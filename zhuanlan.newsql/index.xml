<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Mon, 24 Sep 2018 17:01:00 +0800</lastBuildDate>
<item>
<title>TiKV 集群版本的安全迁移</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-22-45144603.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45144603&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3bc0f0e99ad335bf0ff6992419402210_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：陈书宁&lt;/p&gt;&lt;h2&gt;&lt;b&gt;问题描述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 TiDB 的产品迭代中，不免会碰到一些兼容性问题出现。通常协议上的兼容性 protobuf 已经能帮我们处理的很好，在进行功能开发，性能优化时，通常会保证版本是向后兼容的，但并不保证向前兼容性，因此，当集群中同时有新旧版本节点存在时，旧版本不能兼容新版本的特性，就有可能造成该节点崩溃，影响集群可用性，甚至丢失数据。目前在有不兼容的版本升级时，会要求进行离线升级，但这会影响到服务，我们需要一个适合的机制来进行不停服务的升级。因此我们需要在进行滚动升级时，让这些不能保证整个集群的向后兼容性的功能不被启用。只有在保证集群中所有节点都已经升级完成后，我们才安全的启用这些功能。&lt;/p&gt;&lt;p&gt;常见的当我们对引入新的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt; 的时候，旧版本的 TiKV 并不能识别新的添加的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt;，对于不能认知的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt; TiKV 有不同的处理，可能会报错退出或忽略。比如为了支持 Raft Learner, 在 raftpb 里对添加新的 ConfChange 类型。 当 PD 在进行 Region 调度时，会先发送 &lt;code class=&quot;inline&quot;&gt;AddLearner&lt;/code&gt; 到 TiKV 上，接受到这个命令的肯定是这个 Region 的 Leader，在进行一系列检查后，会将该命令 Proposal, 而 Follwer 如果是旧版本的话，在 Apply 这条 Command 就会出错。而在滚动升级时，很有可能存在 Leader 是新版本，Follwer 是老版本的情况。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;引入版本检查机制&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的版本定义是遵循 Semver 的版本规则的。版本格式一般由主版本号（Major），次版本号（Minor），修订号（Patch），版本号递增规则如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;主版本号：当进行了不兼容的 API 修改。&lt;/li&gt;&lt;li&gt;次版本号：当做了向下兼容的功能性新增。&lt;/li&gt;&lt;li&gt;修订号：当做了向下兼容的问题修正。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;先行版本号（PreRelase）及版本编译信息可以加到“主版本号.次版本号.修订号”的后面，作为延伸。比如 TiDB 目前的版本是 2.1.0-beta，先行版号为 beta 版。&lt;/p&gt;&lt;p&gt;在此之前，集群并没有版本的概念，虽然每个组件都有各自的版本信息，但各个节点的各自组件的版本都可以任意的。没有一个管理机制可以管理或查看所有组件的版本信息。为了解决滚动升级过程中存在多个版本的兼容性问题，这里引入集群版本的概念，并由 TiDB 集群的中心节点 PD 来进行管理和检查。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;具体实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1.升级集群&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 PD 中，会设置一个 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 的键值对，对应当前运行集群中 TiKV 节点中最旧的版本。也就是必须要兼容这个版本， 因此不能打开集群中其他新版本的节点的一些不兼容的特性。&lt;/p&gt;&lt;p&gt;在集群启动的时候，每个 TiKV 都需要向 PD 注册，注册时会带上版本信息。当当前 TiKV 的版本低于集群版本的时候，该 TiKV 会注册失败。因为此时集群的版本已经是更高的版本了，而加入旧版本的节点需要对旧版本进行兼容，为了防止已有的特性降级，直接拒绝不兼容的版本加入，目前默认主版本号和此版本号一样则为兼容的版本。&lt;/p&gt;&lt;p&gt;如果 TiKV 的版本高于或等于当前的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 时， TiKV 能够注册成功并成功启动。每次注册都会触发 PD 的一次检查，会检测当前集群中正常运行的 TiKV 的最低版本，并与当前的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 进行比对，如果最低版本比 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 更加新，则将 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 更新。因此每次滚动升级的时候，能够自动更新集群的版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 版本特性的开启&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 很多功能是需要 PD 的参与，目前这些新功能的开启也是通过 PD 进行控制的。在 PD 中，会将每个版本新特性记录下来，在 TiKV 2.0 中，对应有 Raft Leaner， Region Merge。 TiKV 2.1 中有 Batch Split，Joint Consensus 等。这些特性都需要 PD 的参与与控制。比如说 Add Leaner，Region Merge，Joint Consensus 需要 PD 下发调度给 TiKV，Batch Split 则是 TiKV 主动发起并请求 PD 分配新的 Region ID。因此这些功能都是能通过 PD 进行控制的。PD 会通过比对当前的集群版本，选择开启当前集群版本所支持的新特性。从而保证版本的兼容性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 集群回滚&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当升级完成后，如果遇到问题需要进行集群进行回滚时， 需要手动修改集群版本后。PD 提供了 pdctl 可以通过命令手动修改集群的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;，这时旧版本的 TiKV 就能注册并启动，从而进行回滚。&lt;/p&gt;&lt;p&gt;PD 对 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 是通过 etcd 进行了持久化，在每次 PD 启动的时候，leader 都会从 etcd kv 中加载出 &lt;code class=&quot;inline&quot;&gt;clustrer_version&lt;/code&gt;，然后提供服务。从而保证在 PD leader 切换后 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 的一致性。另外 PD 本身的版本可能会小于当前 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;。因此在滚动升级的时候，需要先升级 PD，如果只升级了 TiKV，虽然 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;已经更新到新的版本的，但 PD 并不能开启新的功能，因为对它来说是不支持的。如果出现这种情况，PD 的日志中会有报警。在升级的时候，最好按 PD，TiKV，TiDB 的顺序逐一对各个组件。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;后续计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面提到的新功能特性一般都是需要 PD 参与的。而有些特性不需要PD的参与，因此需要保证这种特性在 TiKV 之间是可以兼容的，实现的时候可以采用类是 &lt;code class=&quot;inline&quot;&gt;http2 &amp;lt;-&amp;gt; http&lt;/code&gt; 的方式，对请求进行降级装发，保留两套接口等。另为 TiDB 目前是自身保证可以无缝兼容，但与 TiKV 可能存在兼容性问题，往后同样考虑让TiDB 也在 PD上进行注册。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-22-45144603</guid>
<pubDate>Sat, 22 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在爱奇艺的应用及实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-21-45078083.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45078083&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4d6f3202173d72ff80b810466ed58c39_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;爱奇艺，中国高品质视频娱乐服务提供者，2010 年 4 月 22 日正式上线，推崇品质、青春、时尚的品牌内涵如今已深入人心，网罗了全球广大的年轻用户群体，积极推动产品、技术、内容、营销等全方位创新。企业愿景为做一家以科技创新为驱动的伟大娱乐公司。我们在前沿技术领域也保持一定的关注度。&lt;br&gt;&lt;br&gt;随着公司业务的快速发展，原来普遍使用的 MySQL 集群遇到了很多瓶颈，比如单机 MySQL 实例支撑的数据量有限，只能通过不停删除较旧的数据来维持数据库的运转。同时单表的数据行数不断增大导致查询速度变慢。急需一种可扩展、高可用同时又兼容 MySQL 访问方式的数据库来支撑业务的高速发展。&lt;br&gt;&lt;br&gt;我司从 2017 年年中开始调研 TiDB，并且在数据库云部门内部系统中使用了 TiDB 集群。从今年 TiDB 推出 2.0 之后，TiDB 愈发成熟，稳定性与查询效率都有很大提升。今年陆续接入了边控中心、视频转码、用户登录信息等几个业务，这几个业务背景和接入方式如下详述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;项目介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 边控中心&lt;/b&gt;&lt;/p&gt;&lt;p&gt;边控中心存储的是机器的安全统计信息，包括根据 DC、IP、PORT 等不同维度统计的流量信息。上层业务会不定期做统计查询，其业务页面如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-15d28d946b829179d42b8620b349debc_r.jpg&quot; data-caption=&quot;图 1 边控中心上层业务页面（一）&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;452&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-15d28d946b829179d42b8620b349debc&quot; data-watermark-src=&quot;v2-7b9ea19dfc7d3397f0bab3d27e03cb93&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2c4de3f6fcc83f7ea9a62dc54d638216_r.jpg&quot; data-caption=&quot;图 2 边控中心上层业务页面（二）&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;450&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2c4de3f6fcc83f7ea9a62dc54d638216&quot; data-watermark-src=&quot;v2-e5e6c3b09be14fc25b72696dfb9ad41d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;在选型过程中，也考虑过时序型数据库 Apache Druid（http://druid.io），但是 Druid 聚合查询不够灵活，最终放弃 Druid 选择了 TiDB 数据库。TiDB 几乎完全兼容 MySQL 的访问协议，可以使用现有的 MySQL 连接池组件访问 TiDB，业务迁移成本低，开发效率高。&lt;/p&gt;&lt;p&gt;边控中心是爱奇艺第一个在线业务使用 TiDB 的项目，所以我们制定了详细的上线计划。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一，部署单独的 TiDB 集群。然后，为了数据安全，部署了 TokuDB 集群，用作 TiDB 集群的备份数据库。&lt;/li&gt;&lt;li&gt;第二，我们通过 TiDB-Binlog 将 TiDB 集群的数据变更实时同步到 TokuDB 集群中，作为 TiDB 的灾备方案。&lt;/li&gt;&lt;li&gt;第三，前端部署了自研的负载均衡中间件，以最大化利用多个 TiDB 节点的计算能力，同时保证 TiDB 节点的高可用。&lt;/li&gt;&lt;li&gt;第四，部署 Prometheus 和 Grafana 监控 TiDB 集群健康状况，同时 Grafana 接入了公司的告警平台，会及时将告警信息通过短信、邮件等方式通知到运维值班同事。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;边控中心上线过程中，也遇到了一些问题，都比较顺利的解决了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最常见的问题是连接超时。经过仔细排查发现是统计信息严重过时导致执行计划无法选择最优解造成的，这个问题其实是关系型数据库普遍存在的问题，普遍的解决思路是手工进行统计信息收集，或者通过 hint 的方式来固定执行计划，这两种方式对使用者都有一定的要求，而最新版本的 TiDB 完善了统计信息收集策略，比如增加了自动分析功能，目前这个问题已经解决。&lt;/li&gt;&lt;li&gt;还遇到了加索引时间较长的问题。这一方面是由于 DDL 执行信息更新不及时，造成查询 DDL 进度时看到的是滞后的信息。另一方面是由于有时会存在 size 过大的 Region，导致加索引时间变长。这个问题反馈给 PingCAP 官方后得到比较积极的响应，大 Region 已经通过增加 Batch split 等功能在新版的 TiDB 中修复了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;边控中心上线以后，在不中断业务的情况下成功做过版本升级，修改 TiKV 节点内部参数等操作，基本对业务没有影响。在升级 TiKV 节点过程中会有少量报错，如“region is unvailable[try again later]、tikv server timeout”等。这是由于缓存信息滞后性造成的，在分布式系统中是不可避免的，只要业务端有重试机制就不会造成影响。&lt;/p&gt;&lt;p&gt;边控中心上线以后，数据量一直在稳定增长，但查询性能保持稳定，响应时间基本保持不变，能做到这点殊为不易，我个人的理解，这个主要依赖 TiDB 底层自动分片的策略，TiDB 会根据表数据量，按照等长大小的策略（默认 96M）自动分裂出一个分片，然后通过一系列复杂的调度算法打散到各个分布式存储节点上，对一个特定的查询，不管原表数据量多大，都能通过很快定位到某一个具体分片进行数据查询，保证了查询响应时间的稳定。&lt;/p&gt;&lt;p&gt;边控中心数据量增长情况如下：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bd4d272c646f0101976b275af7fce2f1_r.jpg&quot; data-caption=&quot;图 3 边控中心数据量增长情况&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;300&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bd4d272c646f0101976b275af7fce2f1&quot; data-watermark-src=&quot;v2-0d25d4f2f5e63ae12c69de8980bbdc2b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;TiDB 底层自动分片策略：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9ef0617a97f8882565939b6e988fa9db_r.jpg&quot; data-caption=&quot;图 4 TiDB 底层自动分片策略&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9ef0617a97f8882565939b6e988fa9db&quot; data-watermark-src=&quot;v2-6f6ed608bed0599e79e20f89185e27e2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;2. 视频转码&lt;/b&gt;&lt;/p&gt;&lt;p&gt;视频转码业务是很早就接入 TiDB 集群的一个业务。视频转码数据库中主要存储的是转码生产过程中的历史数据，这些数据在生产完成后还需要进一步分析处理，使用 MySQL 集群时因为容量问题只好保留最近几个月的数据，较早的数据都会删掉，失去了做分析处理的机会。&lt;/p&gt;&lt;p&gt;针对业务痛点，在 2017 年年底部署了 TiDB 独立集群，并全量+增量导入数据，保证原有 MySQL 集群和新建 TiDB 集群的数据一致性。在全量同步数据过程中，起初采用 Mydumper+Loader 方式。Loader 是 PingCAP 开发的全量导入工具，但是导入过程总遇到导入过慢的问题，为解决这个问题，PingCAP 研发了 TiDB-Lightning 作为全量同步工具。TiDB-Lightning 会直接将导出的数据直接转化为 sst 格式的文件导入到 TiKV 节点中，极大的提高了效率，1T 数据量在 5-6 个小时内就能完成，在稳定运行一段时间后将流量迁移到了 TiDB 集群，并且扩充了业务功能，迄今稳定运行。&lt;/p&gt;&lt;p&gt;TiDB-Lightning 实现架构图：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-27e095315c85adac271ea24948063dad_r.jpg&quot; data-caption=&quot;图 5 TiDB-Lightning 实现架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;438&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27e095315c85adac271ea24948063dad&quot; data-watermark-src=&quot;v2-1e7d216f79633015a2d4b15ba598ff00&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;3. 用户登录信息&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用户登录信息项目的数据量一直在稳定增长，MySQL 主备集群在不久的将来不能满足存储容量的需求。另外，由于单表数据量巨大，不得不在业务上进行分表处理，业务数据访问层代码变得复杂和缺乏扩展性。在迁移到 TiDB 后，直接去掉了分库分表，简化了业务的使用方式。另外，在 MySQL 中存在双散表并进行双写。在 TiDB 中进一步合并成了一种表，利用 TiDB 的索引支持多种快速查询，进一步简化了业务代码。&lt;/p&gt;&lt;p&gt;在部署增量同步的过程中使用了官方的 Syncer 工具。Syncer 支持通过通配符的方式将多源多表数据汇聚到一个表当中，是个实用的功能，大大简化了我们的增量同步工作。目前的 Syncer 工具还不支持在 Grafana 中展示实时延迟信息，这对同步延迟敏感的业务是个缺点，据官方的消息称已经在改进中，同时 PingCAP 他们重构了整个 Syncer，能自动处理分表主键冲突，多源同时 DDL 自动过滤等功能，总之通过这套工具，可以快速部署 TiDB “实时”同步多个 MySQL，数据迁移体验超赞。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d1610958e01bbddab6a46d66d3d36e11_r.jpg&quot; data-caption=&quot;图 6 Syncer 架构&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;531&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d1610958e01bbddab6a46d66d3d36e11&quot; data-watermark-src=&quot;v2-236621aba796853905372a8aa18c0655&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;在我们公司业务对数据库高可用有两个需求：一个是机房宕机了，服务仍然可用。另一个是，多机房的业务，提供本机房的只读从库，提升响应速度。针对这些不同的需求，TiDB 集群采用了多机房部署的方式，保证其中任一个机房不可用时仍然正常对外提供服务（如下图）。对每个 TiKV 节点设置 label 后，TiDB 集群在每个机房都有一份数据的副本，PD 集群会自动调度到合适的副本进行读操作，也可以满足第二个要求。为了满足迁移过程中的高可用性，会在流量迁移完成后部署 TiDB 到 MySQL 的实时同步。Drainer 支持指定同步开始的时间戳，有力支持了反向同步功能。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f4cc9896480eb2192e3ed6cf8c798d72_r.jpg&quot; data-caption=&quot;图 7  TiDB 集群多机房部署形式&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;848&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f4cc9896480eb2192e3ed6cf8c798d72&quot; data-watermark-src=&quot;v2-fa9e101a090eec110df644e9b64743f7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;在整个运维过程中，PingCAP 的小伙伴们提供了及时的帮助，帮助我们定位问题并提出建议，保证了业务的有序运行。在此表示由衷的感谢！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;适用范围&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在实践过程中感受到 TiDB 解决的痛点主要是横向扩展和高可用。单机数据库支撑的数据量有限，如果采用分库分表 + proxy 的方式，无论 proxy 是在客户端还是服务端都增加了运维的成本。同时 proxy 的查询效率在很多场景下不能满足要求。另外，proxy 对事务的支持都比较弱，不能满足数据强一致性的要求。&lt;b&gt;TiDB 可以直接替代 proxy+MySQL 的架构，服务高可用性、数据高可用性、横向扩展性都由 TiDB 集群完成，完美解决了数据量增量情况下出现的很多问题。而且，TiDB 在数据量越大的情况下性能表现得比 MySQL 越惊艳。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一些改进建议&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;统计信息的收集期望更加的智能化，选择更好的时机自动完成而且不影响线上业务。&lt;/li&gt;&lt;li&gt;OLTP 和 OLAP 混合场景下相互间的隔离和尽量互不影响还有许多工作值得推进。&lt;/li&gt;&lt;li&gt;一些外围工具还需要提供高可用特性。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;未来计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我司仍有其它业务在接入 TiDB 服务，目前正在评估测试中。一些业务场景是 OLTP+OLAP 混合的场景，TiSpark 正好可以大展身手。目前在测试集群发现 TiSpark 查询时对 OLTP 业务的影响还是比较大的，必须限制 TiSpark 对 TiDB 集群造成的压力。还部署了单独 TiDB 集群做 OLAP 场景的测试，对内部参数做了针对性的优化。未来计划会继续加大对 TiDB 的投入，贡献一些 PR 到社区，其中很大的一部分工作是增强 TiDB-Binlog 的功能，和现有的一些数据同步组件整合起来，支持 TiDB 到 Kudu、HBase 等的同步。&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;作者：朱博帅，爱奇艺资深数据库架构师&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-21-45078083</guid>
<pubDate>Fri, 21 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>PingCAP 完成 C 轮 5000 万美元融资</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-12-44322174.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/44322174&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-11fb7be39efa3b3e40af9addc6780530_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;近日，新型分布式关系型数据库公司 PingCAP 宣布完成 5000 万美元 C 轮融资，这是目前为止新型分布式关系型数据库领域的最大笔融资。本轮融资由复星、晨兴资本领投，华创资本、云启资本、经纬中国等多家投资机构跟投，将主要用于技术研发和全球化生态系统建设。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-11fb7be39efa3b3e40af9addc6780530_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2210&quot; data-rawheight=&quot;1266&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-11fb7be39efa3b3e40af9addc6780530&quot; data-watermark-src=&quot;v2-09412e37d5508a38e75aabc1e07680a6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;PingCAP 成立于 2015 年，是面向全球的开源的新型分布式关系型数据库公司，其核心产品 TiDB 项目是基础软件领域的重大创新，实现了水平弹性伸缩、分布式事务、强一致性的多副本数据安全、实时 OLAP 等重要特性，是大数据时代理想的数据库集群和云数据库解决方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;真正落地的 HTAP 融合型通用数据库&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;TiDB 作为一款定位于 HTAP（Hybrid Transactional/Analytical Processing）的融合型通用数据库产品，很早就开始尝试融合 OLTP 和 OLAP 的边界。互联网背景出身的 PingCAP 创始团队的每名成员，都经历过数据指数级增长的时期，具备处理海量数据的经验，这让 TiDB 在满足 OLTP 分布式性能的基础上融合了有场景意义的 OLAP 性能。目前 TiDB 已演进为一套技术栈，既能解决用户业务快速增长过程中海量数据存储、超大规模并发访问问题，又能支持复杂的实时查询分析，极大的提高了用户生产力。&lt;/p&gt;&lt;p&gt;尤其面对风险控制要求更高的金融行业，TiDB 表现出了架构的先进性和高效的性能，水平扩展能力、交易处理能力、功能指标特性均在应用中具备较大的优势，被成功应用在金融核心交易场景中。PingCAP 目前已经成为一家在互联网领域和传统金融领域均有大规模落地的新型分布式数据库公司。&lt;/p&gt;&lt;p&gt;截止目前 TiDB 已有准生产测试（POC）用户 1400 余家，其中 300 余家企业已经将其应用在实际生产环境中，涉及互联网、游戏、银行、证券、保险、第三方支付、政府、电信、航空、制造业、新零售、快消等多个领域。此外，还与国内外多家主流的大型公有云厂商深度集成，提供公有云数据库服务。&lt;/p&gt;&lt;p&gt;复星新技术与新经济产业集团副总裁丛永罡说：“在 PingCAP 持续 3 年的产品打磨和不断实践验证后，TiDB 成为早期将 HTAP 这个概念从实验室带到现实的产品之一。其 TiDB 开源项目获得业内高度认可，并已将国内大多数互联网独角兽企业揽为用户，这具有里程碑式的意义，也是业内高度认可的体现。”&lt;/p&gt;&lt;h2&gt;&lt;b&gt;进一步构建全球化生态体系&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;作为一款基础设施领域的明星级开源项目，TiDB 项目目前在 GitHub 上拥有 15000+ 的 Star，集合了 200 多位 Contributor，入选 CNCF Cloud Native Landscape， 2018 Big Data &amp;amp; AI Landscape。此外， Databricks、Mobike、SpeedyCloud、韩国三星研究院等机构人员也已参与到 TiDB 项目的合作开发中，全球顶级云计算技术基金会 CNCF 也于 8 月 28 日正式接纳 TiDB 项目核心组件之一 TiKV 为基金会项目，TiDB 在全球技术社区的影响力开始爆发。&lt;/p&gt;&lt;p&gt;回顾 PingCAP 的发展历史，用 3 年时间逐步打造产品和布局，以稳健的产品表现及高速的公司发展获得业界瞩目。PingCAP 联合创始人、CEO 刘奇表示，作为一家技术驱动为核心的公司，PingCAP 从创立的第一天起，就专注于解决大数据和云计算时代的海量存储和计算问题，我们将持续保持技术驱动的内核和开源的生命力，不断强化技术和产品优势，为行业带来更多期待。&lt;/p&gt;&lt;p&gt;晨兴资本副总裁刘凯表示：“一直以来，数据库领域都是产品研发能力和商业运营能力的最高级竞技场，PingCAP 持续深耕互联网领域客户，取得了骄人业绩，已成为规模互联网企业和金融企业的主流选择，我们很高兴能够和公司一起成长，共同打造开源软件生态。”&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://pingcap.com/&quot;&gt;PingCAP&lt;/a&gt; 此前曾在 2015 年获得经纬中国的天使轮融资，2016 年获得云启资本领投 A 轮融资，2017 年获得华创资本领投 B 轮融资。2018 年获得复星、晨兴资本领投的 C 轮融资，成为目前为止新型分布式关系型数据库领域的最大笔融资。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-12-44322174</guid>
<pubDate>Wed, 12 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>PingCAP 2019 校园招聘全面启动</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-11-44292776.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/44292776&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-810e53a9d8f2ecf7336f521243084f05_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;我们是谁？&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;以技术产品为驱动的全球化高科技公司；&lt;/li&gt;&lt;li&gt;工程师文化，技术极客的摇篮，ACMer 的聚集地；&lt;/li&gt;&lt;li&gt;挑战计算机科学前沿、工程难度极高的分布式数据库领域&lt;/li&gt;&lt;li&gt;服务于 1000+ 国内外企业，用户覆盖众多国内外独角兽公司&lt;/li&gt;&lt;li&gt;开源理念坚定践行者，TiDB 在 GitHub 上拥有 200+ Contributors、14000+ Stars&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;PingCAP 发展大事记&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-937f01a1eb8603212760ce66964b3c11_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;980&quot; data-rawheight=&quot;615&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-937f01a1eb8603212760ce66964b3c11&quot; data-watermark-src=&quot;v2-4a95b150a60d690babcbf0516c40893d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;典型用户&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;核心产品 TiDB 在生产环境使用超过 200 家，覆盖互联网、游戏、金融、电信、航空、国家部委、制造业、新零售、快消等行业&lt;/li&gt;&lt;li&gt;典型用户：北京银行、饿了么、今日头条、美团、摩拜单车、去哪儿、同程旅游、转转等&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;校招日历&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;9月上旬-10月中旬：投递简历&lt;/li&gt;&lt;li&gt;9月中旬-10月上旬：线下交流会&lt;/li&gt;&lt;li&gt;9月中旬-10月下旬：笔试 &amp;amp; 面试&lt;/li&gt;&lt;li&gt;10月上旬-10月下旬：Offer 发放&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;为什么选择我们&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;挑战分布式领域前沿技术的创新实现，追求个人技术视野和能力的极限&lt;/li&gt;&lt;li&gt;体验纯正的开源和工程师文化，感受 Gmail/GitHub/Slack/JIRA/Trello/Confluence 等协作工具的魅力碰撞&lt;/li&gt;&lt;li&gt;获得改变世界和创造未来的成就感&lt;/li&gt;&lt;li&gt;享受 One-on-One Mentoring 培养方式，为你定制化个人成长路线&lt;/li&gt;&lt;li&gt;加入国际化的发展舞台，北京、上海、杭州、广州、深圳、成都、硅谷虚位以待！Remote 工作模式也别样精彩&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-63678390e72e2d93de56b5d83a212f7e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1500&quot; data-rawheight=&quot;1011&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-63678390e72e2d93de56b5d83a212f7e&quot; data-watermark-src=&quot;v2-8a6207dbe18e5041be114bd4c06a5b5d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;我们需要这样的你&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;拥有快速学习的能力&lt;/li&gt;&lt;li&gt;对新事物充满好奇心&lt;/li&gt;&lt;li&gt;信仰并崇尚开源文化&lt;/li&gt;&lt;li&gt;有强烈的自我驱动力&lt;/li&gt;&lt;li&gt;Get things done, always&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;别犹豫，你就是我们苦苦寻找的那一个！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;在招职位&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;研发类&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Infrastructure Engineer（包括分布式存储、分布式计算、分布式调度、商业工具、SRE、Cloud 等方向）&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;非研发类&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;OPS Engineer&lt;/li&gt;&lt;li&gt;Technical Writer&lt;/li&gt;&lt;li&gt;Marketing Specialist&lt;/li&gt;&lt;li&gt;HR Management Trainee&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;更多职位信息：&lt;a href=&quot;https://www.pingcap.com/recruit-cn/join/#positions&quot;&gt;https://www.pingcap.com/recruit-cn/join/#positions&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ti 星人「推荐」通道&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;提到「推荐」，首先想到的是内推？NO! NO! NO! 在这里，「推荐」通道向全世界小伙伴开放！你可以成为最强伯乐，全局搜索你身边的 Ti 星人并推荐给我们，推荐成功，即有惊喜 Bonus 相送；也可以毛遂自荐，加入 PingCAP，成为改变世界的新力量！&lt;/li&gt;&lt;li&gt;&lt;b&gt;伯乐推荐邮件格式&lt;/b&gt;：[2019 校招伯乐] 学校-姓名-职位名-伯乐姓名-伯乐手机号&lt;/li&gt;&lt;li&gt;&lt;b&gt;毛遂自荐邮件格式&lt;/b&gt;：[2019 校招] 学校-姓名-职位名&lt;/li&gt;&lt;li&gt;&lt;b&gt;简历投递通道：&lt;u&gt;&lt;a href=&quot;mailto:hire@pingcap.com&quot;&gt;hire@pingcap.com&lt;/a&gt;&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-11-44292776</guid>
<pubDate>Tue, 11 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>使用 TiKV 构建分布式类 Redis 服务</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-07-43959766.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43959766&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-828ba4512736d7f2cb3e6664c5fbf3b3_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：唐刘&lt;/p&gt;&lt;h2&gt;&lt;b&gt;什么是 Redis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt; 是一个开源的，高性能的，支持多种数据结构的内存数据库，已经被广泛用于数据库，缓存，消息队列等领域。它有着丰富的数据结构支持，譬如 String，Hash，Set 和 Sorted Set，用户通过它们能构建自己的高性能应用。&lt;/p&gt;&lt;p&gt;Redis 非常快，没准是世界上最快的数据库了，它虽然使用内存，但也提供了一些持久化机制以及异步复制机制来保证数据的安全。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Redis 的不足&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Redis 非常酷，但它也有一些问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;内存很贵，而且并不是无限容量的，所以我们不可能将大量的数据存放到一台机器。&lt;/li&gt;&lt;li&gt;异步复制并不能保证 Redis 的数据安全。&lt;/li&gt;&lt;li&gt;Redis 提供了 transaction mode，但其实并不满足 ACID 特性。&lt;/li&gt;&lt;li&gt;Redis 提供了集群支持，但也不能支持跨多个节点的分布式事务。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以有时候，我们需要一个更强大的数据库，虽然在延迟上面可能赶不上 Redis，但也有足够多的特性，譬如：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;丰富的数据结构&lt;/li&gt;&lt;li&gt;高吞吐，能接受的延迟&lt;/li&gt;&lt;li&gt;强数据一致&lt;/li&gt;&lt;li&gt;水平扩展&lt;/li&gt;&lt;li&gt;分布式事务&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;为什么选择 TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;大约 4 年前，我开始解决上面提到的 Redis 遇到的一些问题。为了让数据持久化，最直观的做法就是将数据保存到硬盘上面，而不是在内存里面。所以我开发了 &lt;a href=&quot;https://github.com/siddontang/ledisdb&quot;&gt;LedisDB&lt;/a&gt;，一个使用 Redis 协议，提供丰富数据结构，但将数据放在 RocksDB 的数据库。LedisDB 并不是完全兼容 Redis，所以后来，我和其他同事继续创建了 &lt;a href=&quot;https://github.com/reborndb/reborn&quot;&gt;RebornDB&lt;/a&gt;，一个完全兼容 Redis 的数据库。&lt;br&gt;无论是 LedisDB 还是 RebornDB，因为他们都是将数据放在硬盘，所以能存储更大量的数据。但它们仍然不能提供 ACID 的支持，另外，虽然我们可以通过 &lt;a href=&quot;https://github.com/CodisLabs/codis&quot;&gt;codis&lt;/a&gt; 去提供集群的支持，我们也不能很好的支持全局的分布式事务。&lt;/p&gt;&lt;p&gt;所以我们需要另一种方式，幸运的是，我们有&lt;a href=&quot;https://github.com/pingcap/tikv&quot;&gt;TiKV&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;TiKV 是一个高性能，支持分布式事务的 key-value 数据库。虽然它仅仅提供了简单的 key-value API，但基于 key-value，我们可以构造自己的逻辑去创建更强大的应用。譬如，我们就构建了 &lt;a href=&quot;https://github.com/pingcap/tidb&quot;&gt;TiDB&lt;/a&gt; ，一个基于 TiKV 的，兼容 MySQL 的分布式关系型数据库。TiDB 通过将 database 的 schema 映射到 key-value 来支持了相关 SQL 特性。所以对于 Redis，我们也可以采用同样的办法 - 构建一个支持 Redis 协议的服务，将 Redis 的数据结构映射到 key-value 上面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何开始&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-22f36db3ad8dd94b0d520e54185f28d7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;600&quot; data-rawheight=&quot;460&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-22f36db3ad8dd94b0d520e54185f28d7&quot; data-watermark-src=&quot;v2-f1bf888ee752ea16863a39c751e33f11&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;整个架构非常简单，我们仅仅需要做的就是构建一个 Redis 的 Proxy，这个 Proxy 会解析 Redis 协议，然后将 Redis 的数据结构映射到 key-value 上面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Redis Protocol&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Redis 协议被叫做 &lt;a href=&quot;https://redis.io/topics/protocol&quot;&gt;RESP&lt;/a&gt;(Redis Serialization Protocol)，它是文本类型的，可读性比较好，并且易于解析。它使用 “rn” 作为每行的分隔符并且用不同的前缀来代表不同的类型。例如，对于简单的 String，第一个字节是 “+”，所以一个 “OK” 行就是 “+OKrn”。&lt;br&gt;大多数时候，客户端会使用最通用的 Request-Response 模型用于跟 Redis 进行交互。客户端会首先发送一个请求，然后等待 Redis返回结果。请求是一个 Array，Array 里面元素都是 bulk strings，而返回值则可能是任意的 RESP 类型。Redis 同样支持其他通讯方式：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Pipeline - 这种模式下面客户端会持续的给 Redis 发送多个请求，然后等待 Redis 返回一个结果。&lt;/li&gt;&lt;li&gt;Push - 客户端会在 Redis 上面订阅一个 channel，然后客户端就会从这个 channel 上面持续受到 Redis push 的数据。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面是一个简单的客户端发送 &lt;code class=&quot;inline&quot;&gt;LLEN mylist&lt;/code&gt; 命令到 Redis 的例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;C: *2\r\n
C: $4\r\n
C: LLEN\r\n
C: $6\r\n
C: mylist\r\n

S: :48293\r\n&lt;/code&gt;&lt;p&gt;客户端会发送一个带有两个 bulk string 的 array，第一个 bulk string 的长度是 4，而第二个则是 6。Redis 会返回一个 48293 整数。正如你所见，RESP 非常简单，自然而然的，写一个 RESP 的解析器也是非常容易的。&lt;/p&gt;&lt;p&gt;作者创建了一个 Go 的库 &lt;a href=&quot;http://github.com/siddontang/goredis&quot;&gt;goredis&lt;/a&gt;，基于这个库，我们能非常容易的从连接上面解析出 RESP，一个简单的例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// Create a buffer IO from the connection.
br := bufio.NewReaderSize(conn, 4096)
// Create a RESP reader.
r := goredis.NewRespReader(br)
// Parse the Request
req := r.ParseRequest()&lt;/code&gt;&lt;p&gt;函数 &lt;code class=&quot;inline&quot;&gt;ParseRequest&lt;/code&gt; 返回一个解析好的 request，它是一个 &lt;code class=&quot;inline&quot;&gt;[][]byte&lt;/code&gt; 类型，第一个字段是函数名字，譬如 “LLEN”，然后后面的字段则是这个命令的参数。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV 事务 API&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们开始之前，作者将会给一个简单实用 TiKV 事务 API 的例子，我们调用 &lt;code class=&quot;inline&quot;&gt;Begin&lt;/code&gt; 开始一个事务：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;txn, err := db.Begin()&lt;/code&gt;&lt;p&gt;函数 &lt;code class=&quot;inline&quot;&gt;Begin&lt;/code&gt; 创建一个事务，如果出错了，我们需要判断 err，不过后面作者都会忽略 err 的处理。&lt;/p&gt;&lt;p&gt;当我们开始了一个事务之后，我们就可以干很多操作了：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;value, err := txn.Get([]byte(“key”))
// Do something with value and then update the newValue to the key.
txn.Put([]byte(“key”), newValue)&lt;/code&gt;&lt;p&gt;上面我们得到了一个 key 的值，并且将其更新为新的值。TiKV 使用乐观事务模型，它会将所有的改动都先缓存到本地，然后在一起提交给 Server。&lt;/p&gt;&lt;code lang=&quot;go&quot;&gt;// Commit the transaction
txn.Commit(context.TODO())&lt;/code&gt;&lt;p&gt;跟其他事务处理一样，我们也可以回滚这个事务：&lt;/p&gt;&lt;code lang=&quot;go&quot;&gt;txn.Rollback()&lt;/code&gt;&lt;p&gt;如果两个事务操作了相同的 key，它们就会冲突。一个事务会提交成功，而另一个事务会出错并且回滚。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;映射 Data structure 到 TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在我们知道了如何解析 Redis 协议，如何在一个事务里面做操作，下一步就是支持 Redis 的数据结构了。Redis 主要有 4 中数据结构：String，Hash，Set 和 Sorted Set，但是对于 TiKV 来说，它只支持 key-value，所以我们需要将这些数据结构映射到 key-value。&lt;/p&gt;&lt;p&gt;首先，我们需要区分不同的数据结构，一个非常容易的方式就是在 key 的后面加上 Type flag。例如，我们可以将 ’s’ 添加到 String，所以一个 String key “abc” 在 TiKV 里面其实就是 “abcs”。&lt;/p&gt;&lt;p&gt;对于其他类型，我们可能需要考虑更多，譬如对于 Hash 类型，我们需要支持如下操作：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;HSET key field1 value1
HSET key field2 value2
HLEN key&lt;/code&gt;&lt;p&gt;一个 Hash 会有很多 fields，我有时候想知道整个 Hash 的个数，所以对于 TiKV，我们不光需要将 Hash 的 key 和 field 合在一起变成 TiKV 的一个 key，也同时需要用另一个 key 来保存整个 Hash 的长度，所以整个 Hash 的布局类似：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;key + ‘h’ -&amp;gt; length
key + ‘f’ + field1 -&amp;gt; value
key + ‘f’ + field2 -&amp;gt; value &lt;/code&gt;&lt;p&gt;如果我们不保存 length，那么如果我们想知道 Hash 的 length，每次都需要去扫整个 Hash 得到所有的 fields，这个其实并不高效。但如果我们用另一个 key 来保存 length，任何时候，当我们加入一个新的 field，我们都需要去更新这个 length 的值，这也是一个开销。对于我来说，我倾向于使用另一个 key 来保存 length，因为 &lt;code class=&quot;inline&quot;&gt;HLEN&lt;/code&gt; 是一个高频的操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;例子&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;作者构建了一个非常简单的例子 &lt;a href=&quot;https://github.com/siddontang/redis-tikv-example&quot;&gt;example&lt;/a&gt; ，里面只支持 String 和 Hash 的一些操作，我们可以 clone 下来并编译：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;git clone https://github.com/siddontang/redis-tikv-example.git $GOPATH/src/github.com/siddontang/redis-tikv-example

cd $GOPATH/src/github.com/siddontang/redis-tikv-example
go build&lt;/code&gt;&lt;p&gt;在运行之前，我们需要启动 TiKV，可以参考&lt;a href=&quot;https://github.com/pingcap/tikv#deploying-to-production&quot;&gt;instruction&lt;/a&gt;，然后执行：&lt;/p&gt;&lt;code lang=&quot;bash&quot;&gt;./redis-tikv-example&lt;/code&gt;&lt;p&gt;这个例子会监听端口 6380，然后我们可以用任意的 Redis 客户端，譬如 &lt;code class=&quot;inline&quot;&gt;redis-cli&lt;/code&gt; 去连接：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;redis-cli -p 6380
127.0.0.1:6380&amp;gt; set k1 a
OK
127.0.0.1:6380&amp;gt; get k1
&quot;a&quot;
127.0.0.1:6380&amp;gt; hset k2 f1 a
(integer) 1
127.0.0.1:6380&amp;gt; hget k2 f1
&quot;a&quot;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;尾声&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在已经有一些公司基于 TiKV 来构建了他们自己的 Redis Server，并且也有一个开源的项目&lt;a href=&quot;https://github.com/yongman/tidis&quot;&gt;tidis&lt;/a&gt; 做了相同的事情。&lt;code class=&quot;inline&quot;&gt;tidis&lt;/code&gt; 已经比较完善，如果你想替换自己的 Redis，可以尝试一下。&lt;br&gt;正如同你所见，TiKV 其实算是一个基础的组件，我们可以在它的上面构建很多其他的应用。如果你对我们现在做的事情感兴趣，欢迎联系我：&lt;a href=&quot;mailto:tl@pingcap.com&quot;&gt;tl@pingcap.com&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文发表于唐刘老师博客：&lt;a href=&quot;https://www.jianshu.com/p/b4dee8372d8d&quot;&gt;使用 TiKV 构建分布式类 Redis 服务&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-07-43959766</guid>
<pubDate>Fri, 07 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读（十八）tikv-client（上）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-06-43926052.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43926052&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-828ba4512736d7f2cb3e6664c5fbf3b3_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：周昱行&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在整个 SQL 执行过程中，需要经过 Parser，Optimizer，Executor，DistSQL 这几个主要的步骤，最终数据的读写是通过 tikv-client 与 TiKV 集群通讯来完成的。&lt;br&gt;为了完成数据读写的任务，tikv-client 需要解决以下几个具体问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如何定位到某一个 key 或 key range 所在的 TiKV 地址？&lt;/li&gt;&lt;li&gt;如何建立和维护和 tikv-server 之间的连接？&lt;/li&gt;&lt;li&gt;如何发送 RPC 请求？&lt;/li&gt;&lt;li&gt;如何处理各种错误？&lt;/li&gt;&lt;li&gt;如何实现分布式读取多个 TiKV 节点的数据？&lt;/li&gt;&lt;li&gt;如何实现 2PC 事务？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们接下来就对以上几个问题逐一解答，其中 5、6 会在下篇中介绍。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何定位 key 所在的 tikv-server&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们需要回顾一下之前 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-internal-1/&quot;&gt;《三篇文章了解 TiDB 技术内幕——说存储》&lt;/a&gt; 。这篇文章中介绍过的一个重要的概念：Region。&lt;/p&gt;&lt;p&gt;TiDB 的数据分布是以 Region 为单位的，一个 Region 包含了一个范围内的数据，通常是 96MB 的大小，Region 的 meta 信息包含了 StartKey 和 EndKey 这两个属性。当某个 key &amp;gt;= StartKey &amp;amp;&amp;amp; key &amp;lt; EndKey 的时候，我们就知道了这个 key 所在的 Region，然后我们就可以通过查找该 Region 所在的 TiKV 地址，去这个地址读取这个 key 的数据。&lt;/p&gt;&lt;p&gt;获取 key 所在的 Region, 是通过向 PD 发送请求完成的。PD client 实现了这样一个接口：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/pd/pd-client/client.go#L49&quot;&gt;GetRegion(ctx context.Context, key []byte) (*metapb.Region, *metapb.Peer, error)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;通过调用这个接口，我们就可以定位这个 key 所在的 Region 了。&lt;/p&gt;&lt;p&gt;如果需要获取一个范围内的多个 Region，我们会从这个范围的 StartKey 开始，多次调用 &lt;code class=&quot;inline&quot;&gt;GetRegion&lt;/code&gt; 这个接口，每次返回的 Region 的 EndKey 做为下次请求的 StartKey，直到返回的 Region 的 EndKey 大于请求范围的 EndKey。&lt;/p&gt;&lt;p&gt;以上执行过程有一个很明显的问题，就是我们每次读取数据的时候，都需要先去访问 PD，这样会给 PD 带来巨大压力，同时影响请求的性能。&lt;/p&gt;&lt;p&gt;为了解决这个问题，tikv-client 实现了一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L50&quot;&gt;RegionCache&lt;/a&gt; 的组件，缓存 Region 信息， 当需要定位 key 所在的 Region 的时候，如果 RegionCache 命中，就不需要访问 PD 了。RegionCache 的内部，有两种数据结构保存 Region 信息，一个是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L55&quot;&gt;map&lt;/a&gt;，另一个是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L56&quot;&gt;b-tree&lt;/a&gt;，用 map 可以快速根据 region ID 查找到 Region，用 b-tree 可以根据一个 key 找到包含该 key 的 Region。&lt;/p&gt;&lt;p&gt;严格来说，PD 上保存的 Region 信息，也是一层 cache，真正最新的 Region 信息是存储在 tikv-server 上的，每个 tikv-server 会自己决定什么时候进行 Region 分裂，在 Region 变化的时候，把信息上报给 PD，PD 用上报上来的 Region 信息，满足 tidb-server 的查询需求。&lt;br&gt;当我们从 cache 获取了 Region 信息，并发送请求以后， tikv-server 会对 Region 信息进行校验，确保请求的 Region 信息是正确的。&lt;/p&gt;&lt;p&gt;如果因为 Region 分裂，Region 迁移导致了 Region 信息变化，请求的 Region 信息就会过期，这时 tikv-server 就会返回 Region 错误。遇到了 Region 错误，我们就需要&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L318&quot;&gt;清理 RegionCache&lt;/a&gt;，重新&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L329&quot;&gt;获取最新的 Region 信息&lt;/a&gt;，并重新发送请求。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何建立和维护和 tikv-server 之间的连接&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当 TiDB 定位到 key 所在的 tikv-server 以后，就需要建立和 TiKV 之间的连接，我们都知道， TCP 连接的建立和关闭有不小的开销，同时会增大延迟，使用连接池可以节省这部分开销，TiDB 和 tikv-server 之间也维护了一个连接池 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L83&quot;&gt;connArray&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;TiDB 和 TiKV 之间通过 gRPC 通信，而 gPRC 支持在单 TCP 连接上多路复用，所以多个并发的请求可以在单个连接上执行而不会相互阻塞。&lt;/p&gt;&lt;p&gt;理论上一个 tidb-server 和一个 tikv-server 之间只需要维护一个连接，但是在性能测试的时候发现，单个连接在并发-高的时候，会成为性能瓶颈，所以实际实现的时候，tidb-server 对每一个 tikv-server 地址维护了多个连接，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L159&quot;&gt;并以 round-robin 算法选择连接&lt;/a&gt;发送请求。连接的个数可以在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/config/config.toml.example#L215&quot;&gt;config&lt;/a&gt; 文件里配置，默认是 16。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何发送 RPC 请求&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;tikv-client 通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/kv.go#L127&quot;&gt;tikvStore&lt;/a&gt; 这个类型，实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L247&quot;&gt;kv.Storage&lt;/a&gt; 这个接口，我们可以把 tikvStore 理解成 tikv-client 的一个包装。外部调用 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 的接口，并不需要关心 RPC 的细节，RPC 请求都是 tikvStore 为了实现 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 接口而发起的。&lt;/p&gt;&lt;p&gt;实现不同的 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 接口需要发送不同的 RPC 请求。比如实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L233&quot;&gt;Snapshot.BatchGet&lt;/a&gt; 需要&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L61&quot;&gt;tikvpb.TikvClient.KvBatchGet&lt;/a&gt;方法；实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L128&quot;&gt;Transaction.Commit&lt;/a&gt;，需要 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L57&quot;&gt;tikvpb.TikvClient.KvPrewrite&lt;/a&gt;, &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L58&quot;&gt;tikvpb.TikvClient.KvCommit&lt;/a&gt; 等多个方法。&lt;/p&gt;&lt;p&gt;在 tikvStore 的实现里，并没有直接调用 RPC 方法，而是通过一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L76&quot;&gt;Client&lt;/a&gt; 接口调用，做这一层的抽象的主要目的是为了让下层可以有不同的实现。比如用来测试的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/mockstore/mocktikv/rpc.go#L493&quot;&gt;mocktikv 就自己实现了 Client 接口&lt;/a&gt;，通过本地调用实现，并不需要调用真正的 RPC。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L180&quot;&gt;rpcClient&lt;/a&gt; 是真正实现 RPC 请求的 Client 实现，通过调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/tikvrpc/tikvrpc.go#L419&quot;&gt;tikvrpc.CallRPC&lt;/a&gt;，发送 RPC 请求。&lt;code class=&quot;inline&quot;&gt;tikvrpc.CallRPC&lt;/code&gt; 再往下层走，就是调用具体&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L152&quot;&gt;每个 RPC  生成的代码&lt;/a&gt;了，到了生成的代码这一层，就已经是 gRPC 框架这一层的内容了，我们就不继续深入解析了，感兴趣的同学可以研究一下 gRPC 的实现。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何处理各种错误&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们前面提到 RPC 请求都是通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L76&quot;&gt;Client&lt;/a&gt; 接口发送的，但实际上这个接口并没有直接被各个 tikvStore 的各个方法调用，而是通过一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L46&quot;&gt;RegionRequestSender&lt;/a&gt; 的对象调用的。&lt;br&gt;&lt;code class=&quot;inline&quot;&gt;RegionRequestSender&lt;/code&gt; 主要的工作除了发送 RPC 请求，还要负责处理各种可以重试的错误，比如网络错误和部分 Region 错误。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RPC 请求遇到的错误主要分为两大类：Region 错误和网络错误。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/tikvrpc/tikvrpc.go#L359&quot;&gt;Region  错误&lt;/a&gt; 是由 tikv-server 收到请求后，在 response 里返回的，常见的有以下几种:&lt;/p&gt;&lt;p&gt;1.&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L207&quot;&gt;NotLeader&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这种错误的原因通常是 Region 的调度，PD 为了负载均衡，可能会把一个热点 Region 的 leader 调度到空闲的 tikv-server 上，而请求只能由 leader 来处理。遇到这种错误就需要 tikv-client 重试，把请求发给新的 leader。&lt;/p&gt;&lt;p&gt;2. &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L210&quot;&gt;StaleEpoch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这种错误主要是因为 Region 的分裂，当 Region 内的数据量增多以后，会分裂成多个新的 Region。新的 Region 包含的 range 是不同的，如果直接执行，返回的结果有可能是错误的，所以 TiKV 就会拒绝这个请求。tikv-client 需要从 PD 获取最新的 Region 信息并重试。&lt;/p&gt;&lt;p&gt;3. &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L211&quot;&gt;ServerIsBusy&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个错误通常是因为 tikv-server 积压了过多的请求处理不完，tikv-server 如果不拒绝这个请求，队列会越来越长，可能等到客户端超时了，请求还没有来的及处理。所以做为一种保护机制，tikv-server 提前返回错误，让客户端等待一段时间后再重试。&lt;/p&gt;&lt;p&gt;另一类错误是网络错误，错误是由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L129&quot;&gt;SendRequest 的返回值&lt;/a&gt; 返回的 error 的，遇到这种错误通常意味着这个 tikv-server 没有正常返回请求，可能是网络隔离或 tikv-server down 了。tikv-client 遇到这种错误，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L140&quot;&gt;OnSendFail&lt;/a&gt; 方法，处理这个错误，会在 RegionCache 里把这个请求失败的 tikv-server 上的&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L453&quot;&gt;所有 region 都 drop 掉&lt;/a&gt;，避免其他请求遇到同样的错误。&lt;br&gt;当遇到可以重试的错误的时候，我们需要等待一段时间后重试，我们需要保证每次重试等待时间不能太短也不能太长，太短会造成多次无谓的请求，增加系统压力和开销，太长会增加请求的延迟。我们用指数退避的算法来计算每一次重试前的等待时间，这部分的逻辑是在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L176&quot;&gt;Backoffer&lt;/a&gt; 里实现的。&lt;/p&gt;&lt;p&gt;在上层执行一个 SQL 语句的时候，在 tikv-client 这一层会触发多个顺序的或并发的请求，发向多个 tikv-server，为了保证上层 SQL 语句的超时时间，我们需要考虑的不仅仅是单个 RPC 请求，还需要考虑一个 query 整体的超时时间。&lt;/p&gt;&lt;p&gt;为了解决这个问题，&lt;code class=&quot;inline&quot;&gt;Backoffer&lt;/code&gt; 实现了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L267&quot;&gt;fork&lt;/a&gt; 功能， 在发送每一个子请求的时候，需要 fork 出一个 &lt;code class=&quot;inline&quot;&gt;child Backoffer&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;child Backoffer&lt;/code&gt; 负责单个 RPC 请求的重试，它记录了 &lt;code class=&quot;inline&quot;&gt;parent Backoffer&lt;/code&gt; 已经等待的时间，保证总的等待时间，不会超过 query 超时时间。&lt;/p&gt;&lt;p&gt;对于不同错误，需要等待的时间是不一样的，每个 &lt;code class=&quot;inline&quot;&gt;Backoffer&lt;/code&gt; 在创建时，会&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L96&quot;&gt;根据不同类型，创建不同的 backoff 函数&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上就是 tikv-client 上篇的内容，我们在下篇会详细介绍实现分布式计算相关的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/coprocessor.go#L354&quot;&gt;copIterator&lt;/a&gt;&lt;/b&gt; &lt;b&gt;和实现分布式事务的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/2pc.go#L66&quot;&gt;twoPCCommiter&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-06-43926052</guid>
<pubDate>Thu, 06 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 加入 CNCF 沙箱托管项目</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-29-43278790.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43278790&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0b95a167ac111d328b7091dd4d9ce487_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;TiKV 加入 CNCF 沙箱托管项目&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://cncf.io/&quot;&gt;云原生计算基金会 (CNCF)&lt;/a&gt;今天宣布接纳 &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt; 开源分布式事务键值数据库作为 CNCF 沙箱的早期发展云原生项目。&lt;/p&gt;&lt;p&gt;TiKV 采用 &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; 构建，由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Raft_(computer_science)&quot;&gt;Raft&lt;/a&gt;（通过 etcd）驱动，并受到 Google Spanner 设计的激励，提供简化的调度和自动平衡，而不依赖于任何分布式文件系统。该项目是一个开源、统一分布式存储层，支持功能强大的数据一致性、分布式事务、水平可扩展性和云原生架构。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27ad63df5d6339a8264a57aec1efaee2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;975&quot; data-rawheight=&quot;513&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27ad63df5d6339a8264a57aec1efaee2&quot; data-watermark-src=&quot;v2-d8fa777cb71f3f36f0d4d0334607aa85&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.pingcap.com/en/&quot;&gt;PingCAP&lt;/a&gt; 的首席工程师 和 TiKV 项目负责人 Siddon Tang 表示：“随着我们产生和收集的数据量继续以惊人的速度增长，各组织需要一种方法确保云原生环境的水平可扩展性和高度可用性。”“通过加入 CNCF，我们期待着建立项目治理，并在这一开发商中立之家培育愈发壮大的贡献者基地，让我们能够构建更多组件，例如，支持更多语言和新的有用功能。”&lt;/p&gt;&lt;p&gt;TiKV 最初于 2016 年在 PingCAP 开发，现在得到三星、摩拜单车、今日头条、饿了么、腾讯云和 UCloud 的支持。用户包括北京银行、饿了么、Hulu、联想、摩拜单车和&lt;a href=&quot;https://github.com/tikv/tikv/blob/master/docs/adopters.md&quot;&gt;诸多其他企业&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;该项目的 TOC 赞助商是 Bryan Cantrill 和 Ben Hindman。&lt;/p&gt;&lt;p&gt;CNCF 沙箱是早期阶段项目的孵化器，如需进一步了解 CNCF 项目成熟度，请访问&lt;a href=&quot;https://github.com/cncf/toc/blob/master/process/graduation_criteria.adoc&quot;&gt;毕业标准&lt;/a&gt;纲要。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;h2&gt;&lt;b&gt;CNCF to Host TiKV in the Sandbox&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, the &lt;a href=&quot;http://cncf.io/&quot;&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; accepted &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt;, an open source distributed transactional key-value database, into the CNCF Sandbox for early stage and evolving cloud native projects.&lt;/p&gt;&lt;p&gt;Built in &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt;, powered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Raft_(computer_science)&quot;&gt;Raft&lt;/a&gt; (via etcd) and inspired by the design of Google Spanner, TiKV offers simplified scheduling and auto-balancing without dependency on any distributed file system. The project serves as an open source, unifying distributed storage layer that supports strong data consistency, distributed transactions, horizontal scalability, and cloud native architecture.&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27ad63df5d6339a8264a57aec1efaee2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;975&quot; data-rawheight=&quot;513&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27ad63df5d6339a8264a57aec1efaee2&quot; data-watermark-src=&quot;v2-d8fa777cb71f3f36f0d4d0334607aa85&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;“As the amount of data we are producing and collecting continues to grow at an astounding pace, organizations need a way to ensure horizontal scalability and high availability for cloud native applications,” said Siddon Tang, Chief Engineer at &lt;a href=&quot;https://www.pingcap.com/en/&quot;&gt;PingCAP&lt;/a&gt; and TiKV project lead. “By joining CNCF, we look forward to establishing project governance and growing a broader contributor base in this vendor neutral home – allowing us to build additional components like support for more languages and new useful features.”&lt;/p&gt;&lt;p&gt;TiKV was originally developed at PingCAP in 2016, and today includes contributions from Samsung, Mobike, Toutiao.com, Ele.me, Tencent Cloud and UCloud. Users include Bank of Beijing, Ele.me, Hulu, Lenovo, Mobike and &lt;a href=&quot;https://github.com/tikv/tikv/blob/master/docs/adopters.md&quot;&gt;many others&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The TOC sponsors of the project are Bryan Cantrill and Ben Hindman.&lt;/p&gt;&lt;p&gt;The CNCF Sandbox is a home for early stage projects, for further clarification around project maturity levels in CNCF, please visit our outlined &lt;a href=&quot;https://github.com/cncf/toc/blob/master/process/graduation_criteria.adoc&quot;&gt;Graduation Criteria&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.cncf.io/blog/2018/08/28/cncf-to-host-tikv-in-the-sandbox/&quot;&gt;CNCF to Host TiKV in the Sandbox - Cloud Native Computing Foundation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-29-43278790</guid>
<pubDate>Wed, 29 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十七）DDL 源码解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-27-43088324.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43088324&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eef68c3b98ddf29ee7a0311e6259636c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：陈霜&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;DDL 是数据库非常核心的组件，其正确性和稳定性是整个 SQL 引擎的基石，在分布式数据库中，如何在保证数据一致性的前提下实现无锁的 DDL 操作是一件有挑战的事情。&lt;/p&gt;&lt;p&gt;本文首先会介绍 TiDB DDL 组件的总体设计，以及如何在分布式场景下支持无锁 shema 变更，并描述这套算法的大致流程，然后详细介绍一些常见的 DDL 语句的源码实现，包括 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;drop column&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;drop table&lt;/code&gt; 这四种。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DDL in TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的 DDL 通过实现 Google F1 的在线异步 schema 变更算法，来完成在分布式场景下的无锁，在线 schema 变更。为了简化设计，TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 &lt;b&gt;owner&lt;/b&gt; 节点的 &lt;b&gt;worker &lt;/b&gt;串行执行的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;worker：每个节点都有一个 worker 用来处理 DDL 操作。&lt;/li&gt;&lt;li&gt;owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里只是简单概述了 TiDB 的 DDL 设计，下两篇文章详细介绍了 TiDB DDL 的设计实现以及优化，推荐阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md&quot;&gt;TiDB 的异步 schema 变更实现  &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://zimulala.github.io/2017/12/24/optimize/&quot;&gt;TiDB 的异步 schema 变更优化&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下图描述了一个 DDL 请求在 TiDB 中的简单处理流程：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b50b7dac9057fcc661c11624a87f5eb7_r.jpg&quot; data-caption=&quot;图 1：TiDB 中 DDL SQL 的处理流程&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1088&quot; data-rawheight=&quot;730&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b50b7dac9057fcc661c11624a87f5eb7&quot; data-watermark-src=&quot;v2-ec295f4a6baf2e4d815cefcca89fcacb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;TiDB 的 DDL 组件相关代码存放在源码目录的 &lt;code class=&quot;inline&quot;&gt;ddl&lt;/code&gt; 目录下。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-830cd4ddaaaf3919c05a60248d81cc42_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;635&quot; data-rawheight=&quot;276&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-830cd4ddaaaf3919c05a60248d81cc42&quot; data-watermark-src=&quot;v2-de4c148aff1c3ed3f886bcce72f5ebc5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;ddl owner&lt;/code&gt; 相关的代码单独放在 &lt;code class=&quot;inline&quot;&gt;owner&lt;/code&gt; 目录下，实现了 owner 选举等功能。&lt;/p&gt;&lt;p&gt;另外，&lt;code class=&quot;inline&quot;&gt;ddl job queue&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;history ddl job queue&lt;/code&gt; 这两个队列都是持久化到 TiKV 中的。&lt;code class=&quot;inline&quot;&gt;structure&lt;/code&gt; 目录下有 list，&lt;code class=&quot;inline&quot;&gt;hash&lt;/code&gt; 等数据结构在 TiKV 上的实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文接下来按照 TiDB 源码的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;origin/source-code&lt;/a&gt;&lt;/b&gt; &lt;b&gt;分支讲解，最新的 master 分支和 source-code 分支代码会稍有一些差异。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Create table&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 需要把 table 的元信息（&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/model/model.go#L95&quot;&gt;TableInfo&lt;/a&gt;）从 SQL 中解析出来，做一些检查，然后把 table 的元信息持久化保存到 TiKV 中。具体流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;语法解析：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/session.go#L790&quot;&gt;ParseSQL&lt;/a&gt; 解析成抽象语法树 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ast/ddl.go#L393&quot;&gt;CreateTableStmt&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;编译生成 Plan：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/session.go#L805&quot;&gt;Compile&lt;/a&gt; 生成 DDL plan , 并 check 权限等。&lt;/li&gt;&lt;li&gt;生成执行器：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L227&quot;&gt;buildExecutor&lt;/a&gt; 生成 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L33&quot;&gt; DDLExec&lt;/a&gt; 执行器。TiDB 的执行器是火山模型。&lt;/li&gt;&lt;li&gt;执行器调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L300&quot;&gt;e.Next&lt;/a&gt; 开始执行，即 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42&quot;&gt;DDLExec.Next&lt;/a&gt; 方法，判断 DDL 类型后执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L68&quot;&gt;executeCreateTable&lt;/a&gt; , 其实质是调用 &lt;code class=&quot;inline&quot;&gt;ddl_api.go&lt;/code&gt; 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739&quot;&gt;CreateTable&lt;/a&gt; 函数。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739&quot;&gt;CreateTable&lt;/a&gt; 方法是主要流程如下：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;会先 check 一些限制，比如 table name 是否已经存在，table 名是否太长，是否有重复定义的列等等限制。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L775&quot;&gt;buildTableInfo&lt;/a&gt; 获取 global table ID，生成 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; , 即 table 的元信息，然后封装成一个 DDL job，这个 job 包含了 &lt;code class=&quot;inline&quot;&gt;table ID&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt;，并将这个 job 的 type 标记为 &lt;code class=&quot;inline&quot;&gt;ActionCreateTable&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L793&quot;&gt;d.doDDLJob(ctx, job)&lt;/a&gt; 函数中的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L423&quot;&gt;d.addDDLJob(ctx, job)&lt;/a&gt; 会先给 job 获取一个 global job ID 然后放到 job queue 中去。&lt;/li&gt;&lt;li&gt;DDL 组件启动后，在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L318&quot;&gt;start&lt;/a&gt; 函数中会启动一个 &lt;code class=&quot;inline&quot;&gt;ddl_worker&lt;/code&gt; 协程运行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L37&quot;&gt;onDDLWorker&lt;/a&gt; 函数（最新 Master 分支函数名已重命名为 start），每隔一段时间调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L193&quot;&gt;handleDDLJobQueu&lt;/a&gt; 函数去尝试处理 DDL job 队列里的 job，&lt;code class=&quot;inline&quot;&gt;ddl_worker&lt;/code&gt; 会先 check 自己是不是 owner，如果不是 owner，就什么也不做，然后返回；如果是 owner，就调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L212&quot;&gt;getFirstDDLJob&lt;/a&gt; 函数获取 DDL 队列中的第一个 job，然后调 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L236&quot;&gt;runDDLJob&lt;/a&gt; 函数执行 job。&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L275&quot;&gt;runDDLJob&lt;/a&gt; 函数里面会根据 job 的类型，然后调用对应的执行函数，对于 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类型的 job，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L31&quot;&gt;onCreateTable&lt;/a&gt; 函数，然后做一些 check 后，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L56&quot;&gt;t.CreateTable&lt;/a&gt; 函数，将 &lt;code class=&quot;inline&quot;&gt;db_ID&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;table_ID&lt;/code&gt; 映射为 &lt;code class=&quot;inline&quot;&gt;key&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 作为 value 存到 TiKV 里面去，并更新 job 的状态。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt; 函数将 job 从 DDL job 队列中移除，然后加入 history ddl job 队列中去。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L451&quot;&gt;doDDLJob&lt;/a&gt; 函数中检测到 history DDL job 队列中有对应的 job 后，返回。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Add index&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 主要做 2 件事：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;修改 table 的元信息，把 &lt;code class=&quot;inline&quot;&gt;indexInfo&lt;/code&gt; 加入到 table 的元信息中去。&lt;/li&gt;&lt;li&gt;把 table 中已有了的数据行，把 &lt;code class=&quot;inline&quot;&gt;index columns&lt;/code&gt; 的值全部回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中去。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;具体执行流程的前部分的 SQL 解析、Compile 等流程，和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 一样，可以直接从 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42&quot;&gt;DDLExec.Next&lt;/a&gt; 开始看，然后调用 &lt;code class=&quot;inline&quot;&gt;alter&lt;/code&gt; 语句的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L78&quot;&gt;e.executeAlterTable(x)&lt;/a&gt; 函数，其实质调 ddl 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L862&quot;&gt;AlterTable&lt;/a&gt; 函数，然后调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1536&quot;&gt;CreateIndex&lt;/a&gt; 函数，开始执行 add index 的主要工作，具体流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Check 一些限制，比如 table 是否存在，索引是否已经存在，索引名是否太长等。&lt;/li&gt;&lt;li&gt;封装成一个 job，包含了索引名，索引列等，并将 job 的 type 标记为 &lt;code class=&quot;inline&quot;&gt;ActionAddIndex&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;给 job 获取一个 global job ID 然后放到 DDL job 队列中去。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job，根据 job 的类型调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L177&quot;&gt;onCreateIndex&lt;/a&gt; 函数。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;buildIndexInfo&lt;/code&gt; 生成 &lt;code class=&quot;inline&quot;&gt;indexInfo&lt;/code&gt;，然后更新 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 中的 &lt;code class=&quot;inline&quot;&gt;Indices&lt;/code&gt;，持久化到 TiKV 中去。&lt;/li&gt;&lt;li&gt;这里引入了 online schema change 的几个步骤，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L237&quot;&gt;需要留意 indexInfo 的状态变化&lt;/a&gt;：&lt;code class=&quot;inline&quot;&gt;none -&amp;gt; delete only -&amp;gt; write only -&amp;gt; reorganization -&amp;gt;  public&lt;/code&gt;。在 &lt;code class=&quot;inline&quot;&gt;reorganization -&amp;gt; public&lt;/code&gt; 时，首先调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L147&quot;&gt;getReorgInfo&lt;/a&gt; 获取 &lt;code class=&quot;inline&quot;&gt;reorgInfo&lt;/code&gt;，主要包含需要 &lt;code class=&quot;inline&quot;&gt;reorganization&lt;/code&gt; 的 range，即从表的第一行一直到最后一行数据都需要回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中。然后调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L72&quot;&gt;runReorgJob&lt;/a&gt; , &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L554&quot;&gt;addTableIndex&lt;/a&gt;函数开始填充数据到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;中去。&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L112&quot;&gt;runReorgJob&lt;/a&gt; 函数会定期保存回填数据的进度到 TiKV。&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L566&quot;&gt;addTableIndex&lt;/a&gt; 的流程如下：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;启动多个 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 用于并发回填数据到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;把 &lt;code class=&quot;inline&quot;&gt;reorgInfo&lt;/code&gt; 中需要 &lt;code class=&quot;inline&quot;&gt;reorganization&lt;/code&gt; 分裂成多个 range。扫描的默认范围是 &lt;code class=&quot;inline&quot;&gt;[startHandle , endHandle]&lt;/code&gt;，然后默认以 128 为间隔分裂成多个 range，之后并行扫描对应数据行。在 master 分支中，range 范围信息是从 PD 中获取。&lt;/li&gt;&lt;li&gt;把 range 包装成多个 task，发给 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 并行回填 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;等待所有 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 完成后，更新 &lt;code class=&quot;inline&quot;&gt;reorg&lt;/code&gt; 进度，然后持续第 3 步直到所有的 task 都做完。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;5. 后续执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt;，检测 history ddl job 流程和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类似。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Drop Column&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;drop Column&lt;/code&gt; 只要修改 table 的元信息，把 table 元信息中对应的要删除的 column 删除。&lt;code class=&quot;inline&quot;&gt;drop Column&lt;/code&gt; 不会删除原有 table 数据行中的对应的 Column 数据，在 decode 一行数据时，会根据 table 的元信息来 decode。&lt;/p&gt;&lt;p&gt;具体执行流程的前部分都类似，直接跳到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1093&quot;&gt;DropColumn&lt;/a&gt; 函数开始，具体执行流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Check table 是否存在，要 drop 的 column 是否存在等。&lt;/li&gt;&lt;li&gt;封装成一个 job, 将 job 类型标记为 &lt;code class=&quot;inline&quot;&gt;ActionDropColumn&lt;/code&gt;，然后放到 DDL job 队列中去&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job，根据 job 的类型调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/column.go#L174&quot;&gt;onDropColumn&lt;/a&gt; 函数：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;这里 &lt;code class=&quot;inline&quot;&gt;column info&lt;/code&gt; 的状态变化和 &lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 时的变化几乎相反：&lt;code class=&quot;inline&quot;&gt;public -&amp;gt; write only -&amp;gt; delete only -&amp;gt; reorganization -&amp;gt; absent&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L362&quot;&gt;updateVersionAndTableInfo&lt;/a&gt; 更新 table 元信息中的 Columns。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4. 后续执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt;，检测 history ddl job 流程和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类似。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Drop table&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;drop table&lt;/code&gt; 需要删除 table 的元信息和 table 中的数据。&lt;/p&gt;&lt;p&gt;具体执行流程的前部分都类似，&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job 后执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L76&quot;&gt;onDropTable&lt;/a&gt; 函数：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 的状态变化是：&lt;code class=&quot;inline&quot;&gt;public -&amp;gt; write only -&amp;gt; delete only -&amp;gt; none&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 的状态变为 &lt;code class=&quot;inline&quot;&gt;none&lt;/code&gt; 之后，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/meta/meta.go#L306&quot;&gt; DropTable&lt;/a&gt; 将 table 的元信息从 TiKV 上删除。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;至于删除 table 中的数据，后面在调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt; 函数将 job 从 job queue 中移除，加入 history ddl job queue 前，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L160&quot;&gt;delRangeManager.addDelRangeJob(job)&lt;/a&gt;，将要删除的 table 数据范围插入到表 &lt;code class=&quot;inline&quot;&gt;gc_delete_range&lt;/code&gt; 中，然后由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/store/tikv/gcworker/gc_worker.go&quot;&gt;GC worker&lt;/a&gt; 根据 &lt;code class=&quot;inline&quot;&gt;gc_delete_range&lt;/code&gt; 中的信息在 GC 过程中做真正的删除数据操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;New Parallel DDL&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前 TiDB 最新的 Master 分支的 DDL 引入了并行 DDL，用来加速多个 DDL 语句的执行速度。因为串行执行 DDL 时，&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 操作需要把 table 中已有的数据回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中，如果 table 中的数据较多，回填数据的耗时较长，就会阻塞后面 DDL 的操作。目前并行 DDL 的设计是将 &lt;code class=&quot;inline&quot;&gt;add index job&lt;/code&gt; 放到新增的 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中去，其它类型的 DDL job 还是放在原来的 job queue。相应的，也增加一个 &lt;code class=&quot;inline&quot;&gt;add index worker&lt;/code&gt; 来处理 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中的 job。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c2a566a46f5c077f474a4e17ccce2b6b_r.jpg&quot; data-caption=&quot;图 2：并行 DDL 处理流程&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1121&quot; data-rawheight=&quot;676&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c2a566a46f5c077f474a4e17ccce2b6b&quot; data-watermark-src=&quot;v2-6b88067473e2e6254b3b8cf922b0de11&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;并行 DDL 同时也引入了 job 依赖的问题。job 依赖是指同一 table 的 DDL job，job ID 小的需要先执行。因为对于同一个 table 的 DDL 操作必须是顺序执行的。比如说，&lt;code class=&quot;inline&quot;&gt;add column a&lt;/code&gt;，然后 &lt;code class=&quot;inline&quot;&gt;add index on column a&lt;/code&gt;, 如果 &lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 先执行，而 &lt;code class=&quot;inline&quot;&gt;add column&lt;/code&gt; 的 DDL 假设还在排队未执行，这时 &lt;code class=&quot;inline&quot;&gt;add index on column a&lt;/code&gt; 就会报错说找不到 &lt;code class=&quot;inline&quot;&gt;column a&lt;/code&gt;。所以当 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中的 job2 执行前，需要检测 job queue 是否有同一 table 的 job1 还未执行，通过对比 job 的 job ID 大小来判断。执行 job queue 中的 job 时也需要检查 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中是否有依赖的 job 还未执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;End&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 目前一共支持 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/model/ddl.go#L32&quot;&gt;十多种 DDL&lt;/a&gt;，具体以及和 MySQL 兼容性对比可以看 &lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/ddl.md&quot;&gt;这里&lt;/a&gt;。剩余其它类型的 DDL 源码实现读者可以自行阅读，流程和上述几种 DDL 类似。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-27-43088324</guid>
<pubDate>Mon, 27 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 2.1 RC1 Release Notes</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-24-42900414.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42900414&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-547ba61d15cf1a27d999b7155098de96_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 8 月 24 日，TiDB 发布 2.1 RC1 版。相比 2.1 Beta 版本，该版本对系统稳定性、优化器、统计信息以及执行引擎做了很多改进。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;SQL 优化器&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;修复某些情况下关联子查询去关联后结果不正确的问题 &lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Explain&lt;/code&gt; 输出结果 &lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 驱动表选择策略&lt;/li&gt;&lt;li&gt;去掉非 &lt;code class=&quot;inline&quot;&gt;PREPARE&lt;/code&gt; 语句的 Plan Cache&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;INSERT&lt;/code&gt; 语句无法正常解析执行的问题&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复某些情况下使用唯一索引不能查询到 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值的问题 &lt;/li&gt;&lt;li&gt;修复 UTF-8 编码情况下前缀索引的范围计算不正确的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;Project&lt;/code&gt; 算子消除导致的结果不正确的问题 &lt;/li&gt;&lt;li&gt;修复主键为整数类型时无法使用 &lt;code class=&quot;inline&quot;&gt;USE INDEX(PRIMARY)&lt;/code&gt; 的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下使用关联列无法计算索引范围的问题&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. SQL 执行引擎&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复某些情况下夏令时时间计算结果不正确的问题&lt;/li&gt;&lt;li&gt;重构聚合函数框架，提升 &lt;code class=&quot;inline&quot;&gt;Stream&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;Hash&lt;/code&gt; 聚合算子的执行效率&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;Hash&lt;/code&gt; 聚合算子不能正常退出的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;BIT_AND&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;BIT_OR&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;BIT_XOR&lt;/code&gt; 没有正确处理非整型数据的问题&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;REPLACE INTO&lt;/code&gt; 语句的执行速度，性能提升近 10 倍&lt;/li&gt;&lt;li&gt;优化时间类型的内存占用，时间类型数据的内存使用降低为原来的一半&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;UNION&lt;/code&gt; 语句整合有符号和无符号型整数结果时与 MySQL 不兼容的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;LPAD&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;RPAD&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;TO_BASE64&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;FROM_BASE64&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;REPEAT&lt;/code&gt; 因为申请过多内存导致 TiDB panic 的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;MergeJoin&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 在处理 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值时结果不正确的问题&lt;/li&gt;&lt;li&gt;修复某些情况下 Outer Join 结果不正确的问题&lt;/li&gt;&lt;li&gt;增强 &lt;code class=&quot;inline&quot;&gt;Data Truncated&lt;/code&gt; 的报错信息，便于定位出错的数据和表中对应的字段&lt;/li&gt;&lt;li&gt;修复某些情况下 Decimal 计算结果不正确的问题&lt;/li&gt;&lt;li&gt;优化点查的查询性能&lt;/li&gt;&lt;li&gt;禁用 &lt;code class=&quot;inline&quot;&gt;Read Commited&lt;/code&gt; 隔离级别，避免潜在的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;LTRIM&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;RTRIM&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;TRIM&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;MaxOneRow&lt;/code&gt; 算子无法保证返回结果不超过 1 行的问题&lt;/li&gt;&lt;li&gt;拆分 range 个数过多的 Coprocessor 请求&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 统计信息&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;优化统计信息动态收集机制&lt;/li&gt;&lt;li&gt;解决数据频繁更新场景下 &lt;code class=&quot;inline&quot;&gt;Auto Analyze&lt;/code&gt; 不工作的问题&lt;/li&gt;&lt;li&gt;减少统计信息动态更新过程中的写入冲突 &lt;/li&gt;&lt;li&gt;优化统计信息不准确情况下的代价估算&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;AccessPath&lt;/code&gt; 的代价估算策略 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. Server&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复加载权限信息时的 bug&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;Kill&lt;/code&gt; 命令对权限的检查过严问题&lt;/li&gt;&lt;li&gt;解决 Binary 协议中某些数值类型移除的问题&lt;/li&gt;&lt;li&gt;精简日志输出 &lt;/li&gt;&lt;li&gt;处理 &lt;code class=&quot;inline&quot;&gt;mismatchClusterID&lt;/code&gt; 问题&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;advertise-address&lt;/code&gt; 配置项&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;GrpcKeepAlive&lt;/code&gt; 选项&lt;/li&gt;&lt;li&gt;增加连接或者 &lt;code class=&quot;inline&quot;&gt;Token&lt;/code&gt; 时间监控&lt;/li&gt;&lt;li&gt;优化数据解码性能 &lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INFORMMATION_SCHEMA&lt;/code&gt; 中增加 &lt;code class=&quot;inline&quot;&gt;PROCESSLIST&lt;/code&gt; 表&lt;/li&gt;&lt;li&gt;解决权限验证时多条规则可以命中情况下的顺序问题&lt;/li&gt;&lt;li&gt;将部分编码相关的系统变量默认值改为 UTF-8 &lt;/li&gt;&lt;li&gt;慢查询日志显示更详细的信息&lt;/li&gt;&lt;li&gt;支持在 PD 注册 tidb-server 的相关信息并通过 HTTP API 获取 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;5. 兼容性&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;Session&lt;/code&gt; 变量 &lt;code class=&quot;inline&quot;&gt;warning_count&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;error_count&lt;/code&gt; &lt;/li&gt;&lt;li&gt;读取系统变量时增加 Scope 检查&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;MAX_EXECUTION_TIME&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持更多的 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 语法 &lt;/li&gt;&lt;li&gt;Set 系统变量值过程中增加合法性校验&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;Prepare&lt;/code&gt; 语句中 &lt;code class=&quot;inline&quot;&gt;PlaceHolder&lt;/code&gt; 数量的校验&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;set character_set_results = null&lt;/code&gt; &lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;flush status&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;ENUM&lt;/code&gt;  类型在 &lt;code class=&quot;inline&quot;&gt;information_schema&lt;/code&gt; 里的 column size&lt;/li&gt;&lt;li&gt;支持建表语句里的 &lt;code class=&quot;inline&quot;&gt;NATIONAL CHARACTER&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;CHARACTER SET&lt;/code&gt; 语法 &lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;ENUM&lt;/code&gt; 类型的 column info&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;CREATE USER&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;IDENTIFIED WITH&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;TIMESTAMP&lt;/code&gt; 类型计算过程中丢失精度的问题 &lt;/li&gt;&lt;li&gt;支持更多 &lt;code class=&quot;inline&quot;&gt;SYSTEM&lt;/code&gt; 变量的合法性验证&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;CHAR_LENGTH&lt;/code&gt; 函数在计算 binary string 时结果不正确的问题&lt;/li&gt;&lt;li&gt;修复在包含 &lt;code class=&quot;inline&quot;&gt;GROUP BY&lt;/code&gt; 的语句里 &lt;code class=&quot;inline&quot;&gt;CONCAT&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;DECIMAL&lt;/code&gt; 类型 CAST 到 &lt;code class=&quot;inline&quot;&gt;STRING&lt;/code&gt; 类型时，类型长度不准确的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;6. DML&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;解决 &lt;code class=&quot;inline&quot;&gt;Load Data&lt;/code&gt; 语句的稳定性 &lt;/li&gt;&lt;li&gt;解决一些 &lt;code class=&quot;inline&quot;&gt;Batch&lt;/code&gt; 操作情况下的内存使用问题 &lt;/li&gt;&lt;li&gt;提升 &lt;code class=&quot;inline&quot;&gt;Replace Into&lt;/code&gt; 语句的性能&lt;/li&gt;&lt;li&gt;修复写入 &lt;code class=&quot;inline&quot;&gt;CURRENT_TIMESTAMP&lt;/code&gt; 时，精度不一致的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;7. DDL&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;改进 DDL 判断 &lt;code class=&quot;inline&quot;&gt;Schema&lt;/code&gt; 是否已经同步的方法, 避免某些情况下的误判&lt;/li&gt;&lt;li&gt;修复在 &lt;code class=&quot;inline&quot;&gt;ADD INDEX&lt;/code&gt; 过程中的 &lt;code class=&quot;inline&quot;&gt;SHOW CREATE TABLE&lt;/code&gt; 结果&lt;/li&gt;&lt;li&gt;非严格 &lt;code class=&quot;inline&quot;&gt;sql-mode&lt;/code&gt; 模式下, &lt;code class=&quot;inline&quot;&gt;text&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;blob&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;json&lt;/code&gt; 的默认值可以为空 &lt;/li&gt;&lt;li&gt;修复某些特定场景下 &lt;code class=&quot;inline&quot;&gt;ADD INDEX&lt;/code&gt; 的问题&lt;/li&gt;&lt;li&gt;大幅度提升添加 &lt;code class=&quot;inline&quot;&gt;UNIQUE-KEY&lt;/code&gt; 索引操作的速度&lt;/li&gt;&lt;li&gt;修复 Prefix-index 在 UTF-8 字符集的场景下的截断问题&lt;/li&gt;&lt;li&gt;增加环境变量 &lt;code class=&quot;inline&quot;&gt;tidb_ddl_reorg_priority&lt;/code&gt; 来控制 &lt;code class=&quot;inline&quot;&gt;add-index&lt;/code&gt; 操作的优先级&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;information_schema.tables&lt;/code&gt; 中 &lt;code class=&quot;inline&quot;&gt;AUTO-INCREMENT&lt;/code&gt; 的显示问题 &lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;admin show ddl jobs &amp;lt;number&amp;gt;&lt;/code&gt; 命令, 支持输出 number 个 DDL jobs&lt;/li&gt;&lt;li&gt;支持并行 DDL 任务执行&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;8. &lt;a href=&quot;https://github.com/pingcap/tidb/projects/6&quot;&gt;Table Partition&lt;/a&gt;（实验性）&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;支持一级分区&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;Range Partition&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;引入版本控制机制，支持集群滚动兼容升级&lt;/li&gt;&lt;li&gt;开启 &lt;code class=&quot;inline&quot;&gt;Region merge&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;GetPrevRegion&lt;/code&gt; 接口&lt;/li&gt;&lt;li&gt;支持批量 &lt;code class=&quot;inline&quot;&gt;split Region&lt;/code&gt; &lt;/li&gt;&lt;li&gt;支持存储 GC safepoint&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 功能改进&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;优化系统时间回退影响 TSO 分配的问题&lt;/li&gt;&lt;li&gt;优化处理 Region heartbeat 的性能&lt;/li&gt;&lt;li&gt;优化 Region tree 性能&lt;/li&gt;&lt;li&gt;优化计算热点统计的性能问题&lt;/li&gt;&lt;li&gt;优化 API 接口错误码返回&lt;/li&gt;&lt;li&gt;新增一些控制调度策略的开关&lt;/li&gt;&lt;li&gt;禁止在 label 中使用特殊字符&lt;/li&gt;&lt;li&gt;完善调度模拟器&lt;/li&gt;&lt;li&gt;pd-ctl 支持使用统计信息进行 Region split&lt;/li&gt;&lt;li&gt;pd-ctl 支持调用 &lt;code class=&quot;inline&quot;&gt;jq&lt;/code&gt; 来格式化 JSON 输出&lt;/li&gt;&lt;li&gt;新增 etcd Raft 状态机相关 metrics&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. Bug 修复&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复 leader 切换后 namespace 未重新加载的问题&lt;/li&gt;&lt;li&gt;修复 namespace 调度超出 schedule limit 配置的问题&lt;/li&gt;&lt;li&gt;修复热点调度超出 schedule limit 的问题&lt;/li&gt;&lt;li&gt;修复 PD client 关闭时输出一些错误日志的问题&lt;/li&gt;&lt;li&gt;修复 Region 心跳延迟统计有误的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;batch split&lt;/code&gt;，防止热点 Region 写入产生超大 Region&lt;/li&gt;&lt;li&gt;支持设置根据数据行数 split Region，提升 index scan 效率&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 性能优化&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;code class=&quot;inline&quot;&gt;LocalReader&lt;/code&gt; 将 Read 操作从 raftstore 线程分离，减少 Read 延迟&lt;/li&gt;&lt;li&gt;重构 MVCC 框架，优化 memory 使用，提升 scan read 性能&lt;/li&gt;&lt;li&gt;支持基于统计估算进行 Region split，减少 I/O 开销&lt;/li&gt;&lt;li&gt;优化连续写入 Rollback 记录后影响读性能的问题&lt;/li&gt;&lt;li&gt;减少下推聚合计算的内存开销&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 功能改进&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;增加大量内建函数下推支持，更完善的 charset 支持&lt;/li&gt;&lt;li&gt;优化 GC 流程，提升 GC 速度并降低 GC 对系统的影响&lt;/li&gt;&lt;li&gt;开启 &lt;code class=&quot;inline&quot;&gt;prevote&lt;/code&gt;，加快网络异常时的恢复服务速度&lt;/li&gt;&lt;li&gt;增加 RocksDB 日志文件相关的配置项&lt;/li&gt;&lt;li&gt;调整 &lt;code class=&quot;inline&quot;&gt;scheduler latch&lt;/code&gt; 默认配置&lt;/li&gt;&lt;li&gt;使用 tikv-ctl 手动 compact 时可设定是否 compact RocksDB 最底层数据&lt;/li&gt;&lt;li&gt;增加启动时的环境变量检查&lt;/li&gt;&lt;li&gt;支持基于已有数据动态设置 &lt;code class=&quot;inline&quot;&gt;dynamic_level_bytes&lt;/code&gt; 参数&lt;/li&gt;&lt;li&gt;支持自定义日志格式&lt;/li&gt;&lt;li&gt;tikv-ctl 整合 tikv-fail 工具&lt;/li&gt;&lt;li&gt;增加 threads IO metrics&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. Bug 修复&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复 decimal 相关问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;gRPC max_send_message_len&lt;/code&gt; 设置有误的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;region_size&lt;/code&gt; 配置不当时产生的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;如今，在社区和 PingCAP 技术团队的共同努力下，TiDB 2.1 RC1 版已发布，在此感谢社区小伙伴们长久以来的参与和贡献。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;作为明星级开源的分布式关系型数据库，TiDB 灵感来自于 Google Spanner/F1，具备『分布式强一致性事务、在线弹性水平扩展、故障自恢复的高可用、跨数据中心多活』等核心特性。TiDB 于 2015 年 5 月在 GitHub 创建，同年 12 月发布 Alpha 版本，而后于 2016 年 6 月发布 Beta 版，12 月发布 RC1 版， 2017 年 3 月发布 RC2 版，6 月发布 RC3 版，8 月发布 RC4 版，10 月发版 TiDB 1.0，2018 年 3 月发版 2.0 RC1，4 月发版 2.0 GA。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-24-42900414</guid>
<pubDate>Fri, 24 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【详解】What’s New in TiDB 2.1 RC1</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-24-42900183.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42900183&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c5471d4c277c4314ece59344787b7a49_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 4 月底，我们发布了 TiDB 2.0 GA 版本，过去的几个月中，这个版本在上百家用户的生产环境中上线，覆盖了多个行业，包括大型互联网、银行、教育、电信、制造业等。与此同时，我们也开始了 2.1 版本的开发，经过 4 个月时间、1058 次代码提交，2.1 RC1 带着更全面的功能和大幅性能提升来到这里，与大家见面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;新增特性&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Raft 新特性&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Raft 是整个 TiKV 存储引擎的基础，2.1 版本中我们引入了 PreVote、Learner、Region Merge、Region Batch Split 这样四个特性，提升这一基础组件的性能和稳定性。其中 Learner 也是由我们贡献给 Etcd 的新特性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;热点调度&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;热点是分布式系统最大的敌人之一，并且大家的业务场景复杂多变，让热点问题捉摸不定，也是最狡猾的敌人。2.1 版本中，我们一方面增强热点检测能力，尽可能详细地统计系统负载，更快的发现热点；另一方面优化热点调度策略，用尽可能小的代价，尽快地打散热点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行 DDL&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DDL 是 SQL 的基础。在 2.1 版本之前，所有的 DDL 操作的都是串行进行，比如在对一张大表进行 Add Index 操作时，所有的 Create Table、Create Database 语句都会被阻塞，我们在 2.1 版本中对此进行了优化。Add Index 操作和其他 DDL 操作的处理分离，不相关的表上面的操作不会相互阻塞。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Table Partition（实验性）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;分布式数据库可以很容易的存储海量数据，但是 Table Parition 也能找到用武之地。比如在存储日志并定期进行数据归档的场景下，可以通过 Drop Partition 来方便的清理历史数据。在高并发写入的场景下，将单表数据分成多个 Parition 也有助于将写入流量打散在集群上。我们期望这个特性能够在 2.2 或者 3.0 版本中稳定下来。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;统计信息动态更新&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 OLTP 场景中，数据的统计分布决定了查询计划的合理性，在数据变化频繁的场景下，维护统计信息的实时性和准确性非常重要。2.1 版本中我们重点优化了统计信息的实时更新，通过执行查询过程中的反馈信息，不断地纠正已有的统计信息。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行聚合算子&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 OLAP 场景中，聚合和 Join 是最重要的两个算子，其性能决定了语句的处理速度，Join 算子在 2.0 版本中已经是并行模式，2.1 版本中我们对聚合算子做了重点优化，一方面将单线程变成多线程模式，另一方面对聚合的框架做了重构，聚合算子的运行速度、内存使用效率都有极大地提升。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;性能优化&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的定位是一个 HTAP 数据库，OLTP 和 OLAP 都是目标场景。2.1 RC1 版本中，我们对点查、区间扫描、聚合运算这些通用场景进行了优化，也对 “replace into” 语句， Add Index 这些特定场景做了优化。这些场景都有很好的性能提升，有的甚至有数量级的提升。&lt;/p&gt;&lt;p&gt;我们将会在 2.1 GA 版本中发布相比 2.0 GA 的 Benchmark 结果，也希望大家在自己的业务场景中实测对比，然后告诉我们在实际业务上的表现。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;开源社区&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这些激动人心的特性背后，一方面是 PingCAP 开发团队的辛勤工作，另一方面是日益壮大的 TiDB 全球社区。我们欣喜地看到，从 TiDB 2.1 Beta 版发布到现在的短短两个月时间，新增 30 多位 Contributor，其中杜川成为了 TiDB Committer。在这里对社区贡献者表示由衷的感谢，也希望更多志同道合的人能加入进来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;做分布式关系型数据库这样一个通用基础软件如同在夜晚的茫茫大海中航行，充满无数的未知和挑战，社区就是照亮我们前进路线的满天星斗。&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;TiDB 2.1 Beta 版到 RC1 版期间新增 Contributor List&lt;/b&gt;&lt;br&gt;mz1999&lt;br&gt;liuzhengyang&lt;br&gt;cityonsky&lt;br&gt;mail2fish&lt;br&gt;ceohockey60&lt;br&gt;crazycs520&lt;br&gt;zbdba&lt;br&gt;laidahe&lt;br&gt;birdstorm&lt;br&gt;gregwebs&lt;br&gt;hhxcc&lt;br&gt;liukun4515&lt;br&gt;morgo&lt;br&gt;supernan1994&lt;br&gt;bb7133&lt;br&gt;maninalift&lt;br&gt;kbacha&lt;br&gt;ceohockey60&lt;br&gt;DorianZheng&lt;br&gt;GuillaumeGomez&lt;br&gt;TennyZhuang&lt;br&gt;lerencao&lt;br&gt;smallyard&lt;br&gt;sweetIan&lt;br&gt;arosspope&lt;br&gt;York Xiang&lt;br&gt;Mason Hua&lt;br&gt;ice1000&lt;br&gt;opensourcegeek&lt;br&gt;xiangyuf&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-24-42900183</guid>
<pubDate>Fri, 24 Aug 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
