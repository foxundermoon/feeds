<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sun, 20 Jan 2019 13:28:17 +0800</lastBuildDate>
<item>
<title>TiDB 在转转的业务实战</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-16-55026939.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55026939&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eb4578b6386a0021b17d7c418e728fbe_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;陈维，转转优品技术部 RD。&lt;/blockquote&gt;&lt;p&gt;世界级的开源分布式数据库 TiDB 自 2016 年 12 月正式发布第一个版本以来，业内诸多公司逐步引入使用，并取得广泛认可。&lt;/p&gt;&lt;p&gt;对于互联网公司，数据存储的重要性不言而喻。在 NewSQL 数据库出现之前，一般采用单机数据库（比如 MySQL）作为存储，随着数据量的增加，“分库分表”是早晚面临的问题，即使有诸如 MyCat、ShardingJDBC 等优秀的中间件，“分库分表”还是给 RD 和 DBA 带来较高的成本；NewSQL 数据库出现后，由于它不仅有 NoSQL 对海量数据的管理存储能力、还支持传统关系数据库的 ACID 和 SQL，所以对业务开发来说，存储问题已经变得更加简单友好，进而可以更专注于业务本身。而 TiDB，正是 NewSQL 的一个杰出代表！&lt;/p&gt;&lt;p&gt;站在业务开发的视角，TiDB 最吸引人的几大特性是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;支持 MySQL 协议（开发接入成本低）；&lt;/li&gt;&lt;li&gt;100% 支持事务（数据一致性实现简单、可靠）；&lt;/li&gt;&lt;li&gt;无限水平拓展（不必考虑分库分表）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;基于这几大特性，TiDB 在业务开发中是值得推广和实践的，但是，它毕竟不是传统的关系型数据库，以致我们对关系型数据库的一些使用经验和积累，在 TiDB 中是存在差异的，现主要阐述“事务”和“查询”两方面的差异。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 事务和 MySQL 事务的差异&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;MySQL 事务和 TiDB 事务对比&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;400&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;400&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;在 TiDB 中执行的事务 b，返回影响条数是 1（认为已经修改成功），但是提交后查询，status 却不是事务 b 修改的值，而是事务 a 修改的值。&lt;/p&gt;&lt;p&gt;可见，MySQL 事务和 TiDB 事务存在这样的差异：&lt;/p&gt;&lt;p&gt;MySQL 事务中，可以通过影响条数，作为写入（或修改）是否成功的依据；而在 TiDB 中，这却是不可行的！&lt;/p&gt;&lt;p&gt;作为开发者我们需要考虑下面的问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;同步 RPC 调用中，如果需要严格依赖影响条数以确认返回值，那将如何是好？&lt;/li&gt;&lt;li&gt;多表操作中，如果需要严格依赖某个主表数据更新结果，作为是否更新（或写入）其他表的判断依据，那又将如何是好？&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原因分析及解决方案&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于 MySQL，当更新某条记录时，会先获取该记录对应的行级锁（排他锁），获取成功则进行后续的事务操作，获取失败则阻塞等待。&lt;/p&gt;&lt;p&gt;对于 TiDB，使用 Percolator 事务模型：可以理解为乐观锁实现，事务开启、事务中都不会加锁，而是在提交时才加锁。参见 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247484286%26idx%3D2%26sn%3D45b7d9e29af3965567f1743f0c2b536c%26chksm%3Deb162414dc61ad02877378fb97d1790a946c72f4c344260c037d1ae0f9817f2e4d14d1c6233e%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt;&lt;/u&gt;（TiDB 事务算法）。&lt;/p&gt;&lt;p&gt;其简要流程如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;733&quot; data-original=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;733&quot; data-original=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;在事务提交的 PreWrite 阶段，当“锁检查”失败时：如果开启冲突重试，事务提交将会进行重试；如果未开启冲突重试，将会抛出写入冲突异常。&lt;/p&gt;&lt;p&gt;可见，对于 MySQL，由于在写入操作时加上了排他锁，变相将并行事务从逻辑上串行化；而对于 TiDB，属于乐观锁模型，在事务提交时才加锁，并使用事务开启时获取的“全局时间戳”作为“锁检查”的依据。&lt;/p&gt;&lt;p&gt;所以，在业务层面避免 TiDB 事务差异的本质在于避免锁冲突，即，当前事务执行时，不产生别的事务时间戳（无其他事务并行）。处理方式为事务串行化。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;TiDB 事务串行化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在业务层，可以借助分布式锁，实现串行化处理，如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;714&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;714&quot; data-original=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;714&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;714&quot; data-original=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;基于 Spring 和分布式锁的事务管理器拓展&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Spring 生态下，spring-tx 中定义了统一的事务管理器接口：&lt;code&gt;PlatformTransactionManager&lt;/code&gt;，其中有获取事务（getTransaction）、提交（commit）、回滚（rollback）三个基本方法；使用装饰器模式，事务串行化组件可做如下设计：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;647&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;800&quot; data-original=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;647&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;800&quot; data-original=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;其中，关键点有：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;超时时间：为避免死锁，锁必须有超时时间；为避免锁超时导致事务并行，事务必须有超时时间，而且锁超时时间必须大于事务超时时间（时间差最好在秒级）。&lt;/li&gt;&lt;li&gt;加锁时机：TiDB 中“锁检查”的依据是事务开启时获取的“全局时间戳”，所以加锁时机必须在事务开启前。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;事务模板接口设计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;隐藏复杂的事务重写逻辑，暴露简单友好的 API：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot;&gt;&lt;/figure&gt;&lt;u&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;213&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;213&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot;&gt;&lt;/figure&gt;&lt;/u&gt;&lt;h2&gt;&lt;b&gt;TiDB 查询和 MySQL 的差异&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 TiDB 使用过程中，偶尔会有这样的情况，某几个字段建立了索引，但是查询过程还是很慢，甚至不经过索引检索。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;索引混淆型（举例）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;表结构：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE `t_test` (
	  `id` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;主键id&#39;,
	  `a` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;a&#39;,
	  `b` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;b&#39;,
	  `c` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;c&#39;,
	  PRIMARY KEY (`id`),
	  KEY `idx_a_b` (`a`,`b`),
	  KEY `idx_c` (`c`)
	) ENGINE=InnoDB;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;查询：&lt;/b&gt;如果需要查询 (a=1 且 b=1）或 c=2 的数据，在 MySQL 中，sql 可以写为：&lt;code&gt;SELECT id from t_test where (a=1 and b=1) or (c=2);&lt;/code&gt;，MySQL 做查询优化时，会检索到 &lt;code&gt;idx_a_b&lt;/code&gt; 和&lt;code&gt;idx_c&lt;/code&gt; 两个索引；但是在 TiDB（v2.0.8-9）中，这个 sql 会成为一个慢 SQL，需要改写为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT id from t_test where (a=1 and b=1) UNION SELECT id from t_test where (c=2);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;小结：导致该问题的原因，可以理解为 TiDB 的 sql 解析还有优化空间（官方回复已在优化计划中）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;冷热数据型（举例）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;表结构：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE `t_job_record` (
	  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键id&#39;,
	  `job_code` varchar(255) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;任务code&#39;,
	  `record_id` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;记录id&#39;,
	  `status` tinyint(3) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;执行状态:0 待处理&#39;,
	  `execute_time` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;执行时间（毫秒）&#39;,
	  PRIMARY KEY (`id`),
	  KEY `idx_status_execute_time` (`status`,`execute_time`),
	  KEY `idx_record_id` (`record_id`)
	) ENGINE=InnoDB COMMENT=&#39;异步任务job&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;数据说明：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;a. 冷数据，&lt;code&gt;status=1&lt;/code&gt; 的数据（已经处理过的数据）；&lt;/p&gt;&lt;p&gt;b. 热数据，&lt;code&gt;status=0 并且 execute_time&amp;lt;= 当前时间&lt;/code&gt; 的数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;慢查询&lt;/b&gt;：对于热数据，数据量一般不大，但是查询频度很高，假设当前（毫秒级）时间为：1546361579646，则在 MySQL 中，查询 sql 为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT * FROM t_job_record where status=0 and execute_time&amp;lt;= 1546361579646
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个在 MySQL 中很高效的查询，在 TiDB 中虽然也可从索引检索，但其耗时却不尽人意（百万级数据量，耗时百毫秒级）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原因分析：&lt;/b&gt;在 TiDB 中，底层索引结构为 LSM-Tree，如下图：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;206&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;206&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;当从内存级的 C0 层查询不到数据时，会逐层扫描硬盘中各层；且 merge 操作为异步操作，索引数据更新会存在一定的延迟，可能存在无效索引。由于逐层扫描和异步 merge，使得查询效率较低。&lt;/p&gt;&lt;p&gt;优化方式：尽可能缩小过滤范围，比如结合异步 job 获取记录频率，在保证不遗漏数据的前提下，合理设置 execute_time 筛选区间，例如 1 小时，sql 改写为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT * FROM t_job_record  where status=0 and execute_time&amp;gt;1546357979646 and execute_time&amp;lt;= 1546361579646
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;优化效果：&lt;/b&gt;耗时 10 毫秒级别（以下）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;关于查询的启发&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在基于 TiDB 的业务开发中，先摒弃传统关系型数据库带来的对 sql 先入为主的理解或经验，谨慎设计每一个 sql，如 DBA 所提倡：设计 sql 时务必关注执行计划，必要时请教 DBA。&lt;/p&gt;&lt;p&gt;和 MySQL 相比，TiDB 的底层存储和结构决定了其特殊性和差异性；但是，TiDB 支持 MySQL 协议，它们也存在一些共同之处，比如在 TiDB 中使用“预编译”和“批处理”，同样可以获得一定的性能提升。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;服务端预编译&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 MySQL 中，可以使用 &lt;code&gt;PREPARE stmt_name FROM preparable_stm&lt;/code&gt;  对 sql 语句进行预编译，然后使用 &lt;code&gt;EXECUTE stmt_name [USING @var_name [, @var_name] ...]&lt;/code&gt; 执行预编译语句。如此，同一 sql 的多次操作，可以获得比常规 sql 更高的性能。&lt;/p&gt;&lt;p&gt;mysql-jdbc 源码中，实现了标准的 &lt;code&gt;Statement&lt;/code&gt; 和 &lt;code&gt;PreparedStatement&lt;/code&gt; 的同时，还有一个&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 实现，&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 属于&lt;code&gt;PreparedStatement&lt;/code&gt;的拓展，三者对比如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;320&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;320&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;容易发现，&lt;code&gt;PreparedStatement&lt;/code&gt; 和  &lt;code&gt;Statement&lt;/code&gt;的区别主要区别在于参数处理，而对于发送数据包，调用服务端的处理逻辑是一样（或类似）的；经测试，二者速度相当。其实，&lt;code&gt;PreparedStatement&lt;/code&gt; 并不是服务端预处理的；&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 才是真正的服务端预处理，速度也较 &lt;code&gt;PreparedStatement&lt;/code&gt; 快；其使用场景一般是：频繁的数据库访问，sql 数量有限（有缓存淘汰策略，使用不宜会导致两次 IO）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;批处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于多条数据写入，常用 sql 为 &lt;code&gt;insert … values(…),(…)&lt;/code&gt;；而对于多条数据更新，亦可以使用&lt;code&gt;update … case … when… then… end&lt;/code&gt; 来减少 IO 次数。但它们都有一个特点，数据条数越多，sql 越加复杂，sql 解析成本也更高，耗时增长可能高于线性增长。而批处理，可以复用一条简单 sql，实现批量数据的写入或更新，为系统带来更低、更稳定的耗时。&lt;/p&gt;&lt;p&gt;对于批处理，作为客户端，&lt;code&gt;java.sql.Statement&lt;/code&gt; 主要定义了两个接口方法，&lt;code&gt;addBatch&lt;/code&gt;  和  &lt;code&gt;executeBatch&lt;/code&gt;  来支持批处理。&lt;/p&gt;&lt;p&gt;批处理的简要流程说明如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;经业务中实践，使用批处理方式的写入（或更新），比常规 &lt;code&gt;insert … values (…),(…)&lt;/code&gt;（或  &lt;code&gt;update … case … when… then… end&lt;/code&gt;）性能更稳定，耗时也更低。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;本文转载自“转转技术”，原文链接：&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Qyvy_YBIBhZJo1uYHTL93g%3Fscene%3D25%23wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic1.zhimg.com/v2-4a5821a1189a52c4708ad4a65139e744_180x120.jpg&quot; data-image-width=&quot;470&quot; data-image-height=&quot;200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB业务实战&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-16-55026939</guid>
<pubDate>Wed, 16 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十四）TiDB Binlog 源码解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-15-54940241.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54940241&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d734c2c18de0b1415b4b39a664766a19_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：姚维&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Binlog Overview&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这篇文章不是讲 TiDB Binlog 组件的源码，而是讲 TiDB 在执行 DML/DDL 语句过程中，如何将 Binlog 数据 发送给 TiDB Binlog 集群的 Pump 组件。目前 TiDB 在 DML 上的 Binlog 用的类似 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/binary-log-formats.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Row-based&lt;/a&gt; 的格式。具体 Binlog 具体的架构细节可以参看这篇 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;文章&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这里只描述 TiDB 中的代码实现。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DML Binlog&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 采用 protobuf 来编码 binlog，具体的格式可以见 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/master/proto/binlog/binlog.proto&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog.proto&lt;/a&gt;。这里讨论 TiDB 写 Binlog 的机制，以及 Binlog 对 TiDB 写入的影响。&lt;/p&gt;&lt;p&gt;TiDB 会在 DML 语句提交，以及 DDL 语句完成的时候，向 pump 输出 Binlog。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Statement 执行阶段&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DML 语句包括 Insert/Replace、Update、Delete，这里挑 Insert 语句来阐述，其他的语句行为都类似。首先在 Insert 语句执行完插入（未提交）之前，会把自己新增的数据记录在 &lt;code&gt;binlog.TableMutation&lt;/code&gt; 结构体中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;// TableMutation 存储表中数据的变化
message TableMutation {
	    // 表的 id，唯一标识一个表
	    optional int64 table_id      = 1 [(gogoproto.nullable) = false]; 
	    
	    // 保存插入的每行数据
	    repeated bytes inserted_rows = 2;
	    
	    // 保存修改前和修改后的每行的数据
	    repeated bytes updated_rows  = 3;
	    
	    // 已废弃
	    repeated int64 deleted_ids   = 4;
	    
	    // 已废弃
	    repeated bytes deleted_pks   = 5;
	     
	    // 删除行的数据
	    repeated bytes deleted_rows  = 6;
	    
	    // 记录数据变更的顺序
	    repeated MutationType sequence = 7;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个结构体保存于跟每个 Session 链接相关的事务上下文结构体中 &lt;code&gt;TxnState.mutations&lt;/code&gt;。 一张表对应一个 &lt;code&gt;TableMutation&lt;/code&gt; 对象，&lt;code&gt;TableMutation&lt;/code&gt; 里面保存了这个事务对这张表的所有变更数据。Insert 会把当前语句插入的行，根据 &lt;code&gt;RowID&lt;/code&gt; + &lt;code&gt;Row-value&lt;/code&gt; 的格式编码之后，追加到 &lt;code&gt;TableMutation.InsertedRows&lt;/code&gt; 中：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;func (t *Table) addInsertBinlog(ctx context.Context, h int64, row []types.Datum, colIDs []int64) error {
	mutation := t.getMutation(ctx)
	pk, err := codec.EncodeValue(ctx.GetSessionVars().StmtCtx, nil, types.NewIntDatum(h))
	if err != nil {
		return errors.Trace(err)
	}
	value, err := tablecodec.EncodeRow(ctx.GetSessionVars().StmtCtx, row, colIDs, nil, nil)
	if err != nil {
		return errors.Trace(err)
	}
	bin := append(pk, value...)
	mutation.InsertedRows = append(mutation.InsertedRows, bin)
	mutation.Sequence = append(mutation.Sequence, binlog.MutationType_Insert)
	return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等到所有的语句都执行完之后，在 &lt;code&gt;TxnState.mutations&lt;/code&gt; 中就保存了当前事务对所有表的变更数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Commit 阶段&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于 DML 而言，TiDB 的事务采用 2-phase-commit 算法，一次事务提交会分为 Prewrite 阶段，以及 Commit 阶段。这里分两个阶段来看看 TiDB 具体的行为。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Prewrite Binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 &lt;code&gt;session.doCommit&lt;/code&gt; 函数中，TiDB 会构造 &lt;code&gt;binlog.PrewriteValue&lt;/code&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;message PrewriteValue {
    optional int64         schema_version = 1 [(gogoproto.nullable) = false];
    repeated TableMutation mutations      = 2 [(gogoproto.nullable) = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个 &lt;code&gt;PrewriteValue&lt;/code&gt; 中包含了跟这次变动相关的所有行数据，TiDB 会填充一个类型为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 的 Binlog：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;info := &amp;amp;binloginfo.BinlogInfo{
	Data: &amp;amp;binlog.Binlog{
		Tp:            binlog.BinlogType_Prewrite,
		PrewriteValue: prewriteData,
	},
	Client: s.sessionVars.BinlogClient.(binlog.PumpClient),
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;TiDB 这里用一个事务的 Option &lt;code&gt;kv.BinlogInfo&lt;/code&gt; 来把 &lt;code&gt;BinlogInfo&lt;/code&gt; 绑定到当前要提交的 transaction 对象中：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;s.txn.SetOption(kv.BinlogInfo, info)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 &lt;code&gt;twoPhaseCommitter.execute&lt;/code&gt; 中，在把数据 prewrite 到 TiKV 的同时，会调用 &lt;code&gt;twoPhaseCommitter.prewriteBinlog&lt;/code&gt;，这里会把关联的 &lt;code&gt;binloginfo.BinlogInfo&lt;/code&gt; 取出来，把 Binlog 的 &lt;code&gt;binlog.PrewriteValue&lt;/code&gt; 输出到 Pump。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;binlogChan := c.prewriteBinlog()
err := c.prewriteKeys(NewBackoffer(prewriteMaxBackoff, ctx), c.keys)
if binlogChan != nil {
	binlogErr := &amp;lt;-binlogChan // 等待 write prewrite binlog 完成
	if binlogErr != nil {
		return errors.Trace(binlogErr)
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里值得注意的是，在 prewrite 阶段，是需要等待 write prewrite binlog 完成之后，才能继续做接下去的提交的，这里是为了保证 TiDB 成功提交的事务，Pump 至少一定能收到 Prewrite Binlog。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Commit Binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 &lt;code&gt;twoPhaseCommitter.execute&lt;/code&gt; 事务提交结束之后，事务可能提交成功，也可能提交失败。TiDB 需要把这个状态告知 Pump：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;err = committer.execute(ctx)
if err != nil {
	committer.writeFinishBinlog(binlog.BinlogType_Rollback, 0)
	return errors.Trace(err)
}
committer.writeFinishBinlog(binlog.BinlogType_Commit, int64(committer.commitTS))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果发生了 error，那么输出的 Binlog 类型就为 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt;，如果成功提交，那么输出的 Binlog 类型就为 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;func (c *twoPhaseCommitter) writeFinishBinlog(tp binlog.BinlogType, commitTS int64) {
	if !c.shouldWriteBinlog() {
		return
	}
	binInfo := c.txn.us.GetOption(kv.BinlogInfo).(*binloginfo.BinlogInfo)
	binInfo.Data.Tp = tp
	binInfo.Data.CommitTs = commitTS
	go func() {
		err := binInfo.WriteBinlog(c.store.clusterID)
		if err != nil {
			log.Errorf(&quot;failed to write binlog: %v&quot;, err)
		}
	}()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;值得注意的是，这里 WriteBinlog 是单独启动 goroutine 异步完成的，也就是 Commit 阶段，是不再需要等待写 binlog 完成的。这里可以节省一点 commit 的等待时间，这里不需要等待是因为 Pump 即使接收不到这个 Commit Binlog，在超过 timeout 时间后，Pump 会自行根据 Prewrite Binlog 到 TiKV 中确认当条事务的提交状态。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DDL Binlog&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一个 DDL 有如下几个状态：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;const (
	JobStateNone    		JobState = 0
	JobStateRunning 		JobState = 1
	JobStateRollingback  	JobState = 2
	JobStateRollbackDone 	JobState = 3
	JobStateDone         	JobState = 4
	JobStateSynced 			JobState = 6
	JobStateCancelling 		JobState = 7
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这些状态代表了一个 DDL 任务所处的状态：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateNone&lt;/code&gt;，代表 DDL 任务还在处理队列，TiDB 还没有开始做这个 DDL。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRunning&lt;/code&gt;，当 DDL Owner 开始处理这个任务的时候，会把状态设置为 &lt;code&gt;JobStateRunning&lt;/code&gt;，之后 DDL 会开始变更，TiDB 的 Schema 可能会涉及多个状态的变更，这中间不会改变 DDL job 的状态，只会变更 Schema 的状态。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateDone&lt;/code&gt;， 当 TiDB 完成自己所有的 Schema 状态变更之后，会把 Job 的状态改为 Done。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateSynced&lt;/code&gt;，当 TiDB 每做一次 schema 状态变更，就会需要跟集群中的其他 TiDB 做一次同步，但是当 Job 状态为 &lt;code&gt;JobStateDone&lt;/code&gt; 之后，在 TiDB 等到所有的 TiDB 节点同步之后，会将状态修改为 &lt;code&gt;JobStateSynced&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateCancelling&lt;/code&gt;，TiDB 提供语法 &lt;code&gt;ADMIN CANCEL DDL JOBS job_ids&lt;/code&gt; 用于取消某个正在执行或者还未执行的 DDL 任务，当成功执行这个命令之后，DDL 任务的状态会变为 &lt;code&gt;JobStateCancelling&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollingback&lt;/code&gt;，当 DDL Owner 发现 Job 的状态变为 &lt;code&gt;JobStateCancelling&lt;/code&gt; 之后，它会将 job 的状态改变为 &lt;code&gt;JobStateRollingback&lt;/code&gt;，以示已经开始处理 cancel 请求。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;，在做 cancel 的过程，也会涉及 Schema 状态的变更，也需要经历 Schema 的同步，等到状态回滚已经做完了，TiDB 会将 Job 的状态设置为 &lt;code&gt;JobStateRollbackDone&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;对于 Binlog 而言，DDL 的 Binlog 输出机制，跟 DML 语句也是类似的，只有开始处理事务提交阶段，才会开始写 Binlog 出去。那么对于 DDL 来说，跟 DML 不一样，DML 有事务的概念，对于 DDL 来说，SQL 的事务是不影响 DDL 语句的。但是 DDL 里面，上面提到的 Job 的状态变更，是作为一个事务来提交的（保证状态一致性）。所以在每个状态变更，都会有一个事务与之对应，但是上面提到的中间状态，DDL 并不会往外写 Binlog，只有 &lt;code&gt;JobStateRollbackDone&lt;/code&gt; 以及 &lt;code&gt;JobStateDone&lt;/code&gt; 这两种状态，TiDB 会认为 DDL 语句已经完成，会对外发送 Binlog，发送之前，会把 Job 的状态从 &lt;code&gt;JobStateDone&lt;/code&gt; 修改为 &lt;code&gt;JobStateSynced&lt;/code&gt;，这次修改，也涉及一次事务提交。这块逻辑的代码如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;worker.handleDDLJobQueue():

if job.IsDone() || job.IsRollbackDone() {
		binloginfo.SetDDLBinlog(d.binlogCli, txn, job.ID, job.Query)
		if !job.IsRollbackDone() {
			job.State = model.JobStateSynced
		}
		err = w.finishDDLJob(t, job)
		return errors.Trace(err)
}

type Binlog struct {
	DdlQuery []byte
	DdlJobId         int64
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;DdlQuery&lt;/code&gt; 会设置为原始的 DDL 语句，&lt;code&gt;DdlJobId&lt;/code&gt; 会设置为 DDL 的任务 ID。&lt;/p&gt;&lt;p&gt;对于最后一次 Job 状态的提交，会有两条 Binlog 与之对应，这里有几种情况：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如果事务提交成功，类型分别为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 和 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;如果事务提交失败，类型分别为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 和 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以，Pumps 收到的 DDL Binlog，如果类型为 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt; 应该只认为如下状态是合法的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateDone&lt;/code&gt; （因为修改为 &lt;code&gt;JobStateSynced&lt;/code&gt; 还未成功）&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果类型为 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;，应该只认为如下状态是合法的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateSynced&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;当 TiDB 在提交最后一个 Job 状态的时候，如果事务提交失败了，那么 TiDB Owner 会尝试继续修改这个 Job，直到成功。也就是对于同一个 &lt;code&gt;DdlJobId&lt;/code&gt;，后续还可能会有多次 Binlog，直到出现 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-15-54940241</guid>
<pubDate>Tue, 15 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十三）Prepare/Execute 请求处理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-03-53967950.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53967950&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-44348f6cb008b2c6fef5670d96cda457_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：苏立&lt;/p&gt;&lt;blockquote&gt;在之前的一篇文章&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB 源码阅读系列文章（三）SQL 的一生》&lt;/a&gt;中，我们介绍了 TiDB 在收到客户端请求包时，最常见的&lt;code&gt;Command --- COM_QUERY&lt;/code&gt;的请求处理流程。本文我们将介绍另外一种大家经常使用的&lt;code&gt;Command --- Prepare/Execute&lt;/code&gt;请求在 TiDB 中的处理过程。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;Prepare/Execute Statement 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先我们先简单回顾下客户端使用 Prepare 请求过程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;客户端发起 Prepare 命令将带 “?” 参数占位符的 SQL 语句发送到数据库，成功后返回 &lt;code&gt;stmtID&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;具体执行 SQL 时，客户端使用之前返回的 &lt;code&gt;stmtID&lt;/code&gt;，并带上请求参数发起 Execute 命令来执行 SQL。&lt;/li&gt;&lt;li&gt;不再需要 Prepare 的语句时，关闭 &lt;code&gt;stmtID&lt;/code&gt; 对应的 Prepare 语句。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;相比普通请求，Prepare 带来的好处是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;减少每次执行经过 Parser 带来的负担，因为很多场景，线上运行的 SQL 多是相同的内容，仅是参数部分不同，通过 Prepare 可以通过首次准备好带占位符的 SQL，后续只需要填充参数执行就好，可以做到“一次 Parse，多次使用”。&lt;/li&gt;&lt;li&gt;在开启 PreparePlanCache 后可以达到“一次优化，多次使用”，不用进行重复的逻辑和物理优化过程。&lt;/li&gt;&lt;li&gt;更少的网络传输，因为多次执行只用传输参数部分，并且返回结果 Binary 协议。&lt;/li&gt;&lt;li&gt;因为是在执行的同时填充参数，可以防止 SQL 注入风险。&lt;/li&gt;&lt;li&gt;某些特性比如 serverSideCursor 需要是通过 Prepare statement 才能使用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/sql-syntax-prepared-statements.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MySQL 协议&lt;/a&gt; 一样，对于发起 Prepare/Execute 这种使用访问模式提供两种方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Binary 协议：即上述的使用 &lt;code&gt;COM_STMT_PREPARE&lt;/code&gt;，&lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt;，&lt;code&gt;COM_STMT_CLOSE&lt;/code&gt; 命令并且通过 Binary 协议获取返回结果，这是目前各种应用开发常使用的方式。&lt;/li&gt;&lt;li&gt;文本协议：使用 &lt;code&gt;COM_QUERY&lt;/code&gt;，并且用 &lt;code&gt;PREPARE&lt;/code&gt;，&lt;code&gt;EXECUTE&lt;/code&gt;，&lt;code&gt;DEALLOCATE PREPARE&lt;/code&gt; 使用文本协议获取结果，这个效率不如上一种，多用于非程序调用场景，比如在 MySQL 客户端中手工执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下面我们主要以 Binary 协议来看下 TiDB 的处理过程。文本协议的处理与 Binary 协议处理过程比较类似，我们会在后面简要介绍一下它们的差异点。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_PREPARE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;首先，客户端发起 &lt;code&gt;COM_STMT_PREPARE&lt;/code&gt;，在 TiDB 收到后会进入 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L51&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtPrepare&lt;/a&gt;&lt;/code&gt;，这个函数会通过调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L305&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#Prepare&lt;/a&gt;&lt;/code&gt; 来进行实际 Prepare 操作并返回 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-prepare-response.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;结果&lt;/a&gt; 给客户端，实际的 Prepare 处理主要在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/session/session.go%23L924&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;session#PrepareStmt&lt;/a&gt;&lt;/code&gt;和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L73&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrepareExec&lt;/a&gt;&lt;/code&gt; 中完成：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;调用 Parser 完成文本到 AST 的转换，这部分可以参考&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-5/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB 源码阅读系列文章（五）TiDB SQL Parser 的实现》&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;使用名为 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;paramMarkerExtractor&lt;/a&gt;&lt;/code&gt; 的 visitor 从 AST 中提取 “?” 表达式，并根据出现位置（offset）构建排序 Slice，后面我们会看到在 Execute 时会通过这个 Slice 值来快速定位并替换 “?” 占位符。&lt;/li&gt;&lt;li&gt;检查参数个数是否超过 Uint16 最大值（这个是 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-prepare-response.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;协议限制&lt;/a&gt;，对于参数只提供 2 个 Byte）。&lt;/li&gt;&lt;li&gt;进行 Preprocess， 并且创建 LogicPlan， 这部分实现可以参考之前关于 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;逻辑优化的介绍&lt;/a&gt;，这里生成 LogicPlan 主要为了获取并检查组成 Prepare 响应中需要的列信息。&lt;/li&gt;&lt;li&gt;生成 &lt;code&gt;stmtID&lt;/code&gt;，生成的方式是当前会话中的递增 int。&lt;/li&gt;&lt;li&gt;保存 &lt;code&gt;stmtID&lt;/code&gt; 到 &lt;code&gt;ast.Prepared&lt;/code&gt; (由 AST，参数类型信息，schema 版本，是否使用 &lt;code&gt;PreparedPlanCache&lt;/code&gt; 标记组成) 的映射信息到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L185&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SessionVars#PreparedStmts&lt;/a&gt;&lt;/code&gt; 中供 Execute 部分使用。&lt;/li&gt;&lt;li&gt;保存 &lt;code&gt;stmtID&lt;/code&gt; 到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBStatement&lt;/a&gt;&lt;/code&gt; （由 &lt;code&gt;stmtID&lt;/code&gt;，参数个数，SQL 返回列类型信息，&lt;code&gt;sendLongData&lt;/code&gt; 预 &lt;code&gt;BoundParams&lt;/code&gt; 组成）的映射信息保存到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#stmts&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在处理完成之后客户端会收到并持有 &lt;code&gt;stmtID&lt;/code&gt; 和参数类型信息，返回列类型信息，后续即可通过 &lt;code&gt;stmtID&lt;/code&gt; 进行执行时，server 可以通过 6、7 步保存映射找到已经 Prepare 的信息。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_EXECUTE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt; Prepare 成功之后，客户端会通过 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 命令请求执行，TiDB 会进入 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L108&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtExecute&lt;/a&gt;&lt;/code&gt;，首先会通过 stmtID 在上节介绍中保存的 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#stmts&lt;/a&gt;&lt;/code&gt; 中获取前面保存的 &lt;code&gt;TiDBStatement&lt;/code&gt;，并解析出是否使用 &lt;code&gt;userCursor&lt;/code&gt; 和请求参数信息，并且调用对应 &lt;code&gt;TiDBStatement&lt;/code&gt; 的 Execute 进行实际的 Execute 逻辑：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;生成 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/732efe993f70da99fdc18acb380737be33f2333a/ast/misc.go%23L218&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ast.ExecuteStmt&lt;/a&gt;&lt;/code&gt; 并调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/optimize.go%23L28&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;planer.Optimize&lt;/a&gt;&lt;/code&gt; 生成 &lt;code&gt;plancore.Execute&lt;/code&gt;，和普通优化过程不同的是会执行 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/optimize.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exeucte#OptimizePreparedPlan&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;使用 &lt;code&gt;stmtID&lt;/code&gt; 通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SessionVars#PreparedStmts&lt;/a&gt;&lt;/code&gt; 获取到到 Prepare 阶段的 &lt;code&gt;ast.Prepared&lt;/code&gt; 信息。&lt;/li&gt;&lt;li&gt;使用上一节第 2 步中准备的 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;prepared.Params&lt;/a&gt;&lt;/code&gt; 来快速查找并填充参数值；同时会保存一份参数到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;sessionVars.PreparedParams&lt;/a&gt;&lt;/code&gt; 中，这个主要用于支持 &lt;code&gt;PreparePlanCache&lt;/code&gt; 延迟获取参数。&lt;/li&gt;&lt;li&gt;判断对比判断 Prepare 和 Execute 之间 schema 是否有变化，如果有变化则重新 Preprocess。&lt;/li&gt;&lt;li&gt;之后调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L188&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Execute#getPhysicalPlan&lt;/a&gt;&lt;/code&gt; 获取物理计划，实现中首先会根据是否启用 PreparedPlanCache 来查找已缓存的 Plan，本文后面我们也会专门介绍这个。&lt;/li&gt;&lt;li&gt;在没有开启 PreparedPlanCache 或者开启了但没命中 cache 时，会对 AST 进行一次正常的 Optimize。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在获取到 PhysicalPlan 后就是正常的 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot; class=&quot;internal&quot;&gt;Executing 执行&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_CLOSE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt; 在客户不再需要执行之前的 Prepared 的语句时，可以通过&lt;code&gt;COM_STMT_CLOSE&lt;/code&gt;来释放服务器资源，TiDB 收到后会进入&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L501&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtClose&lt;/a&gt;&lt;/code&gt;，会通过&lt;code&gt;stmtID&lt;/code&gt;在&lt;code&gt;TiDBContext#stmts&lt;/code&gt;中找到对应的&lt;code&gt;TiDBStatement&lt;/code&gt;，并且执行&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L152&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Close&lt;/a&gt;清理之前的保存的&lt;code&gt;TiDBContext#stmts&lt;/code&gt;和&lt;code&gt;SessionVars#PrepareStmts&lt;/code&gt;，不过通过代码我们看到，对于前者的确直接进行了清理，对于后者不会删除而是加入到&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/session/session.go%23L1020&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RetryInfo#DroppedPreparedStmtIDs&lt;/a&gt;&lt;/code&gt;中，等待当前事务提交或回滚才会从&lt;code&gt;SessionVars#PrepareStmts&lt;/code&gt;中清理，之所以延迟删除是由于 TiDB 在事务提交阶段遇到冲突会根据配置决定是否重试事务，参与重试的语句可能只有 Execute 和 Deallocate，为了保证重试还能通过&lt;code&gt;stmtID&lt;/code&gt;找到 prepared 的语句 TiDB 目前使用延迟到事务执行完成后才做清理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;其他 &lt;code&gt;COM_STMT&lt;/code&gt;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;br&gt;除了上面介绍的 3 个 &lt;code&gt;COM_STMT&lt;/code&gt;，还有另外几个 &lt;code&gt;COM_STMT_SEND_LONG_DATA&lt;/code&gt;，&lt;code&gt;COM_STMT_FETCH&lt;/code&gt;，&lt;code&gt;COM_STMT_RESET&lt;/code&gt; 也会在 Prepare 中使用到。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_SEND_LONG_DATA&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;某些场景我们 SQL 中的参数是 &lt;code&gt;TEXT&lt;/code&gt;，&lt;code&gt;TINYTEXT&lt;/code&gt;，&lt;code&gt;MEDIUMTEXT&lt;/code&gt;，&lt;code&gt;LONGTEXT&lt;/code&gt; and &lt;code&gt;BLOB&lt;/code&gt;，&lt;code&gt;TINYBLOB&lt;/code&gt;，&lt;code&gt;MEDIUMBLOB&lt;/code&gt;，&lt;code&gt;LONGBLOB&lt;/code&gt; 列时，客户端通常不会在一次 Execute 中带大量的参数，而是单独通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-send-long-data.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;COM_SEND_LONG_DATA&lt;/a&gt;&lt;/code&gt; 预先发到 TiDB，最后再进行 Execute。&lt;/p&gt;&lt;p&gt;TiDB 的处理在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L514&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;client#handleStmtSendLongData&lt;/a&gt;&lt;/code&gt;，通过 &lt;code&gt;stmtID&lt;/code&gt; 在 &lt;code&gt;TiDBContext#stmts&lt;/code&gt; 中找到 &lt;code&gt;TiDBStatement&lt;/code&gt; 并提前放置 &lt;code&gt;paramID&lt;/code&gt; 对应的参数信息，进行追加参数到 &lt;code&gt;boundParams&lt;/code&gt;（所以客户端其实可以多次 send 数据并追加到一个参数上），Execute 时会通过 &lt;code&gt;stmt.BoundParams()&lt;/code&gt; 获取到提前传过来的参数并和 Execute 命令带的参数 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L176&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;一起执行&lt;/a&gt;，在每次执行完成后会重置 &lt;code&gt;boundParams&lt;/code&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_FETCH&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通常的 Execute 执行后，TiDB 会向客户端持续返回结果，返回速率受 &lt;code&gt;max_chunk_size&lt;/code&gt; 控制（见《&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-10/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章（十）Chunk 和执行框架简介&lt;/a&gt;》）， 但实际中返回的结果集可能非常大。客户端受限于资源（一般是内存）无法一次处理那么多数据，就希望服务端一批批返回，&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-fetch.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;COM_STMT_FETCH&lt;/a&gt;&lt;/code&gt; 正好解决这个问题。&lt;/p&gt;&lt;p&gt;它的使用首先要和 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 配合（也就是必须使用 Prepared 语句执行）， &lt;code&gt;handleStmtExeucte&lt;/code&gt; 请求协议 flag 中有标记要使用 cursor，execute 在完成 plan 拿到结果集后并不立即执行而是把它缓存到 &lt;code&gt;TiDBStatement&lt;/code&gt; 中，并立刻向客户端回包中带上列信息并标记 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/status-flags.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ServerStatusCursorExists&lt;/a&gt;&lt;/code&gt;，这部分逻辑可以参看 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L193&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;handleStmtExecute&lt;/a&gt;&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;客户端看到 &lt;code&gt;ServerStatusCursorExists&lt;/code&gt; 后，会用 &lt;code&gt;COM_STMT_FETCH&lt;/code&gt; 向 TiDB 拉去指定 fetchSize 大小的结果集，在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L210&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;connClient#handleStmtFetch&lt;/a&gt;&lt;/code&gt; 中，会通过 session 找到 &lt;code&gt;TiDBStatement&lt;/code&gt; 进而找到之前缓存的结果集，开始实际调用执行器的 Next 获取满足 fetchSize 的数据并返回客户端，如果执行器一次 Next 超过了 fetchSize 会只返回 fetchSize 大小的数据并把剩下的数据留着下次再给客户端，最后对于结果集最后一次返回会标记 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/status-flags.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ServerStatusLastRowSend&lt;/a&gt;&lt;/code&gt; 的 flag 通知客户端没有后续数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_RESET&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;主要用于客户端主动重置 &lt;code&gt;COM_SEND_LONG_DATA&lt;/code&gt; 发来的数据，正常 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 后会自动重置，主要针对客户端希望主动废弃之前数据的情况，因为 &lt;code&gt;COM_STMT_SEND_LONG_DATA&lt;/code&gt; 是一直追加的操作，客户端某些场景需要主动放弃之前预存的参数，这部分逻辑主要位于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L531&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;connClient#handleStmtReset&lt;/a&gt;&lt;/code&gt; 中。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Prepared Plan Cache&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;通过前面的解析过程我们看到在 Prepare 时完成了 AST 转换，在之后的 Execute 会通过 &lt;code&gt;stmtID&lt;/code&gt; 找之前的 AST 来进行 Plan 跳过每次都进行 Parse SQL 的开销。如果开启了 Prepare Plan Cache，可进一步在 Execute 处理中重用上次的 PhysicalPlan 结果，省掉查询优化过程的开销。&lt;/p&gt;&lt;p&gt;TiDB 可以通过 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/config/config.toml.example%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;修改配置文件&lt;/a&gt; 开启 Prepare Plan Cache， 开启后每个新 Session 创建时会初始化一个 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/util/kvcache/simple_lru.go%23L38&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SimpleLRUCache&lt;/a&gt;&lt;/code&gt; 类型的 &lt;code&gt;preparedPlanCache&lt;/code&gt; 用于保存用于缓存 Plan 结果，缓存的 key 是 &lt;code&gt;pstmtPlanCacheKey&lt;/code&gt;（由当前 DB，连接 ID，&lt;code&gt;statementID&lt;/code&gt;，&lt;code&gt;schemaVersion&lt;/code&gt;， &lt;code&gt;snapshotTs&lt;/code&gt;，&lt;code&gt;sqlMode&lt;/code&gt;，&lt;code&gt;timezone&lt;/code&gt; 组成，所以要命中 plan cache 这以上元素必须都和上次缓存的一致），并根据配置的缓存大小和内存大小做 LRU。&lt;/p&gt;&lt;p&gt;在 Execute 的处理逻辑 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L161&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrepareExec&lt;/a&gt;&lt;/code&gt; 中除了检查 &lt;code&gt;PreparePlanCache&lt;/code&gt; 是否开启外，还会判断当前的语句是否能使用 &lt;code&gt;PreparePlanCache&lt;/code&gt;。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只有 &lt;code&gt;SELECT&lt;/code&gt;，&lt;code&gt;INSERT&lt;/code&gt;，&lt;code&gt;UPDATE&lt;/code&gt;，&lt;code&gt;DELETE&lt;/code&gt; 有可能可以使用 &lt;code&gt;PreparedPlanCache&lt;/code&gt;	。&lt;/li&gt;&lt;li&gt;并进一步通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/cacheable_checker.go%23L43&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;cacheableChecker&lt;/a&gt;&lt;/code&gt; visitor 检查 AST 中是否有变量表达式，子查询，&quot;order by ?&quot;，&quot;limit ?，?&quot; 和 UnCacheableFunctions 的函数调用等不可以使用 PlanCache 的情况。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果检查都通过则在 &lt;code&gt;Execute#getPhysicalPlan&lt;/code&gt; 中会用当前环境构建 cache key 查找 &lt;code&gt;preparePlanCache&lt;/code&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;未命中 Cache&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们首先来看下没有命中 Cache 的情况。发现没有命中后会用 &lt;code&gt;stmtID&lt;/code&gt; 找到的 AST 执行 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L161&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Optimize&lt;/a&gt;，但和正常执行 Optimize 不同对于 Cache 的 Plan， 我需要对 “?” 做延迟求值处理， 即将占位符转换为一个 function 做 Plan 并 Cache， 后续从 Cache 获取后 function 在执行时再从具体执行上下文中实际获取执行参数。&lt;/p&gt;&lt;p&gt;回顾下构建 LogicPlan 的过程中会通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/expression_rewriter.go%23L151&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;expressionRewriter&lt;/a&gt;&lt;/code&gt; 将 AST 转换为各类 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/expression/expression.go%23L42&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;expression.Expression&lt;/a&gt;&lt;/code&gt;，通常对于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/types/parser_driver/value_expr.go%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ParamMarkerExpr&lt;/a&gt;&lt;/code&gt; 会重写为 Constant 类型的 expression，但如果该条 stmt 支持 Cache 的话会重写为 Constant 并带上一个特殊的 &lt;code&gt;DeferredExpr&lt;/code&gt; 指向一个 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/expression/builtin_other.go%23L787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;GetParam&lt;/a&gt;&lt;/code&gt; 的函数表达式，而这个函数会在执行时实际从前面 Execute 保存到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;sessionVars.PreparedParams&lt;/a&gt;&lt;/code&gt; 中获取，这样就做到了 Plan 并 Cache 一个参数无关的 Plan，然后实际执行的时填充参数。&lt;/p&gt;&lt;p&gt;新获取 Plan 后会保存到 &lt;code&gt;preparedPlanCache&lt;/code&gt; 供后续使用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;命中 Cache&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;让我们回到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L188&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;getPhysicalPlan&lt;/a&gt;&lt;/code&gt;，如果 Cache 命中在获取 Plan 后我们需要重新 build plan 的 range，因为前面我们保存的 Plan 是一个带 &lt;code&gt;GetParam&lt;/code&gt; 的函数表达式，而再次获取后，当前参数值已经变化，我们需要根据当前 Execute 的参数来重新修正 range，这部分逻辑代码位于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L214&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Execute#rebuildRange&lt;/a&gt;&lt;/code&gt; 中，之后就是正常的执行过程了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;文本协议的 Prepared&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;前面主要介绍了二进制协议的 Prepared 执行流程，还有一种执行方式是通过二进制协议来执行。&lt;/p&gt;&lt;p&gt;客户端可以通过 &lt;code&gt;COM_QUREY&lt;/code&gt; 发送：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;PREPARE stmt_name FROM prepareable_stmt;
EXECUTE stmt_name USING @var_name1, @var_name2,...
DEALLOCTE PREPARE stmt_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;来进行 Prepared，TiDB 会走正常 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot; class=&quot;internal&quot;&gt;文本 Query 处理流程&lt;/a&gt;，将 SQL 转换 Prepare，Execute，Deallocate 的 Plan， 并最终转换为和二进制协议一样的 &lt;code&gt;PrepareExec&lt;/code&gt;，&lt;code&gt;ExecuteExec&lt;/code&gt;，&lt;code&gt;DealocateExec&lt;/code&gt; 的执行器进行执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prepared 是提高程序 SQL 执行效率的有效手段之一。熟悉 TiDB 的 Prepared 实现，可以帮助各位读者在将来使用 Prepared 时更加得心应手。另外，如果有兴趣向 TiDB 贡献代码的读者，也可以通过本文更快的理解这部分的实现。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-03-53967950</guid>
<pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>写给社区的回顾和展望：TiDB 2019, Level Up !</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-03-53915150.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53915150&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1ba53738a37e83c05ad220d4100d9092_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄东旭 &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hash=&quot;5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hovercard=&quot;p$b$5940b1ec1c21a3538c6cfcf5711a75a6&quot;&gt;@Ed Huang&lt;/a&gt; &lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;2018 年对于 TiDB 和 PingCAP 来说是一个由少年向成年的转换的一年，如果用一个关键字来概括就是「蜕变」。&lt;/b&gt;在这一年很欣喜的看到 TiDB 和 TiKV 在越来越多的用户使用在了越来越广泛的场景中，作为一个刚刚 3 岁多的开源项目，没有背后强大的社区的话，是没有办法取得这样的进展的。&lt;br&gt;同时在技术上，2018 年我觉得也交出了一份令人满意的答卷，TiDB 的几个主要项目今年一共合并了 4380 个提交，这几天在整理 2018 年的 Change Log 时候，对比了一下年初的版本，这 4380 个 Commits 背后代表了什么，这里简单写一个文章总结一下。&lt;/blockquote&gt;&lt;p&gt;回想起来，TiDB 是最早定位为 HTAP 的通用分布式数据库之一，如果熟悉我们的老朋友一定知道，我们最早时候一直都是定位 NewSQL，当然现在也是。但是 NewSQL 这个词有个问题，到底 New 在哪，解决了哪些问题，很难一目了然，其实一开始我们就想解决一个 MySQL 分库分表的问题，但是后来慢慢随着我们的用户越来越多，使用的场景也越来越清晰，很多用户的场景已经开始超出了一个「更大的 MySQL 」的使用范围，于是我们从实验室和学术界找到了我们觉得更加清晰的定义：HTAP，希望能构建一个融合 OLTP 和 OLAP 通用型分布式数据库。但是要达成这个目标非常复杂，我们的判断是如果不是从最底层重新设计，很难达到我们的目标，&lt;b&gt;我们认为这是一条更困难但是正确的路，现在看来，这条路是走对了，而且未来会越走越快，越走越稳。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;在 SQL 层这边，TiDB 选择了 MySQL 的协议兼容，一方面持续的加强语法兼容性，另一方面选择自研优化器和执行器，带来的好处就是没有任何历史负担持续优化。回顾今年最大的一个工作应该是重构了执行器框架，&lt;/b&gt;TiDB的 SQL 层还是经典的 Volcano 模型，我们引入了新的内存数据结构 Chunk 来批量处理多行数据，并对各个算子都实现了基于 Chunk 的迭代器接口，这个改进对于 OLAP 请求的改进非常明显，在 TiDB 的 TPC-H 测试集上能看出来（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs-cn/blob/master/benchmark/tpch.md&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/docs&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-cn/blob/master/benchmark/tpch.md&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），Chunk 的引入为我们全面的向量化执行和 CodeGen 支持打下了基础。目前在 TiKV 内部对于下推算子的执行还没有使用 Chunk 改造，不过这个已经在计划中，在 TiKV 中这个改进，预期对查询性能的提升也将非常显著。&lt;/p&gt;&lt;p&gt;&lt;b&gt;另一方面，一个数据库查询引擎最核心的组件之一：优化器，在今年也有长足的进步。&lt;/b&gt;我们在 2017 年就已经全面引入了基于代价的 SQL 优化（CBO，Cost-Based Optimization），我们在今年改进了我们的代价评估模型，加入了一些新的优化规则，同时实现了 Join Re-Order 等一系列优化，从结果上来看，目前在 TPC-H 的测试集上，对于所有 Query，TiDB 的 SQL 优化器大多已给出了最优的执行计划。CBO 的另一个关键模块是统计信息收集，在今年，我们引入了自动的统计信息收集算法，使优化器的适应性更强。另外针对 OLTP 的场景 TiDB 仍然保留了轻量的 RBO 甚至直接 Bypass 优化器，以提升 OLTP 性能。另外，感谢三星韩国研究院的几位工程师的贡献，他们给 TiDB 引入了 Query Plan Cache，对高并发场景下查询性能的提升也很明显。另外在功能上，我们引入了 Partition Table 的支持，对于一些 Partition 特性很明显的业务，TiDB 能够更加高效的调度数据的写入读取和更新。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;787&quot; data-rawheight=&quot;362&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;787&quot; data-original=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;787&quot; data-rawheight=&quot;362&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;787&quot; data-original=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;一直以来，TiDB 的 SQL 层作为纯 Go 语言实现的最完备的 MySQL 语法兼容层，很多第三方的 MySQL 工具在使用着 TiDB 的 SQL Parser，其中的优秀代表比如&lt;u&gt;&lt;b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/user-case-xiaomi/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;小米的 Soar&lt;/a&gt;&lt;/b&gt;&lt;/u&gt;（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/XiaoMi/soar&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/XiaoMi/soar&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;）。&lt;b&gt;为了方便第三方更好的复用 TiDB Parser，我们在 2018 年将 Parser 从主项目中剥离了出来，成为了一个独立的项目：pingcap/parser，希望能帮到更多的人。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;说到 TiDB 的底层存储 TiKV 今年也有很多让人眼前一亮的更新。&lt;/b&gt;在 TiKV 的基石——一致性算法 Raft 这边，大家知道 TiKV 采用的是 Multi-Raft 的架构，内部通过无数个 Raft Group 动态的分裂、合并、移动以达到动态伸缩和动态负载均衡。我们在今年仍然持续在扩展 Multi-Raft 的边界，我们今年加入了动态的 Raft Group 合并，以减轻元信息存储和心跳通信的负担；给 Raft 扩展了 Learner 角色（只同步 Log 不投票的角色） 为 OLAP Read 打下基础；给 Raft 的基础算法加入了 Pre-Vote 的阶段，让整个系统在异常网络状态下可靠性更高。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;412&quot; data-rawheight=&quot;395&quot; class=&quot;content_image&quot; width=&quot;412&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;412&quot; data-rawheight=&quot;395&quot; class=&quot;content_image lazy&quot; width=&quot;412&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot;&gt;&lt;figcaption&gt;Raft Group Merge&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在性能方面，我们花了很大的精力重构了我们单机上多 Raft Group 的线程模型（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/pull/3568&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/tikv/tikv/pu&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ll/3568&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）， 虽然还没有合并到 master 分支，在我们测试中，这个优化带来了两倍以上的吞吐提升，同时写入延迟降低至现在的版本的 1/2 ，预计在这两周我们会完成这个巨大的 PR 的 Code Review，各位同学可以期待一下 :) &lt;/p&gt;&lt;p&gt;第三件事情是我们开始将 TiKV 的本地存储引擎的接口彻底抽象出来，目标是能做到对 RocksDB 的弱耦合，这点的意义很大，不管是社区还是我们自己，对新的单机存储引擎支持将变得更加方便。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;510&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;510&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;510&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;510&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;在 TiKV 社区这边，今年的另外一件大事是加入了 CNCF，变成了 CNCF 的托管项目，也是 CNCF 基金会第一个非结构化数据库项目。&lt;/b&gt; 后来很多朋友问我，为什么捐赠的是 TiKV 而不是 TiDB，其实主要的原因就像我在当天的一条 Tweet 说的，TiKV 更像是的一个更加通用的组件，当你有一个可以弹性伸缩的，支持跨行 ACID 事务的 Key-Value 数据库时，你会发现构建其他很多可靠的分布式系统会容易很多，这在我们之后的 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/mKygN5EQoaaeFMDIFeAzsw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon&lt;/a&gt;&lt;/u&gt; 中得到了很好的体现。另外社区已经开始出现基于 TiKV 构建的 Redis 协议支持，以及分布式队列系统，例如 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/2tyAtcmKUU2L1yoE_V3SsA&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;meitu/titan&lt;/a&gt;&lt;/u&gt; 项目。作为一个基金会项目，社区不仅仅可以直接使用，更能够将它作为构建其他系统的基石，我觉得更加有意义。类似的，今年我们将我们的 Raft 实现从主项目中独立了出来（pingcap/raft-rs），也是希望更多的人能从中受益。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;“……其 KV与 SQL分层的方式，刚好符合我们提供 NoSQL 存储和关系型存储的需求，另外，PingCAP 的文档齐全，社区活跃，也已经在实际应用场景有大规模的应用，公司在北京，技术交流也非常方便，事实证明，后面提到的这几个优势都是对的……”&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;                                                          ——美图公司 Titan 项目负责人任勇全对 TiKV 的评论&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;在 TiDB 的设计之初，我们坚定将调度和元信息从存储层剥离出来（PD），现在看来，好处正渐渐开始显示出来。&lt;/b&gt;今年在 PD 上我们花了很大精力在处理热点探测和快速热点调度，调度和存储分离的架构让我们不管是在开发，测试还是上线新的调度策略时效率很高。瞬时热点一直是分布式存储的最大敌人，如何快速发现和处理，我们也有计划尝试将机器学习引入 PD 的调度中，这是 2019 会尝试的一个事情。总体来说，这个是一个长期的课题。&lt;/p&gt;&lt;p&gt;我在几个月前的一篇文章提到过 TiDB 为什么从 Day-1 起就 All-in Kubernetes （《&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/m2_Mf0-x_KpPHbnOawyy2A&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;十问 TiDB ：关于架构设计的一些思考&lt;/a&gt;》），今年很欣喜的看到，Kubernetes 及其周边生态已经渐渐成熟，已经开始有很多公司用 Kubernetes 来运行 Mission-critical 的系统，这也佐证了我们当年的判断。2018 年下半年，我们也开源了我们的 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/9Buo-pFMHF892muIdTITPQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Operator&lt;/a&gt;&lt;/u&gt;（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-operator&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），这个项目并不止是一个简单的在 K8s 上自动化运维 TiDB 的工具，在我们的战略里面，是作为 Cloud TiDB 的重要基座，过去设计一个完善的多租户系统是一件非常困难的事情，同时调度对象是数据库这种带状态服务，更是难上加难，TiDB-Operator 的开源也是希望能够借助社区的力量，一起将它做好。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;543&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;543&quot; data-original=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;543&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;543&quot; data-original=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot;&gt;&lt;figcaption&gt;多租户 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;今年还做了一件很大的事情，我们成立了一个新的部门 TEP（TiDB Enterprise Platform）专注于商业化组件及相关的交付质量控制。作为一个企业级的分布式数据库，TiDB 今年完成了商业化从0到1的跨越，越来越的付费客户证明 TiDB 的核心的成熟度已经可以委以重任，成立 TEP 小组也是希望在企业级产品方向上继续发力。从 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Lightning&lt;/a&gt;&lt;/u&gt;（MySQL 到 TiDB 高速离线数据导入工具）到 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM&lt;/a&gt;&lt;/u&gt;（TiDB-DataMigration，端到端的数据迁移-同步工具）能看到发力的重点在让用户无缝的从上游迁移到 TiDB 上。另一方面，&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Binlog&lt;/a&gt;&lt;/u&gt; 虽然不是今年的新东西，但是今年这一年在无数个社区用户的场景中锻炼，越来越稳定。&lt;b&gt;做工具可能在很多人看来并不是那么「高科技」， 很多时候也确实是脏活累活，但是这些经过无数用户场景打磨的周边工具和生态才是一个成熟的基础软件的护城河和竞争壁垒，在 PingCAP 内部，负责工具和外围系统研发的团队规模几乎和内核团队是 1:1 的配比，重要性可见一斑。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在使用场景上，TiDB 的使用规模也越来越大，下面这张图是我们统计的我们已知 TiDB 的用户，包括上线和准上线的用户，&lt;b&gt;从 1.0 GA 后，几乎是以一个指数函数的曲线在增长，应用的场景也从简单的 MySQL Sharding 替代方案变成横跨 OLTP 到实时数据中台的通用数据平台组件。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;598&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1004&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;598&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1004&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 的用户数统计&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;今年几个比较典型的用户案例，  &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-meituan/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;美团&lt;/a&gt;&lt;/u&gt; 的横跨 OLTP 和实时数仓的深度实践，到 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/user-case-zhuanzhuan/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;转转&lt;/a&gt;&lt;/u&gt; 的 All-in TiDB 的体验，再到 TiDB 支撑的北京银行的核心交易系统。可以看到，这些案例从互联网公司的离线线数据存储到要求极端 SLA 的传统银行核心交易系统，TiDB 在这些场景里面都发光发热，甚至有互联网公司（转转）都喊出了 All-in TiDB 的口号，我们非常珍视这份信任，一定尽全力做出漂亮的产品，高质量的服务好我们的用户和客户。另一方面，TiDB 也慢慢开始产生国际影响力的，在线视频巨头葫芦软件（&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//Hulu.com&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;Hulu.com&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;），印度最大的在线票务网站 BookMyShow，东南亚最大的电商之一 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-shopee/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Shopee&lt;/a&gt;&lt;/u&gt; 等等都在大规模的使用 TiDB，在北美和欧洲也已经不少准上线和测试中的的巨头互联网公司。&lt;/p&gt;&lt;p&gt;&lt;b&gt;简单回顾了一下过去的 2018 年，我们看看未来在哪里。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其实从我们在 2018 年做的几个比较大的技术决策就能看到，2019 年将是上面几个方向的延续。大的方向的几个指导思想是：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Predicable.&lt;/b&gt; （靠谱，在更广泛的场景中，做到行为可预测。）&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Make it right before making it fast.&lt;/b&gt;（稳定，先做稳，再做快。）&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Ease of use.&lt;/b&gt; （好用，简单交给用户，复杂留给自己。）&lt;/p&gt;&lt;p&gt;对于真正的 HTAP 场景来说，最大的挑战的是如何很好的做不同类型的 workload 隔离和数据结构根据访问特性自适应。我们在这个问题上给出了自己的答案：通过拓展 Raft 的算法，将不同的副本存储成异构的数据结构以适应不同类型的查询。&lt;/p&gt;&lt;p&gt;这个方法有以下好处：&lt;/p&gt;&lt;p&gt;1. 本身在 Multi-Raft 的层面上修改，不会出现由数据传输组件造成的瓶颈（类似 Kafka 或者 DTS），因为 Multi-Raft 本身就是可扩展的，数据同步的单位从 binlog，变成 Raft log，这个效率会更高，进一步降低了同步的延迟。&lt;/p&gt;&lt;p&gt;2. 更好的资源隔离，通过 PD 的调度，可以真正将不同的副本调度到隔离的物理机器上，真正做到互不影响。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;314&quot; data-rawheight=&quot;493&quot; class=&quot;content_image&quot; width=&quot;314&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;314&quot; data-rawheight=&quot;493&quot; class=&quot;content_image lazy&quot; width=&quot;314&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 2019 年会变成这个样子&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;341&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;691&quot; data-original=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;341&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;691&quot; data-original=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot;&gt;&lt;figcaption&gt;Learner 在 HTAP 中的应用&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在执行器方面，我们会继续推进向量化，不出意外的话，今年会完成所有算子的全路径的向量化执行。&lt;/p&gt;&lt;p&gt;这个 HTAP 方案的另一个关键是存储引擎本身。2019 年，我们会引入新的存储引擎，当然第一阶段仍然会继续在 RocksDB 上改进，改进的目标仍然是减小 LSM-Tree 本身的写放大问题。选用的模型是 WiscKey （FAST16，&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;usenix.org/system/files&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/conference/fast16/fast16-papers-lu.pdf&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），WiscKey 的核心思想是将 Value 从 LSM-Tree 中剥离出来，以减少写放大，如果关注 TiKV 的朋友，已经能注意到我们已经在前几天将一个&lt;b&gt;新存储引擎 Titan&lt;/b&gt;（PingCAP 的 Titan，很遗憾和美图那个项目重名了） 合并到了 TiKV 的主干分支，这个 Titan 是我们在 RocksDB 上的 WiscKey 模型的一个实现，除了 WiscKey 的核心本身，我们还加入了对小 KV 的 inline 等优化，Titan 在我们的内部测试中效果很好，对长度随机的 key-value 写入的吞吐基本能达到原生 RocksDB 的 2 - 3 倍，当然性能提升并不是我最关注的，这个引擎对于 TiDB 最大的意义就是，这个引擎将让 TiDB 适应性更强，做到更加稳定，更加「可预测」。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;618&quot; data-rawheight=&quot;266&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;618&quot; data-original=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;618&quot; data-rawheight=&quot;266&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;618&quot; data-original=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot;&gt;&lt;figcaption&gt;TiKV 新的本地存储引擎 Titan&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 Titan 走向稳定的同时，我们也在调研从头构建一个更适合 TiDB 的 OLTP workload 的存储引擎，前面说到 2018 年做了抽象 TiKV 的本地存储引擎的事情就是为了这个打基础，当然我们仍然会走 LSM-Tree 的路线。这里多提一句，其实很多人都误解了 LSM-Tree 模型的真正优势，在我看来并不是性能，而是：做到可接受的性能的同时，LSM-Tree 的实现非常简单可维护，只有简单的东西才可以依赖，这个决定和我们在 Raft 与 Paxos 之间的选择偏好也是一致的。另外 LSM-Tree 的设计从宏观上来说，更加符合「冷热分层」以适配异构存储介质的想法，这个我相信是未来在存储硬件上的大趋势。&lt;/p&gt;&lt;p&gt;&lt;b&gt;至于在 OLAP 的存储引擎这边，我们走的就是纯列式存储的路线了，但是会和传统的 Columnar 数据结构的设计不太一样，这块的进展，我们会在&lt;/b&gt; &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/community/devcon2019/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;1 月 19 号的 TiDB DevCon&lt;/a&gt; &lt;b&gt;&lt;u&gt;上首秀，这里先卖个关子。&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;另一个大的方向是事务模型，目前来说，TiDB 从诞生起，事务模型就没有改变过，走的是传统的 Percolator 的 2PC。这个模型的好处是简单，吞吐也不是瓶颈，但是一个比较大的问题是延迟，尤其在跨数据中心的场景中，这里的延迟主要表现在往 TSO 拿时间戳的网络 roundtrip 上，当然了，我目前仍然认为时钟（TSO）是一个必不可少组件，在不降低一致性和隔离级别的大前提下也是目前我们的最好选择，另外 Percolator 的模型也不是没有办法对延迟进行优化，我们其实在 2018 年，针对 Percolator 本身做了一些理论上的改进，减少了几次网络的 roundtrip，也在年中书写了新的 2PC 改进的完整的 TLA+ 的证明（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tla-plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tla-&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），证明了这个新算法的正确性，2019 年在这块还会有比较多的改进，其实我们一直在思考，怎么样能够做得更好，选择合适的时机做合适的优化。另外一方面，在事务模型这方面，2PC 在理论和工程上还有很多可以改进的空间，但是现在的当务之急继续的优化代码结构和整体的稳定性，这部分的工作在未来一段时间还是会专注在理论和证明上。另外一点大家可以期待的是，2019 年我们会引入安全的 Follower/Learner Read，这对保持一致性的前提下读的吞吐会提升，另外在跨数据中心的场景，读的延迟会更小。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1030&quot; data-rawheight=&quot;506&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1030&quot; data-original=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1030&quot; data-rawheight=&quot;506&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1030&quot; data-original=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;差不多就这些吧，最后放一句我特别喜欢的丘吉尔的一句名言作为结尾。&lt;/p&gt;&lt;p&gt;Success is not final, failure is not fatal: it is the courage to continue that counts.&lt;/p&gt;&lt;p&gt;成功不是终点，失败也并非终结，最重要的是继续前进的勇气。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-03-53915150</guid>
<pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-28-53457577.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53457577&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f0ec6cb07659b51a79c3ac8332284862_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是&lt;b&gt;菜哥和他的朋友们队&lt;/b&gt;的&lt;b&gt;于畅&lt;/b&gt;同学，他们的项目&lt;b&gt; TiPrometheus&lt;/b&gt;已经被 Prometheus adapter 合并。该项目分两个小项目，分别解决了时序数据的存储与计算问题。存储主要兼容 Prometheus 语法和数据格式，实现了精确查询、模糊查询，完全兼容现有语法。所有数据仅存在 TiKV 中。计算主要通过 TiKV 调用 Lua 实现，通过 Lua 动态扩展实现数据计算的功能。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;项目简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;既然你关注了 TiDB， 想必你一定是个关注 Infrastructure 的硬汉子。监控作为 Infra 不可或缺的一环，其核心便是 &lt;b&gt;TSDB（time series database）&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;TSDB 是一种以时间为主要索引的数据库。主要用来存储大量以时间为序列的指标数据，数据结构也比较简单，通常包括特征信息，指标数据和 timestamp。常见的 TSDB 包括 InfluxDB，OpenTSDB，Prometheus。&lt;/p&gt;&lt;p&gt;而 Prometheus 是一整套监控系统，时序数据库是它的存储部分，下面这张架构图来自于 Prometheus 官方，简单概括了其架构和生态的组成。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;564&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;564&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Prometheus 还支持一个图上没有体现的功能 Remote Storage，可以进行远程的读写，对查询是透明的。这个功能主要是用来做长存储。&lt;b&gt;我们的项目就是实现了一个基于 TiKV 的TSDB 来做 Prometheus 的 Remote Storage。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;核心实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prometheus 记录的数据结构分为两部分 label, samples。label 记录了一些特征信息。samples 包含了指标数据和 timestamp。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;&quot;labels&quot;: [{
 &quot;job&quot;:        &quot;node&quot;,
 &quot;instance&quot;:   &quot;123.123.1.211:9090&quot;,
}]
&quot;samples&quot;:[{
 &quot;timestamp&quot;: 1473305798
 &quot;value&quot;: 0.9
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;label 和时间范围结合，可以查询到需要的 value。&lt;/p&gt;&lt;p&gt;为了查询这些记录，我们需要构建两种索引 label index 和 time index，并以特殊的 key 存储 value。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;label index&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;每对 label 为会以 index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; 为key，labelID 为 value 存入。新的记录会追加到 value 后面。这是一种搜索中常用的倒排索引。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;time index&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;每个 sample 项会以 index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; 为 key，timestamp 为 value。splitTime为时间切片的起始点。新的 timestamp 会追加到 value 后面。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;doc 存储&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们将每一条 samples 记录以 timeseries:doc:&amp;lt;labelID&amp;gt;:&amp;lt;timestamp&amp;gt; 为 key 存入 TiKV，其中 labelID 是 label 全文的散列值。&lt;/p&gt;&lt;p&gt;下面做一个梳理。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;893&quot; data-rawheight=&quot;679&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;893&quot; data-original=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;893&quot; data-rawheight=&quot;679&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;893&quot; data-original=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;写入过程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;生成 labelID&lt;/li&gt;&lt;li&gt;构建 label index，index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; &quot;labelID,labelID&quot;&lt;/li&gt;&lt;li&gt;构建 time index，index:timeseries:&amp;lt;labelID&amp;gt;:&amp;lt;splitTime&amp;gt; &quot;ts,ts&quot;&lt;/li&gt;&lt;li&gt;写入时序数据，timeseries:doc:&amp;lt;labelID&amp;gt;:&amp;lt;timestamp&amp;gt; &quot;value&quot;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;查询过程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. 根据倒排索引查出 labelID 的集合，多对 label 的查询会对 labelID 集合求交集。&lt;/p&gt;&lt;p&gt;2. 根据 labelID 和时间范围内的时间分片查询包含的 timestamp。&lt;/p&gt;&lt;p&gt;3. 根据 labelID 和 timestamp 查出所需的 value。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;扯完这些没用的我们来聊些正经的。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;我们为什么要做这样一个项目&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 2018 年下半年，PingCAP 组织的 Hackathon，当时作为萌新即将参加比赛，想着一定要文体两开花，弘扬开源文化。&lt;/p&gt;&lt;p&gt;萌生了四个想法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiKV TSDB&lt;/li&gt;&lt;li&gt;Machine Learning on TiSpark&lt;/li&gt;&lt;li&gt;魔改 TiKV + Lua 做成 mapreduce&lt;/li&gt;&lt;li&gt;geo 全文检索&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;核心想法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 能做出来，符合参赛要求。&lt;/p&gt;&lt;p&gt;2. 确实能解决生产问题而不是一个比赛项目。&lt;/p&gt;&lt;p&gt;摸了摸头发，觉得 ML on TiSpark 太硬核，根本做不完。&lt;/p&gt;&lt;p&gt;TiHaoop 也太硬核，也做不完。&lt;/p&gt;&lt;p&gt;geo 没在厂里的生产中遇到什么问题。&lt;/p&gt;&lt;p&gt;最后辗转反侧思考一番，拍脑袋决定双线操作，做基于 TiKV 的 TSDB 和 TiKV + Lua，完成时序检索功能的同时，增加更丰富的算子（比赛前两天才想好做什么）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;比赛过程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;周五&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原计划，提前看看 rust，作为 rust 萌新。&lt;/p&gt;&lt;p&gt;于是前一天和同事借了本 rust 书，准备一天速成 rust。&lt;/p&gt;&lt;p&gt;后来发现还是看电视剧更管用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 1（周六）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;周六参加比赛的时候，原以为会有个很长的开场致辞，所以决定 10 点再去。&lt;/p&gt;&lt;p&gt;到了现场，发现大家已经开始撸代码了？？？&lt;/p&gt;&lt;p&gt;整体过程还算顺利，但其中也遇到了一些问题。&lt;/p&gt;&lt;p&gt;Prometheus 的依赖和 TiKV 的一些依赖不兼容，于是 fork 一份 Prometheus 依赖，野路子改两行，兼容了。&lt;/p&gt;&lt;p&gt;下午 5 点的时候，时序基本实现了，但联调发现有数据读写不一致的情况。因菜哥的一个 bug 导致，然后开始了漫长的 debug，一共历时 5 个小时（特别说明，我们组叫菜哥和他的朋友们）。&lt;/p&gt;&lt;p&gt;晚 10 点，准备回家了，不准备再 debug 了，一个 bug 查了 5 个小时。作为娱乐队，熬夜写代码是不可能。&lt;/p&gt;&lt;p&gt;各回各家，各找各妈。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 2（周日）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;开始漫长的半天精通 Lua 虚拟机 + rust。&lt;/p&gt;&lt;p&gt;也遇到了一些问题，比如为什么 TiKV 编译这么慢？？？一天只有 24 次编译机会？？？&lt;/p&gt;&lt;p&gt;下午 2 点，作为第一个讲的团队，及时生成了一个 PPT ，毕竟 PPT 工程师的基础还在。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;648&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;648&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一周后的周一&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;之前写的渣代码，简单写了个 README。抱着尝试的心态，给 Prometheus adapter 提了个 PR。&lt;/p&gt;&lt;p&gt;&lt;b&gt;然后，居然被合进去了！！！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一下午写的代码居然被合进去了！！！&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;成果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;彻底打通了 TiKV 和 Prometheus。&lt;/p&gt;&lt;p&gt;为 TiKV 的时序存储和计算提供了一个思路（之前做过 TiDB 存储时序数据）。&lt;/p&gt;&lt;p&gt;为 Prometheus 的长存储提供了一个还算好用的方案（M3 其实还可以，Thanos 是分片机制，不能算真正意义的分布式存储）。&lt;/p&gt;&lt;p&gt;已在公司生产环境试用，需要经过大数据量的测试，如果没问题计划替代现有方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感悟&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;参加 Hackathon，和周末加两天班没有太大的区别。&lt;/p&gt;&lt;p&gt;最先开始来，只是想混个奖品，比如说书包。去年参加 DevCon 给的布袋用了一年，还没坏，今年准备再领一个。&lt;/p&gt;&lt;p&gt;见到了很多年龄比我们小，但技术又还不错的小伙伴，比如兰海他们组，udf 那个组。也见到了一些年龄稍长的参赛者。&lt;/p&gt;&lt;p&gt;他们的存在，让我们在充满杂事的日常工作中又有了继续奋斗的动力。&lt;/p&gt;&lt;p&gt;似乎，当时选择这个行业没有错，而不仅仅是一份工作。&lt;/p&gt;&lt;p&gt;Just for fun。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;感谢唐刘老师和申砾老师的指导。&lt;/p&gt;&lt;p&gt;感谢 PingCAP 举办了这场大型网友见面活动，收获颇丰。&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目地址：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bragfoo/TiPrometheus&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/bragfoo/TiPr&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ometheus&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（代码比较渣，思路供参考）。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;打个广告：由菜哥和他的朋友们翻译的书：《Go 语言并发之道》已登陆京东、淘宝。非常棒一本 Go 语言书籍，搜索即可购买。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;参考资料：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;1. &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//fabxc.org/tsdb/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;fabxc.org/tsdb/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;2.&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;docs.influxdata.com/inf&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;luxdb/v1.7/concepts/storage_engine/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3. &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/prometheus/prometheus/tree/release-1.8/storage&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/prometheus/p&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;rometheus/tree/release-1.8/storage&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D1%26sn%3D72d9d52558e83eb97cd709c67b5a4149%26chksm%3Deb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D2%26sn%3D7eb3d41b2b5cf2a8a440b12121796e2d%26chksm%3Deb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D2%26sn%3D5f1ee6e838c3a86556fcd556662112c5%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D1%26sn%3D8a8861419dd22344a021667545005769%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D2%26sn%3D3a601b2ff9100a9797605a825e478c01%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/_o_4XRJ22NfAfBvUFQqueQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 2018 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/sg3RiE8Fp96aQZlo1MM0hg&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/q3ZHJ2rxMqfpdRd2oof2xw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TBSSQL 的那些事 | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-28-53457577</guid>
<pubDate>Fri, 28 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TBSSQL 的那些事 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-27-53455060.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53455060&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1409789f904133511501fd27f2be0c7e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自 TiBoys 队的崔秋同学，他们的项目 TBSSQL 在 TiDB Hackathon 2018 中获得了一等奖。&lt;br&gt;&lt;b&gt;TiDB Batch and Streaming SQL（简称 TBSSQL）&lt;/b&gt;扩展了 TiDB 的 SQL 引擎，支持用户以类似 StreamSQL 的语法将 Kafka, Pulsar 等外部数据源以流式表的方式接入 TiDB。通过简单的 SQL 语句，用户可以实现对流式数据的过滤，流式表与普通表的 Join（比如流式事实表与多个普通维度表），甚至通过 CREATE TABLE AS SELECT 语法将处理过的流式数据写入普通表中。此外，针对流式数据的时间属性, 我们实现了基于时间窗口的聚合/排序算子, 使得我们可以对流式数据进行时间维度的聚合/排序。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;序&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;算起来这应该是第三次参加的 Hackathon 了，第一次参加的时候还是在小西天的豌豆荚，和东旭一起，做跨平台数据传输的工具，两天一夜；第二次和奇叔一起在 3W 咖啡，又是两天一夜；这次在自己家举办 Hackathon 比赛，下定决心一定要佛性一些，本着能抱大腿就不单干的心态，迅速决定拉唐长老（唐刘）下水。接下来就计划着折腾点啥，因为我们两个前端都不怎么样，所以只能硬核一些，于是拍了两个方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案一&lt;/b&gt;：之前跟唐长老合作过很长一段时间，我们两个对于测试质量之类的事情也都非常关注，所以想着能不能在 Chaos 系统上做一些文章，把一些前沿的测试理论和经验方法结合到系统里面来，做一套通用的分布式系统测试框架，就像 Jepsen 那样，用这套系统去测试和验证主流的开源分布式项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案二&lt;/b&gt;：越接近于业务实时性的数据处理越有价值，不管是 Kafka/KSQL，Flink/Spark Streaming 都是在向着实时流计算领域方向进行未来的探索。TiDB 虽然已经能够支持类 Real Time OLAP 的场景，但是对于更实时的流式数据处理方面还没有合适的解决方案，不过 TiDB 具有非常好的 Scale 能力，天然的能存储海量的数据库表数据，所以在 Streaming Event 和 Table 关联的场景下具有非常明显的优势。如果在 TiDB 上能够实现一个 Streaming SQL 的引擎，实现 Batch/Streaming 的计算融合，那将会是一件非常有意思的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因为打 Hackathon 比赛主要是希望折腾一些新的东西，所以我们两个简单讨论完了之后还是倾向于方案二，当然做不做的出来另说。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当我们正准备做前期调研和设计的时候，Hackathon 主办方把唐长老拉去做现场导师，参赛规则规定导师不能下场比赛，囧，于是就这样被被动放了鸽子。好在后来遇到了同样被霸哥（韩飞）当导师而放鸽子的川总（杜川），川总对于 Streaming SQL 非常感兴趣，于是难兄难弟一拍即合，迅速决定抱团取暖。随后，Robot 又介绍了同样还没有组队的社区小伙伴 GZY（高志远），这样算是凑齐了三个人，但是一想到没有前端肯定搞不定，于是就拜托娘家人（Dashbase）的交际小王子 WPH（王鹏翰）出马，帮助去召唤一个靠谱的前端小伙伴，后来交际未果直接把自己卖进了队伍，&lt;b&gt;这样终于凑齐了四后端，不，应该是三后端 + 一伪前端的组合。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;因为马上要准备提交项目和团队名称，大家都一致觉得方案二非常有意思，所以就选定了更加儒雅的 TBSSQL（TiDB Batch and Streaming SQL）作为项目名称，TSBSQL 遗憾落选。在团队名称方面，打酱油老男孩 / Scboy / TiStream / 养生 Hackathon / 佛系 Hackathon 都因为不够符合气质被遗憾淘汰，最后代表更有青春气息的 TiBoys 入选（跟着我左手右手一个慢动作，逃……&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前期准备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;所谓 “三军未动, 粮草先行”，既然已经报名了，还是要稍作准备，虽然已经确定了大的方向，但是具体的落地方案还没有细化，而且人员的分工也不是太明确。又经过一轮简单的讨论之后，明确了大家的职责方向，我这边主要负责项目整体设计，进度管理以及和 TiDB 核心相关的代码，川总主要负责 TiDB 核心技术攻关，GZY 负责流数据源数据的采集部分，WPH 负责前端展现以及 Hackathon 当天的 Demo 演示，分工之后大家就开始分头调研动工。&lt;/p&gt;&lt;p&gt;作为这两年来基本没怎么写过代码的退役型选手来说，心里还是非常没底的，也不知道现在 TiDB 代码结构和细节变成什么样了，不求有功，但求别太拖后腿。&lt;/p&gt;&lt;p&gt;对于项目本身的典型应用场景，大家还是比较明确的，觉得这个方向是非常有意义的。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;应用层系统：实时流事件和离线数据的关联查询，比如在线广告推荐系统，在线推荐系统，在线搜索，以及实时反欺诈系统等。&lt;/li&gt;&lt;li&gt;内部数据系统：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;实时数据采样统计，比如内部监控系统；&lt;/li&gt;&lt;li&gt;时间窗口数据分析系统，比如实时的数据流数据分析（分析一段时间内异常的数据流量和系统指标），用于辅助做 AI Ops 相关的事情（比如根据数据流量做节点自动扩容/自动提供参数调优/异常流量和风险报告等等）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;业界 Streaming 相关的系统很多，前期我这边快速地看了下能不能站在巨人的肩膀上做事情，有没有可借鉴或者可借用的开源项目。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Apache Beam&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本质上 Apache Beam 还是一个批处理和流处理融合的 SDK Model，用户可以在应用层使用更简单通用的函数接口实现业务的处理，如果使用 Beam 的话，还需要实现自定义的 Runner，因为 TiDB 本身主要的架构设计非常偏重于数据库方向，内部并没有特别明确的通用型计算引擎，所以现阶段基本上没有太大的可行性。当然也可以选择用 Flink 作为 Runner 连接 TiDB 数据源，但是这就变成了 Flink&amp;amp;TiDB 的事情了，和 Beam 本身关系其实就不大了。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Apache Flink / Spark Streaming&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Flink 是一个典型的流处理系统，批处理可以用流处理来模拟出来。&lt;br&gt;本身 Flink 也是支持 SQL 的，但是是一种嵌入式 SQL，也就是 SQL 和应用程序代码写在一起，这种做法的好处是可以直接和应用层进行整合，但是不好的地方在于，接口不是太清晰，有业务侵入性。阿里内部有一个增强版的 Flink 项目叫 Blink，在这个领域比较活跃。如果要实现批处理和流处理融合的话，需要内部定制和修改 Flink 的代码，把 TiDB 作为数据源对接起来，还有可能需要把一些环境信息提交给 TiDB 以便得到更好的查询结果，当然或许像 TiSpark 那样，直接 Flink 对接 TiKV 的数据源应该也是可以的。因为本身团队对于 Scala/Java 代码不是很熟悉，而且 Flink 的模式会有一定的侵入性，所以就没有在这方面进行更多的探索。同理，没有选择 Spark Streaming 也是类似的原因。当然有兴趣的小伙伴可以尝试下这个方向，也是非常有意思的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Kafka SQL&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为 Kafka 本身只是一个 MQ，以后会向着流处理方向演进，但是目前并没有实现批处理和流处理统一的潜力，所以更多的我们只是借鉴 Kafka SQL 的语法。目前 Streaming SQL 还没有一个统一的标准 SQL，Kafka SQL 也只是一个 SQL 方言，支持的语法还比较简单，但是非常实用，而且是偏交互式的，没有业务侵入性。非常适合在 Hackathon 上做 Demo 演示，我们在项目实现中也是主要参考了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/confluentinc/ksql/blob/0.1.x/docs/syntax-reference.md%23ksql-statements&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kafka SQL&lt;/a&gt; 的定义，当然，&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//ci.apache.org/projects/flink/flink-docs-stable/dev/table/sql.html%23specifying-a-query&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Flink&lt;/a&gt; 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//calcite.apache.org/docs/stream.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Calcite&lt;/a&gt; 也有自己定义的 Streaming 语法，这里就不再讨论了。&lt;/p&gt;&lt;p&gt;调研准备工作讨论到这里基本上也就差不多了，于是我们开始各自备（hua）战（shui），出差的出差，加班的加班，接客户的接客户，学 Golang 的学 Golang，在这种紧（fang）张（fei）无（zi）比（wo）的节奏中，迎来了 Hackathon 比赛的到来。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 流水账&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;具体的技术实现方面都是比较硬核的东西，细节也比较多，扔在最后面写，免的大家看到一半就点×了。&lt;/i&gt;&lt;br&gt;&lt;i&gt;至于参加 Hackathon 的感受，因为不像龙哥那么文豪，也不像马老师那么俏皮，而且本来读书也不多，所以也只能喊一句“黑客马拉松真是太好玩了”！&lt;/i&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 1&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3:30 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于飞机晚点，川总这个点儿才辗转到酒店。睡觉之前非常担心一觉睡过头，让这趟 Hackathon 之旅还没开始就结束了，没想到躺下以后满脑子都是技术细节，怎么都睡不着。漫漫长夜，无眠。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7:45 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;川总早早来到 Hackathon 现场。由于来太早，其他选手都还没到，所以他提前刺探刺探敌情的计划也泡汤了，只好在赛场瞎晃悠一番熟悉熟悉环境，顺道跟大奖合了个影。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;11:00 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;简单的开幕式之后，Hackathon 正式开始。我们首先搞定的是 Streaming SQL 的语法定义以及 Parser 相关改动。这一部分在之前就经过比较详细的在线讨论了，所以现场只需要根据碰头后统一的想法一顿敲敲敲就搞定了。快速搞定这一块以后，我们就有了 SQL 语法层面的 Streaming 实现。当然此时 Streaming 也仅限于语法层面，Streaming 在 SQL 引擎层面对应的其实还是普通的TiDB Table。&lt;/p&gt;&lt;p&gt;接下来是 DDL 部分。这一块我们已经想好了要复用 TiDB Table 的 Meta 结构 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/parser/blob/e5d56f38f4b2fdfb1d7010180cb038bd9f58c071/model/model.go%23L140&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TableInfo&lt;/a&gt; ，因此主要工作就是按照 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-17/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL源码解析&lt;/a&gt; 依葫芦画瓢，难度也不大，以至于我们还有闲心纠结一下 SHOW TABLES 语法里到底要不要屏蔽掉 Streaming Table 的问题。&lt;/p&gt;&lt;p&gt;整体上来看上午的热身活动还是进行的比较顺利的，起码 Streaming DDL 这块没有成为太大的问题。这里面有个插曲就是我在 Hackathon 之前下载编译 TiDB，结果发现 TiDB 的 parser 已经用上时髦的 go module 了（也是好久好久没看 TiDB 代码），折腾好半天，不过好处就是 Hackathon 当天的时候改起来 parser 就比较轻车熟路了，所以赛前编译一个 TiDB 还是非常有必要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;15:30 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随着热身的结束，马上迎来了稳定的敲敲敲阶段。川总简单弄了一个 Mock 的 StreamReader 然后丢给了我，因为我之前写 TiDB 的时候，时代比较遥远，那时候都还在用周 sir 的 Datum，现在一看，为了提高内存效率和性能，已经换成了高大上的 Chunk，于是一个很常见的问题：如何用最正确的做法把一个传过来的 Json 数据格式化成 Table Row 数据放到 Chunk 里面，让彻底我懵逼了。&lt;/p&gt;&lt;p&gt;这里面倒不是技术的问题，主要是类型太多，如果枚举所有类型，搞起来很麻烦，按道理应该有更轻快的办法，但是翻了源代码还是没找到解决方案。这个时候果断去求助现场导师，也顺便去赛场溜（ci）达（tan）一（di）圈（qing）。随便扫了一眼，惊呆了，龙哥他们竟然已经开始写 PPT 了，之前知道龙哥他们强，但是没想到强到这个地步，还让不让大家一块欢快地玩耍了。同时，也了解到了不少非常有意思的项目，比如用机器学习方法去自动调节 TiDB 的调度参数，用 Lua 给 TiKV 添加 UDF 之类的，在 TiDB 上面实现异构数据库的关联查询（简直就是 F1 的大一统，而且听小道消息，他们都已经把 Join 推到 PG 上面去了，然而我们还没开始进入到核心开发流程），在 TiKV 上面实现时序数据库和 Memcached 协议等等，甚至东旭都按捺不住自己 Hackathon 起来了（嘻嘻，可以学学我啊 ;D ）。&lt;/p&gt;&lt;p&gt;本来还想去聊聊各个项目的具体实现方案，但是一想到自己挖了一堆坑还没填，只能默默回去膜拜 TiNiuB 项目。看起来不能太佛系了，于是乎我赶紧召开了一次内部团队 sync 的 catch up，明确下分工，川总开始死磕 TBSSQL 的核心逻辑 Streaming Aggregation 的实现，我这边继续搞不带 Aggregation 的 Streaming SQL 的其他实现，GZY 已经部署起来了 Pulsar，开始准备 Mock 数据，WPH 辅助 GZY 同时也快速理解我们的 Demo 场景，着手设计实现前端展现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;18:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我这边和面带慈父般欣慰笑容的老师（张建）进行了一些技术方案实现上的交流后，了解到目前社区小伙伴已经在搞 CREATE TABLE AS SELECT 的重要信息（后续证明此信息值大概一千块 RMB）。&lt;/p&gt;&lt;p&gt;此时，在解决了之前的问题之后，TBSSQL 终于能跑通简单的 SELECT 语句了。我们心里稍微有点底了，于是一鼓作气，顺路也实现了带 Where 条件的 Stream Table 的 SELECT，以及 Stream Table 和 TiDB Table 的多表 Join，到这里，此时，按照分工，我这边的主体工作除了 Streaming Position 的持久化支持以外，已经写的差不多了，剩下就是去实现一些 Nice to have 的 DDL 的语法支持。川总这里首先要搞的是基于时间窗口的 Streaming Aggregation。按照我们的如意算盘，这里基本上可以复用 TiDB 现有的 Hash Aggregation 的计算逻辑，只需要加上窗口的处理就完事儿了。&lt;/p&gt;&lt;p&gt;不过实际下手的时候仔细一研究代码，发现 Aggregation 这一块代码在川总疏于研究这一段时间已经被重构了一把，加上了一个并发执行的分支，看起来还挺复杂。于是一不做二不休，川总把 Hash Aggregation 的代码拷了一份，删除了并发执行的逻辑，在比较简单的非并发分支加上窗口相关实现。不过这种方法意味着带时间窗口的 Aggregation 得单独出 Plan，Planner 上又得改一大圈。这一块弄完以后，还没来得及调试，就到吃晚饭的点儿了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;21:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;吃完晚饭，因为下午死磕的比较厉害，我和张建、川总出门去园区溜达了一圈。期间张建问我们搞得咋样了，我望了一眼川总，语重心长地说主要成败已经不在我了（后续证明这句语重心长至少也得值一千块 RMB），川总果断信心满满地说问题不大，一切尽在掌握之中。&lt;/p&gt;&lt;p&gt;没想到这个 Flag 刚立起来还是温的，就立马被打脸了。问题出在吃饭前搞的聚合那块（具体细节可以看下后面的坑系列），为了支持时间窗口，我们必须确保 Streaming 上的窗口列能透传到聚合算子当中，为此我们屏蔽了优化器中窗口聚合上的列裁剪规则。可是实际运行当中，我们的修改并没有生效？？？而此时，川总昨天一整晚没睡觉的副作用开始显现出来了，思路已经有点不太清醒了。于是我们把张建拖过来一起 debug。然后我这边也把用 TiDB Global Variable 控制 Streaming Position 的功能实现了，并且和 GZY 这边也实现了 Mock 数据。&lt;/p&gt;&lt;p&gt;之后，我也顺路休息休息，毕竟川总这边搞不定，我们这边搞的再好也没啥用。除了观摩川总和张建手把手，不，肩并肩结对小黑屋编程之外，我也顺便申请了部署 Kafka 联调的机器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;23:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们这边最核心的功能还没突破，亮眼的 CREATE TABLE AS SELECT Streaming 也还没影，其实中期进度还是偏慢了（或者说之前我设计实现的功能的工作量太大了，看起来今天晚上只能死磕了，囧）。我调试 Kafka 死活调不通，端口可以 Telnet 登陆，但是写入和获取数据的时候一直报超时错误，而且我这边已经开始困上来了，有点扛不动了，后来在 Kafka 老司机 WPH 一起看了下配置参数，才发现 Advertise URL 设置成了本地地址，换成对外的 IP 就好了，当然为了简单方便，我们设置了单 Partition 的 Topic，这样 collector 的 Kafka 部分就搞的差不多了，剩下就是实现一个 http 的 restful api 来提供给 TiDB 的 StreamReader 读取，整个连通工作就差不多了。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 2&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;00:00 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这时候川总那边也传来了好消息，终于从 Streaming Aggregation 这个大坑里面爬出来了，后面也比较顺利地搞定了时间窗口上的聚合这块。此时时间已经到了 Hackathon 的第二天，不少其他项目的小伙伴已经收摊回家了。不过我们抱着能多做一个 Feature 是一个的心态，决定挑灯夜战。首先，川总把 Sort Executor 改了一把以支持时间窗口，可能刚刚的踩坑经历为我们攒了人品，Sort 上的改动竟然一次 AC 了。借着这股劲儿，我们又回头优化了一把 SHOW CREATE STREAM 的输出。&lt;/p&gt;&lt;p&gt;这里有个插曲就是为了近距离再回味和感受下之前的开发流程，我们特意在 TiDB 的 repo 里面开了一个 tiboys/hackathon 的分支，然后提交的时候用了标准的 Pull Request 的方式，点赞了才能 merge（后来想想打 Hackathon 不是太可取，没什么用，还挺耽误时间，不知道当时怎么想的），所以在 master 分支和 tiboys/hackathon 分支看的时候都没有任何提交记录。嘻嘻，估计龙哥也没仔细看我们的 repo，所以其实在龙哥的激励下，我们的效率还是可以的 :) 。&lt;/p&gt;&lt;p&gt;GZY 和 WPH 把今天安排的工作完成的差不多了，而且第二天还靠他们主要准备 Demo Show，就去睡觉了，川总也已经困得不行了，准备打烊睡觉。我和川总合计了一下，还差一个最重要的 Feature，抱着就试一把，不行就手工的心态，我们把社区的小伙伴王聪（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bb7133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;bb7133&lt;/a&gt;）提的支持 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/pull/7787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;CREATE TABLE AS SELECT&lt;/a&gt; 语法的 PR 合到了我们的分支，冲突竟然不是太多，然后稍微改了一下来支持 Streaming，结果一运行奇迹般地发现竟然能够运行，RP 全面爆发了，于是我们就近乎免费地增加了一个 Feature。改完这个地方，川总实在坚持不住了，就回去睡了。我这边的 http restful api 也搞的差不多了，准备联调一把，StreamReader 通过 http client 从 collector 读数据，collector 通过 kafka consumer 从 kafka broker 获取数据，结果获取的 Json 数据序列化成 TiDB 自定义的 Time 类型老是出问题，于是我又花了一些时间给 Time 增加了 Marshall 和 Unmarshal 的格式化支持，到这里基本上可以 work 了，看了看时间，凌晨四点半，我也准备去睡了。期间好几次看到霸哥（韩飞）凌晨还在一直帮小（tian）伙（zi）伴（ji）查（wa）问（de）题（keng），其实霸哥认真的时候还是非常靠谱的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7:30 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个时候人陆陆续续地来了，我这边也进入了打酱油的角色，年纪大了确实刚不动了，吃了早餐之后，开始准备思考接下来的分工。因为大家都是临时组队，到了 Hackathon 才碰面，基本上没有太多磨合，而且普遍第二天状态都不大好。虽然大家都很努力，但是在我之前设计的宏大项目面前，还是感觉人力不太够，所以早上 10 点我们开了第二次 sync 的 catch up，讨论接下来的安排。我去负责更新代码和 GitHub 的 Readme，川总最后再简单对代码扫尾，顺便和 GZY 去录屏（罗伯特小姐姐介绍的不翻车经验），WPH 准备画图和 PPT，因为时间有限，前端展现部分打算从卖家秀直接转到买家秀。11 点敲定代码完全封板，然后安心准备 PPT 和下午的 Demo。&lt;/p&gt;&lt;p&gt;&lt;b&gt;14:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;因为抽签抽的比较靠后，主要事情在 WPH 这边，我和川总基本上也没什么大事了，顺手搞了几幅图，然后跟马老师还有其他项目的小伙伴们开始八卦聊天。因为正好周末，家里妹子买东西顺便过来慰问了下。下午主要听了各个 Team 的介绍，欣赏到了极尽浮夸的 LOGO 动画，Get 到了有困难找 Big Brother 的新技能，学习和了解了很有意思的 Idea，真心觉得这届 Hackathon 做的非常值得回忆。&lt;/p&gt;&lt;p&gt;从最后的现场展示情况来看，因为 TBSSQL 内容比较多，真的展示下来，感觉 6 分钟时间还是太赶，好在 WPH Demo 的还是非常顺利的，把我们做的事情都展示出来了。因为砍掉了一些前端展现的部分(这块我们也确实不怎么擅长)，其实对于 Hackathon 项目是非常吃亏的，不过有一点比较欣慰，就像某光头大佬说的，评委们都是懂技术的。因为实现完整性方面能做的也都搞差不多了，打的虽然很累但是也很开心，对于结果也就不怎么纠结了。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;因为川总晚上的飞机，小伙伴们简单沟通了几句，一致同意去园区找个地吃个晚饭，于是大家拉上霸哥去了“头一号”，也是第一次吃了大油条，中间小伙伴们各种黑谁谁谁写的 bug 巴拉巴拉的，后来看手机群里有人 @ 我说拿奖了。&lt;/p&gt;&lt;p&gt;其实很多项目各方面综合实力都不错，可以说是各有特色，很难说的上哪个项目有绝对的优势。我们之前有讨论过，TBSSQL 有获奖的赢面，毕竟从完整性，实用性和生态方面都是有潜质的，但是能获得大家最高的认可还是小意外的，特别感谢各位技术大佬们，也特别感谢帮助我们领奖的满分罗伯特小姐姐。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;最后大家补了一张合照，算是为这次 Hackathon 画下一个句号。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;至此，基本上 Hackathon 的流水账就记录完了，整个项目地址在&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/qiuyesuifeng&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/tidb&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/b&gt; &lt;b&gt;欢迎大家关注和讨论。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;选读：技术实现&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;TLDR: 文章很长，挑感兴趣的部分看看就可以了。&lt;/blockquote&gt;&lt;p&gt;在前期分析和准备之后，基本上就只有在 TiDB 上做 SQL Streaming 引擎一条路可选了，细化了下要实现的功能以及简单的系统架构，感觉工作量还是非常大的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;903&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;903&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;下面简单介绍下系统架构和各个模块的功能：&lt;/p&gt;&lt;p&gt;在数据源采集部分（collector），我们计划选取几种典型的数据源作为适配支持。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;br&gt;最流行的开源 MQ 系统，很多 Streaming 系统对接的都是 Kafka。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Pulsar&lt;/b&gt;&lt;br&gt;流行的开源 MQ 系统，目前比较火爆，有赶超 Kafka 的势头。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Binlog&lt;/b&gt;&lt;br&gt;支持 MySQL/TiDB Binlog 处理，相当于是 MySQL Trigger 功能的升级加强版了。我们对之前的 MySQL -&amp;gt; TiDB 的数据同步工具 Syncer 也比较熟悉，所以这块工作量应该也不大。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Log&lt;/b&gt;&lt;br&gt;常见的 Log 日志，这个就没什么好解释的了。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了方便 Demo 和协作，collector 除了适配不同的数据源，还会提供一个 restful api 的接口，这样 TBSSQL 就可以通过 pull 的方式一直获取 streaming 的数据。因为 collector 主要是具体的工程实现，所以就不在这里细节展开了，感兴趣的话，可以参考下 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/collector&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;相关代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;要在 TiDB 中实现 Streaming 的功能即 TBSSQL，就需要在 TiDB 内部深入定制和修改 TiDB 的核心代码。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Streaming 有两个比较本质的特征：&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Streaming 具有流式特性，也就是说，其数据可以是一直增长，无穷无尽的。而在 Batch 系统(暂时把 MySQL/TIDB 这种数据在一定时间内相对稳定的系统简称 Batch 系统，下面都会沿用这种说法)当中，每个 SQL 的输入数据集是固定，静态的。&lt;/li&gt;&lt;li&gt;Streaming 具有时序特性。每一条数据都有其内在的时间属性（比如说事件发生时间等），数据之间有先后顺序关系。而在 Batch 系统当中，一个表中的数据在时间维度上是无序的。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;因此，要在 TiDB SQL 引擎上支持 Streaming SQL，所涉及到的算子都需要根据 Streaming 的这两个特点做修改。以聚合函数（Aggregation）为例，按照 SQL 语义，聚合算子的实现应该分成两步：首先是 Grouping, 即对输入按照聚合列进行分组；然后是 Execute, 即在各个分组上应用聚合函数进行计算，如下图所示。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;731&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;731&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;对于 Streaming，因为其输入可以是无尽的，Grouping 这个阶段永远不可能结束，所以按照老套路，聚合计算就没法做了。这时，就要根据 Streaming 的时序特性对 Streaming 数据进行分组。每一个分组被称为一个 Time Window（时间窗口）。就拿最简单的 Tumbling Window 来说，可以按照固定的时间间隔把 Streaming 输入切分成一个个相互无交集的窗口，然后在每一个窗口上就可以按照之前的方式进行聚合了。&lt;/p&gt;&lt;p&gt;聚合算子只是一个比较简单的例子，因为其只涉及一路输入。如果要修改多路输入的算子（比如说 Join 多个 Streaming），改动更复杂。此外，时间窗口的类型也是多种多样，刚刚例子中的 Tumbling Window 只是基础款，还有复杂一点的 Hopping Window 以及更复杂的 Sliding Window。在 Hackathon 的有限时间内，我们既要考虑实现难度，又要突出 Batch / Streaming 融合处理的特点，因此在技术上我们做出如下抉择：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;时间窗口只做最基本的 Tumbling Window。&lt;/li&gt;&lt;li&gt;实现基于时间窗口的 Aggregation 和 Sort 作为经典流式算子的代表。&lt;/li&gt;&lt;li&gt;实现单 Streaming Join 多 Batch Table 作为 Batch / Streaming 融合的示例, 多个 Streaming Join 太复杂，因为时间有限就先不做了。&lt;/li&gt;&lt;li&gt;支持 Streaming 处理结果写入 Batch Table（TiDB Table）这种常见但是非常实用的功能。也就是说要支持 &lt;code&gt;CREATE TABLE AS SELECT xxx FROM streaming&lt;/code&gt; 的类似语法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外，既然是要支持 Streaming SQL，选择合适的 SQL 语法也是必要的，需要在 Parser 和 DDL 部分做相应的修改。单整理下，我们的 Feature List 如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;287&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;287&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;下面具体聊聊我们实现方案中的一些关键选择。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Streaming SQL 语法&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Streaming SQL 语法的核心是时间窗口的定义，Time Window 和一般 SQL 中的 Window Function 其实语义上是有区别的。在 Streaming SQL 中，Time Window 主要作用是为后续的 SQL 算子限定输入的范围，而在一般的 SQL 中，Window Funtion 本身就是一个 SQL 算子，里面的 Window 其实起到一个 Partition 的作用。&lt;br&gt;在纯 Streaming 系统当中，这种语义的差别影响不大，反而还会因为语法的一致性降低用户的学习成本，但是在 TBSSQL 这种 Batch / Streaming 混合场景下，同一套语法支持两种语义，会对用户的使用造成一定困扰，特别是在 TiDB 已经被众多用户应用到生产环境这种背景下，这种语义上的差别一定要体现在语法的差异上。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Sreaming DDL&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DDL 这一块实现难度不大，只要照着 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-17/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL源码解析&lt;/a&gt; 依葫芦画瓢就行。这里值得一提的是在 Meta 层，我们直接（偷懒）复用了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/parser/blob/e5d56f38f4b2fdfb1d7010180cb038bd9f58c071/model/model.go%23L140&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TableInfo&lt;/a&gt; 结构（加了判断是否为 Streaming 的 Flag 和一些表示 Streaming 属性的字段）来表示 Streaming Table。这个选择主要是从实现难度上考虑的，毕竟复用现有的结构是最快最安全的。但是从设计思想上看，这个决定其实也暗示了在 TBSSQL 当中，Streaming 是 Table 的一种特殊形式，而不是一个独立的概念。理解这一点很重要，因为这是一些其他设计的依据。比如按照以上设定，那么从语义上讲，在同一个 DB 下 Streaming 和普通 Table 就不能重名，反之的话这种重名就是可以接受的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;StreamReader&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一块主要有两个部分，一个是适配不同的数据源（collector），另一个是将 Streaming 数据源引入 TiDB 计算引擎（StreamReader）。collector 这部分上面已经介绍过了，这里就不再过多介绍了。StreamReader 这一块，主要要修改由 LogicalPlan 生成 PhysicalPlan（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/planner/core/find_best_task.go%23L206&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;），以及由 PhysicalPlan 生成 Executor Operator Tree 的过程（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/executor/builder.go%23L171&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;）。&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/executor/stream_reader.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;StreamReader&lt;/a&gt; 的 Open 方法中，会利用 Meta 中的各种元信息来初始化与 collector 之间的连接，然后在 Next 方法中通过 Pull 的方式不断拉取数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;对时间窗口的处理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;前面我们提到，时间窗口是 Streaming 系统中的核心概念。那么这里就有一个重要的问题，Time Window 中的 Time 如何界定？如何判断什么时候应该切换 Window？最容易想到，也是最简单粗暴的方式，就是按照系统的当前时间来进行切割。这种方式问题很大，因为：&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;数据从生成到被 TBSSQL 系统接收到，肯定会有一定的延迟，而且这个延迟时间是没有办法精确预估的。因此在用户实际场景中，除非是要测量收发延迟，这个系统时间对用户没有太大意义。&lt;/li&gt;&lt;li&gt;考虑到算子并发执行的可能性（虽然还没有实现），不同机器的系统时间可能会有些许偏差，这个偏差对于 Window 操作来说可能导致致命的误差，也会导致结果的不精确（因为 Streaming 源的数据 Shuffle 到不同的处理节点上，系统时间的误差可能不太一样,可能会导致 Window 划分的不一样）。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;因此，比较合理的方式是以 Streaming 中的某一 Timestamp 类型的列来切分窗口，这个值由用户在应用层来指定。当然 Streaming 的 Schema 中可能有多个 Timestamp 列，这里可以要求用户指定一个作为 Window 列。在实现 Demo 的时候，为了省事，我们直接限定了用户 Schema 中只能有一个时间列，并且以该列作为 Window 列（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/ddl/table.go%23L58&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;）。当然这里带来一个问题，就是 Streaming 的 Schema 中必须有 Timestamp 列，不然这里就没法玩了。为此，我们在创建 Streaming 的 DDL 中加了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/ddl/ddl_api.go%23L149&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;检查逻辑&lt;/a&gt;，强制 Streaming 的 Schema 必须有 Timestamp 列（其实我们也没想明白当初 Hackathon 为啥要写的这么细，这些细节为后来通宵埋下了浓重的伏笔，只能理解为程序猿的本能，希望这些代码大家看的时候吐槽少一些）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Streaming DML&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;620&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;620&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;这里简单 DML 指的就是不依赖时间窗口的 DML，比如说只带 Selection 和 Projection 的SELECT 语句，或者单个 Streaming Join 多个 Table。因为不依赖时间窗口，支持这类 DML 实际上不需要对计算层做任何改动，只要接入 Streaming 数据源就可以了。&lt;br&gt;对于 Streaming Join Table(如上图表示的是 Stream Join User&amp;amp;Ads 表的示意图) 可以多说一点，如果不带 Time Window，其实这里需要修改一下Planner。因为 Streaming 的流式特性，这里可能没法获取其完整输入集，因此就没法对 Streaming 的整个输入进行排序，所以 Merge Join 算法这里就没法使用了。同理，也无法基于 Streaming 的整个输入建 Hash 表，因此在 Hash Join 算法当中也只能某个普通表 Build Hash Table。不过，在我们的 Demo 阶段，输入其实也是还是有限的，所以这里其实没有做，倒也影响不大。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;基于时间窗口的 Aggregation 和 Sort&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 TBSSQL 当中，我们实现了基于固定时间窗的 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/executor/aggregate.go%23L934&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hash Aggregation Operator&lt;/a&gt; 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/commit/d36b70bdb2d54b8c34216746ff7a716cba8f4d3c&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sort Operator&lt;/a&gt;。这里比较正规的打法其实应该是实现一个独立的 TimeWindow，各种基于时间窗口的 Operator 可以切换时间窗的逻辑，然后比如 Aggregation 和 Sort 这类算子只关心自己的计算逻辑。 但是这样一来要对 Planner 做比较大的改动，想想看难度太大了，所以我们再一次采取了直（tou）接（lan）的方法，将时间窗口直接实现分别实现在 Aggregation 和 Sort 内部，这样 Planner 这块不用做伤筋动骨的改动，只要在各个分支逻辑上修修补补就可以了。&lt;br&gt;对于 Aggregation，我们还做了一些额外的修改。Aggregation 的输出 Schema 语义上来说只包括聚合列和聚合算子的输出列。但是在引入时间窗口的情况下，为了区分不同的窗口的聚合输出，我们为聚合结果显式加上了两个 Timestamp 列 &lt;code&gt;window_start&lt;/code&gt; 和 &lt;code&gt;window_end&lt;/code&gt;, 来表示窗口的开始时间和结束时间。为了这次这个小特性，我们踩到一个大坑，费了不少劲，这个后面再仔细聊聊。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;支持 Streaming 处理结果写入 Batch Table&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为 TiDB 本身目前还暂时不支持 CREATE TABLE AS SELECT … 语法，而从头开始搞的话工作量又太大，因此我们一度打算放弃这个 Feature。后面经过老司机提醒，我们发现社区的小伙伴王聪（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bb7133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;bb7133&lt;/a&gt;）已经提了一个 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/pull/7787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PR&lt;/a&gt; 在做这个事情了。本着试一把的想法我们把这个 PR 合到我们的分支上一跑，结果竟然没多少冲突，还真能 Work……稍微有点问题的是如果 SELECT 子句中有带时间窗口的聚合，输出的结果不太对。仔细研究了一下发现，CREATE TABLE AS SELECT 语句中做 LogicalPlan 的路径和直接执行 SELECT 时做 LogicalPlan 的入口不太一致，以至于对于前者，我们做 LogicalPlan 的时候遗漏了一些 Streaming 相关信息。这里稍作修改以后，也能够正常运行了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;遇到的困难和坑&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本着前人采坑，后人尽量少踩的心态聊聊遇到的一些问题，主要的技术方案上面已经介绍的比较多了。限于篇幅，只描述遇到的最大的坑——消失的窗口列的故事。在做基于时间窗口的 Aggregation 的时候，我们要按照用户指定的窗口列来切窗口。但是根据 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/planner/core/rule_column_pruning.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;列裁剪&lt;/a&gt; 规则，如果这个窗口列没有被用作聚合列或者在聚合函数中被使用，那么这一列基本上会被优化器裁掉。这里的修改很简单（我们以为），只需要在聚合的列裁剪逻辑中，如果发现聚合带时间窗口，那么直接不做裁剪就完事儿了（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/planner/core/rule_column_pruning.go%23L96&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;代码&lt;/a&gt;）。三下五除二修改完代码，编译完后一运行，结果……瞬间 Panic 了……Debug 一看，发现刚刚的修改没有生效，Streaming 的窗口列还是被裁剪掉了，随后我们又把 Planner 的主要流程看了一遍，还是没有在其他地方发现有类似的裁剪逻辑。&lt;/p&gt;&lt;p&gt;这时我们意识到事情没有这么简单了，赶忙从导师团搬来老司机（还是上面那位）。我们一起用简单粗暴的二分大法和 Print 大法，在生成 LogicalPlan，PhysicalPlan 和 Executor 前后将各个算子的 Schema 打印出来。结果发现，在 PhysicalPlan 完成后，窗口列还是存在的，也就是说我们的修改是生效了的，但是在生成 Executor 以后，这一列却神秘消失了。所以一开始我们定位的思路就错了，问题出在生成 Executor 的过程，但是我们一直在 Planner 中定位，当然找不到问题。&lt;/p&gt;&lt;p&gt;明确了方向以后，我们很快就发现了元凶。在 Build HashAggregation 的时候，有一个不起眼的函数调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/executor/builder.go%23L1111&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;buildProjBelowAgg&lt;/a&gt;，这个函数悄悄地在 Aggregation 算子下面加塞了一个 Projection 算子，顺道又做了一把列裁剪，最为头疼的是，因为这个 Projection 算子是在生成 Executor 阶段才塞进去的，而 EXPLAIN 语句是走不到这里来的，所以这个 Projection 算子在做 Explain 的时候是看不见的，想当于是一个隐形的算子，所以我们就这样华丽丽地被坑了，于是就有了罗伯特小姐姐听到的那句 “xxx，出来挨打” 的桥段。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;今后的计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从立项之初，我们就期望 TBSSQL 能够作为一个正式的 Feature 投入生产环境。为此，在设计和实现过程中，如果能用比较优雅的解决方案，我们都尽量不 Hack。但是由于时间紧迫和能力有限，目前 TBSSQL 还是处于 Demo 的阶段，离实现这个目标还有很长的路要走。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Streaming 数据源&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在对接 Streaming 数据源这块，目前 TBSSQL 有两个问题。首先，TBSSQL 默认输入数据是按照窗口时间戳严格有序的。这一点在生产环境中并不一定成立（比如因为网络原因，某一段数据出现了乱序）。为此，我们需要引入类似 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub41378&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google MillWheel&lt;/a&gt; 系统中 Low Watermark 的机制来保证数据的有序性。其次，为了保证有序，目前 StreamReader 只能单线程运行。在实际生产环境当中，这里很可能因为数据消费速度赶不上上游数据生产速度，导致上游数据源的堆积，这又会反过来导致产生计算结果的时间和数据生产时间之间的延迟越来越大。为了解决这个问题，我们需要将 StreamReader 并行化，而这又要求基于时间窗口的计算算子能够对多路数据进行归并排序。另外，目前采用 TiDB Global Variable 来模拟 Streaming 的位置信息，其实更好地方案是设计用一个 TiDB Table 来记录每个不同 StreamReader 读取到的数据位置，这种做法更标准。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Planner&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Planner 这块，从前面的方案介绍可以看出，Streaming 的流式特性和时序特性决定了 Streaming SQL 的优化方式和一般 SQL 有所不同。目前 TBSSQL 的实现方式是在现有 Planner 的执行路径上加上一系列针对 Streaming SQL 的特殊分支。这种做法很不优雅，既难以理解，也难以扩展。目前，TiDB 正在基于 &lt;a href=&quot;http://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.98.9460%26rep%3Drep1%26type%3Dpdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cascade&lt;/a&gt; 重构 Planner 架构，我们希望今后 Streaming SQL 的相关优化也基于新的 Planner 框架来完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 时间窗口&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前，TBSSQL 只实现了最简单的固定窗口。在固定窗口上，Aggregation、Sort 等算子很大程度能复用现有逻辑。但是在滑动窗口上，Aggregation、Sort 的计算方式和在 Batch Table 上的计算方式会完全不一样。今后，我们希望 TBSSQL 能够支持完善对各种时间窗口类型的支持。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. 多 Streaming 处理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前 TBSSQL 只能处理单路 Streaming 输入，比如单个 Streaming 的聚合，排序，以及单个Streaming 和多个 Table 之间的 Join。多个 Streaming 之间的 Join 因为涉及多个 Streaming 窗口的对齐，目前 TBSSQL 暂不支持，所以 TBSSQL 目前并不是一个完整的 Streaming SQL 引擎。我们计划今后对这一块加以完善。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TBSSQL 是一个复杂的工程，要实现 Batch/Streaming 的融合，除了以上提到这四点，TBSSQL 还有很有很多工作要做，这里就不一一详述了。或许，下次 Hackathon 可以再继续搞一把 TBSSQL 2.0 玩玩:) 有点遗憾的是作为选手出场，没有和所有优秀的参赛的小伙伴们畅谈交流，希望有机会可以补上。属于大家的青春不散场，TiDB Hackathon 2019，不见不散～～&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D1%26sn%3D72d9d52558e83eb97cd709c67b5a4149%26chksm%3Deb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D2%26sn%3D7eb3d41b2b5cf2a8a440b12121796e2d%26chksm%3Deb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D2%26sn%3D5f1ee6e838c3a86556fcd556662112c5%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D1%26sn%3D8a8861419dd22344a021667545005769%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D2%26sn%3D3a601b2ff9100a9797605a825e478c01%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/_o_4XRJ22NfAfBvUFQqueQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 2018 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/sg3RiE8Fp96aQZlo1MM0hg&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-27-53455060</guid>
<pubDate>Thu, 27 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Ecosystem Tools 原理解读系列（三）TiDB-DM 架构设计与实现原理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-26-53357816.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53357816&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d68d4233f88a929df432cf4e63c5e7f9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张学程&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;TiDB-DM（Data Migration）是用于将数据从 MySQL/MariaDB 迁移到 TiDB 的工具。&lt;/b&gt;该工具既支持以全量备份文件的方式将 MySQL/MariaDB 的数据导入到 TiDB，也支持通过解析执行 MySQL/MariaDB binlog 的方式将数据增量同步到 TiDB。特别地，对于有多个 MySQL/MariaDB 实例的分库分表需要合并后同步到同一个 TiDB 集群的场景，DM 提供了良好的支持。如果你需要从 MySQL/MariaDB 迁移到 TiDB，或者需要将 TiDB 作为 MySQL/MariaDB 的从库，DM 将是一个非常好的选择。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;架构设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DM 是集群模式的，其主要由 DM-master、DM-worker 与 DM-ctl 三个组件组成，能够以多对多的方式将多个上游 MySQL 实例的数据同步到多个下游 TiDB 集群，其架构图如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f997a346fbb15217c2aef4a941abb33_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;601&quot; data-rawheight=&quot;402&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7f997a346fbb15217c2aef4a941abb33&quot; data-watermark-src=&quot;v2-3d3f8c27cc6c1d13685c6faaa2047725&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;DM-master：管理整个 DM 集群，维护集群的拓扑信息，监控各个 DM-worker 实例的运行状态；进行数据同步任务的拆解与分发，监控数据同步任务的执行状态；在进行合库合表的增量数据同步时，协调各 DM-worker 上 DDL 的执行或跳过；提供数据同步任务管理的统一入口。&lt;/li&gt;&lt;li&gt;DM-worker：与上游 MySQL 实例一一对应，执行具体的全量、增量数据同步任务；将上游 MySQL 的 binlog 拉取到本地并持久化保存；根据定义的数据同步任务，将上游 MySQL 数据全量导出成 SQL 文件后导入到下游 TiDB，或解析本地持久化的 binlog 后增量同步到下游 TiDB；编排 DM-master 拆解后的数据同步子任务，监控子任务的运行状态。&lt;/li&gt;&lt;li&gt;DM-ctl：命令行交互工具，通过连接到 DM-master 后，执行 DM 集群的管理与数据同步任务的管理。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;实现原理&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据迁移流程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;单个 DM 集群可以同时运行多个数据同步任务；对于每一个同步任务，可以拆解为多个子任务同时由多个 DM-worker 节点承担，其中每个 DM-worker 节点负责同步来自对应的上游 MySQL 实例的数据。对于单个 DM-worker 节点上的单个数据同步子任务，其数据迁移流程如下，其中上部的数据流向为全量数据迁移、下部的数据流向为增量数据同步：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9682a9ee2fcdbec6a350fe9d64cdad6e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;601&quot; data-rawheight=&quot;260&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9682a9ee2fcdbec6a350fe9d64cdad6e&quot; data-watermark-src=&quot;v2-6958ab643e8ebd83b8dac03d5b748a15&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在每个 DM-worker 节点内部，对于特定的数据同步子任务，主要由 dumper、loader、relay 与 syncer（binlog replication）等数据同步处理单元执行具体的数据同步操作。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于全量数据迁移，DM 首先使用 dumper 单元从上游 MySQL 中将表结构与数据导出成 SQL 文件；然后使用 loader 单元读取这些 SQL 文件并同步到下游 TiDB。&lt;/li&gt;&lt;li&gt;对于增量数据同步，首先使用 relay 单元作为 slave 连接到上游 MySQL 并拉取 binlog 数据后作为 relay log 持久化存储在本地，然后使用 syncer 单元读取这些 relay log 并解析构造成 SQL 语句后同步到下游 TiDB。这个增量同步的过程与 MySQL 的主从复制类似，主要区别在于在 DM 中，本地持久化的 relay log 可以同时供多个不同子任务的 syncer 单元所共用，避免了多个任务需要重复从上游 MySQL 拉取 binlog 的问题。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据迁移并发模型&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为加快数据导入速度，在 DM 中不论是全量数据迁移，还是增量数据同步，都在其中部分阶段使用了并发处理。&lt;/p&gt;&lt;p&gt;对于全量数据迁移，在导出阶段，dumper 单元调用 mydumper 导出工具执行实际的数据导出操作，对应的并发模型可以直接参考 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper 的源码&lt;/a&gt;。在使用 loader 单元执行的导入阶段，对应的并发模型结构如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-500d7c88efa869029888578be09f73bd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;147&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-500d7c88efa869029888578be09f73bd&quot; data-watermark-src=&quot;v2-4289423da132976eb7cfa4a1547e54f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;使用 mydumper 执行导出时，可以通过 &lt;code class=&quot;inline&quot;&gt;--chunk-filesize&lt;/code&gt; 等参数将单个表拆分成多个 SQL 文件，这些 SQL 文件对应的都是上游 MySQL 某一个时刻的静态快照数据，且各 SQL 文件间的数据不存在关联。因此，在使用 loader 单元执行导入时，可以直接在一个 loader 单元内启动多个 worker 工作协程，由各 worker 协程并发、独立地每次读取一个待导入的 SQL 文件进行导入。即 loader 导入阶段，是以 SQL 文件级别粒度并发进行的。在 DM 的任务配置中，对于 loader 单元，其中的 &lt;code class=&quot;inline&quot;&gt;pool-size&lt;/code&gt; 参数即用于控制此处 worker 协程数量。&lt;/p&gt;&lt;p&gt;对于增量数据同步，在从上游拉取 binlog 并持久化到本地的阶段，由于上游 MySQL 上 binlog 的产生与发送是以 stream 形式进行的，因此这部分只能串行处理。在使用 syncer 单元执行的导入阶段，在一定的限制条件下，可以执行并发导入，对应的模型结构如下：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-94c16769b0f7075bc7e30ce81ed1cc66_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;166&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-94c16769b0f7075bc7e30ce81ed1cc66&quot; data-watermark-src=&quot;v2-a192eca1e8e3f9ae33da328e5fc781ff&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;当 syncer 读取与解析本地 relay log 时，与从上游拉取 binlog 类似，是以 stream 形式进行的，因此也只能串行处理。当 syncer 解析出各 binlog event 并构造成待同步的 job 后，则可以根据对应行数据的主键、索引等信息经过 hash 计算后分发到多个不同的待同步 job channel 中；在 channel 的另一端，与各个 channel 对应的 worker 协程并发地从 channel 中取出 job 后同步到下游的 TiDB。即 syncer 导入阶段，是以 binlog event 级别粒度并发进行的。在 DM 的任务配置中，对于 syncer 单元，其中的 &lt;code class=&quot;inline&quot;&gt;worker-count&lt;/code&gt; 参数即用于控制此处 worker 协程数量。&lt;/p&gt;&lt;p&gt;但 syncer 并发同步到下游 TiDB 时，存在一些限制，主要包括：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于 DDL，由于会变更下游的表结构，因此必须确保在旧表结构对应的 DML 都同步完成后，才能进行同步。在 DM 中，当解析 binlog event 得到 DDL 后，会向每一个 job channel 发送一个特殊的 flush job；当各 worker 协程遇到 flush job 时，会立刻向下游 TiDB 同步之前已经取出的所有 job；等各 job channel 中的 job 都同步到下游 TiDB 后，开始同步 DDL；等待 DDL 同步完成后，继续同步后续的 DML。即 DDL 不能与 DML 并发同步，且 DDL 之前与之后的 DML 也不能并发同步。sharding 场景下 DDL 的同步处理见后文。&lt;/li&gt;&lt;li&gt;对于 DML，多条 DML 可能会修改同一行的数据，甚至是主键。如果并发地同步这些 DML，则可能造成同步后数据的不一致。DM 中对于 DML 之间的冲突检测与处理，与 TiDB-Binlog 中的处理类似，具体原理可以阅读 《&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487391&amp;amp;idx=1&amp;amp;sn=3e173b9c634e028824a69f67a506dd11&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB EcoSystem Tools 原理解读（一）TiDB-Binlog 架构演进与实现原理&lt;/a&gt;&lt;/u&gt;》中关于 Drainer 内 SQL 之间冲突检测的讨论。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;合库合表数据同步&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在使用 MySQL 支撑大量数据时，经常会选择使用分库分表的方案。但当将数据同步到 TiDB 后，通常希望逻辑上进行合库合表。DM 为支持合库合表的数据同步，主要实现了以下的一些功能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;table router&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为说明 DM 中 table router（表名路由）功能，先看如下图所示的一个例子：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-43726bdeba8135376cf56c41f422d81e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;291&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-43726bdeba8135376cf56c41f422d81e&quot; data-watermark-src=&quot;v2-4259409243fc8ccb4daaa510365bf859&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，上游有 2 个 MySQL 实例，每个实例有 2 个逻辑库，每个库有 2 个表，总共 8 个表。当同步到下游 TiDB 后，希望所有的这 8 个表最终都合并同步到同一个表中。&lt;/p&gt;&lt;p&gt;但为了能将 8 个来自不同实例、不同库且有不同名的表同步到同一个表中，首先要处理的，就是要能根据某些定义好的规则 ，将来自不同表的数据都路由到下游的同一个表中。在 DM 中，这类规则叫做 router-rules。对于上面的示例，其规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-router-rule:
    schema-pattern: &quot;schema_*&quot;
    table-pattern: &quot;table_*&quot;
    target-schema: &quot;schema&quot;
    target-table: &quot;table&quot;&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;name-of-router-rule&lt;/code&gt;：规则名，用户指定。当有多个上游实例需要使用相同的规则时，可以只定义一条规则，多个不同的实例通过规则名进行引用。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt;：用于匹配上游库（schema）名的模式，支持在尾部使用通配符（*）。这里使用 &lt;code class=&quot;inline&quot;&gt;schema_*&lt;/code&gt; 即可匹配到示例中的两个库名。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt;：用于匹配上游表名的模式，与 &lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; 类似。这里使用 &lt;code class=&quot;inline&quot;&gt;table_*&lt;/code&gt; 即可匹配到示例中的两个表名。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-schema&lt;/code&gt;：目标库名。对于库名、表名匹配的数据，将被路由到这个库中。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-table&lt;/code&gt;：目标表名。对于库名、表名匹配的数据，将被路由到 &lt;code class=&quot;inline&quot;&gt;target-schema&lt;/code&gt; 库下的这个表中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 DM 内部实现上，首先根据 &lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; / &lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt; 构造对应的 trie 结构，并将规则存储在 trie 节点中；当有 SQL 需要同步到下游时，通过使用上游库名、表名查询 trie 即可得到对应的规则，并根据规则替换原 SQL 中的库名、表名；通过向下游 TiDB 执行替换后的 SQL 即完成了根据表名的路由同步。有关 &lt;code class=&quot;inline&quot;&gt;router-rules&lt;/code&gt; 规则的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/table-router&quot;&gt;table-router pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;column mapping&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 table router 功能，已经可以完成基本的合库合表数据同步了。但在数据库中，我们经常会使用自增类型的列作为主键。如果多个上游分表的主键各自独立地自增，将它们合并同步到下游后，就很可能会出现主键冲突，造成数据的不一致。我们可看一个如下的例子：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6b7af10cbcef959e18a3bbc920719989_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;309&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6b7af10cbcef959e18a3bbc920719989&quot; data-watermark-src=&quot;v2-1f6ff66889ddae1a696f4203ba5b9bfe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，上游 4 个需要合并同步到下游的表中，都存在 id 列值为 1 的记录。假设这个 id 列是表的主键。在同步到下游的过程中，由于相关更新操作是以 id 列作为条件来确定需要更新的记录，因此会造成后同步的数据覆盖前面已经同步过的数据，导致部分数据的丢失。&lt;/p&gt;&lt;p&gt;在 DM 中，我们通过 column mapping 功能在数据同步的过程中依据指定规则对相关列的数据进行转换改写来避免数据冲突与丢失。对于上面的示例，其中 MySQL 实例 1 的 column mapping 规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;mapping-rule-of-instance-1:
    schema-pattern: &quot;schema_*&quot;
    table-pattern: &quot;table_*&quot;
    expression: &quot;partition id&quot;
    source-column: &quot;id&quot;
    target-column: &quot;id&quot;
    arguments: [&quot;1&quot;, &quot;schema_&quot;, &quot;table_&quot;]  &lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;mapping-rule-of-instance-1&lt;/code&gt;：规则名，用户指定。由于不同的上游 MySQL 实例需要转换得到不同的值，因此通常每个 MySQL 实例使用一条专有的规则。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; / &lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt;：上游库名、表名匹配模式，与 &lt;code class=&quot;inline&quot;&gt;router-rules&lt;/code&gt; 中的对应配置项一致。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;expression&lt;/code&gt;：进行数据转换的表达式名。目前常用的表达式即为 &lt;code class=&quot;inline&quot;&gt;&quot;partition id&quot;&lt;/code&gt;，有关该表达式的具体说明见下文。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;source-column&lt;/code&gt;：转换表达式的输入数据对应的来源列名，&lt;code class=&quot;inline&quot;&gt;&quot;id&quot;&lt;/code&gt; 表示这个表达式将作用于表中名为 id 的列。暂时只支持对单个来源列进行数据转换。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-column&lt;/code&gt;：转换表达式的输出数据对应的目标列名，与 &lt;code class=&quot;inline&quot;&gt;source-column&lt;/code&gt; 类似。暂时只支持对单个目标列进行数据转换，且对应的目标列必须已经存在。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;arguments&lt;/code&gt;：转换表达式所依赖的参数。参数个数与含义依具体表达式而定。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;partition id&lt;/code&gt; 是目前主要受支持的转换表达式，其通过为 bigint 类型的值增加二进制前缀来解决来自不同表的数据合并同步后可能产生冲突的问题。&lt;code class=&quot;inline&quot;&gt;partition id&lt;/code&gt; 的 arguments 包括 3 个参数，分别为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MySQL 实例 ID：标识数据的来源 MySQL 实例，用户自由指定。如 &lt;code class=&quot;inline&quot;&gt;&quot;1&quot;&lt;/code&gt; 表示匹配该规则的数据来自于 MySQL 实例 1，且这个标识将被转换成数值后以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;li&gt;库名前缀：标识数据的来源逻辑库。如 &lt;code class=&quot;inline&quot;&gt;&quot;schema_&quot;&lt;/code&gt; 应用于 &lt;code class=&quot;inline&quot;&gt;schema_2&lt;/code&gt; 逻辑库时，表示去除前缀后剩下的部分（数字 &lt;code class=&quot;inline&quot;&gt;2&lt;/code&gt;）将以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;li&gt;表名前缀：标识数据的来源表。如 &lt;code class=&quot;inline&quot;&gt;&quot;table_&quot;&lt;/code&gt; 应用于 &lt;code class=&quot;inline&quot;&gt;table_3&lt;/code&gt; 表时，表示去除前缀后剩下的部分（数字 &lt;code class=&quot;inline&quot;&gt;3&lt;/code&gt;）将以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;各部分在经过转换后的数值中的二进制分布如下图所示（各部分默认所占用的 bits 位数如图所示）：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77e7308f132193e01dc614ccadb81ebb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;588&quot; data-rawheight=&quot;48&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;假如转换前的原始数据为 &lt;code class=&quot;inline&quot;&gt;123&lt;/code&gt;，且有如上的 arguments 参数设置，则转换后的值为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;1&amp;lt;&amp;lt;(64-1-4) | 2&amp;lt;&amp;lt;(64-1-4-7) | 3&amp;lt;&amp;lt;(64-1-4-7-8) | 123&lt;/code&gt;&lt;p&gt;另外，arguments 中的 3 个参数均可设置为空字符串（&lt;code class=&quot;inline&quot;&gt;&quot;&quot;&lt;/code&gt;），即表示该部分不被添加到转换后的值中，且不占用额外的 bits。比如将其设置为&lt;code class=&quot;inline&quot;&gt;[&quot;1&quot;, &quot;&quot;, &quot;table_&quot;]&lt;/code&gt;，则转换后的值为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;1 &amp;lt;&amp;lt; (64-1-4) | 3&amp;lt;&amp;lt; (64-1-4-8) | 123&lt;/code&gt;&lt;p&gt;有关 column mapping 功能的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/column-mapping&quot;&gt;column-mapping pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;sharding DDL&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 table router 和 column mapping 功能，DML 的合库合表数据同步已经可以正常进行了。但如果在增量数据同步的过程中，上游待合并的分表上执行了 DDL 操作，则可能出现问题。我们先来看一个简化后的在分表上执行 DDL 的例子。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-428a086f546ef3d7ab3b0223f179c2e6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-428a086f546ef3d7ab3b0223f179c2e6&quot; data-watermark-src=&quot;v2-a786537af890679590b690c35f516fd3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在上图的例子中，分表的合库合表简化成了上游只有两个 MySQL 实例，每个实例内只有一个表。假设在开始数据同步时，将两个分表的表结构 schema 的版本记为 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt;，将 DDL 执行完成后的表结构 schema 的版本记为 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;现在，假设数据同步过程中，从两个上游分表收到的 binlog 数据有如下的时序：&lt;/p&gt;&lt;p&gt;1. 开始同步时，从两个分表收到的都是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;2. 在 t1 时刻，收到实例 1 上分表的 DDL。&lt;/p&gt;&lt;p&gt;3. 从 t2 时刻开始，从实例 1 收到的是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML；但从实例 2 收到的仍是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻，收到实例 2 上分表的 DDL。&lt;/p&gt;&lt;p&gt;5. 从 t4 时刻开始，从实例 2 收到的也是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;假设在数据同步过程中，不对分表的 DDL 进行处理。当将实例 1 的 DDL 同步到下游后，下游的表结构会变更成为 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt;。但对于实例 2，在 t2 时刻到 t3 时刻这段时间内收到的仍然是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。当尝试把这些与 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 对应的 DML 同步到下游时，就会由于 DML 与表结构的不一致而发生错误，造成数据无法正确同步。&lt;/p&gt;&lt;p&gt;继续使用上面的例子，来看看我们在 DM 中是如何处理合库合表过程中的 DDL 同步的。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b986329d81a846f97d3f1612a8a61402_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;600&quot; data-rawheight=&quot;277&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b986329d81a846f97d3f1612a8a61402&quot; data-watermark-src=&quot;v2-4decdb25db89abc60ad666693e38a44f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，DM-worker-1 用于同步来自 MySQL 实例 1 的数据，DM-worker-2 用于同步来自 MySQL 实例 2 的数据，DM-master 用于协调多个 DM-worker 间的 DDL 同步。从 DM-worker-1 收到 DDL 开始，简化后的 DDL 同步流程为：&lt;/p&gt;&lt;p&gt;1. DM-worker-1 在 t1 时刻收到来自 MySQL 实例 1 的 DDL，自身暂停该 DDL 对应任务的 DDL 及 DML 数据同步，并将 DDL 相关信息发送给 DM-master。&lt;/p&gt;&lt;p&gt;2. DM-master 根据 DDL 信息判断需要协调该 DDL 的同步，为该 DDL 创建一个锁，并将 DDL 锁信息发回给 DM-worker-1，同时将 DM-worker-1 标记为这个锁的 owner。&lt;/p&gt;&lt;p&gt;3. DM-worker-2 继续进行 DML 的同步，直到在 t3 时刻收到来自 MySQL 实例 2 的 DDL，自身暂停该 DDL 对应任务的数据同步，并将 DDL 相关信息发送给 DM-master。&lt;/p&gt;&lt;p&gt;4. DM-master 根据 DDL 信息判断该 DDL 对应的锁信息已经存在，直接将对应锁信息发回给 DM-worker-2。&lt;/p&gt;&lt;p&gt;5. DM-master 根据启动任务时的配置信息、上游 MySQL 实例分表信息、部署拓扑信息等，判断得知已经收到了需要合表的所有上游分表的该 DDL，请求 DDL 锁的 owner（DM-worker-1）向下游同步执行该 DDL。&lt;/p&gt;&lt;p&gt;6. DM-worker-1 根据 step 2 时收到的 DDL 锁信息验证 DDL 执行请求；向下游执行 DDL，并将执行结果反馈给 DM-master；若执行 DDL 成功，则自身开始继续同步后续的（从 t2 时刻对应的 binlog 开始的）DML。&lt;/p&gt;&lt;p&gt;7. DM-master 收到来自 owner 执行 DDL 成功的响应，请求在等待该 DDL 锁的所有其他 DM-worker（DM-worker-2）忽略该 DDL ，直接继续同步后续的（从 t4 时刻对应的 binlog 开始的）DML。&lt;/p&gt;&lt;p&gt;根据上面 DM 处理多个 DM-worker 间的 DDL 同步的流程，归纳一下 DM 内处理多个 DM-worker 间 sharding DDL 同步的特点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;根据任务配置与 DM 集群部署拓扑信息，在 DM-master 内建立一个需要协调 DDL 同步的逻辑 sharding group，group 中的成员为处理该任务拆解后各子任务的 DM-worker。&lt;/li&gt;&lt;li&gt;各 DM-worker 在从 binlog event 中获取到 DDL 后，会将 DDL 信息发送给 DM-master。&lt;/li&gt;&lt;li&gt;DM-master 根据来自 DM-worker 的 DDL 信息及 sharding group 信息创建/更新 DDL 锁。&lt;/li&gt;&lt;li&gt;如果 sharding group 的所有成员都收到了某一条 DDL，则表明上游分表在该 DDL 执行前的 DML 都已经同步完成，可以执行 DDL，并继续后续的 DML 同步。&lt;/li&gt;&lt;li&gt;上游分表的 DDL 在经过 table router 转换后，对应需要在下游执行的 DDL 应该一致，因此仅需 DDL 锁的 owner 执行一次即可，其他 DM-worker 可直接忽略对应的 DDL。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从 DM 处理 DM-worker 间 sharding DDL 同步的特点，可以看出该功能存在以下一些限制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游的分表必须以相同的顺序执行（table router 转换后相同的）DDL，比如表 1 先增加列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt; 后再增加列 &lt;code class=&quot;inline&quot;&gt;b&lt;/code&gt;，而表 2 先增加列 &lt;code class=&quot;inline&quot;&gt;b&lt;/code&gt; 后再增加列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt;，这种不同顺序的 DDL 执行方式是不支持的。&lt;/li&gt;&lt;li&gt;一个逻辑 sharding group 内的所有 DM-worker 对应的上游分表，都应该执行对应的 DDL，比如其中有 DM-worker-2 对应的上游分表未执行 DDL，则其他已执行 DDL 的 DM-worker 都会暂停同步任务，等待 DM-worker-2 收到对应上游的 DDL。&lt;/li&gt;&lt;li&gt;由于已经收到的 DDL 的 DM-worker 会暂停任务以等待其他 DM-worker 收到对应的 DDL，因此数据同步延迟会增加。&lt;/li&gt;&lt;li&gt;增量同步开始时，需要合并的所有上游分表结构必须一致，才能确保来自不同分表的 DML 可以同步到一个确定表结构的下游，也才能确保后续各分表的 DDL 能够正确匹配与同步。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在上面的示例中，每个 DM-worker 对应的上游 MySQL 实例中只有一个需要进行合并的分表。但在实际场景下，一个 MySQL 实例可能有多个分库内的多个分表需要进行合并，比如前面介绍 table router 与 column mapping 功能时的例子。当一个 MySQL 实例中有多个分表需要合并时，sharding DDL 的协调同步过程增加了更多的复杂性。&lt;/p&gt;&lt;p&gt;假设同一个 MySQL 实例中有 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 两个分表需要进行合并，如下图：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d03c0bdab2df38bb7776ba74b13c0a82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;96&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;由于数据来自同一个 MySQL 实例，因此所有数据都是从同一个 binlog 流中获得。在这个例子中，时序如下：&lt;/p&gt;&lt;p&gt;1. 开始同步时，两个分表收到的数据都是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;2. 在 t1 时刻，收到了 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL。&lt;/p&gt;&lt;p&gt;3. 从 t2 时刻到 t3 时刻，收到的数据同时包含 table_1 schema V2 的 DML 及 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻，收到了 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 DDL。&lt;/p&gt;&lt;p&gt;5. 从 t4 时刻开始，两个分表收到的数据都是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;假设在数据同步过程中不对 DDL 进行特殊处理，当 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL 同步到下游、变更下游表结构后，&lt;code class=&quot;inline&quot;&gt;table_2 schema V1&lt;/code&gt;的 DML 将无法正常同步。因此，在单个 DM-worker 内部，我们也构造了与 DM-master 内类似的逻辑 sharding group，但 group 的成员是同一个上游 MySQL 实例的不同分表。&lt;/p&gt;&lt;p&gt;但 DM-worker 内协调处理 sharding group 的同步不能完全与 DM-master 处理时一致，主要原因包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当收到 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL 时，同步不能暂停，需要继续解析 binlog 才能获得后续 &lt;code class=&quot;inline&quot;&gt;table_2 &lt;/code&gt;的 DDL，即需要从 t2 时刻继续向前解析直到 t3 时刻。&lt;/li&gt;&lt;li&gt;在继续解析 t2 时刻到 t3 时刻的 binlog 的过程中，&lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML 不能向下游同步；但在 sharding DDL 同步并执行成功后，这些 DML 需要同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;在 DM 中，简化后的 DM-worker 内 sharding DDL 同步流程为：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 在 t1 时刻收到 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL，记录 DDL 信息及此时的 binlog 位置点信息。&lt;/p&gt;&lt;p&gt;2. 继续向前解析 t2 时刻到 t3 时刻的 binlog。&lt;/p&gt;&lt;p&gt;3. 对于属于 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; DML，忽略；对于属于 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; DML，正常同步到下游。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻收到 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 DDL，记录 DDL 信息及此时的 binlog 位置点信息。&lt;/p&gt;&lt;p&gt;5. 根据同步任务配置信息、上游库表信息等，判断该 MySQL 实例上所有分表的 DDL 都已经收到；将 DDL 同步到下游执行、变更下游表结构。&lt;/p&gt;&lt;p&gt;6. 设置新的 binlog 流的解析起始位置点为 step 1 时保存的位置点。&lt;/p&gt;&lt;p&gt;7. 重新开始解析从 t2 时刻到 t3 时刻的 binlog。&lt;/p&gt;&lt;p&gt;8. 对于属于 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; DML，正常同步到下游；对于属于 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;shema V1&lt;/code&gt; DML，忽略。&lt;/p&gt;&lt;p&gt;9. 解析到达 step 4 时保存的 binlog 位置点，可得知在 step 3 时被忽略的所有 DML 都已经重新同步到下游。&lt;/p&gt;&lt;p&gt;10. 继续从 t4 时刻对应的 binlog 位置点正常同步。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从上面的分析可以知道，DM 在处理 sharding DDL 同步时，主要通过两级 sharding group 来进行协调控制，简化的流程为：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 各 DM-worker 独立地协调对应上游 MySQL 实例内多个分表组成的 sharding group 的 DDL 同步。&lt;/p&gt;&lt;p&gt;2. 当 DM-worker 内所有分表的 DDL 都收到时，向 DM-master 发送 DDL 相关信息。&lt;/p&gt;&lt;p&gt;3. DM-master 根据 DM-worker 发来的 DDL 信息，协调由各 DM-worker 组成的 sharing group 的 DDL 同步。&lt;/p&gt;&lt;p&gt;4. 当 DM-master 收到所有 DM-worker 的 DDL 信息时，请求 DDL lock 的 owner（某个 DM-worker） 执行 DDL。&lt;/p&gt;&lt;p&gt;5. owner 执行 DDL，并将结果反馈给 DM-master；自身开始重新同步在内部协调 DDL 同步过程中被忽略的 DML。&lt;/p&gt;&lt;p&gt;6. 当 DM-master 发现 owner 执行 DDL 成功后，请求其他所有 DM-worker 开始继续同步。&lt;/p&gt;&lt;p&gt;7. 其他所有 DM-worker 各自开始重新同步在内部协调 DDL 同步过程中被忽略的 DML。&lt;/p&gt;&lt;p&gt;8. 所有 DM-worker 在重新同步完成被忽略的 DML 后，继续正常同步。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据同步过滤&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在进行数据同步的过程中，有时可能并不需要将上游所有的数据都同步到下游，这时一般期望能在同步过程中根据某些规则，过滤掉部分不期望同步的数据。在 DM 中，支持 2 种不同级别的同步过滤方式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;库表黑白名单&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DM 在 dumper、loader、syncer 三个处理单元中都支持配置规则只同步/不同步部分库或表。&lt;/p&gt;&lt;p&gt;对于 dumper 单元，其实际调用 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper&lt;/a&gt; 来 dump 上游 MySQL 的数据。比如只期望导出 test 库中的 t1、t2 两个表的数据，则可以为 dumper 单元配置如下规则：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-dump-rule:
    extra-args: &quot;-B test -T t1,t2&quot;&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;name-of-dump-rule&lt;/code&gt;：规则名，用户指定。当有多个上游实例需要使用相同的规则时，可以只定义一条规则，多个不同的实例通过规则名进行引用。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;extra-args&lt;/code&gt;：dumper 单元额外参数。除 dumper 单元中明确定义的配置项外的其他所有 mydumper 配置项都通过此参数传入，格式与使用 mydumper 时一致。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有关 mydumper 对库表黑白名单的支持，可查看 mydumper 的参数及 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper 的源码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;对于 loader 和 syncer 单元，其对应的库表黑白名单规则为 &lt;code class=&quot;inline&quot;&gt;black-white-list&lt;/code&gt;。假设只期望同步 test 库中的 t1、t2 两个表的数据，则可配置如下规则：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-bwl-rule:
    do-tables:
    - db-name: &quot;test&quot;
      tbl-name: &quot;t1&quot;
    - db-name: &quot;test&quot;
      tbl-name: &quot;t2&quot;&lt;/code&gt;&lt;p&gt;示例中只使用了该规则的部分配置项，完整的配置项及各配置项的含义，可阅读该功能对应的用户文档。DM 中该规则与 MySQL 的主从同步过滤规则类似，因此也可参考 &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-rules-db-options.html&quot;&gt;Evaluation of Database-Level Replication and Binary Logging Options&lt;/a&gt; 与 &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-rules-table-options.html&quot;&gt;Evaluation of Table-Level Replication Options&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;对于 loader 单元，在解析 SQL 文件名获得库名表名后，会与配置的黑白名单规则进行匹配，如果匹配结果为不需要同步，则会忽略对应的整个 SQL 文件。对于 syncer 单元，在解析 binlog 获得库名表名后，会与配置的黑白名单规则进行匹配，如果匹配结果为不需要同步，则会忽略对应的（部分） binlog event 数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;binlog event 过滤&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在进行增量数据同步时，有时会期望过滤掉某些特定类型的 binlog event，两个典型的场景包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游执行 &lt;code class=&quot;inline&quot;&gt;TRUNCATE TABLE&lt;/code&gt; 时不希望清空下游表中的数据。&lt;/li&gt;&lt;li&gt;上游分表上执行 &lt;code class=&quot;inline&quot;&gt;DROP TABLE&lt;/code&gt; 时不希望 &lt;code class=&quot;inline&quot;&gt;DROP&lt;/code&gt; 下游合并后的表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 DM 中支持根据 binlog event 的类型进行过滤，对于需要过滤 &lt;code class=&quot;inline&quot;&gt;TRUNCATE TABLE&lt;/code&gt; 与 &lt;code class=&quot;inline&quot;&gt;DROP TABLE&lt;/code&gt; 的场景，可配置规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-filter-rule:
    schema-pattern: &quot;test_*&quot;
    table-pattern: &quot;t_*&quot;
    events: [&quot;truncate table&quot;, &quot;drop table&quot;]
    action: Ignore&lt;/code&gt;&lt;p&gt;规则的匹配模式与 table router、column mapping 类似，具体的配置项可阅读该功能对应的用户文档。&lt;/p&gt;&lt;p&gt;在实现上，当解析 binlog event 获得库名、表名及 binlog event 类型后，与配置的规则进行匹配，并在匹配后依据 action 配置项来决定是否需要进行过滤。有关 binlog event 过滤功能的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/binlog-filter&quot;&gt;binlog-filter pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/#TiDB-Ecosystem-Tools&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-26-53357816</guid>
<pubDate>Wed, 26 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十二）Hash Aggregation</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-26-52969666.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52969666&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2e665a8d2aeb4f9ccf89d305b5638fe8_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：徐怀宇&lt;/p&gt;&lt;h2&gt;&lt;b&gt;聚合算法执行原理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 SQL 中，聚合操作对一组值执行计算，并返回单个值。TiDB 实现了 2 种聚合算法：Hash Aggregation 和 Stream Aggregation。&lt;/p&gt;&lt;p&gt;我们首先以 &lt;code&gt;AVG&lt;/code&gt; 函数为例（案例参考 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/1471147/how-do-aggregates-group-by-work-on-sql-server/1471167%231471167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Stack Overflow&lt;/a&gt;），简述这两种算法的执行原理。&lt;/p&gt;&lt;p&gt;假设表 &lt;code&gt;t&lt;/code&gt; 如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8db01296a4397381f572bfd5a329c488_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1232&quot; data-rawheight=&quot;504&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1232&quot; data-original=&quot;https://pic1.zhimg.com/v2-8db01296a4397381f572bfd5a329c488_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8db01296a4397381f572bfd5a329c488_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1232&quot; data-rawheight=&quot;504&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1232&quot; data-original=&quot;https://pic1.zhimg.com/v2-8db01296a4397381f572bfd5a329c488_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-8db01296a4397381f572bfd5a329c488_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;SQL:&lt;code&gt;select avg(b) from t group by a&lt;/code&gt;, 要求将表&lt;code&gt;t&lt;/code&gt;的数据按照&lt;code&gt;a&lt;/code&gt;的值分组，对每一组的&lt;code&gt;b&lt;/code&gt;值计算平均值。不管 Hash 还是 Stream 聚合，在&lt;code&gt;AVG&lt;/code&gt;函数的计算过程中，我们都需要维护 2 个中间结果变量&lt;code&gt;sum&lt;/code&gt;和&lt;code&gt;count&lt;/code&gt;。Hash 和 Stream 聚合算法的执行原理如下。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Hash Aggregate 的执行原理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Hash Aggregate 的计算过程中，我们需要维护一个 Hash 表，Hash 表的键为聚合计算的 &lt;code&gt;Group-By&lt;/code&gt; 列，值为聚合函数的中间结果 &lt;code&gt;sum&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt;。在本例中，键为 &lt;code&gt;列 a&lt;/code&gt; 的值，值为 &lt;code&gt;sum(b)&lt;/code&gt; 和 &lt;code&gt;count(b)&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;计算过程中，只需要根据每行输入数据计算出键，在 Hash 表中找到对应值进行更新即可。对本例的执行过程模拟如下。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a811115327639d12cea5c777a01121dc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;508&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-a811115327639d12cea5c777a01121dc_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a811115327639d12cea5c777a01121dc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;508&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-a811115327639d12cea5c777a01121dc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-a811115327639d12cea5c777a01121dc_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;输入数据输入完后，扫描 Hash 表并计算，便可以得到最终结果：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-99647b66dfb1d3e931cb250e08325d72_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;230&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1238&quot; data-original=&quot;https://pic3.zhimg.com/v2-99647b66dfb1d3e931cb250e08325d72_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-99647b66dfb1d3e931cb250e08325d72_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;230&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1238&quot; data-original=&quot;https://pic3.zhimg.com/v2-99647b66dfb1d3e931cb250e08325d72_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-99647b66dfb1d3e931cb250e08325d72_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Stream Aggregation 的执行原理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Stream Aggregate 的计算需要保证输入数据&lt;b&gt;按照&lt;/b&gt; &lt;b&gt;&lt;code&gt;Group-By&lt;/code&gt;&lt;/b&gt; &lt;b&gt;列有序&lt;/b&gt;。在计算过程中，每当读到一个新的 Group 的值或所有数据输入完成时，便对前一个 Group 的聚合最终结果进行计算。&lt;/p&gt;&lt;p&gt;对于本例，我们首先对输入数据按照 &lt;code&gt;a&lt;/code&gt; 列进行排序。排序后，本例执行过程模拟如下。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ce83f2fe1e9f559028fd5c0552a46552_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1592&quot; data-rawheight=&quot;658&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1592&quot; data-original=&quot;https://pic3.zhimg.com/v2-ce83f2fe1e9f559028fd5c0552a46552_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ce83f2fe1e9f559028fd5c0552a46552_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1592&quot; data-rawheight=&quot;658&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1592&quot; data-original=&quot;https://pic3.zhimg.com/v2-ce83f2fe1e9f559028fd5c0552a46552_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-ce83f2fe1e9f559028fd5c0552a46552_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;因为 Stream Aggregate 的输入数据需要保证同一个 Group 的数据连续输入，所以 Stream Aggregate 处理完一个 Group 的数据后可以立刻向上返回结果，不用像 Hash Aggregate 一样需要处理完所有数据后才能正确的对外返回结果。当上层算子只需要计算部分结果时，比如 Limit，当获取到需要的行数后，可以提前中断 Stream Aggregate 后续的无用计算。&lt;/p&gt;&lt;p&gt;当 &lt;code&gt;Group-By&lt;/code&gt; 列上存在索引时，由索引读入数据可以保证输入数据按照 &lt;code&gt;Group-By&lt;/code&gt; 列有序，此时同一个 Group 的数据连续输入 Stream Aggregate 算子，可以避免额外的排序操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 聚合函数的计算模式&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于分布式计算的需要，TiDB 对于聚合函数的计算阶段进行划分，相应定义了 5 种计算模式：CompleteMode，FinalMode，Partial1Mode，Partial2Mode，DedupMode。不同的计算模式下，所处理的输入值和输出值会有所差异，如下表所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e0330d54b46bfda4ffe89a871977b0b6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;826&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-e0330d54b46bfda4ffe89a871977b0b6_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e0330d54b46bfda4ffe89a871977b0b6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;826&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-e0330d54b46bfda4ffe89a871977b0b6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e0330d54b46bfda4ffe89a871977b0b6_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;以上文提到的 &lt;code&gt;select avg(b) from t group by a&lt;/code&gt; 为例，通过对计算阶段进行划分，可以有多种不同的计算模式的组合，如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CompleteMode&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此时 &lt;code&gt;AVG&lt;/code&gt; 函数的整个计算过程只有一个阶段，如图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-18d7246583684dfce6c22c1769a8f4e7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;470&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;470&quot; data-original=&quot;https://pic4.zhimg.com/v2-18d7246583684dfce6c22c1769a8f4e7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-18d7246583684dfce6c22c1769a8f4e7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;470&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;470&quot; data-original=&quot;https://pic4.zhimg.com/v2-18d7246583684dfce6c22c1769a8f4e7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-18d7246583684dfce6c22c1769a8f4e7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Partial1Mode –&amp;gt; FinalMode&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此时我们将 &lt;code&gt;AVG&lt;/code&gt; 函数的计算过程拆成两个阶段进行，如图所示：&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-638ea8080df76ae1255c6e340518b5ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;556&quot; data-rawheight=&quot;637&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;556&quot; data-original=&quot;https://pic3.zhimg.com/v2-638ea8080df76ae1255c6e340518b5ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-638ea8080df76ae1255c6e340518b5ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;556&quot; data-rawheight=&quot;637&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;556&quot; data-original=&quot;https://pic3.zhimg.com/v2-638ea8080df76ae1255c6e340518b5ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-638ea8080df76ae1255c6e340518b5ce_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;除了上面的两个例子外，还可能有如下的几种计算方式：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;聚合被下推到 TiKV 上进行计算（Partial1Mode），并返回经过预聚合的中间结果。为了充分利用 TiDB server 所在机器的 CPU 和内存资源，加快 TiDB 层的聚合计算，TiDB 层的聚合函数计算可以这样进行：Partial2Mode –&amp;gt; FinalMode。&lt;/li&gt;&lt;li&gt;当聚合函数需要对参数进行去重，也就是包含 &lt;code&gt;DISTINCT&lt;/code&gt; 属性，且聚合算子因为一些原因不能下推到 TiKV 时，TiDB 层的聚合函数计算可以这样进行：DedupMode –&amp;gt; Partial1Mode –&amp;gt; FinalMode。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;聚合函数分为几个阶段执行， 每个阶段对应的模式是什么，是否要下推到 TiKV，使用 Hash 还是 Stream 聚合算子等都由优化器根据数据分布、估算的计算代价等来决定。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 并行 Hash Aggregation 的实现&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;如何构建 Hash Aggregation 执行器&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/planner/core/logical_plan_builder.go%23L95&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;构建逻辑执行计划&lt;/a&gt; 时，会调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/blob/v2.1.0/expression/aggregation/descriptor.go%23L49&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;NewAggFuncDesc&lt;/a&gt; 将聚合函数的元信息封装为一个 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/descriptor.go%23L35-L46&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AggFuncDesc&lt;/a&gt;。 其中 &lt;code&gt;AggFuncDesc.RetTp&lt;/code&gt; 由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/descriptor.go%23L146-L163&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AggFuncDesc.typeInfer&lt;/a&gt; 根据聚合函数类型及参数类型推导而来；&lt;code&gt;AggFuncDesc.Mode&lt;/code&gt; 统一初始化为 CompleteMode。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go%23L487&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;构建物理执行计划&lt;/a&gt;时，&lt;code&gt;PhysicalHashAgg&lt;/code&gt; 和 &lt;code&gt;PhysicalStreamAgg&lt;/code&gt; 的 &lt;code&gt;attach2Task&lt;/code&gt; 方法会根据当前 &lt;code&gt;task&lt;/code&gt; 的类型尝试进行下推聚合计算，如果 &lt;code&gt;task&lt;/code&gt; 类型满足下推的基本要求，比如 &lt;code&gt;copTask&lt;/code&gt;，接着会调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go%23L380&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;newPartialAggregate&lt;/a&gt; 尝试将聚合算子拆成 TiKV 上执行的 Partial 算子和 TiDB 上执行的 &lt;code&gt;Final&lt;/code&gt; 算子，其中 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/agg_to_pb.go%23L25&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AggFuncToPBExpr&lt;/a&gt; 函数用来判断某个聚合函数是否可以下推。若聚合函数可以下推，则会在 TiKV 中进行预聚合并返回中间结果，因此需要将 TiDB 层执行的 &lt;code&gt;Final&lt;/code&gt; 聚合算子的 &lt;code&gt;AggFuncDesc.Mode&lt;/code&gt; &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go%23L427&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;修改为 FinalMode&lt;/a&gt;，并将其 &lt;code&gt;AggFuncDesc.Args&lt;/code&gt; &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go%23L403-L426&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;修改为 TiKV 预聚合后返回的中间结果&lt;/a&gt;，TiKV 层的 Partial 聚合算子的 &lt;code&gt;AggFuncDesc&lt;/code&gt; 也需要作出对应的修改，这里不再详述。若聚合函数不可以下推，则 &lt;code&gt;AggFuncDesc.Mode&lt;/code&gt; 保持不变。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/blob/v2.1.0/executor/builder.go%23L999&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;构建 HashAgg 执行器&lt;/a&gt;时，首先检查当前 &lt;code&gt;HashAgg&lt;/code&gt; 算子&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/blob/v2.1.0/executor/builder.go%23L1037-L1047&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;是否可以并行执行&lt;/a&gt;。目前当且仅当两种情况下 &lt;code&gt;HashAgg&lt;/code&gt; 不可以并行执行：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;存在某个聚合函数参数为 DISTINCT 时。TiDB 暂未实现对 DedupMode 的支持，因此对于含有 &lt;code&gt;DISTINCT&lt;/code&gt; 的情况目前仅能单线程执行。&lt;/li&gt;&lt;li&gt;系统变量 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs-cn/blob/master/sql/tidb-specific.md%23tidb_hashagg_partial_concurrency&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb_hashagg_partial_concurrency&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs-cn/blob/master/sql/tidb-specific.md%23tidb_hashagg_final_concurrency&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb_hashagg_final_concurrency&lt;/a&gt;&lt;/code&gt; 被同时设置为 1 时。这两个系统变量分别用来控制 Hash Aggregation 并行计算时候，TiDB 层聚合计算 partial 和 final 阶段 worker 的并发数。当它们都被设置为 1 时，选择单线程执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;若&lt;code&gt;HashAgg&lt;/code&gt;算子可以并行执行，使用&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/builder.go%23L1062&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AggFuncDesc.Split&lt;/a&gt;根据&lt;code&gt;AggFuncDesc.Mode&lt;/code&gt;将 TiDB 层的聚合算子的计算拆分为 partial 和 final 两个阶段，并分别生成对应的&lt;code&gt;AggFuncDesc&lt;/code&gt;，设为&lt;code&gt;partialAggDesc&lt;/code&gt;和&lt;code&gt;finalAggDesc&lt;/code&gt;。若&lt;code&gt;AggFuncDesc.Mode == CompleteMode&lt;/code&gt;，则将 TiDB 层的计算阶段拆分为&lt;code&gt;Partial1Mode --&amp;gt; FinalMode&lt;/code&gt;；若&lt;code&gt;AggFuncDesc.Mode == FinalMode&lt;/code&gt;，则将 TiDB 层的计算阶段拆分为&lt;code&gt;Partial2Mode --&amp;gt; FinalMode&lt;/code&gt;。进一步的，我们可以根据&lt;code&gt;partialAggDesc&lt;/code&gt;和&lt;code&gt;finalAggDesc&lt;/code&gt;分别&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/builder.go%23L1063-L1066&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;构造出对应的执行函数&lt;/a&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行 Hash Aggregation 执行过程详述&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 的并行 Hash Aggregation 算子执行过程中的主要线程有：Main Thead，Data Fetcher，Partial Worker，和 Final Worker：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Main Thread 一个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;启动 Input Reader，Partial Workers 及 Final Workers&lt;/li&gt;&lt;li&gt;等待 Final Worker 的执行结果并返回&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Data Fetcher 一个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;按 batch 读取子节点数据并分发给 Partial Worker&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Partial Worker 多个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取 Data Fetcher 发送来的数据，并做预聚合&lt;/li&gt;&lt;li&gt;将预聚合结果根据 Group 值 shuffle 给对应的 Final Worker&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Final Worker 多个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取 PartialWorker 发送来的数据，计算最终结果，发送给 Main Thread&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;Hash Aggregation 的执行阶段可分为如下图所示的 5 步：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65f2027935adc477a4bed0fa5f275d34_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-65f2027935adc477a4bed0fa5f275d34_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65f2027935adc477a4bed0fa5f275d34_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-65f2027935adc477a4bed0fa5f275d34_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-65f2027935adc477a4bed0fa5f275d34_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;启动 Data Fetcher，Partial Workers 及 Final Workers。&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;prepare4Parallel&lt;/a&gt; 函数完成。该函数会启动一个 Data Fetcher，&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L589-L591&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;多个 Partial Worker&lt;/a&gt; 以及 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L596-L598&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;多个 Final Worker&lt;/a&gt;。Partial Worker 和 Final Worker 的数量可以分别通过 &lt;code&gt;tidb_hashgg_partial_concurrency&lt;/code&gt; 和 &lt;code&gt;tidb_hashagg_final_concurrency&lt;/code&gt; 系统变量进行控制，这两个系统变量的默认值都为 4。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.DataFetcher 读取子节点的数据并分发给 Partial Workers。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L535&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;fetchChildData&lt;/a&gt; 函数完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.Partial Workers 预聚合计算，及根据 Group Key shuffle 给对应的 Final Workers。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L326&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;HashAggPartialWorker.run&lt;/a&gt; 函数完成。该函数调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L351&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;updatePartialResult&lt;/a&gt; 函数对 DataFetcher 发来数据执行 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L358-L363&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;预聚合计算&lt;/a&gt;，并将预聚合结果存储到 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L63&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;partialResultMap&lt;/a&gt; 中。其中 &lt;code&gt;partialResultMap&lt;/code&gt; 的 key 为根据 &lt;code&gt;Group-By&lt;/code&gt; 的值 encode 的结果，value 为 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggfuncs/aggfuncs.go%23L89&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PartialResult&lt;/a&gt; 类型的数组，数组中的每个元素表示该下标处的聚合函数在对应 Group 中的预聚合结果。&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L370&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;shuffleIntermData&lt;/a&gt; 函数完成根据 Group 值 shuffle 给对应的 Final Worker。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.Final Worker 计算最终结果，发送给 Main Thread。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L505&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;HashAggFinalWorker.run&lt;/a&gt; 函数完成。该函数调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L434&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;consumeIntermData&lt;/a&gt; 函数 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/aggregate.go%23L443&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;接收 PartialWorkers 发送来的预聚合结果&lt;/a&gt;，进而 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L459&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;合并&lt;/a&gt; 得到最终结果。&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go%23L459&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;getFinalResult&lt;/a&gt; 函数完成发送最终结果给 Main Thread。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Main Thread 接收最终结果并返回。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 并行 Hash Aggregation 的性能提升&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;此处以 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-bench/blob/master/tpch/queries/17.sql&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TPC-H query-17&lt;/a&gt; 为例，测试并行 Hash Aggregation 相较于单线程计算时的性能提升。引入并行 Hash Aggregation 前，它的计算瓶颈在 &lt;code&gt;HashAgg_35&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;该查询执行计划如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f47abed183d2edbddd6fda0d8ce6098c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;941&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-f47abed183d2edbddd6fda0d8ce6098c_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f47abed183d2edbddd6fda0d8ce6098c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;941&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-f47abed183d2edbddd6fda0d8ce6098c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-f47abed183d2edbddd6fda0d8ce6098c_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;在 TiDB 中，使用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs-cn/blob/master/sql/understanding-the-query-execution-plan.md%23explain-analyze-%25E8%25BE%2593%25E5%2587%25BA%25E6%25A0%25BC%25E5%25BC%258F&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;EXPLAIN ANALYZE&lt;/a&gt; 可以获取 SQL 的执行统计信息。因篇幅原因此处仅贴出 TPC-H query-17 部分算子的 EXPLAIN ANALYZE 结果。&lt;/p&gt;&lt;p&gt;&lt;code&gt;HashAgg&lt;/code&gt; 单线程计算时：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;查询总执行时间 23 分 24 秒，其中 &lt;code&gt;HashAgg&lt;/code&gt; 执行时间约 17 分 9 秒。&lt;/p&gt;&lt;p&gt;&lt;code&gt;HashAgg&lt;/code&gt; 并行计算时（此时 TiDB 层 Partial 和 Final 阶段的 worker 数量都设置为 16）：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;总查询时间 8 分 37 秒，其中 &lt;code&gt;HashAgg&lt;/code&gt; 执行时间约 1 分 4 秒。&lt;/p&gt;&lt;p&gt;并行计算时，Hash Aggregation 的计算速度提升约 16 倍。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-26-52969666</guid>
<pubDate>Wed, 26 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 助力东南亚领先电商 Shopee 业务升级</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-25-53257583.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53257583&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-03b7f66215d7a0d664732db2fb9d5c0b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;刘春辉，Shopee DBA&lt;br&gt;洪超，Shopee DBA&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;一、业务场景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Shopee（https://shopee.com/）是东南亚和台湾地区领先的电子商务平台，覆盖新加坡、马来西亚、菲律宾、印度尼西亚、泰国、越南和台湾等七个市场。Shopee 母公司 Sea（https://seagroup.com/）为首家在纽约证券交易所上市的东南亚互联网企业。2015 年底上线以来，Shopee 业务规模迅速扩张，逐步成长为区域内发展最为迅猛的电商平台之一：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;截止 2018 年第三季度 Shopee APP 总下载量达到 1.95 亿次，平台卖家数量超过 700 万。&lt;/li&gt;&lt;li&gt;2018 年第一季度和第二季度 GMV 分别为 19 亿美金和 22 亿美金，2018 上半年的 GMV 已达到 2017 全年水平。2018 年第三季度 GMV 达到了创纪录的 27 亿美元, 较 2017 年同期年增长率为 153%。&lt;/li&gt;&lt;li&gt;2018 年双 11 促销日，Shopee 单日订单超过 1100 万，是 2017 年双 11 的 4.5 倍；刚刚过去的双 12 促销日再创新高，实现单日 1200 万订单。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-322edab2ce7fb2f69f744aa4d20709c5_r.jpg&quot; data-caption=&quot;图 1  Shopee 电商平台展示图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;841&quot; data-rawheight=&quot;583&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-322edab2ce7fb2f69f744aa4d20709c5&quot; data-watermark-src=&quot;v2-738d13f716f51b01868c64e5424c4fbe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;我们从 2018 年初开始调研 TiDB，6 月份上线了第一个 TiDB 集群。到目前为止我们已经有两个集群、60 多个节点在线运行，主要用于以下 Shopee 业务领域：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;风控系统：风控日志数据库是我们最早上线的一个 TiDB 集群，稍后详细展开。&lt;/li&gt;&lt;li&gt;审计日志系统：审计日志数据库存储每一个电商订单的支付和物流等状态变化日志。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文将重点展开风控日志数据库选型和上线的过程，后面也会约略提及上线后系统扩容和性能监控状况。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、选型：MySQL 分库分表 vs TiDB&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-38c37f53b5fba1b2f40dd4db3ecfbb7c_r.jpg&quot; data-caption=&quot;图 2  风控日志收集和处理示意图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;689&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-38c37f53b5fba1b2f40dd4db3ecfbb7c&quot; data-watermark-src=&quot;v2-046de8c9fd315c566c34c8d3d3f52dab&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;风控系统基于大量历史订单以及用户行为日志，以实时和离线两种方式识别平台上的异常行为和欺诈交易。它的重要数据源之一是各种用户行为日志数据。最初我们将其存储于 MySQL 数据库，并按照 USER_ID 把数据均分为 100 个表。随着 Shopee 用户活跃度见长，数据体积开始疯长，到 2017 年底磁盘空间显得十分捉襟见肘了。作为应急措施，我们启用了 InnoDB 表透明压缩将数据体积减半；同时，我们把 MySQL 服务器磁盘空间从 2.5TB 升级到了 6TB。这两个措施为后续迁移 MySQL 数据到 TiDB 多争取了几个月时间。&lt;/p&gt;&lt;p&gt;关于水平扩容的实现方案，当时内部有两种意见：MySQL 分库分表和直接采用 TiDB。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. MySQL 分库分表&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基本思路：按照 USER_ID 重新均分数据（Re-sharding），从现有的 100 个表增加到1000 个甚至 10000 个表，然后将其分散到若干组 MySQL 数据库。&lt;/li&gt;&lt;li&gt;优点：继续使用 MySQL 数据库 ，不论开发团队还是 DBA 团队都驾轻就熟。&lt;/li&gt;&lt;li&gt;缺点：业务代码复杂度高。Shopee 内部若干个系统都在使用该数据库，同时我们还在使用 Golang 和 Python 两种编程语言，每一个系统都要改动代码以支持新的分库分表规则。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 直接采用 TiDB&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基本思路：把数据从 MySQL 搬迁至 TiDB，把 100 个表合并为一个表。&lt;/li&gt;&lt;li&gt;优点：数据库结构和业务逻辑都得以大幅简化。TiDB 会自动实现数据分片，无须客户端手动分表；支持弹性水平扩容，数据量变大之后可以通过添加新的 TiKV 节点实现水平扩展。理想状况下，我们可以把 TiDB 当做一个「无限大的 MySQL」来用，这一点对我们极具诱惑力。&lt;/li&gt;&lt;li&gt;缺点：TiDB 作为新组件首次引入 Shopee 线上系统，我们要做好「踩坑」的准备。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，我们决定采用 TiDB 方案，在 Shopee 内部做「第一个吃螃蟹的人」。风控日志数据库以服务离线系统为主，只有少许在线查询；这个特点使得它适合作为第一个迁移到 TiDB 的数据库。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、上线：先双写，后切换&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们的上线步骤大致如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;应用程序开启双写：日志数据会同时写入 MySQL 和 TiDB。&lt;/li&gt;&lt;li&gt;搬迁旧数据：把旧数据从 MySQL 搬到 TiDB，并完成校验确保新旧数据一致。&lt;/li&gt;&lt;li&gt;迁移只读流量：应用程序把只读流量从 MySQL 逐步迁移至 TiDB（如图 3 所示）。&lt;/li&gt;&lt;li&gt;停止双写：迁移过程至此结束。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b94de1689b47ff8554e67c1139c2bcba_r.jpg&quot; data-caption=&quot;图 3  迁移过程图：保持双写，逐步从读 MySQL 改为读 TiDB&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;609&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b94de1689b47ff8554e67c1139c2bcba&quot; data-watermark-src=&quot;v2-61c865b27080aae5a95c74cda41c7b45&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;双写方式使得我们可以把整个切换过程拖长至几个月时间。这期间开发团队和 DBA 团队有机会逐步熟悉新的 TiDB 集群，并充分对比新旧数据库的表现。理论上，在双写停掉之前，若新的 TiDB 集群遭遇短时间内无法修复的问题，则应用程序有可能快速回退到 MySQL。&lt;/p&gt;&lt;p&gt;除此之外，采用双写方式也让我们有了重构数据库设计的机会。这一次我们就借机按照用户所属地区把风控日志数据分别存入了七个不同的逻辑数据库：rc_sg，rc_my，rc_ph，…，rc_tw。Shopee 用户分布于七个不同地区。迁移到 TiDB 之前，所有日志数据共存于同一个逻辑数据库。按照地区分别存储使得我们能够更为方便地为每个地区的日志定制不同的数据结构。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、硬件配置和水平扩容&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上线之初我们一共从 MySQL 迁移了大约 4TB 数据到 TiDB 上。当时 TiDB 由 14 个节点构成，包括 3 个 PD 节点，3 个 SQL 节点和 8 个 TiKV 节点。服务器硬件配置如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiKV 节点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CPU: 2 * Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 40 cores&lt;/li&gt;&lt;li&gt;内存: 192GB&lt;/li&gt;&lt;li&gt;磁盘: 4 * 960GB Read Intensive SAS SSD Raid 5&lt;/li&gt;&lt;li&gt;网卡: 2 * 10gbps NIC Bonding&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;PD 节点和 SQL 节点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CPU: 2 * Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 40 cores&lt;/li&gt;&lt;li&gt;内存: 64GB&lt;/li&gt;&lt;li&gt;磁盘: 2 * 960GB Read Intensive SAS SSD Raid 1&lt;/li&gt;&lt;li&gt;网卡: 2 * 10gbps NIC Bonding&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;截至目前，系统已经平稳运行了六个多月，数据量增长至 35TB（如图 4 所示），经历了两次扩容后现在集群共包含 42 个节点。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bee608fafd6029e42b6132886a0d8a62_r.jpg&quot; data-caption=&quot;图 4  风控日志 TiDB 数据库存储容量和使用状况&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bee608fafd6029e42b6132886a0d8a62&quot; data-watermark-src=&quot;v2-ff047ea9ed148260290d1a14929b8a46&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;性能&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a5f96a061bb0aeb697528dc3c156ef70_r.jpg&quot; data-caption=&quot;图 5  风控日志 TiDB 数据库 QPS Total 曲线&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;406&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a5f96a061bb0aeb697528dc3c156ef70&quot; data-watermark-src=&quot;v2-bd9a725c717194b816d7a0815a6b79be&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;风控日志数据库的日常 QPS（如图 5 所示）一般低于每秒 20K，在最近的双 12 促销日我们看到峰值一度攀升到了每秒 100K 以上。&lt;/p&gt;&lt;p&gt;&lt;b&gt;尽管数据量较之 6 个月前涨了 8 倍，目前整个集群的查询响应质量仍然良好，大部分时间 pct99 响应时间（如图 6 所示）都小于 60ms。对于以大型复杂 SQL 查询为主的风控系统而言，这个级别的响应时间已经足够好了。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c53d50d8d4eba74de7bcbea4bc1636cc_r.jpg&quot; data-caption=&quot;图 6  风控日志 TiDB 数据库两天 pct99 查询响应时间曲线&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;356&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c53d50d8d4eba74de7bcbea4bc1636cc&quot; data-watermark-src=&quot;v2-68e0d62112838b47540305d8db0dd691&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;五、问题和对策&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;TiDB 的字符串匹配区分大小写（Case Sensitive）。目前尚不支持 Case Insensitive 方式。应用程序做了适配以实现 Case Insensitive 方式的字符串匹配。&lt;/li&gt;&lt;li&gt;TiDB 对于 MySQL 用户授权 SQL 语法的兼容支持尚不完善。例如，目前不支持 SHOW CREATE USER 语法，有时候不得不读取系统表（mysql.user）来查看一个数据库账户的基本信息。&lt;/li&gt;&lt;li&gt;添加 TiKV 节点后需要较长时间才能完成数据再平衡。据我们观察，1TB 数据大约需要 24 个小时才能完成拷贝。因此促销前我们会提前几天扩容和观察数据平衡状况。&lt;/li&gt;&lt;li&gt;TiDB  v1.x 版本以 region 数目为准在各个 TiKV 节点之间平衡数据。不过每个 region 的大小其实不太一致。这个问题导致不同 TiKV 节点的磁盘空间使用率存在明显差异。据说新的 TiDB v2.x 对此已经做了优化，我们未来会尝试在线验证一下。&lt;/li&gt;&lt;li&gt;TiDB v1.x 版本需要定期手动执行 Analyze Table 以确保元信息准确。PingCAP 的同学告诉我们说：当 (Modify_count / Row_count) 大于 0.3 就要手动 Analyze Table 了。v2.x 版本已经支持自动更新元数据了。我们后续会考虑升级到新版本。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;mysql&amp;gt; show stats_meta where db_name = &#39;aaa_db&#39;  \G
*************************** 1. row ***************************
     Db_name: aaa_db
  Table_name: xxx_tab
 Update_time: 2018-12-16 23:49:02
Modify_count: 166545248
   Row_count: 8568560708
1 row in set (0.00 sec)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;六、未来规划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;过去一年亲密接触之下，我们对 TiDB 的未来充满信心，相信 TiDB 会成为 Shopee 数据库未来实现弹性水平扩容和分布式事务的关键组件。当前我们正在努力让更多 Shopee 业务使用 TiDB。&lt;/p&gt;&lt;p&gt;我们规划把 Shopee 数据从 MySQL 迁移到 TiDB 上的路线是「先 Non-transactional Data（非交易型数据），后 Transactional Data（交易型数据）」。目前线上运行的集群都属于 Non-transactional Data，他们的特点是数据量超大（TB 级别），写入过程中基本不牵涉数据库事务。接下来我们会探索如何把一些 Transactional Data 迁移到 TiDB 上。&lt;/p&gt;&lt;p&gt;MySQL Replica 是另一个工作重点。MySQL Replica 指的是把 TiDB 作为 MySQL 的从库，实现从 MySQL 到 TiDB 实时复制数据。我们最近把订单数据从 MySQL 实时复制到 TiDB。后续来自 BI 系统以及部分对数据实时性要求不那么高的只读查询就可以尝试改为从 TiDB 读取数据了。这一类查询的特点是全表扫描或者扫描整个索引的现象较多，跑在 TiDB 可能比 MySQL 更快。当然，BI 系统也可以借助 TiSpark 绕过 SQL 层直接读取 TiKV 以提升性能。&lt;/p&gt;&lt;p&gt;目前我们基于物理机运行 TiDB 集群，DBA 日常要耗费不少精力去照顾这些服务器的硬件、网络和 OS。我们有计划把 TiDB 搬到 Shopee 内部的容器平台上，并构建一套工具实现自助式资源申请和配置管理，以期把 DBA 从日常运维的琐碎中解放出来。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;七、致谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;感谢 PingCAP 的同学一年来对我们的帮助和支持。每一次我们在微信群里提问，都能快速获得回应。官方架构师同学还不辞辛劳定期和我们跟进，详细了解项目进度和难点，总是能给出非常棒的建议。&lt;/p&gt;&lt;p&gt;PingCAP 的文档非常棒，结构层次完整清晰，细节翔实，英文文档也非常扎实。一路跟着读下来，受益良多。&lt;/p&gt;&lt;p&gt;TiDB 选择了 Golang 和 RocksDB，并坚信 SSD 会在数据库领域取代传统机械硬盘。这些也是 Shopee 技术团队的共识。过去几年间我们陆续把这些技术引入了公司的技术栈，在一线做开发和运维的同学相信都能真切体会到它们为 Shopee 带来的改变。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多 TiDB 用户实践：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-25-53257583</guid>
<pubDate>Tue, 25 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-21-52972108.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52972108&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91a6142a4423a2d939a72a9f4837d32c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是&lt;b&gt;矛盾螺旋队&lt;/b&gt;的成员刘玮，他们的项目&lt;b&gt;TiEye&lt;/b&gt;在 TiDB Hackathon 2018 中获得了三等奖。TiEye 是 Region 信息变迁历史可视化工具，通过 PD记录 Region 的Split、Merge、ConfChange、LeaderChange 等信息，可以方便的回溯 Region 某个时间的具体状态，为开发人员提供了方便的可视化展示界面及查询功能。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;TiKV 的 Region&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Region 是 TiKV 的一个数据调度单元，TiKV 将数据按照键值范围划分为很多个 Region，分在集群的多台机器上，通过调度 Region 来实现负载均衡以及数据存储的扩展，同时一个 Region 也是一个 Raft Group，一个 Region 分布在多个 TiKV 实例上（通常是 3 个或者 5 个），通过 Raft 算法保证多副本的强一致性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;动机&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这个项目的灵感是之前在查一些问题的时候想到的，因为我们很多时候需要去知道 Region 在某个时间的状态，这就需要通过日志从杂乱的信息中提取出来有用的信息来复原当时的场景，但实际并不是特别方便高效，尤其是在看多个 Region 之间的关系的时候。因此通过将 Region 信息变化历史可视化，希望能为开发者们在定位问题的时候提供一个方便直观的工具，同时还能通过它来分析 PD 的调度策略，以及调度带来的写放大问题等等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实际方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一开始我们考虑的是通过一个独立的服务去解析 PD 的日志来获取 Region 信息的变化历史，后来讨论后认为这样做不仅依赖于 PD 中的日志格式，造成系统耦合，同时 PD 的 leader 变迁导致日志内容不连续，以及日志中的信息并不是特别充分等问题也增加了开发难度。因此我们最后决定直接修改 PD 的源代码，在每次 PD 变更 Region 的时候，记录下这些信息并持久化。这样既能保证在 PD 切换 leader 后的变化信息的连续性，又提供了更加丰富的历史信息。同时，PD 添加相关的 API，以供前端进行查询。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 回顾&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们的团队由三个人组成，分别是我（刘玮）、周振靖和张博康，都毕业于北京邮电大学。我们在这次 Hackathon 之前就认识，因为大家都在北京，因此交流还是蛮方便的，在开赛大约一周前就确定了这个题目。其实我最初的想法是做一些有关于性能优化的事情，但是在跟队友们交流后还是决定做 Region 历史可视化，其更具有实用性，也更适合在 Hackathon 上来做。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;10:00 比赛正式开始。我们之前已经讨论好了项目的大体架构，因此没有再做过多的讨论就各自开始码代码了。博康负责后端框架以及 PD 相应的修改，我负责后端查询 API，振靖负责前端可视化。&lt;/li&gt;&lt;li&gt;12:15 午餐。休息片刻，继续码代码。&lt;/li&gt;&lt;li&gt;14:00 后端框架大体完成，已经可以在 PD 中收集 Region 相应的状态变化；前端部分已经画出简单的 Region 分裂、合并等示意图。&lt;/li&gt;&lt;li&gt;17:30 完成了最简单的查询逻辑，进行了第一次联调，发现大家对于 Region 状态的展示方式理解不一样，于是再次讨论统一了意见。&lt;/li&gt;&lt;li&gt;18:00 晚餐时间。&lt;/li&gt;&lt;li&gt;19:00 ~ 次日 2:30: 我们基本完成了后端开发，而前端这时还剩比较多的工作量。同时晚上在前端展示，后端查询 API，数据持久化方面都发现了几个 bug，大家一直忙到很晚才一一解决。&lt;/li&gt;&lt;li&gt;次日 9:00 返回赛场，抽签确定 Demo 时间，最终为第四个出场。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7c8a46afd26bf52c3480899fc07e9c44_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7c8a46afd26bf52c3480899fc07e9c44&quot; data-watermark-src=&quot;v2-48b604cf5bc5bd3891d258d8cd0e99f0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;次日 12:00 前端可视化基本完善，为界面做最后的调整。&lt;/li&gt;&lt;li&gt;次日 12:00 ~ 12:30 午餐时间&lt;/li&gt;&lt;li&gt;次日 13:00 ~ 14:00 准备 PPT 和展示录屏&lt;/li&gt;&lt;li&gt;次日 14:30 ~ 18:30 Demo Time（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiEye 架构&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们采取了前后端分离的架构。&lt;/p&gt;&lt;p&gt;前端是 Vue.js 框架，使用 Typescript 语言开发。由于看上去现有的图表库啥的并不能很好地满足我们的需求，所以前端同学决定手撸 SVG。&lt;/p&gt;&lt;p&gt;后端则是 PD 提供的 API。数据存储目前暂时存储在 etcd，将来会考虑其它方案来应对数据规模太大的情况。我们将 Region 的变化分成了以下四种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;LeaderChange：Raft Group 选举（或者是主动移交）了新的 leader&lt;/li&gt;&lt;li&gt;ConfChange：Raft Group 成员变更&lt;/li&gt;&lt;li&gt;Split：当某个 Region 数据超过一定阙值时（或被手动干预时）会分裂成键值范围相邻的两个 Region&lt;/li&gt;&lt;li&gt;Merge：两个键值范围连续的 Region 合并成一个&lt;/li&gt;&lt;li&gt;Bootstrap：一个新的集群中第一个 Region 产生&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在前端的表示方式如图所示：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a09e108c7302afe3c25b0e4ec20ed9e0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;912&quot; data-rawheight=&quot;431&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a09e108c7302afe3c25b0e4ec20ed9e0&quot; data-watermark-src=&quot;v2-05f0cdcb662a552e165ed910254fd6f1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;给 PD 添加的 API 则有如下几种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/list&lt;/code&gt;，GET 方法，返回全部历史。&lt;/li&gt;&lt;li&gt;返回结果:&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;[
   {
       &quot;timestamp&quot;:1544286220000000,
       &quot;leader_store_id&quot;:0,
       &quot;event_type&quot;:&quot;Bootstrap&quot;,
       &quot;Region&quot;:{
           &quot;id&quot;:2,
           &quot;start_key&quot;:&quot;&quot;,
           &quot;end_key&quot;:&quot;&quot;,
           &quot;Region_epoch&quot;:{
               &quot;conf_ver&quot;:1,
               &quot;version&quot;:1
           },

           &quot;peers&quot;:[
               {
                   &quot;id&quot;:3,
                   &quot;store_id&quot;:1
               }
           ]
       },
       &quot;parents&quot;:[],
       &quot;children&quot;:26
   },
   ...
]
&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/Region/{RegionId}&lt;/code&gt;，GET 方法，查询某个 Region 的变化历史，返回结果同上。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/key/{key}&lt;/code&gt;，GET 方法，查询某个 key 所属 Region 的变化历史，返回结果同上。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上几个 API 均可附加起止时间参数（时间戳），如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;/pd/api/v1/history/list?start=0&amp;amp;end=1544286229000000&lt;/code&gt;&lt;p&gt;顺便一提，前端部分原先打算作为 PD 的一部分来提供，与 PD 一起构建（于是前端的代码也放进了 PD 的一个单独的文件夹里）。但是后来觉得对于不涉及这些前端代码的开发者来说这样做不太好，所以我们之后会抽时间将这些前端代码放进一个单独的仓库里。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;测试过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;测试的时候我们部署了1个 TiDB，6个 TiKV，3个 PD，通过 sysbench 导入少量数据，最后通过开启 random-merge-scheduler 来进行随机合并 Region。下图是我们的测试过程中的结果展示：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-52b313df099b00a39576a6fc7714cc05_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;449&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-52b313df099b00a39576a6fc7714cc05&quot; data-watermark-src=&quot;v2-eaef3f65ba836a74cdfa28aeae033769&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;此时通过我们的工具还意外发现了一个 bug。&lt;/p&gt;&lt;p&gt;可以在上图看到，在第一个红框处 Region 2 合并进 Region 28，然后第二红框那里已经被 merge 进 Region 28 的 Region 2 莫名其妙地又连接了后面的 Region 40，显然这里是有问题的。经过通过日志确认，这是由于 PD 收到了一个含有过期 Region 2 信息的心跳导致的，追根溯源发现是 TiKV 中的 pd-client 的一个 Bug 导致了在与 PD 重连后会发送一个过期的心跳信息（Bug 地址在 &lt;a href=&quot;https://github.com/tikv/tikv/issues/3868&quot;&gt;https://github.com/tikv/tikv/issues/3868&lt;/a&gt;）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实际运行结果&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;横轴表示时间，纵轴表示 Region 的存储键值的顺序（仅表示顺序，不代表实际的数据量），矩形上的数字表示 Region id，为了便于理解，所有的 Region 的最终状态都会在最后的时间点上展示出来（即使在这个时间点没有发生 Region 的改变）。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2e3dc2e9d527601c390b67262d612a28_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;775&quot; data-rawheight=&quot;504&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2e3dc2e9d527601c390b67262d612a28&quot; data-watermark-src=&quot;v2-5a386471b93335a3eff5a5d593c74de4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;点击右上角可以更改查下的时间范围。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c47ebd4cd54100f5c1a89a0f898d7d1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;747&quot; data-rawheight=&quot;493&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c47ebd4cd54100f5c1a89a0f898d7d1b&quot; data-watermark-src=&quot;v2-c0d25e05977246c27133a4b13df90eef&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;右上角可以设置按照 key 的范围对齐，效果如下图：&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ebf0ac8520a5bf184bebb85558805eb9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;616&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ebf0ac8520a5bf184bebb85558805eb9&quot; data-watermark-src=&quot;v2-64a3bb508b3b6d094bb3952bf4831f40&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;点击任何一个节点，会展示当时 Region 的详细信息。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fa062306dd0f7ac8196cb021e520aa89_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;877&quot; data-rawheight=&quot;589&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fa062306dd0f7ac8196cb021e520aa89&quot; data-watermark-src=&quot;v2-5874a61e30526155791697ea710a4900&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;拖动下方的框可以对局部进行缩放（你也可以通过查询更小的时间范围达到同样的效果）。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bbf63e89becb32ab0bbbdd53164edec3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;849&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bbf63e89becb32ab0bbbdd53164edec3&quot; data-watermark-src=&quot;v2-1a043712c79c51d425063ce7b51f6785&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Hackathon Demo&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们团队的 Demo 展示是博康负责的。一开始他还担心如果演讲的时候忘词了怎么办，不过最后展示效果很不错，整个 Demo show 进行得非常顺利（P.S. 要是展示时间能多给几分钟就好了）。&lt;/p&gt;&lt;p&gt;在展示中，我们也看见了其他团队的作品也都非常棒。这其中让我最感兴趣的是有个团队做的是以 TiKV 作为数据存储的 etcd。这个选题一开始我也考虑过，因为我在工作中实际已经遇到了这个问题，不过最后和队友商量后还是选择了现在这个题目。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们“矛盾螺旋”团队最终获得了三等奖，这对我来说简直是意外之喜。在演示中，很多别的团队也都做得十分优秀，我们在观看其它团队的演示时几乎都觉得获奖无望了。最后却拿到了三等奖，实在是意料之外。这次我们之所以能够获奖，一方面是选题选得恰到好处，具有一定的实际作用，同时工作量又能保证在 Hackathon 期间完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后感谢我的两位队友，谢谢导师，谢谢评委老师，谢谢 PingCAP 的所有工作人员为这次 Hackathon 所做的努力。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487370&amp;amp;idx=1&amp;amp;sn=72d9d52558e83eb97cd709c67b5a4149&amp;amp;chksm=eb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487370&amp;amp;idx=2&amp;amp;sn=7eb3d41b2b5cf2a8a440b12121796e2d&amp;amp;chksm=eb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7&amp;amp;scene=21#wechat_redirect&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487451&amp;amp;idx=2&amp;amp;sn=5f1ee6e838c3a86556fcd556662112c5&amp;amp;scene=21#wechat_redirect&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487479&amp;amp;idx=1&amp;amp;sn=8a8861419dd22344a021667545005769&amp;amp;scene=21#wechat_redirect&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487479&amp;amp;idx=2&amp;amp;sn=3a601b2ff9100a9797605a825e478c01&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-21-52972108</guid>
<pubDate>Fri, 21 Dec 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
