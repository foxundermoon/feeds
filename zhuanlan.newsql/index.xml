<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Wed, 19 Dec 2018 18:02:23 +0800</lastBuildDate>
<item>
<title>TiDB Ecosystem Tools 原理解读之 TiDB-Lightning Toolset</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-18-52699499.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52699499&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8b0bba75055cd143356e76a82064b113_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Lightning Toolset 是一套快速全量导入 SQL dump 文件到 TiDB 集群的工具集，自 2.1.0 版本起随 TiDB 发布，速度可达到传统执行 SQL 导入方式的至少 3 倍、大约每小时 100 GB，适合在上线前用作迁移现有的大型数据库到全新的 TiDB 集群。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 从 2017 年开始提供全量导入工具 &lt;a href=&quot;https://pingcap.com/docs-cn/tools/loader/&quot;&gt;Loader&lt;/a&gt;，它以多线程操作、错误重试、断点续传以及修改一些 TiDB 专属配置来提升数据导入速度。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-23ac26045113e913212b950ab743d557_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;835&quot; data-rawheight=&quot;458&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-23ac26045113e913212b950ab743d557&quot; data-watermark-src=&quot;v2-9abe9f1df7771b4219e03c60f6924dad&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;然而，当我们全新初始化一个 TiDB 集群时，Loader 这种逐条 INSERT 指令在线上执行的方式从根本上是无法尽用性能的。原因在于 SQL 层的操作有太强的保证了。在整个导入过程中，TiDB 需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;保证 ACID 特性，需要执行完整的事务流程。&lt;/li&gt;&lt;li&gt;保证各个 TiKV 服务器数据量平衡及有足够的副本，在数据增长的时候需要不断的分裂、调度 Regions。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些动作确保 TiDB 整段导入的期间是稳定的，但在导入完毕前我们根本不会对外提供服务，这些保证就变成多此一举了。此外，多线程的线上导入也代表资料是乱序插入的，新的数据范围会与旧的重叠。TiKV 要求储存的数据是有序的，大量的乱序写入会令 TiKV 要不断地移动原有的数据（这称为 Compaction），这也会拖慢写入过程。&lt;/p&gt;&lt;p&gt;TiKV 是使用 RocksDB 以 KV 对的形式储存数据，这些数据会压缩成一个个 SST 格式文件。TiDB-Lightning Toolset使用新的思路，绕过SQL层，在线下将整个 SQL dump 转化为 KV 对、生成排好序的 SST 文件，然后直接用 &lt;a href=&quot;https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files&quot;&gt;Ingestion&lt;/a&gt; 推送到 RocksDB 里面。这样批量处理的方法略过 ACID 和线上排序等耗时步骤，让我们提升最终的速度。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;架构&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-55cd8f3a33a4f958cd86d036b5a39dde_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;854&quot; data-rawheight=&quot;625&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-55cd8f3a33a4f958cd86d036b5a39dde&quot; data-watermark-src=&quot;v2-53068d757bfdd30eea0a8589478bf276&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;TiDB-Lightning Toolset 包含两个组件：tidb-lightning 和 tikv-importer。Lightning 负责解析 SQL 成为 KV 对，而 Importer 负责将 KV 对排序与调度、上传到 TiKV 服务器。&lt;/p&gt;&lt;p&gt;为什么要把一个流程拆分成两个程式呢？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Importer 与 TiKV 密不可分、Lightning 与 TiDB 密不可分，Toolset 的两者皆引用后者为库，而这样 Lightning 与 Importer 之间就出现语言冲突：TiKV 是使用 Rust 而 TiDB 是使用 Go 的。把它们拆分为独立的程式更方便开发，而双方都需要的 KV 对可以透过 gRPC 传递。&lt;/li&gt;&lt;li&gt;分开 Importer 和 Lightning 也使横向扩展的方式更为灵活，例如可以运行多个 Lightning，传送给同一个 Importer。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;以下我们会详细分析每个组件的操作原理。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Lightning&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9844bda340c571347842d5bd1c945fbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;343&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9844bda340c571347842d5bd1c945fbf&quot; data-watermark-src=&quot;v2-a8e7de501a49d2f57fb1e6ce50ecd95c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Lightning 现时只支持经 mydumper 导出的 SQL 备份。mydumper 将每个表的内容分别储存到不同的文件，与 mysqldump 不同。这样不用解析整个数据库就能平行处理每个表。&lt;/p&gt;&lt;p&gt;首先，Lightning 会扫描 SQL 备份，区分出结构文件（包含 CREATE TABLE 语句）和数据文件（包含 INSERT 语句）。结构文件的内容会直接发送到 TiDB，用以建立数据库构型。&lt;/p&gt;&lt;p&gt;然后 Lightning 就会并发处理每一张表的数据。这里我们只集中看一张表的流程。每个数据文件的内容都是规律的 INSERT 语句，像是：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;INSERT INTO `tbl` VALUES (1, 2, 3), (4, 5, 6), (7, 8, 9);  
INSERT INTO `tbl` VALUES (10, 11, 12), (13, 14, 15), (16, 17, 18);
INSERT INTO `tbl` VALUES (19, 20, 21), (22, 23, 24), (25, 26, 27);&lt;/code&gt;&lt;p&gt;Lightning 会作初步分析，找出每行在文件的位置并分配一个行号，使得没有主键的表可以唯一的区分每一行。此外亦同时将文件分割为大小差不多的区块（默认 256 MiB）。这些区块也会并发处理，让数据量大的表也能快速导入。以下的例子把文件以 20 字节为限分割成 5 块：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f671b93f00f67d5a0b3451d8694d7638_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1068&quot; data-rawheight=&quot;180&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f671b93f00f67d5a0b3451d8694d7638&quot; data-watermark-src=&quot;v2-d8e0c081f70d453289ba24500fd83d70&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Lightning 会直接使用 TiDB 实例来把 SQL 转换为 KV 对，称为「KV 编码器」。与外部的 TiDB 集群不同，KV 编码器是寄存在 Lightning 进程内的，而且使用内存存储，所以每执行完一个 INSERT 之后，Lightning 可以直接读取内存获取转换后的 KV 对（这些 KV 对包含数据及索引）。&lt;/p&gt;&lt;p&gt;得到 KV 对之后便可以发送到 Importer。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Importer&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-777753c0442df7718aa158c3333fb52e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;397&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-777753c0442df7718aa158c3333fb52e&quot; data-watermark-src=&quot;v2-8b1fb35fcb9de6b81fec9a8d35b28afe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;因异步操作的缘故，Importer 得到的原始 KV 对注定是无序的。所以，Importer 要做的第一件事就是要排序。这需要给每个表划定准备排序的储存空间，我们称之为 engine file。&lt;/p&gt;&lt;p&gt;对大数据排序是个解决了很多遍的问题，我们在此使用现有的答案：直接使用 RocksDB。一个 engine file 就相等于本地的 RocksDB，并设置为优化大量写入操作。而「排序」就相等于将 KV 对全写入到 engine file 里，RocksDB 就会帮我们合并、排序，并得到 SST 格式的文件。&lt;/p&gt;&lt;p&gt;这个 SST 文件包含整个表的数据和索引，比起 TiKV 的储存单位 Regions 实在太大了。所以接下来就是要切分成合适的大小（默认为 96 MiB）。Importer 会根据要导入的数据范围预先把 Region 分裂好，然后让 PD 把这些分裂出来的 Region 分散调度到不同的 TiKV 实例上。&lt;/p&gt;&lt;p&gt;最后，Importer 将 SST 上传到对应 Region 的每个副本上。然后通过 Leader 发起 Ingest 命令，把这个 SST 文件导入到 Raft group 里，完成一个 Region 的导入过程。&lt;/p&gt;&lt;p&gt;我们传输大量数据时，需要自动检查数据完整，避免忽略掉错误。Lightning 会在整个表的 Region 全部导入后，对比传送到 Importer 之前这个表的 Checksum，以及在 TiKV 集群里面时的 Checksum。如果两者一样，我们就有信心说这个表的数据没有问题。&lt;/p&gt;&lt;p&gt;一个表的 Checksum 是透过计算 KV 对的哈希值（Hash）产生的。因为 KV 对分布在不同的 TiKV 实例上，这个 Checksum 函数应该具备结合性；另外，Lightning 传送 KV 对之前它们是无序的，所以 Checksum 也不应该考虑顺序，即服从交换律。也就是说 Checksum 不是简单的把整个 SST 文件计算 SHA-256 这样就了事。&lt;/p&gt;&lt;p&gt;我们的解决办法是这样的：先计算每个 KV 对的 CRC64，然后用 XOR 结合在一起，得出一个 64 位元的校验数字。为减低 Checksum 值冲突的概率，我们目时会计算 KV 对的数量和大小。若速度允许，将来会加入更先进的 Checksum 方式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结和下一步计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从这篇文章大家可以看到，Lightning 因为跳过了一些复杂、耗时的步骤使得整个导入进程更快，适合大数据量的初次导入，接下来我们还会做进一步的改进。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;提升导入速度&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现时 Lightning 会原封不动把整条 SQL 命令抛给 KV 编码器。所以即使我们省去执行分布式 SQL 的开销，但仍需要进行解析、规划及优化语句这些不必要或未被专门化的步骤。Lightning 可以调用更底层的 TiDB API，缩短 SQL 转 KV 的行程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行导入&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b3b31a81e0d75bc2783f64ad28f72904_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;279&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b3b31a81e0d75bc2783f64ad28f72904&quot; data-watermark-src=&quot;v2-da8c69e0cd6f70648ccf994cd7b3adbd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;另一方面，尽管我们可以不断的优化程序代码，单机的性能总是有限的。要突破这个界限就需要横向扩展：增加机器来同时导入。如前面所述，只要每套 TiDB-Lightning Toolset 操作不同的表，它们就能平行导进同一个集群。可是，现在的版本只支持读取本机文件系统上的 SQL dump，设置成多机版就显得比较麻烦了（要安装一个共享的网络盘，并且手动分配哪台机读取哪张表）。我们计划让 Lightning 能从网路获取 SQL dump（例如通过 S3 API），并提供一个工具自动分割数据库，降低设置成本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;在线导入&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB-Lightning 在导入时会把集群切换到一个专供 Lightning 写入的模式。目前来说 Lightning 主要用于在进入生产环境之前导入全量数据，所以在此期间暂停对外提供服务还可以接受。但我们希望支持更多的应用场景，例如回复备份、储存 OLAP 的大规模计算结果等等，这些都需要维持集群在线上。所以接下来的一大方向是考虑怎样降低 Lightning 对集群的影响。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;延伸阅读：&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;TiDB EcoSystem Tools 原理解读（一）：TiDB-Binlog 架构演进与实现原理&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-18-52699499</guid>
<pubDate>Tue, 18 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>天真贝叶斯学习机 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-18-52647715.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52647715&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3dafb6e9e934bab634b27352ad757cd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦～&lt;br&gt;本文作者是来自 DSG 团队的杨文同学，他们的项目《PD 热点调度贝叶斯模型》在本届 Hackathon 中获得了三等奖+最佳创意奖。&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3dafb6e9e934bab634b27352ad757cd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2124&quot; data-rawheight=&quot;928&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e3dafb6e9e934bab634b27352ad757cd&quot; data-watermark-src=&quot;v2-6e73710c0e3a84e11baf80348b95e8ba&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;“在 TiDB Hackathon 2018 学习到不少东西&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;希望明年再来”&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;简述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;“pd ctl 天真学习机”&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;具体做法：用 naive bayes 模型来根据系统指标和人的 pd ctl 调用，来得到一个模型去根据系统指标去自动提供 pd ctl 调用的命令。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 贝叶斯算法举例&lt;/b&gt;&lt;/p&gt;&lt;p&gt;贝叶斯模型可以用来干这种事：&lt;/p&gt;&lt;p&gt;比如一个妈妈根据天气预报来跟儿子在出们的时候叮嘱：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;天气预报[ 晴, 温度: 28, 风力: 中 ], 妈妈会说 [好好玩]
天气预报[ 雨, 温度: 15, 风力: 低 ], 妈妈会说 [带上伞]
天气预报[ 阴, 温度: 02, 风力: 大 ], 妈妈会说 [多穿点]...&lt;/code&gt;&lt;p&gt;把这些输入输入到贝叶斯模型里以后, 模型可以根据天气预报来输出：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;天气预报[ 晴, 温度: 00, 风力中], 模型会说 [ 多穿点:0.7, 好好玩0.2, 带上伞0.1]
天气预报[ 雨, 温度: 10, 风力大], 模型会说 [ 带上伞:0.8, 多穿点0.1, 好好玩0.1]&lt;/code&gt;&lt;p&gt;这样通过一个妈妈的叮嘱就可以训练出一个也会根据天气预报给出叮嘱的模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 初步想法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们可以把一个模型单独的部署在一个 pod 里, 暴露一个 service ，然后集群上每次有人去调用 pd_ctl 的时候就在后台用 rest call 到模型服务上记录一下操作（叮嘱）和当前的系统指标(好比天气预报). 这样慢慢用一段时间以后，积累的操作多了以后，就可以打开某个自动响应，或者打开自动建议应该执行的命令的功能。&lt;/p&gt;&lt;p&gt;这样模型可以在某一组系统指标出现之前类似学习过的状态之后，给出相应的建议，当这些建议都很正确的时候直接让 pd 直接采纳，完全智能的自动化运作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 实际 Hackathon 方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在跟导师交流探讨后发现，目前 PD 已经比较自动化了，很少需要人为介入进行操作，需要的时候也是比较复杂的场景，或者自动化运作比较慢的场景。&lt;/p&gt;&lt;p&gt;我们团队在跟多名导师的沟通交流下，将初步想法进行了一些调整：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从热点调度策略入手，用热点调度策略的数值去用 naive bayes 模型去训练他们，然后再根据这些数值再去模型中去获取建议值。&lt;/li&gt;&lt;li&gt;统计建议值和热点调度策略进行比较；（从开始的测试结果来看，大概有 70% 匹配，但是我们实测发现，使用我们模型的建议值去真正的调度，热点 region 还是非常均衡的）&lt;/li&gt;&lt;li&gt;三组对照试验：不进行调度，只打印调度数据；正常使用原来的热点调度策略；使用原来的热点调度策略的数值，但是使用模型训练的建议值进行实际调度；&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Hackathon 回顾&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先，介绍一下我们团队（DSG），分别来自：丹麦、北京（山西）、广州。&lt;/p&gt;&lt;p&gt;D 先生是在比赛前一天早上到达北京的，我是比赛前一天晚上从广州出发，于比赛当日早上 6:38 才抵达北京的。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;说实话，时差和疲惫对于参赛还是有一点影响的。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;废话不多说，我就来回顾一下我的整个参赛过程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;比赛前一日 20:05 从广州南站出发，次日 6:38 抵达北京西站。&lt;/li&gt;&lt;li&gt;7:58 抵达地铁西小口&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-15e3857dfc8600b934bf1aa6edc63659_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1917&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-15e3857dfc8600b934bf1aa6edc63659&quot; data-watermark-src=&quot;v2-591490d18ed33c3cb18194f73ac275d6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;8:06 经过转转&lt;/li&gt;&lt;li&gt;8:12 抵达比赛所在地：东升科技园 C-1 楼&lt;/li&gt;&lt;li&gt;8:16 签到，逛 PingCAP&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2f2e7223147d1bd1e5b8a7a867f219bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1154&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2f2e7223147d1bd1e5b8a7a867f219bb&quot; data-watermark-src=&quot;v2-094e38deefe87c8e62e49ad79ac4d784&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;8:40 跟 D 先生汇合，了解贝叶斯模型&lt;/li&gt;&lt;li&gt;9:20 DSG 团队成员全部集结完毕&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-341eeab6fc17ded51af3a4fe8fe9b848_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;733&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-341eeab6fc17ded51af3a4fe8fe9b848&quot; data-watermark-src=&quot;v2-17c83d958e278ed7c96013dba27dae0a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;10:00 比赛正式开始&lt;/li&gt;&lt;li&gt;10:00 Hacking Time: Trello 构建整个比赛分工、准备工作、需求分析&lt;/li&gt;&lt;li&gt;搭建 TiDB 集群（2套）【熟悉 TiDB 集群，实操 PD-CTL】&lt;/li&gt;&lt;li&gt;12:17 午餐&lt;/li&gt;&lt;li&gt;13:00 Hacking Time: 熟悉 PD Command，贝叶斯模型，导师指导，本地 TiDB 环境构建（坑），分析 PD 热点调度，剖析调度流程，模拟热点数据&lt;/li&gt;&lt;li&gt;18:20 外出用餐（芦月轩羊蝎子(西三旗店)）【沾 D 先生的光，蹭吃蹭喝】&lt;/li&gt;&lt;li&gt;20:40 回到东升科技园&lt;/li&gt;&lt;li&gt;20:50 ~ 次日 1:10 Hacking Time: 模拟热点数据，实测调度上报和获取模型返回结果，本地测通调度参数上报和得到模型返回值&lt;/li&gt;&lt;li&gt;次日 1:10 ~ 5:50 会议室休息（在此期间，我的队友 D 先生，调好了模型，并将此模型通过 Docker 构建部署到 PD 机器上）&lt;/li&gt;&lt;li&gt;次日 5:50 Hacking Time: 部署修改过的 PD 服务到线上服务器，并打通 rust-nb-server，实时上报和实时获取模型返回结果&lt;/li&gt;&lt;li&gt;次日 7:30 早餐&lt;/li&gt;&lt;li&gt;次日 8:00 正式调试&lt;/li&gt;&lt;li&gt;次日 9:00 抽签确定 Demo 时间&lt;/li&gt;&lt;li&gt;次日 9:00 ~ 12:00 Hacking Time: 调优&lt;/li&gt;&lt;li&gt;次日 12:00 ~ 12:30 午餐时间&lt;/li&gt;&lt;li&gt;次日 13:00 ~ 14:00 Hacking Time:  PPT，调优&lt;/li&gt;&lt;li&gt;次日 14:30 ~ 18:30 Demo Time（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3227038cf7a5c0fceea2cf8dbe7b0ec8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;810&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3227038cf7a5c0fceea2cf8dbe7b0ec8&quot; data-watermark-src=&quot;v2-43c58a1dff6f5bf247b2315a873136bf&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f8d50461f274aab577f0629570103c53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;503&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f8d50461f274aab577f0629570103c53&quot; data-watermark-src=&quot;v2-4295925384afa95673a6d0a9eb1a78c7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17c0306743ed229a59ad4fca13237e76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-17c0306743ed229a59ad4fca13237e76&quot; data-watermark-src=&quot;v2-59b2751df6e57d6ad7639ecda0d31b0d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;次日 18:30 ~ 19:00 颁奖（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5be44353701d44af3fcfba2562379147_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5be44353701d44af3fcfba2562379147&quot; data-watermark-src=&quot;v2-27fe422c71dd698de08e22f7b0e037af&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3819854910b872e8c8131fbd54bef68d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3819854910b872e8c8131fbd54bef68d&quot; data-watermark-src=&quot;v2-ed6851744c7ee2430773c8e51b143067&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Hackathon 实操&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 搭建 TiDB 集群&lt;/b&gt;&lt;/p&gt;&lt;p&gt;完全参考&lt;u&gt;文档&lt;/u&gt;。&lt;/p&gt;&lt;p&gt;测试 TiDB 集群，可能遇到的坑（MySQL 8 client On MacOSX）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mysql client connect : Unknown charset 255 (MySQL 8 Client 不支持字符集，需要指定默认字符集为 UTF8) &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;mysql -hx.x.x.x --default-character-set utf8&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 天真贝叶斯的服务接口&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/model/service1&lt;/code&gt; PUT 上报数据：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;updates&quot;: [
    [
      &quot;transfer leader from store 7 to store 2&quot;,
      [
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;hotRegionsCount1&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;minRegionsCount1&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;hotRegionsCount2&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;minRegionsCount2&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;srcRegion&quot;,
          &quot;value&quot;: &quot;7&quot;
        }
      ]
    ],
  ]}&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/model/service1&lt;/code&gt; POST 获取模型结果：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;输入参数：上报的参数&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;predictions&quot;: [
    {
      &quot;transfer leader from store 1 to store 2&quot;: 0.27432775221072137,
      &quot;transfer leader from store 1 to store 7&quot;: 0.6209064350448428,
      &quot;transfer leader from store 2 to store 1&quot;: 0.024587894827775753,
      &quot;transfer leader from store 2 to store 7&quot;: 0.01862719305134528,
      &quot;transfer leader from store 7 to store 1&quot;: 0.02591609468013258,
      &quot;transfer leader from store 7 to store 2&quot;: 0.03563463018518229
    }
  ]} &lt;/code&gt;&lt;p&gt;&lt;b&gt;3. PD 集群部署&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先将 pd-server 替换到集群所在 &lt;code class=&quot;inline&quot;&gt;ansible/resources/bin&lt;/code&gt; 目录下，那如何让集群上的 PD 更新生效呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;更新：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;$ ansible-playbook rolling_update.yml --tags=pd&lt;/code&gt;&lt;p&gt;在实操过程中， 如果你在更新到一半的时候就关门了，可能会导致整个 PD 挂掉（非集群环境），可能是因为逻辑不严谨所导致的问题&lt;/p&gt;&lt;p&gt;直接停止了 ansible，导致 PD 集群机器节点有停止的情况，这个时候你可以通过以下命令启动它。&lt;/p&gt;&lt;p&gt;&lt;b&gt;启动：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;$ ansible-playbook start.yml --tags=pd&lt;/code&gt;&lt;p&gt;&lt;b&gt;4. PD 调度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1 取消热点数据调度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家都以为可以通过配置来解决：(调度开关方法: 用 config set xxx 0 来关闭调度)&lt;/p&gt;&lt;p&gt;配置如下：（虽然找的地方错误了，但是错打错着，我们来到了 Demo Time：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;config set leader-schedule-limit 0
config set region-schedule-limit 0
scheduler add hot-region-scheduler
config show
config set leader-schedule-limit 4
config set region-schedule-limit 8&lt;/code&gt;&lt;p&gt;实测发现，根本不生效，必须要改源代码。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;func (h *balanceHotRegionsScheduler) dispatch(typ BalanceType, cluster schedule.Cluster) []*schedule.Operator {
    h.Lock()
    defer h.Unlock()
    switch typ {
    case hotReadRegionBalance:
        h.stats.readStatAsLeader = h.calcScore(cluster.RegionReadStats(), cluster, core.LeaderKind)
        // return h.balanceHotReadRegions(cluster) // 将这一行注释
    case hotWriteRegionBalance:
        h.stats.writeStatAsLeader = h.calcScore(cluster.RegionWriteStats(), cluster, core.LeaderKind)
        h.stats.writeStatAsPeer = h.calcScore(cluster.RegionWriteStats(), cluster, core.RegionKind)
        // return h.balanceHotWriteRegions(cluster) // 将这一行注释
    }
    return nil
}&lt;/code&gt;&lt;p&gt;但是，我们要的不是不调度，而只是不给调度结果：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;func (h *balanceHotRegionsScheduler) balanceHotReadRegions(cluster schedule.Cluster) []*schedule.Operator {
    // balance by leader
    srcRegion, newLeader := h.balanceByLeader(cluster, h.stats.readStatAsLeader)
    if srcRegion != nil {
        schedulerCounter.WithLabelValues(h.GetName(), &quot;move_leader&quot;).Inc()
        // step := schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()} // 修改为不返回值或者返回 _
        _ = schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()}
        // return []*schedule.Operator{schedule.NewOperator(&quot;transferHotReadLeader&quot;, srcRegion.GetID(), srcRegion.GetRegionEpoch(), schedule.OpHotRegion|schedule.OpLeader, step)} // 注释这一行，并 return nil
        return nil
    }

    // balance by peer
    srcRegion, srcPeer, destPeer := h.balanceByPeer(cluster, h.stats.readStatAsLeader)
    if srcRegion != nil {
        schedulerCounter.WithLabelValues(h.GetName(), &quot;move_peer&quot;).Inc()
        return []*schedule.Operator{schedule.CreateMovePeerOperator(&quot;moveHotReadRegion&quot;, cluster, srcRegion, schedule.OpHotRegion, srcPeer.GetStoreId(), destPeer.GetStoreId(), destPeer.GetId())}
    }
    schedulerCounter.WithLabelValues(h.GetName(), &quot;skip&quot;).Inc()
    return nil
}

......

func (h *balanceHotRegionsScheduler) balanceHotWriteRegions(cluster schedule.Cluster) []*schedule.Operator {
    for i := 0; i &amp;lt; balanceHotRetryLimit; i++ {
        switch h.r.Int() % 2 {
        case 0:
            // balance by peer
            srcRegion, srcPeer, destPeer := h.balanceByPeer(cluster, h.stats.writeStatAsPeer)
            if srcRegion != nil {
                schedulerCounter.WithLabelValues(h.GetName(), &quot;move_peer&quot;).Inc()
                fmt.Println(srcRegion, srcPeer, destPeer)
                // return []*schedule.Operator{schedule.CreateMovePeerOperator(&quot;moveHotWriteRegion&quot;, cluster, srcRegion, schedule.OpHotRegion, srcPeer.GetStoreId(), destPeer.GetStoreId(), destPeer.GetId())} // 注释这一行，并 return nil
                return nil
            }
        case 1:
            // balance by leader
            srcRegion, newLeader := h.balanceByLeader(cluster, h.stats.writeStatAsLeader)
            if srcRegion != nil {
                schedulerCounter.WithLabelValues(h.GetName(), &quot;move_leader&quot;).Inc()
                // step := schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()} // 修改为不返回值或者返回 _
                _ = schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()}

                // return []*schedule.Operator{schedule.NewOperator(&quot;transferHotWriteLeader&quot;, srcRegion.GetID(), srcRegion.GetRegionEpoch(), schedule.OpHotRegion|schedule.OpLeader, step)} // 注释这一行，并 return nil
                return nil
            }
        }
    }

    schedulerCounter.WithLabelValues(h.GetName(), &quot;skip&quot;).Inc()
    return nil
}&lt;/code&gt;&lt;p&gt;当修改了 PD 再重新编译得到 pd-server，将其放到 &lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;tidb-ansible/resources/bin/pd-server&lt;/code&gt; 并替换原来的文件，然后执行 &lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;ansible-playbook rolling_update.yml --tags=pd&lt;/code&gt;，即可重启 pd-server 服务。&lt;/p&gt;&lt;p&gt;在调优的过程中发现，当前 &lt;code class=&quot;inline&quot;&gt;hot-region-scheduler&lt;/code&gt; 的调度时对于目标机器的选择并不是最优的，代码如下：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/pd/blob/master/server/schedulers/hot_region.go#L374&quot;&gt;https://github.com/pingcap/pd/blob/master/server/schedulers/hot_region.go#L374&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;简述：循环遍历 candidateStoreIDs 的时候，如果满足条件有多台，那么最后一个总会覆盖前面已经存储到 destStoreID 里面的数据，最终我们拿到的 destStoreID 有可能不是最优的。（&lt;/b&gt;pd issue: &lt;a href=&quot;https://github.com/pingcap/pd/issues/1359&quot;&gt;https://github.com/pingcap/pd/issues/1359&lt;/a&gt;）&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// selectDestStore selects a target store to hold the region of the source region.
// We choose a target store based on the hot region number and flow bytes of this store.
func (h *balanceHotRegionsScheduler) selectDestStore(candidateStoreIDs []uint64, regionFlowBytes uint64, srcStoreID uint64, storesStat core.StoreHotRegionsStat) (destStoreID uint64) {
    sr := storesStat[srcStoreID]
    srcFlowBytes := sr.TotalFlowBytes
    srcHotRegionsCount := sr.RegionsStat.Len()

    var (
        minFlowBytes    uint64 = math.MaxUint64
        minRegionsCount        = int(math.MaxInt32)
    )
    for _, storeID := range candidateStoreIDs {
        if s, ok := storesStat[storeID]; ok {
            if srcHotRegionsCount-s.RegionsStat.Len() &amp;gt; 1 &amp;amp;&amp;amp; minRegionsCount &amp;gt; s.RegionsStat.Len() {
                destStoreID = storeID
                minFlowBytes = s.TotalFlowBytes
                minRegionsCount = s.RegionsStat.Len()
                continue // 这里
            }
            if minRegionsCount == s.RegionsStat.Len() &amp;amp;&amp;amp; minFlowBytes &amp;gt; s.TotalFlowBytes &amp;amp;&amp;amp;
                uint64(float64(srcFlowBytes)*hotRegionScheduleFactor) &amp;gt; s.TotalFlowBytes+2*regionFlowBytes {
                minFlowBytes = s.TotalFlowBytes
                destStoreID = storeID
            }
        } else {
            destStoreID = storeID
            return
        }
    }
    return
}&lt;/code&gt;&lt;p&gt;&lt;b&gt;4.2 PD 重要监控指标详解之 HotRegion：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Hot write Region&#39;s leader distribution：每个 TiKV 实例上是写入热点的 leader 的数量&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s peer distribution：每个 TiKV 实例上是写入热点的 peer 的数量&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s leader written bytes：每个 TiKV 实例上热点的 leader 的写入大小&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s peer written bytes：每个 TiKV 实例上热点的 peer 的写入大小&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s leader distribution：每个 TiKV 实例上是读取热点的 leader 的数量&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s peer distribution：每个 TiKV 实例上是读取热点的 peer 的数量&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s leader read bytes：每个 TiKV 实例上热点的 leader 的读取大小&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s peer read bytes：每个 TiKV 实例上热点的 peer 的读取大小&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本次我们只 hack 验证了 Write Region Leader 这部分，所以我们重点关注一下监控和问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Hot write Region&#39;s leader distribution&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;i&gt;监控数据有一定的延时（粗略估计1-2分钟）&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;5. 模拟热点数据&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从本地往服务器 load 数据：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;修改 &lt;code class=&quot;inline&quot;&gt;tidb-bench&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Makefile#load&lt;/code&gt; 模块对应的主机地址，然后执行 &lt;code class=&quot;inline&quot;&gt;make tbl&lt;/code&gt;, &lt;code class=&quot;inline&quot;&gt;make load&lt;/code&gt; 即可往服务器 load 数据了。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;注意，这里你也需要进行一些配置修改：&lt;/i&gt;&lt;code class=&quot;inline&quot;&gt;--default-character-set utf8&lt;/code&gt;&lt;br&gt;&lt;i&gt;犯的错：受限于本地-服务器间网络带宽，导入数据很慢。&lt;/i&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;线上服务器上：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;$ ./go-ycsb run mysql -p mysql.host=10.9.x.x -p mysql.port=4000 -p mysql.db=test1 -P workloads/workloada&lt;/code&gt;&lt;p&gt;注：&lt;code class=&quot;inline&quot;&gt;go-ycsb&lt;/code&gt; 支持 insert，也支持 update，你可以根据你的需要进行相对应的调整&lt;code class=&quot;inline&quot;&gt;workloada#recordcount&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;workloada#operationcount&lt;/code&gt; 参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.本地构建 rust-nb-server&lt;/b&gt;&lt;/p&gt;&lt;p&gt;rust 一天速成……&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;Demo Time 的时候听好几个团队都说失败了。我以前也尝试过，但是被编译的速度以及耗能给击败了。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;环境都可以把你 de 自信心击溃。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;rustup install nightly
cargo run
...&lt;/code&gt;&lt;p&gt;Mac 本地打包 Linux 失败：缺少 std 库，通过 Docker 临时解决。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7. 导师指导&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从比赛一开始，导师团就非常积极和主动，直接去每个项目组，给予直接指导和建议，我们遇到问题去找导师时，他们也非常的配合。&lt;/p&gt;&lt;p&gt;导师不仅帮我们解决问题（特别是热点数据构建，包括对于代码级别的指导），还跟我们一起探讨课题方向和实际可操作性，以及可以达到的目标。&lt;/p&gt;&lt;p&gt;非常感谢！！！&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;我们的准备和主动性真的不足，值得反思--也希望大家以后不要怕麻烦，有问题就大胆的去问。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;Hackathon Demo&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;整个 Demo show 进行的非常顺利，为每一个团队点赞！&lt;/p&gt;&lt;p&gt;很多团队的作品都让人尖叫，可想而知他们的作品是多么的酷炫和牛逼，印象中只有一个团队在 Demo 环境出现了演示时程序崩溃的问题（用Java Netty 基于 TiKV 做的 memcache（实现了大部分的协议））。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 颁奖&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;遗憾！！！&lt;/p&gt;&lt;p&gt;我们 DSG 团队荣获三等奖+最佳创意两项大奖，但是很遗憾我未能跟团队一起分享这一刻。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;因为我要赶着去火车站，所以在周日下午6点的时候，我跟队友和一些朋友道别后，我就去火车站了，后面几组的 Demo Show 也很非常遗憾未能参加。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;得奖感言：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;谢谢 DSG 团队，谢谢导师，谢谢评委老师，谢谢 PingCAP 给大家筹备了这么好的一次黑客马拉松比赛活动。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Hackathon 2018 总结&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;本次比赛的各个方面都做的完美，除了&lt;b&gt;网络&lt;/b&gt;。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;1. 环境（一定要提前准备）----这次被坑了不少时间和精力；&lt;/p&gt;&lt;p&gt;2. 配置文档中有一些注意事项，一定要认真阅读：&lt;i&gt;ext4&lt;/i&gt; 必须要每台机器都更新；&lt;/p&gt;&lt;p&gt;3. [10.9.97.254]: Ansible FAILED! =&amp;gt; playbook: bootstrap.yml; TASK: check_system_optional : Preflight check - Check TiDB server&#39;s RAM; message: {&quot;changed&quot;: false, &quot;msg&quot;: &quot;This machine does not have sufficient RAM to run TiDB, at least 16000 MB.&quot;}  - 内存不足的问题&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可以在执行的时候增加参数来避免&lt;/li&gt;&lt;li&gt;ansible-playbook bootstrap.yml --extra-vars &quot;dev_mode=True&quot;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4. 如果磁盘挂载有问题，可以重新清除数据后再重新启动；&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;ansible-playbook unsafe_cleanup_data.yml&lt;/code&gt;  （&lt;a href=&quot;https://github.com/pingcap/docs/blob/master/op-guide/ansible-operation.md&quot;&gt;https://github.com/pingcap/docs/blob/master/op-guide/ansible-operation.md&lt;/a&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;参考资料&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/pd&quot;&gt;https://github.com/pingcap/pd&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb-bench/tree/master/tpch&quot;&gt;tidb-bench tpch&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/go-ycsb&quot;&gt;https://github.com/pingcap/go-ycsb&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/op-guide/ansible-deployment.md&quot;&gt;Ansible 部署&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/op-guide/dashboard-pd-info.md&quot;&gt;PD 重要监控指标详解&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.github.io/docs/op-guide/ansible-deployment-rolling-update/&quot;&gt;使用 TiDB-Ansible 升级 TiDB&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://tool.oschina.net/codeformat/json&quot;&gt;在线代码格式化&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/liufuyang/rust-nb-server&quot;&gt;rust-nb-server&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-18-52647715</guid>
<pubDate>Tue, 18 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB DevCon 2019 报名开启 ：年度最高规格的 TiDB 技术大会</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-17-52603507.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52603507&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6a42d7ca1d0fcdcda28d72e7485505e1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;&lt;i&gt;年度最高规格的 TiDB 技术大会&lt;/i&gt;&lt;/b&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;海内外动态及成果的综合呈现&lt;/i&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;最新核心技术解读&lt;/i&gt;&lt;br&gt;&lt;i&gt;多个成果首次亮相&lt;/i&gt;&lt;br&gt;&lt;i&gt;2019 RoadMap 展望&lt;/i&gt;&lt;br&gt;&lt;i&gt;14 位海内外基础架构领域技术大咖&lt;/i&gt;&lt;br&gt;&lt;i&gt;8 个跨行业多场景的用户实战经验&lt;/i&gt;&lt;br&gt;&lt;i&gt;1 小时 Demo Show&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;只为向你描述可以预见的&lt;/i&gt;&lt;br&gt;&lt;i&gt;数据库的未来&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;1970 年关系模型的提出，在数据库领域如同打开了一扇创世之门，近半个世纪以来相关产品迅速迭代，新旧共生，我们试图洞悉其中规律，寻求一个问题的答案：&lt;b&gt;什么是未来的数据库？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;然而谈到“未来”难免总是宏观而模糊，我们尝试&lt;b&gt;用自己的方式&lt;/b&gt;来表达对未来的定义：年度规格最高的 TiDB 技术大会&lt;b&gt;#TiDB DevCon 2019&lt;/b&gt; 正式开启。&lt;/p&gt;&lt;p&gt;我们将从技术理念开始，尽所能地向前&lt;b&gt;眺望&lt;/b&gt;，分享我们所看到的，和即将付诸行动的一切探索。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间：&lt;/b&gt;2019 年 1 月 19 日&lt;/li&gt;&lt;li&gt;&lt;b&gt;地点：&lt;/b&gt;北京市 朝阳区 樱花园东街甲 2 号 北京服装学院内 中关村时尚产业创新园 二层&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;一、讲师介绍&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2a26fa50a77cf471c140d147c90ae197_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2132&quot; data-rawheight=&quot;1222&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2a26fa50a77cf471c140d147c90ae197&quot; data-watermark-src=&quot;v2-a46f637fcb69f4c004b5823f8c894e04&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-398e8319675f83c5a139ad62a1a04736_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2192&quot; data-rawheight=&quot;1228&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-398e8319675f83c5a139ad62a1a04736&quot; data-watermark-src=&quot;v2-f189a9c9b3c7ed2bc84b78203c5d6eb4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;二、会议议程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;8:40 - 9:30&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;     入场签到&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;9:30 - 9:50  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Opening Keynote &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     刘奇，PingCAP 联合创始人兼 CEO    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;9:50 - 10:10    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Global Scale of TiDB (English)   &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     Morgan Tocker, PingCAP Senior Product and Community Manager&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;10:10 - 11:10   &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区实践专场 1    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     - 美团数据库架构演变及展望&lt;/p&gt;&lt;p&gt;        美团点评 - 赵应钢    &lt;/p&gt;&lt;p&gt;     - 智能物联网的高写入场景解决之道&lt;/p&gt;&lt;p&gt;        小米 - 潘友飞    &lt;/p&gt;&lt;p&gt;     - 新东方新一代报名系统数据库选型&lt;/p&gt;&lt;p&gt;        新东方 - 傅少峰    &lt;/p&gt;&lt;p&gt;     - TiDB 在银行核心金融领域的研究与实践&lt;/p&gt;&lt;p&gt;       北京银行 - 于振华    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;11:10 - 11:50    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Way to TiDB 3.0&lt;/b&gt;&lt;/p&gt;&lt;p&gt;     申砾，PingCAP Engineering VP  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;11:50 - 13:20  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;     午餐    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;13:20 - 14:00&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区荣誉时刻  &lt;/b&gt; &lt;/p&gt;&lt;p&gt;     崔秋,  PingCAP 联合创始人  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;14:00 - 15:00    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区实践专场 2    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     - TiDB 在微众银行的实践与应用&lt;/p&gt;&lt;p&gt;        微众银行 - 胡盼盼    &lt;/p&gt;&lt;p&gt;    - All in！转转 TiDB 大规模实践之旅&lt;/p&gt;&lt;p&gt;        转转 - 孙玄    &lt;/p&gt;&lt;p&gt;    - TiDB 在小红书实时销售数据分析的最佳实践  &lt;/p&gt;&lt;p&gt;       小红书 - 张俊骏  &lt;/p&gt;&lt;p&gt;    - How to Add a New Feature to TiDB - From a Contributor’s “View”&lt;/p&gt;&lt;p&gt;       某金融机构 - 潘迪&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:00 - 15:20    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;      茶歇  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:20 - 15:40  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;TiKV - A Strong Foundation (English)    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;      Ana Hobden，PingCAP Senior Database Engineer on TiKV&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:40 - 16:40    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Demo Session    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     黄东旭，PingCAP 联合创始人兼 CTO&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;16:40 - 17:00&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Future of Database    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     黄东旭，PingCAP 联合创始人兼 CTO&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、报名注册开启&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB DevCon 2019 即日起开放报名注册，分为社区贡献者注册和标准注册两种类型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名链接👉：&lt;a href=&quot;https://pingcap.com/community/devcon2019/&quot;&gt;DevCon 2019&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 社区贡献者注册（¥0）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们已经将 TiDB 社区贡献者专属&lt;b&gt;【验证码】&lt;/b&gt;发送到大家在 GitHub 上的预留个人邮箱，请注意查收。如果没有收到，可以联系 TiDB Robot（微信号：tidbai）。&lt;/li&gt;&lt;li&gt;大家填写报名表单时务必填写 GitHub ID 和验证码，否则无法审核通过。&lt;/li&gt;&lt;li&gt;即日起至 1 月 17 日 18:00 期间新增社区贡献者亦可注册。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. 标准注册（¥1970）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;标准注册无需审核。&lt;/li&gt;&lt;li&gt;另外，我们为教职人员和在校学生提供学术优惠（票价 ¥499），仅限优惠码注册，申请材料： &lt;/li&gt;&lt;ul&gt;&lt;li&gt;教职人员：校园网站个人信息页截图（或教师资格证） + 本人身份证扫描件 &lt;/li&gt;&lt;li&gt;在校学生：本人有效学生证 + 本人身份证扫描件 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;请将申请材料发送到 &lt;a href=&quot;mailto:%20langyuemeng@pingcap.com&quot;&gt;langyuemeng@pingcap.com&lt;/a&gt;，审核结果将通过邮件通知。优惠码申请截止时间 1 月 17 日 18:00。&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/community/devcon2019/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;DevCon 2019&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-17-52603507</guid>
<pubDate>Mon, 17 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiQuery：All Diagnosis in SQL | TiDB Hackathon 项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-17-52554018.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52554018&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e54c5c646a5e75cff2f65c5e0545b6ab_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自 TiNiuB 队的黄梦龙同学，他们的项目 TiQuery 在本届 TiDB Hackathon  2018 中获得了三等奖。 TiQuery 可以搜集诊断集群问题所需要的信息，包括集群拓扑，Region 分布，配置，各种系统信息，整理成结构化的数据，并在 TiDB 中支持直接使用 SQL 语言进行查询，开发和运维人员可以在 SQL 环境方便高效地进行问题诊断。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;“距离 Hackathon 结束已经一个多星期了，感觉心情还是没有从激情中平复过来。不过由于我读书少，这时候好像只能感慨一句，黑客马拉松真是太好玩了……”&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;组队和选题&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;组队的过程有些崎岖，过程不细表，总之最后我们凑成了 4 人团队：&lt;/p&gt;&lt;p&gt;我（menglong）和阿毛哥是 PingCAP 内部员工，我在 TiKV 团队，目前主要是负责 PD 部分的。阿毛哥是 TiDB 组的开发，同时也是 Go 语言圈的大佬。&lt;/p&gt;&lt;p&gt;晓峰（ID 米麒麟）是大数据圈的网红，可能很多人都在各种社区或各种微信群偶遇过，另外他的公司其实也是上线了 TiDB 集群的客户之一。&lt;/p&gt;&lt;p&gt;胡争来自小米，是 HBase 的 committer，在分布式系统方面有丰富的经验，Hackathon 的过程中还顺便给 TiKV 集群的全局备份方案提了几个很好的建议，也是不得不服……他还有另外一个身份，是我们 TiKV 组员大妹子的老公，大妹子回老家休产假了于是只得派家属来代为过个瘾。&lt;/p&gt;&lt;p&gt;我们大约从一周之前开始讨论选题的事情，我们所有人都是第一次参加 Hackathon，也没什么选题的经验，经过微信语音长时间的头脑风暴，前后大约提了有五六个方案，最终敲定了 TiQuery 这个方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案的主要灵感是来自 facebook 的开源项目 osquery，它能把系统的各种信息（CPU，内存，文件，挂载，设备，网络连接，crontab，ulimit，iptable，等等等等）整理成结构化数据并支持以 SQL 的方式进行查询。当然它是一个单机的，我们需要做个 proxy 把集群中所有节点的数据收集在一起，放在 TiDB 里供用户查询。再考虑 TiDB 产品生态的实际情况，我们还可以搜集 region 分布，配置，日志，metrics 等诊断所需要信息统一到 SQL 接口里，这样就升级成了一套完整的诊断工具。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后选 TiQuery 这个方案主要考虑了这几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;不需要写前端。我们几个虽然说平时或多或少写过点前端，但是毕竟手生，短时间可能很难搞出酷炫的效果，而且还有翻车的风险。事后证明这个扬长避短的思路无疑是正确的，最后拿到名次的 6 个团队只有 2 个是重点在可视化这块的，而且呈现出来的效果以我们的水平应该难以企及。&lt;/li&gt;&lt;li&gt;不容易翻车。因为有 facebook/osquery 这个强力项目做后盾，基本上不存在翻车的可能。做完 osquery 后，计划的其他部分可以到时候根据情况决定要不要做，可以说是既稳又浪。&lt;/li&gt;&lt;li&gt;&lt;b&gt;实用性强。&lt;/b&gt;我们在日常帮助用户排查问题的时候，经常需要在 SQL / 日志 / Grafana / pd-ctl / tidb-ctl / ssh 各种工具来回切换搜集各种信息。甚至有时候无法直接访问用户的环境，需要一步一步向用户说明如何去排查，在交流上花费了大量不必要的时间和精力。所以 TiQuery 这个项目解决的是切实存在的痛点，听闻已经有客户准备在生产集群中把 TiQuery 上线，也是很好的佐证。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;比赛过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Day 1&lt;/b&gt;&lt;/p&gt;&lt;p&gt;比赛第一天。早上 10 点踩着点来到公司，在前台简单签了到领了周边，转身进入办公区域，一瞬间就被扑面而来的热烈的气氛给感染到了。平常安静空旷的工作空间此时已是济济一堂，各路大神有的围在一起探讨方案，有的已经打开电脑开始攻克技术难题，还有不少相互网上熟识已久的网友们在热情地打着招呼。&lt;/p&gt;&lt;p&gt;我也很快找到 TiNiuB 队的根据地，短暂寒暄后就进入了工作状态。根据之前商量的分工，我主要负责把 PD 相关的数据转成 table 的形式。因为对 Go 语言和 PD 的接口都很熟悉，很快就把 TiQuery 的大体框架给撸出来了，PD 相关的数据源也依次给整理出来了。&lt;/p&gt;&lt;p&gt;阿毛哥那边计划是魔改一版 TiDB，来达成特定表的数据从远程服务加载这个需求。本来我们想的是需要 hack 一下 physical plan，或者实现一个新的 storage 什么的，想想还有些复杂。到他具体做的时候猛然发现 InformationSchema 这个神奇的存在，简单介绍下，InformationSchema 包含了 TiDB 中一系列特殊的表，它们的数据是直接从内存中捞来的，不需要经过 physical plan，也不需要走 storage。所以我们仿照 InformationSchema 的方式处理一下就行了，只不过数据是从 TiQuery 获取而不是直接在内存里。&lt;/p&gt;&lt;p&gt;两边写完之后我们很快进行了简单的联调，直接就通过测试了。看到 “SELECT * FROM pd_store”跑出结果后大家不由地一阵欢呼。不得不说，当不需要考虑异常错误处理，不需要写测试，不需要 review 的时候写代码的效率是真高……&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6808c88d870791b49185215a3cba3efb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6808c88d870791b49185215a3cba3efb&quot; data-watermark-src=&quot;v2-0133f41bf3f82ff2a72446c402dbe5c2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;中午我们在园区的“那家小馆”吃午餐。这里有个小插曲，胡争早了为了方便进园区把大妹子的员工卡给带来了，我们一合计估摸她休假之前应该卡里还留了不少钱，便决定饱餐一顿后强行用她的卡买单。等到了结账的时候才发现卡里只给留了 8 分钱……最后只好我掏了卡，并暗地里下决心好歹得拿个奖回来，不然偷鸡不成还蚀把米，与此同时不禁为胡争未来漫长的婚姻生活捏了把汗。&lt;/p&gt;&lt;p&gt;下午我们加入了 osquery-agent 服务负责从不同节点搜集上报系统信息，并用脚本把 osquery 的所有 schema 转成了 MySQL 兼容的形式导入进 TiDB。跑通后简单的试玩了下，基本上能按预期的方式运行，但是实用性方面有一些不足。随后我们主要从这几个方面进行优化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增加集群拓扑结构的信息。osquery-agent 变身成 tiquery-agent，除了 wrap osquery 之外还上报节点所运行的所有服务信息。&lt;/li&gt;&lt;li&gt;引入 psutil 库，tiquery-agent 支持查询针对单个服务更精确的 CPU，内存等信息。&lt;/li&gt;&lt;li&gt;调研 prometheus 转成 table 的可行性，这个我们发现短时间不太好做，就放弃了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他同步在做的事情包括用 ansible 部署了一套测试集群，开始做演示用的 slides，梳理 TiQuery 能提供的功能整理一个有说服力的 user story。期间我还客串了下导师的角色，支持了几个涉及到 PD 的项目。&lt;/p&gt;&lt;p&gt;到了晚上，测试集群上已经能完全顺畅地跑起来了，slides 基本上完成，演示要用的查询也都准备好了。霸哥（导师团成员&lt;b&gt;韩飞&lt;/b&gt;，人称 SQL 小王子）帮我们手写了一个复杂的 4 表 JOIN，一气呵成，大家纷纷表示向大佬低头。&lt;/p&gt;&lt;p&gt;23 点左右，我又去整个赛场溜达了一圈，发现比较有竞争力的几个项目要么还在埋头苦干，要么就是 block 在技术难点上痛不欲生。当时我们就感觉胜券在握了，简单商量了一下就各自回家休息。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Day 2&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第二天早上过来先是又转了一圈探查敌情，当时脑海里就冒出来“龟兔赛跑”的故事。&lt;br&gt;&lt;/p&gt;&lt;p&gt;几个可视化的项目，昨天晚上走之前还几乎是白板一片，一夜之间就酷炫到没朋友了，尤其是凤凰队，真给人一种山鸡变凤凰的感觉。还有 TiBoys，昨天我琢磨着项目太宏大，铁定没法搞出来的，早上去一看，readme 里的 todo list 已经基本上全给勾上了，很难想像这一夜他们经历了什么……&lt;/p&gt;&lt;p&gt;我们当天其实就没做什么事情了，就整理了下项目文档什么的，还找了个马里奥的图片 ps 了下。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-068739583a743493fa8c42ee150f4bb7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;600&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-068739583a743493fa8c42ee150f4bb7&quot; data-watermark-src=&quot;v2-bc600f7bf47f245cd4610e8a97926983&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下午的 Demo 我们排在最后出场，由胡争出场演示。整个过程出奇地顺畅，评委的疑问也顺畅的解答了，其实我们的项目本身比较简单，要做的事情一两句话就能说清楚。略有遗憾的是当时胡争可能是过于激动，关于 TiQuery 具体有哪些实用场景没有细说。&lt;/p&gt;&lt;p&gt;最后我们在众多优秀的作品中杀出重围，侥幸拿到了三等奖，第一次参赛的大家都很兴奋，晚上自然是又出去好好腐败了一把，并相约以后再找机会参加类似的活动……&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7030f9a748d5ae252c05215e771c1edc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;594&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7030f9a748d5ae252c05215e771c1edc&quot; data-watermark-src=&quot;v2-e67afd03449cb4452535ebaab0959e9e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;感想和总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我本人喜欢跑步，也参加过多次马拉松赛，其实马拉松赛不仅在于竞技，也不仅在于坚持跑完那 42km，更重要的是它是一大群有共同爱好的人聚在一起的一次狂欢。从这个角度看，“黑客马拉松”真的是非常贴切的一个名字。在这里我想感谢 PingCAP 和志愿者们的精心组织，提供了这么好的一个机会让大家有机会互相欣赏，切磋，交流，学习。&lt;/p&gt;&lt;p&gt;通过这次比赛我也积攒了一些经验，或者说下次参加 Hackathon 会去考虑的一些点吧，在这里分享出来供大家探讨：&lt;/p&gt;&lt;p&gt;&lt;b&gt;THINK BIG&lt;/b&gt;。我们在选方案的时候特别怕想复杂了到时候做不出来然后翻车，实际上在 Hackathon 比赛时爆发出的潜能会远超自己的想象，结果就是我们迅速就完工了后面其实无所事事……可以简单针对编程速度算下账：不用考虑异常处理，效率 x2，不用写单元测试，效率 x3，不用 code review，效率 x2，不用考虑优雅设计前后兼容，效率 x2，全天 24 小时工作，效率 x3（根据贵司具体情况可调整为 x2），再加上是几个人一起做的，粗略算下来 24 小时内足以做完平时需要几个月才能完成的事情，因此我们设计方案时可以尽管往大了想。&lt;/p&gt;&lt;p&gt;&lt;b&gt;SHOW OFF ALL&lt;/b&gt;。比赛之前我们就已经意识到 Demo 展示环节非常关键，但是可惜这块还是做得不够好，有些花力气做了的功能最终没有最后在 Demo 时展现出来，今后参加类似的比赛时会更加注意这一点。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiQuery 项目及其未来&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后再花一些篇幅来推广下 TiQuery 这个项目吧。&lt;/p&gt;&lt;p&gt;首先明确一点，它不是要替换掉已有的查日志，看 metrics，调用 PD API 等现有的诊断问题手段，而是提供一种新的途径和可能，即直接使用 SQL 语言，并且这种途径在很多场景下会更方便甚至是变不可能为可能。&lt;/p&gt;&lt;p&gt;比如我们平常定位一个慢 SQL，可能需要先在 SQL 环境中确认有问题的语句，然后去日志中找出响应时间长的 Region，随后使用 pd-ctl 去查询 Region 的信息，然后再根据 leader 所在的 TiKV ID 查询到对应 TiKV 所在的节点，然后再 ssh 登录到对应的节点查询关键线程的 CPU 占用情况……&lt;/p&gt;&lt;p&gt;如果部署了 TiQuery，以上操作都可以在 SQL 环境中搞定，不用各种工具来回切换，而且通过 SQL 的关联查询功能，以上整个流程甚至只需要一条语句。&lt;/p&gt;&lt;p&gt;再比如，客户的环境可能对访问某些资源有限制，比如没有权限 ssh 登录对应的服务器，防火墙的原因无法查看 Grafana，这时 TiQuery 就能帮且我们拿到原本拿不到的信息。还有的时候，我们无法直接连接客户的集群，只能远程指导客户去诊断问题，这种情况下不用去费力教会客户用各种工具了，直接扔过去一条 SQL，岂不美哉。&lt;/p&gt;&lt;p&gt;另外，SQL 作为一门标准化的查询语言，在易用性方面有着天然的优势，不仅方便不了解 TiDB 的 DBA 快速上手，其他外部系统也能方便地对接（比如外部监控报警系统）。&lt;/p&gt;&lt;p&gt;当然了，TiQuery 项目如果真要在严肃的生产环境上线，还有许多工作要做：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;完善异常处理，重构，文档的完善&lt;/li&gt;&lt;li&gt;更多 schema 支持，包括日志文件，prometheus metrics 等&lt;/li&gt;&lt;li&gt;tidb-ansible 集成，包括部署 tiquery-agent 服务，配置生成，安装 osquery 等&lt;/li&gt;&lt;li&gt;TiDB 支持外部数据源加载数据（这个特性在 Hackathon 其他项目也有各自的实现，期待能合入 TiDB 主干）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;最后，项目的地址在&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://github.com/TiNiuB/tiquery&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-b79fd3efb6bcd737367065f0bda81a7a&quot; data-image-width=&quot;400&quot; data-image-height=&quot;400&quot; data-image-size=&quot;ipico&quot;&gt;TiNiuB/tiquery&lt;/a&gt;&lt;p&gt;&lt;b&gt;期待大家来共同完善！&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-17-52554018</guid>
<pubDate>Mon, 17 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Lab 诞生记 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-14-52414551.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52414551&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-391401a44b5cfc4f07ef0d321a10fce0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文由&lt;b&gt;红凤凰粉凤凰粉红凤凰队&lt;/b&gt;的成员主笔，他们的项目 &lt;b&gt;TiDB Lab&lt;/b&gt; 在本届 TiDB Hackathon 2018 中获得了二等奖。TiDB Lab 为 TiDB 培训体系增加了一个可以动态观测 TiDB / TiKV / PD 细节的动画教学 Lab，让用户可以一边进行真实操作一边观察组件之间的变化，例如 SQL 的解析，Region 的变更等等，从而生动地理解 TiDB 的工作原理。&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a8ca052d52e744806a93b786c597e257_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;246&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a8ca052d52e744806a93b786c597e257&quot; data-watermark-src=&quot;v2-1698678ea9ba1448ad082b2e043d13e5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;项目简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Lab，全称 TiDB Laboratory&lt;/b&gt;，是一个集 TiDB 集群状态的在线实时可视化与交互式教学的平台。用户可以一边对 TiDB 集群各个组件 TiKV、TiDB、PD 进行各种操作，包括上下线、启动关闭、迁移数据、插入查询数据等，一边在 TiDB Lab 上以动画形式观察操作对集群的影响，例如数据是怎么流动的，Region 副本在什么情况下发生了变更等等。通过 TiDB Lab 这种对操作进行可视化反馈的交互模式，用户可以快速且生动地理解 TiDB 内部原理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.功能&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;实时动态展示 TiDB、TiKV 节点的新增、启动与关闭。&lt;/li&gt;&lt;li&gt;实时动态展示 TiDB 收到 SQL 后，物理算子将具体请求发送给某些 TiKV Region Leader 并获取数据的过程。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本状态的变化，例如新增、删除、分裂。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本内的数据量情况。&lt;/li&gt;&lt;li&gt;浏览集群事件历史（事件指上述四条功能所展示的各项内容）并查看事件的详细情况，包括事件的具体数据内容、SQL Plan 等等。&lt;/li&gt;&lt;li&gt;按 TiDB、TiKV 或 Region 过滤事件历史。&lt;/li&gt;&lt;li&gt;对事件历史进行时间穿梭：回到任意事件发生时刻重新观察当时的集群状态，或按事件单步重放观察集群状态的变化。&lt;/li&gt;&lt;li&gt;在线获取常用运维操作的操作指南。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3.愿景&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们其实为 TiDB Lab 规划了更大的愿景，但由于 Hackathon 时间关系，还来不及实现。我们希望能实现 TiDB Lab + TiDB 生态组件的沙盒，从而在 TiDB Lab 在线平台上直接提供命令执行与 SQL 执行功能。这样用户无需离开平台，无需自行准备机器下载部署，就可以直接在平台上根据提供的操作指南进行各类操作，并能观察操作带来的具体影响，形成操作与反馈的闭环，真正地实现零门槛浏览器在线教学。我们期望平台能提供用户以下的操作流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户获得一个 TiDB Lab 账户并登录（考虑到沙盒是占用实际资源的，需要通过账户许可来限制避免资源快速耗尽）。&lt;/li&gt;&lt;li&gt;用户在 TiDB Lab 上获得若干虚拟机器的访问权限，每个机器处于同一内网并具有独立 IP。这些虚拟机器的实际实现是资源受限的虚拟机或沙盒，因为作为教学实验不需要占用很多资源。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;例如：平台为用户自动分配了 5 个 IP 独立的沙盒的访问权限，地址为 192.168.0.1 ~ 192.168.0.5。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户在平台上进行第零章「架构原理」的学习。平台提供了一个默认的拓扑部署，用户可以在平台提供的在线 SQL Shell 中进行数据的插入、删除、更新等操作。用户通过平台观察到 SQL 是如何对应到 TiKV 存储节点上的，以及数据是怎么切分到不同 Region 的等等。&lt;/li&gt;&lt;li&gt;用户在平台上进行第一章「TiDB 部署」的学习，了解到可以通过 ansible 进行部署。教学样例是一个典型的 TiDB + TiKV 三副本部署。对于这个教学样例，平台告知用户 inventory.ini 具体内容应当写成什么样子。用户可以在平台提供的在线 Terminal 上修改 inventory 文件，并执行部署与集群启动命令。部署和启动均能在平台上实时反馈可视化。&lt;/li&gt;&lt;li&gt;用户继续进行后续章节学习，例如「TiDB 单一服务启动与关闭」。用户在可视化界面上点击某个刚才已经部署出来的节点，可以了解启动或关闭单个 TiDB 的命令。用户可以在平台提供的在线 Terminal 上执行这些命令，尝试启动或关闭单一 TiDB。&lt;/li&gt;&lt;li&gt;用户继续学习基础运维操作，例如「TiKV 扩容」。平台告知 inventory.ini 应当如何进行修改，用户可以根据指南在在线 Terminal 上进行实践，并通过可视化界面观察扩容的过程，例如其他节点上的副本被逐渐搬迁到新节点上。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b00fb1e54cf86282b73e86667a939732_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b00fb1e54cf86282b73e86667a939732&quot; data-watermark-src=&quot;v2-66a6b03f86e2e8fbc23c88126c249016&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;前期准备&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;团队&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们团队有三个人，是一个 PingCAP 同学与 TiDB 社区小伙伴钱同学的混合组队，其中 PingCAP 成员分别来自 TiKV 组与 OLAP 组。我们本着搞事情的想法，团队取名叫「&lt;b&gt;红凤凰粉凤凰粉红凤凰」&lt;/b&gt;，想围观主持人念团队名称（然而机智的主持人小姐姐让我们自报团队名称）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 原计划：干掉 gRPC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;鉴于从报名开始直到 Hackathon 正式开始前几小时，我们都在为原计划做准备，因此值得详细说一下…&lt;/p&gt;&lt;p&gt;我们一开始规划的 Hackathon 项目是换掉 TiKV、TiDB 之间的 RPC 框架 gRPC，原因有几个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一是发现 TiKV、TiDB 中 gRPC 经常占用了大量 CPU，尤其是在请求较多但很简单的 benchmark 场景中经常比 Coprocessor 这块儿还高，这在客户机器 CPU 资源比较少的情况下是性能瓶颈；&lt;/li&gt;&lt;li&gt;二是发现 gRPC 性能一般，在各类常用 RPC 框架的性能测试中 gRPC 经常是垫底水平；&lt;/li&gt;&lt;li&gt;三是 gRPC 主要设计用于用户产品与 Google 服务进行通讯，因而考虑到了包括负载均衡友好、流量控制等方面，但对 TiKV 与 TiDB 这类内部通讯来说这些都是用不上的功能，为此牺牲的性能是无谓的开销；&lt;/li&gt;&lt;li&gt;另外近期 TiKV 内部有一个实验是将多个 RPC 请求 batch 到一起再发送（当然处理时候再拆开一个一个执行原来的 handler），性能可以瞬间提高一倍以上，这也从侧面说明了 gRPC 框架自身开销很大，因为用户侧请求总量是一致的，处理模式也是一致的，唯一的区别就是 RPC 框架批量发送或一条一条发送。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们早在几周前就开始写简单 Echo Server 进行可行性验证和性能测试，是以下三个方面的正交：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;协议：CapnProto RPC、brpc over gRPC、裸写 echo。&lt;/li&gt;&lt;li&gt;服务端：Rust、C++，对应用于 TiKV。&lt;/li&gt;&lt;li&gt;客户端：Golang，对应用于 TiDB。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中 TiKV 侧调研了 Rust 和 C++ 的服务端实现，原因是 Rust 可以通过 binding 方式调用 C++ 服务端。而 TiDB 侧客户端实现不包含 C++ 的原因是 Golang 进行 C / C++ FFI 性能很差，因此可以直接放弃 C/C++ 包装一层 binding 用于 TiDB 的想法。最后测试下来，有以下结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CapnProto：序列化性能很高，但其 RPC 性能没有很突出。最重要的是，CapnProto 的 Golang Client 实现有 bug，并不能稳定地与 Rust 或 C++ 的 Server 进行 RPC 交互。作者回复说这是一个已知缺陷，涉及重构。这对于 Hackathon 来说是一个致命的问题，我们并没有充足的时间解决这个问题，直接导致我们放弃这个方案。&lt;/li&gt;&lt;li&gt;brpc over gRPC：可以实现使用 brpc C++ 客户端 &amp;amp; gRPC Golang 服务端配合（注：brpc 没有 Golang 的实现）。但这个方案本质只是替换服务端的实现，并没有替换协议，并不彻底，我们不是特别喜欢。另外在这个换汤不换药的方案下，测试下来性能的提升有限，且随着 payload 越大会越小。我们最终觉得作为一个 Hackathon 项目如果仅有有限的性能提升（虽然可以在展示的时候掩盖缺陷只展示优点），那么意义不是很大，最终无法用于产品，因而放弃。&lt;/li&gt;&lt;li&gt;裸写：我们三个成员都不是这方面的老司机，裸写大概是写不完的。。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 新计划：做一个用于培训的可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Hackathon 开始的前一个晚上，我们决定推翻重来，于是 brain storm 了几个想法，最后觉得做一个用于教学的可视化比较可行，并且具有比较大的实际意义。另外，这个新项目「集群可视化」相比原项目「换 RPC」来说更适合 Hackathon，主要在于：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;具有图形化界面，容易拿奖可以直观地展现成果。&lt;/li&gt;&lt;li&gt;并不是一个「非零即一」的任务。新项目有很多子功能，可以逐一进行实现，很稳妥，且不像老项目那样只有换完才知道效果。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们继续在这个「可视化」的想法上进行了进一步的思考，想到如果可以做成在线教学的模式，则可以进一步扩展其项目意义，形成一个完整的在线教学体系，因此最终决定了项目的 scope。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;具体实现&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;在 TiKV、TiDB 与 PD 中各个关键路径上将发生的具体「事件」记录下来，并在前端进行一一可视化。&lt;/i&gt;&lt;/blockquote&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;事件&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们将 TiDB Lab 进行可视化所需要的信号称为「事件」，并规划了以下「事件」：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiDB&lt;/li&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiKV&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 启动&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到一条 SQL&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 KvGet 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 PreWrite / Commit 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 Coprocessor 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 创建&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 删除&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 分裂&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Snapshot 复制&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 数据量发生显著变化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，由于时间关系、技术难度和可视化需要，实际实现的是以下事件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 事件：TiDB 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 KvGet 读请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 PreWrite / Commit 写入请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 Coprocessor 读请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 创建（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 删除（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 分裂（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 数据量发生显著变化（通过 Region 心跳实现）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;可视化部分由前端（lab-frontend）和事件收集服务（lab-gateway）组成。&lt;/p&gt;&lt;p&gt;事件收集服务是一个简单的 HTTP Server，各个组件通过 HTTP Post 方式告知事件，事件收集服务将其通过 WebSocket 协议实时发送给前端。事件收集服务非常简单，使用的是 Node.js 开发，基于 ExpressJs 启动 HTTP Server 并基于 SocketIO 实现与浏览器的实时通讯。ExpressJs 收到事件 JSON 后将其通过 SocketIO 进行广播，总代码仅仅十几行。&lt;/p&gt;&lt;p&gt;可视化前端采用 Vue 实现，动画使用 animejs 和 CSS3。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过模板实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一部分事件通过「由事件更新集群状态数据 – 由集群状态通过 Vue 渲染模板」进行可视化。&lt;/p&gt;&lt;p&gt;这类可视化是最简单的，以 TiDB 启动与否为例，TiDB 的启动与否在界面上呈现为一个标签显示为「Started」或「Stopped」，那么就是一个传统的 Vue MVVM 流程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;数据变量 instances.x.online 代表 TiDB x 是否已启动。&lt;/li&gt;&lt;li&gt;收到 TiDB Started 事件后，更新 instances.x.online = true 。&lt;/li&gt;&lt;li&gt;收到 TiDB Stopped 事件后，更新 instances.x.online = false 。&lt;/li&gt;&lt;li&gt;前端模板上，根据 instances.x.online 渲染成 Started 或 Stopped 对应的界面。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这类可视化的动画采用的是 CSS3 动画。由于 Region Peer 位置是由 left, top CSS 属性给出的，因此为其加上 transition，即可实现 Region Peer 在屏幕上显示的位置改变的动画。位置改变会主要发生在分裂时，分裂时 Region 列表中按顺序会新增一个，那么后面各个 Region 都要向后移动（或换到下一行等）。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;244&quot; data-rawheight=&quot;232&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_b.jpg&quot;&gt;&lt;p&gt;最后，使用 Vue Group Transition 功能，即可为 Region Peer 的新增与删除也加入动画效果。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过动画实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另一部分事件并不反应为一个持久化 DOM 的变化，例如 TiDB 收到 SQL 并发请求到某个具体 TiKV 上 Region peer 的事件，在前端展示为一个 TiDB 节点到 TiKV Region Peer 的过渡动画。动画开始前和动画结束后，DOM 没有什么变化，动画是一个临时的可视元素。这类动画通过 animejs 实现。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;748&quot; data-rawheight=&quot;496&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_b.jpg&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_r.gif&quot; data-caption=&quot;这个浮夸的开场动画效果也是 animjs 做的&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;200&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_b.jpg&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间穿梭&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;时间穿梭是一个在目前前端框架中提供的很时髦的功能，我们准备借鉴一波。主要包括：1. 回退到任意一个历史事件发生的时刻展示集群的状态；2. 从当前事件开始往后进行单步可视化重现。&lt;/p&gt;&lt;p&gt;时间穿梭的本质是需要实现两个基础操作：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于单一事件实现正向执行，即事件发生后，更新对应集群数据信息（如果采用「通过模板实现的可视化」），或创建临时动画 DOM（如果采用「通过动画实现的可视化」）。另外允许跳过「通过动画实现的可视化」这一步。&lt;/li&gt;&lt;li&gt;对于单一事件实现反向执行，即撤销这个事件造成的影响。对于「通过模板实现的可视化」，我们需要根据事件内容反向撤销它对集群数据信息的修改。对于「通过动画实现的可视化」，我们什么都不用做。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;时间穿梭的功能可以通过组合这两个基础操作实现。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;回退到任意历史事件发生时刻：若想要前往的事件早于当前呈现的事件，则对于这期间的事件逐一进行反向执行。若想要前往的事件晚于当前呈现的事件，则进行无动画的正向执行。&lt;/li&gt;&lt;li&gt;从当前事件开始单步可视化：执行一次有动画的正向执行。&lt;/li&gt;&lt;li&gt;实时展示新事件：执行一次有动画的正向执行。&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;950&quot; data-rawheight=&quot;572&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;3. TiDB 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的历史事件收集略为 Hack。由于需要过滤任何非用户发起的查询（类似 GC 或者 meta 查询会由背景协程频繁发起打扰使用体验），因此在用户链接入口处添加了 context 标记一路携带到执行层，再修改相应的协程同步数据结构添加需要转发的标记信息。比较麻烦的是类似 Point Get 这样接口允许携带信息非常少的调用，只好将标记位编码进 Key 本身了。Plan 的可视化其实并没有花多少功夫，因为找到 TiDB 本身已经做了类似的功能，我们无非只是将这块代码直接偷来了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. PD 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;原本不少事件希望在 TiKV 端完成侦听，不过显然 KV 一小时编译一次的效率无法满足 Hackathon 中多次试错的需要。因此我们改为在 PD 中侦测心跳和汇报事件。其实并没有什么神秘，在原本 PD 自己检查 Region 变更的代码拆分成 Region 和 Peer 变更：每次 PD 接到 Region 心跳会在 PD Cache 中进行 Version 和 confVer 变更的检测，主要涉及 Peer 的增加和减少等。而 Region Split 会单独由 RegionSplitReport 进行汇报，这里也会做一次 Hook。另外就是每次心跳会检查是否有未上报给 Lab 的 Region 信息，如果有就转换成 Peer 信息进行补发。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. TiKV 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上，原本我们计划了很多 TiKV 事件，但由于开发机器配置不佳，每次修改都要等待一小时进行一次编译，考虑到 Hackathon 上时间紧迫，因此最终大大缩减了 TiKV 上收集的事件数量，改为只收集启动和停止。基本架构是事件发生的时候，事件异步发送给一个 Channel，Channel 的另一端有一个异步的 worker 负责不断处理各个事件并通过 Hyper HTTP Client 发送出去。这个流程其实与 TiKV 中汇报 PD 事件有些类似，只是事件内容和汇报目标不一样。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感悟&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;以下文字 by 马晓宇 @ OLAP Team, PingCAP&lt;/i&gt;&lt;br&gt;&lt;i&gt;“由于比较能调侃干的活相对少一些，所以大王要我来巡山写感悟。”&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;就像队长说的，这个项目原本是希望做一个 bRPC 替换 gRPC 的试验，只是由于种种原因临到 Hackathon 前夜我们才确定这是个大概率翻车的点子。于是乎我们只好在酒店里抓耳挠腮，一边讨论可能的补救措施。&lt;/p&gt;&lt;p&gt;参加过、围观过几次 Hackathon，见过现场 Demo 效果最震撼的一次其实并不是一个技术上最优秀的作品，但是它的确赢得了大奖。那是一个脑洞奇大的点子：用 Kinect 体感加上 DirectX 的全 3D 展示做的网络流量实时展示；程序根据实时数据（举办方提供了实时网络测量统计信息）聚合显示不同粗细的炫酷弧光特效，而演示者则用手势操控地球模型的旋转。&lt;/p&gt;&lt;p&gt;于是在完全不了解大家是否能搞定前端的情况下，我们还是很轻率地决定了要做个重前端的项目。至于展示什么？既然时间不够，那么秀一个需要生产上线的可视化工具，翻车的概率就大的多，不如直接定位为 For Educational Purpose Only。而且大概是过度解读了炫酷对于成功的重要性，因此队长也轻率地决定了这个项目的基调是「极尽浮夸」。就在这样友好且不靠谱的讨论氛围下，这个项目的策划出炉了。&lt;/p&gt;&lt;p&gt;之后就是艰苦卓绝连绵无休的代码过程了。&lt;/p&gt;&lt;p&gt;龙毛二哥，晓峰和胡家属的 &lt;u&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487451&amp;amp;idx=2&amp;amp;sn=5f1ee6e838c3a86556fcd556662112c5&amp;amp;chksm=eb1628b1dc61a1a7e8f4cb82e2bfaab40cbfb27e986f9705f9166d629ff31a812f7ae45b1d73&amp;amp;scene=21#wechat_redirect&quot;&gt;TiNiuB Team&lt;/a&gt;&lt;/u&gt; 就在我们对面开工。讲道理这其实是我个人最喜欢的项目之一。第一天放学的时候，看着他们的进度其实我心里虚得不行：看看别人家的项目，我们的绝对主力还在折腾 TiDB Logo 动画，是我敢怒不敢言的状态。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f2ade45b86ee55be8b85a1198f684000_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;240&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f2ade45b86ee55be8b85a1198f684000&quot; data-watermark-src=&quot;v2-e650574cbe2a265f474817c1ac2c88e2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;原来我想偷一下懒回家睡个觉啥的，就临时改变主意继续配合队长努力干活；钱同学也抱着「TiKV 一天只能 Build 24 次必须珍惜」的态度写着人生的第一个 Rust 项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因此，这里必须鸣谢龙哥他们对我们的鞭策。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;事实证明，误打误撞但又深谋远虑的浮夸战略是有效的。Demo 的时候我很认真地盯着评委：在本项目最浮夸的 Logo 展示环节，大家的眼睛是发着光的，一如目睹了摩西分开红海。我个人认为这个动画 Logo 生生拉高了项目 50% 的评分。&lt;/p&gt;&lt;p&gt;只是具体要说感悟的话（似乎好多年没写感悟了呢），首先这次我们三个组员虽然有两个是 PingCAP 员工，不过由于技能的缺口大于人力，因此负责的任务都不是自己所属的模块：队长是 TiKV 组的但是在写前端，我是 OLAP 组的但是在改 PD 和 DB，钱同学也在做自己从来没写过的 Rust（参赛前一周简单入门了一下），因此其实还蛮有挑战的（笑）。然后偶尔写写自己不熟悉的语言，搞搞自己不熟悉的模块，会有一种别样的新鲜刺激感，这大概就是所谓的路边野花更香吧（嗯）？除此之外，Hackathon 更像一个大型社交活动，满足了猫一样孤僻的程序员群体被隐藏的社交欲，有利于码农的身心发展，因此可以多搞搞 :) 。&lt;/p&gt;&lt;p&gt;回到我们项目本身的话，其实 TiDB 的源码阅读或者其他介绍类文章其实并不能非常直观地帮助一头雾水的初学者理解这个系统。我们做这个项目的最大目的是能降低学习门槛，让所有人能以非常直观，互动的方式近距离理解她。所以希望这个项目能给大家带来方便吧。&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-14-52414551</guid>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>让 TiDB 访问多种数据源 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-14-52413872.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52413872&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4dcc3a79d7a6c17e1fcf431ec2440e84_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自&lt;b&gt;CC 组的兰海同学&lt;/b&gt;，他们的项目&lt;b&gt;《让 TiDB 访问多种数据源》&lt;/b&gt;在本届 TiDB Hackathon 2018 中获得了二等奖。该项目可以让 TiDB 支持多种外部数据源的访问，针对不同数据源的特点会不同的下推工作，使 TiDB 成为一个更加通用的数据库查询优化和计算平台。&lt;/blockquote&gt;&lt;p&gt;我们队伍是由武汉大学在校学生组成。我们选择的课题是让 TiDB 接入若干外部的数据源，使得 TiDB 成为一个更加通用的查询优化和计算平台。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么选这个课题&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;刚开始我们选择课题是 TiDB 执行计划的实时动态可视化。但是填了报名单后，TiDB Robot 回复我们说做可视化的人太多了。我们担心和别人太多冲突，所以咨询了导师的意见，改成了 TiDB 外部数据源访问。这期间也阅读了 F1 Query 和 Calcite 论文，看了东旭哥（PingCAP CTO）在 PingCAP 内部的论文阅读的分享视频。感觉写一个简单 Demo，还是可行的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;系统架构和效果展示&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08d5704dedbfe4ea20221beb6dabcf52_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;449&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-08d5704dedbfe4ea20221beb6dabcf52&quot; data-watermark-src=&quot;v2-c2844ea0dd651526266cace4ae7eac28&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如上图所示，TiDB 通过 RPC 接入多个不同的数据源。TiDB 利用 RPC 发送请求给远端数据源，远端数据源收到请求后，进行查询处理，返回结果。TiDB 拿到返回结果进一步的进行计算处理。&lt;/p&gt;&lt;p&gt;我们通过定义一张系统表 foreign_register(table_name,source_type,rpc_info) 记录一个表上的数据具体来自哪种数据源类型，以及对应的 RPC 连接信息。对于来自 TiKV 的我们不用在这个表中写入，默认的数据就是来自 TiKV。&lt;/p&gt;&lt;p&gt;我们想访问一张 PostgreSQL（后面简称为 PG）上的表：首先，我们在 TiDB 上定义一个表(记为表 a)，然后利用我们 register_foreign(a,postgresql,ip#port#table_name) 注册相关信息。之后我们就可以通过 select * from a 来读取在 PG 上名为 table_name 的表。&lt;/p&gt;&lt;p&gt;我们在设计各个数据源上数据访问时，充分考虑各个数据源自身的特点。将合适的操作下推到具体的数据源来做。例如，PG 本身就是一个完整的数据库系统，我们支持投影、条件、连接下推给 PG 来做。Redis 是一个内存键值数据库，我们考虑到其 Get 以及用正则来匹配键值很快，我们将在 Key 值列的点查询以及模糊匹配查询都推给了 Redis 来做，其他条件查询我们就没有进行下推。&lt;/p&gt;&lt;p&gt;具体的运行效果如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad72ad983498655bfb7831ce6711d645_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;266&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ad72ad983498655bfb7831ce6711d645&quot; data-watermark-src=&quot;v2-413d53a2fae37a96dcf666b492bb54d4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如图所示，我们在远程开了 3 个 RPC Server，负责接收 TiDB 执行过程中的外部表请求，并在内部的系统表中注册三张表，并在 TiDB 本地进行了模式的创建——分别是remotecsv，remoteredis，remotepg，还有一张本地 KV Store 上的 localkv 表。我们对 4 张表进行 Join 操作，效果如图所示，说明如下。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;远程 csv 文件我们不做选择下推，所以可以发现 csv 上的条件还是在 root（即本地）上做。&lt;/li&gt;&lt;li&gt;远程的 PG 表，我们会进行选择下推，所以可以发现 PG 表的 selection 被推到了 PG 上。&lt;/li&gt;&lt;li&gt;远程的 Redis 表，我们也会进行选择下推，同时还可以包括模型查询条件（Like）的下推。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;P.S. 此外，对于 PostgreSQL 源上两个表的 Join 操作，我们也做了Join 的下推，Join 节点也被推送到了 PostgreSQL 来做，具体的图示如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-eed13dc021d5362ada026473113cd194_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;270&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-eed13dc021d5362ada026473113cd194&quot; data-watermark-src=&quot;v2-63305dae95a51fbaa876ecde30b8342f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;如何做的&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于项目偏硬核的，需要充分理解 TiDB 的优化器，执行器等代码细节。所以在比赛前期，我们花了两三天去研读 TiDB 的优化器，执行器代码，弄清楚一个简单的 Select 语句扔进 TiDB 是如何进行逻辑优化，物理优化，以及生成执行器。之前我们对 TiDB 这些细节都不了解，硬着去啃。发现 TiDB 生成完执行器，会调用一个 Open 函数，这个函数还是一个递归调用，最终到 TableReader 才发出数据读取请求，并且已经开始拿返回结果。这个和以前分析的数据库系统还有些不同。前期为了检验我们自己对 TiDB 的执行流程理解的是否清楚，我们尝试着去让 TiDB 读取本地 csv 文件。&lt;/p&gt;&lt;p&gt;比赛正式开始，我们一方面完善 csv，不让其进行条件下推，因为我们远端 RPC 没有处理条件的能力，我们修改了逻辑计划的条件下推规则，遇到数据源是 csv 的，我们拒绝条件下推。另一方面，首先得啃下硬骨头 PostgreSQL。我们考虑了两种方案，第一种是拿到 TiDB 的物理计划后，我们将其转换为 SQL，然后发给 PG；第二种方案我们直接将 TiDB 的物理计划序列化为 PG 的物理计划，发给 PG。我们考虑到第二种方案需要给 PG 本身加接受物理计划的钩子，就果断放弃，不然可能两天都费在改 PG 代码上了。我们首先实现了 select * from pgtable。主要修改了增加 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/distsql/select_result.go#L45&quot;&gt;pgSelectResult&lt;/a&gt; 结构体实现对应的结构体。通过看该结构体以及其对应接口函数，大家就知道如何去读取一个数据源上的数据，以及是如何做投影下推。修改 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/planner/core/logical_plans.go#L314&quot;&gt;Datasource&lt;/a&gt; 数据结构增加对数据源类型，RPC 信息，以及条件字符串，在部分物理计划内，我们也增加相关信息。同时根据数据源信息，在 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/table_reader.go#L132&quot;&gt;（e*TableReaderExecutor）buildResp&lt;/a&gt; 增加对来源是 PG 的表处理。&lt;/p&gt;&lt;p&gt;接着我们开始尝试条件下推：select * from pgtable where … 将 where 推下去。我们发现第一问题：由于我们的注册表里面没有记录外部源数据表的模式信息导致，下推去构建 SQL 的时候根本拿不到外部数据源 PG 上正确的属性名。所以我们暂时保证 TiDB 创建的表模式与 PG 创建的表模式完全一样来解决这个问题。条件下推，我们对条件的转换为字符串在函数 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/expression/expr_to_pb.go#L72&quot;&gt;ExpressionToString&lt;/a&gt; 中，看该函数调用即可明白是如何转换的。当前我们支持等于、大于、小于三种操作符的下推。&lt;/p&gt;&lt;p&gt;很快就到了 1 号下午了，我们主要工作就是进行 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/builder.go#L873&quot;&gt;Join下推&lt;/a&gt; 的工作。Join 下推主要是当我们发现两个 Join 的表都来来自于同一个 PG 实例时，我们就将该 Join 下推给 PG。我们增加一种 Join 执行器：&lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/join.go#L60&quot;&gt;PushDownJoinExec&lt;/a&gt; 。弄完 Join 已经是晚上了。而且中间还遇到几个 Bug，首先，PG 等数据源没有一条结果满足时的边界条件没有进行检查，其次是，在 Join 下推时，某些情况下 Join 条件未必都是在 On 子句，这个时候需要考虑 Where 子句的信息。最后使得连接和条件同时下推没有问题。因为不同表的相同属性需要进行区分。主要难点就是对各个物理计划的结构体中的解析工作。&lt;/p&gt;&lt;p&gt;到了晚上，我们准备开始着手接入 Redis。考虑到 Redis 本身是 KV 型，对于给定 Key 的 Get  以及给定 Key 模式的匹配是很快。我们直接想到对于 Redis，我们允许 Key 值列上的条件下推，让 Redis 来做过滤。因为 Redis 是 API 形式，我们单独定义一个简单请求协议，来区别单值，模糊，以及全库查询三种基本情况，见 &lt;a href=&quot;https://github.com/hailanwhu/tidbhack2018rpcserver/blob/b132bfed58d5b9565250d9584bd6e280bc452ad0/rpcserverforredis.go#L12&quot;&gt;RequestRedis&lt;/a&gt; 定义。Redis 整体也像是 PG 一样的处理，主要没有 Join 下推这一个比较复杂的点。&lt;/p&gt;&lt;p&gt;我们之后又对 Explain 部分进行修改，使得能够打印能够反映我们现在加入外部数据源后算子真实的执行情况，可以见 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/planner/core/common_plans.go#L525&quot;&gt;explainPlanInRowFormat&lt;/a&gt; 部分代码。之后我们开始进行测试每个数据源上的，以及多个数据源融合起来进行测试。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;不足之处&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;我们很多物理计划都是复用 TiDB 本身的，给物理计划加上很多附属属性。其实最好是将这些物理计划单独抽取出来形成一个，不去复用。&lt;/li&gt;&lt;li&gt;Cost 没有进行细致考虑，例如对于 Join 下推，其实两张 100 万的表进行 Join 可能使得结果成为 1000 万，那么网络传输的代价反而更大了。这些具体算子下推的代价还需要细致的考虑。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;比较痛苦的经历&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;TiDB 不支持 Create Funtion，我们就只好写内置函数，就把另外一个 Parser 模块拖下来，自己修改加上语法，然后在加上自己设计的内置函数。&lt;/li&gt;&lt;li&gt;最痛苦还是两个方面，首先 Golang 语言，我们之前没有用得很多，经常遇到些小问题，例如 interface 的灵活使用等。其次就是涉及的 TiDB 的源码模块很多，从优化器、执行器、内置函数以及各种各样的结构。虽然思路很简单，但是改动的地方都很细节。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;收获&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3071d2d5a54975795c7008af298db4b7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;831&quot; data-rawheight=&quot;616&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3071d2d5a54975795c7008af298db4b7&quot; data-watermark-src=&quot;v2-5e8c67840493cada3362f35c86f6136a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;比赛过程中，看到了非常多优秀选手，以及他们酷炫的作业，感觉还是有很长的路要走。Hackathon 的选手都好厉害，听到大家噼里啪啦敲键盘的声音，似乎自己也不觉得有多累了。人啊，逼一下自己，会感受到自己无穷的力量。通过这次活动，我们最终能够灵活使用 Golang 语言，对 TiDB 整体也有了更深入的认识，希望自己以后能够称为 TiDB 的代码贡献者。&lt;/p&gt;&lt;p&gt;最后非常感谢 PingCAP 这次组织的 Hackathon 活动，感谢导师团、志愿者，以及还有特别感谢导师张建的指导。&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-14-52413872</guid>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十一）基于规则的优化 II</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-11-52138596.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52138596&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-180de02d6f7269fdf95e2294d895caf8_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;在 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;TiDB 源码阅读系列文章（七）基于规则的优化&lt;/a&gt; 一文中，我们介绍了几种 TiDB 中的逻辑优化规则，包括列剪裁，最大最小消除，投影消除，谓词下推和构建节点属性，本篇将继续介绍更多的优化规则：聚合消除、外连接消除和子查询优化。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;聚合消除&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;聚合消除会检查 SQL 查询中 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 语句所使用的列是否具有唯一性属性，如果满足，则会将执行计划中相应的 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 算子替换为 &lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt; 算子。这里的逻辑是当聚合函数按照具有唯一性属性的一列或多列分组时，下层算子输出的每一行都是一个单独的分组，这时就可以将聚合函数展开成具体的参数列或者包含参数列的普通函数表达式，具体的代码实现在 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_aggregation_elimination.go&quot;&gt;rule_aggregation_elimination.go&lt;/a&gt;&lt;/code&gt; 文件中。下面举一些具体的例子。&lt;/p&gt;&lt;p&gt;例一：&lt;/p&gt;&lt;p&gt;下面这个 Query 可以将聚合函数展开成列的查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select max(a) from t group by t.pk;&lt;/code&gt;&lt;p&gt;被等价地改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select a from t;&lt;/code&gt;&lt;p&gt;例二：&lt;/p&gt;&lt;p&gt;下面这个 Query 可以将聚合函数展开为包含参数列的内置函数的查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select count(a) from t group by t.pk;&lt;/code&gt;&lt;p&gt;被等价地改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select if(isnull(a), 0, 1) from t;&lt;/code&gt;&lt;p&gt;这里其实还可以做进一步的优化：如果列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt; 具有 &lt;code class=&quot;inline&quot;&gt;Not Null&lt;/code&gt; 的属性，那么可以将 &lt;code class=&quot;inline&quot;&gt;if(isnull(a), 0, 1)&lt;/code&gt; 直接替换为常量 1（目前 TiDB 还没做这个优化，感兴趣的同学可以来贡献一个 PR）。&lt;/p&gt;&lt;p&gt;另外提一点，对于大部分聚合函数，参数的类型和返回结果的类型一般是不同的，所以在展开聚合函数的时候一般会在参数列上构造 cast 函数做类型转换，展开后的表达式会保存在作为替换 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 算子的 &lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt; 算子中。&lt;/p&gt;&lt;p&gt;这个优化过程中，有一点非常关键，就是如何知道 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 使用的列是否满足唯一性属性，尤其是当聚合算子的下层节点不是 &lt;code class=&quot;inline&quot;&gt;DataSource&lt;/code&gt; 的时候？我们在 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;基于规则的优化&lt;/a&gt; 一文中的“构建节点属性”章节提到过，执行计划中每个算子节点会维护这样一个信息：当前算子的输出会按照哪一列或者哪几列满足唯一性属性。因此，在聚合消除中，我们可以通过查看下层算子保存的这个信息，再结合 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 用到的列判断当前聚合算子是否可以被消除。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;外连接消除&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;不同于 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;基于规则的优化&lt;/a&gt; 一文中“谓词下推”章节提到的将外连接转换为内连接，这里外连接消除指的是将整个连接操作从查询中移除。&lt;/p&gt;&lt;p&gt;外连接消除需要满足一定条件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;条件 1 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的父亲算子只会用到 &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的 outer plan 所输出的列&lt;/li&gt;&lt;li&gt;条件 2 :&lt;/li&gt;&lt;ul&gt;&lt;li&gt;条件 2.1 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 中的 join key 在 inner plan 的输出结果中满足唯一性属性&lt;/li&gt;&lt;li&gt;条件 2.2 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的父亲算子会对输入的记录去重&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;条件 1 和条件 2 必须同时满足，但条件 2.1 和条件 2.2 只需满足一条即可。&lt;/p&gt;&lt;p&gt;满足条件 1 和 条件 2.1 的一个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.a from t1 left join t2 on t1.b = t2.pk;&lt;/code&gt;&lt;p&gt;可以被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.a from t1;&lt;/code&gt;&lt;p&gt;满足条件 1 和条件 2.2 的一个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select distinct(t1.a) from t1 left join t2 on t1.b = t2.b;&lt;/code&gt;&lt;p&gt;可以被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select distinct(t1.a) from t1;&lt;/code&gt;&lt;p&gt;具体的原理是，对于外连接，outer plan 的每一行记录肯定会在连接的结果集里出现一次或多次，当 outer plan 的行不能找到匹配时，或者只能找到一行匹配时，这行 outer plan 的记录在连接结果中只出现一次；当 outer plan 的行能找到多行匹配时，它会在连接结果中出现多次；那么如果 inner plan 在 join key 上满足唯一性属性，就不可能存在 outer plan 的行能够找到多行匹配，所以这时 outer plan 的每一行都会且仅会在连接结果中出现一次。同时，上层算子只需要 outer plan 的数据，那么外连接可以直接从查询中被去除掉。同理就可以很容易理解当上层算子只需要 outer plan 的去重后结果时，外连接也可以被消除。&lt;/p&gt;&lt;p&gt;这部分优化的具体代码实现在 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_join_elimination.go&quot;&gt;rule_join_elimination.go&lt;/a&gt; 文件中。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;子查询优化 / 去相关&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;子查询分为非相关子查询和相关子查询，例如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;-- 非相关子查询
select * from t1 where t1.a &amp;gt; (select t2.a from t2 limit 1);
-- 相关子查询
select * from t1 where t1.a &amp;gt; (select t2.a from t2 where t2.b &amp;gt; t1.b limit 1);&lt;/code&gt;&lt;p&gt;对于非相关子查询， TiDB 会在 &lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt; 的逻辑中做两类操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;子查询展开&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;即直接执行子查询获得结果，再利用这个结果改写原本包含子查询的表达式；比如上述的非相关子查询，如果其返回的结果为一行记录 “1” ，那么整个查询会被改写为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a &amp;gt; 1;&lt;/code&gt;&lt;p&gt;详细的代码逻辑可以参考&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go&quot;&gt;expression_rewriter.go&lt;/a&gt;中的&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L685&quot;&gt;handleScalarSubquery&lt;/a&gt;和&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L535&quot;&gt;handleExistSubquery&lt;/a&gt;函数。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;子查询转为 Join&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于包含 IN (subquery) 的查询，比如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a in (select t2.a from t2);&lt;/code&gt;&lt;p&gt;会被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.* from t1 inner join (select distinct(t2.a) as a from t2) as sub on t1.a = sub.a;&lt;/code&gt;&lt;p&gt;如果 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; 满足唯一性属性，根据上面介绍的聚合消除规则，查询会被进一步改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.* from t1 inner join t2 on t1.a = t2.a;&lt;/code&gt;&lt;p&gt;这里选择将子查询转化为 inner join 的 inner plan 而不是执行子查询的原因是：以上述查询为例，子查询的结果集可能会很大，展开子查询需要一次性将 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 的全部数据从 TiKV 返回到 TiDB 中缓存，并作为 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 扫描的过滤条件；如果将子查询转化为 inner join 的 inner plan ，我们可以更灵活地对 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 选择访问方式，比如我们可以对 join 选择 &lt;code class=&quot;inline&quot;&gt;IndexLookUpJoin&lt;/code&gt; 实现方式，那么对于拿到的每一条 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表数据，我们只需拿 &lt;code class=&quot;inline&quot;&gt;t1.a&lt;/code&gt; 作为 range 对 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 做一次索引扫描，如果 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表很小，相比于展开子查询返回 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 全部数据，我们可能总共只需要从 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 返回很少的几条数据。&lt;/p&gt;&lt;p&gt;注意这个转换的结果不一定会比展开子查询更好，其具体情况会受 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表和 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表数据的影响，如果在上述查询中， &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt;表很大而 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表很小，那么展开子查询再对 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 选择索引扫描可能才是最好的方案，所以现在有参数控制这个转化是否打开，详细的代码可以参考 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go&quot;&gt;expression_rewriter.go&lt;/a&gt; 中的 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L596&quot;&gt;handleInSubquery&lt;/a&gt; 函数。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;相关子查询&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于相关子查询，TiDB 会在&lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt;中将整个包含相关子查询的表达式转化为&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;算子。&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;算子是一类特殊的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;，特殊之处体现在执行逻辑上：对于 outer plan 返回的每一行记录，取出相关列的具体值传递给子查询，再执行根据子查询生成的 inner plan ，即&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;在执行时只能选择类似 循环嵌套连接的方式，而普通的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;则可以在物理优化阶段根据代价模型选择最合适的执行方式，包括&lt;code class=&quot;inline&quot;&gt;HashJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;MergeJoin&lt;/code&gt;和&lt;code class=&quot;inline&quot;&gt;IndexLookUpJoin&lt;/code&gt;，理论上后者生成的物理执行计划一定会比前者更优，所以在逻辑优化阶段我们会检查是否可以应用“去相关”这一优化规则，试图将&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;转化为等价的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;。其核心思想是将&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的 inner plan 中包含相关列的那些算子提升到&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;之中或之上，在算子提升后如果 inner plan 中不再包含任何的相关列，即不再引用任何 outer plan 中的列，那么&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;就会被转换为普通的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;，这部分代码逻辑实现在&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_decorrelate.go&quot;&gt;rule_decorrelate.go&lt;/a&gt;文件中。&lt;/p&gt;&lt;p&gt;具体的算子提升方式分为以下几种情况：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;则将其过滤条件添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中，然后将该 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 从 inner plan 中删除，再递归地对 inner plan 提升算子。&lt;/p&gt;&lt;p&gt;以如下查询为例：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a in (select t2.a from t2 where t2.b = t1.b);&lt;/code&gt;&lt;p&gt;其生成的最初执行计划片段会是：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a5982ed5121da2abdc05167506e48e09_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;383&quot; data-rawheight=&quot;277&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a5982ed5121da2abdc05167506e48e09&quot; data-watermark-src=&quot;v2-8a2d9d63bd08f8d908fd93ad370a140d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 提升后会变成如下片段：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-62f3075f148145ff308de465a07ff53b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;404&quot; data-rawheight=&quot;191&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-62f3075f148145ff308de465a07ff53b&quot; data-watermark-src=&quot;v2-62cff68b480f307f9001d16bba429797&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;到此 inner plan 中不再包含相关列，于是 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 会被转换为如下 LogicalJoin ：&lt;/p&gt;&lt;u&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-22d5560a1d5a1513063c1083de52c887_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;418&quot; data-rawheight=&quot;202&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-22d5560a1d5a1513063c1083de52c887&quot; data-watermark-src=&quot;v2-7bb00d9abf4fffa883e0ca832215cc93&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;/u&gt;&lt;p&gt;&lt;b&gt;2.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt;&lt;/b&gt;&lt;br&gt;即要求子查询最多输出一行记录，比如这个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select t2.a from t2 where t2.pk = t1.a) from t1;&lt;/code&gt;&lt;p&gt;因为子查询出现在整个查询的投影项里，所以 &lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt; 在处理子查询时会对其生成的执行计划在根节点上加一个 &lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt; 限制最多产生一行记录，如果在执行时发现下层输出多于一行记录，则会报错。在这个例子中，子查询的过滤条件是 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表的主键上的等值条件，所以子查询肯定最多只会输出一行记录，而这个信息在“构建节点属性”这一步时会被发掘出来并记录在算子节点的 &lt;code class=&quot;inline&quot;&gt;MaxOneRow&lt;/code&gt; 属性中，所以这里的 &lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt; 节点实际上是冗余的，于是我们可以将其从 inner plan 中移除，然后再递归地对 inner plan 做算子提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;则首先将这个投影算子从 inner plan 中移除，再根据&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的连接类型判断是否需要在&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;之上再加上一个&lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt;，具体来说是：对于非 semi-join 这一类的连接（包括 inner join 和 left join ），inner plan 的输出列会保留在&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的结果中，所以这个投影操作需要保留，反之则不需要。最后，再递归地对删除投影后的 inner plan 提升下层算子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;a.&lt;/b&gt;首先我们会检查这个聚合算子是否可以被提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上再执行。以如下查询为例：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select sum(t2.b) from t2 where t2.a = t1.pk) from t1;&lt;/code&gt;&lt;p&gt;其最初生成的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3a476443a3930e657a52023f7ec82983_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;427&quot; data-rawheight=&quot;414&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3a476443a3930e657a52023f7ec82983&quot; data-watermark-src=&quot;v2-2f0771f0f6b99f56ef089d61c847b1e0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;将聚合提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 后的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bcbb359b6fc8771c984fe21351e599bc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;385&quot; data-rawheight=&quot;331&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bcbb359b6fc8771c984fe21351e599bc&quot; data-watermark-src=&quot;v2-bab6573026a3946820f9e4b66b465ead&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;即先对 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 做连接，再在连接结果上按照 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 分组后做聚合。这里有两个关键变化：第一是不管提升前 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型是 inner join 还是 left join ，提升后必须被改为 left join ；第二是提升后的聚合新增了 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 的列，即要按照 outer plan 传进 inner plan 中的相关列做分组。这两个变化背后的原因都会在后面进行阐述。因为提升后 inner plan 不再包含相关列，去相关后最终生成的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3af11844da7786a930af425723d7ec90_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;383&quot; data-rawheight=&quot;343&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3af11844da7786a930af425723d7ec90&quot; data-watermark-src=&quot;v2-c2e2c8792666624bcfe5ae7b5c39813c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;聚合提升有很多限定条件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型必须是 inner join 或者 left join 。 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 是根据相关子查询生成的，只可能有 3 类连接类型，除了 inner join 和 left join 外，第三类是 semi join （包括 &lt;code class=&quot;inline&quot;&gt;SemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;LeftOuterSemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;AntiSemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;AntiLeftOuterSemiJoin&lt;/code&gt;），具体可以参考 &lt;code class=&quot;inline&quot;&gt;expression_rewriter.go&lt;/code&gt; 中的代码，限于篇幅在这里就不对此做展开了。对于 semi join 类型的 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; ，因为 inner plan 的输出列不会出现在连接的结果中，所以很容易理解我们无法将聚合算子提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 本身不能包含 join condition 。以上面给出的查询为例，可以看到聚合提升后会将子查询中包含相关列的过滤条件 (&lt;code class=&quot;inline&quot;&gt;t2.a = t1.pk&lt;/code&gt;) 添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中，如果 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 本身存在 join condition ，那么聚合提升后聚合算子的输入（连接算子的输出）就会和在子查询中时聚合算子的输入不同，导致聚合算子结果不正确。&lt;/li&gt;&lt;li&gt;子查询中用到的相关列在 outer plan 输出里具有唯一性属性。以上面查询为例，如果 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 不满足唯一性，假设 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 有两条记录满足 &lt;code class=&quot;inline&quot;&gt;t1.pk = 1&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 只有一条记录 &lt;code class=&quot;inline&quot;&gt;{ (t2.a: 1, t2.b: 2) }&lt;/code&gt; ，那么该查询会输出两行结果 &lt;code class=&quot;inline&quot;&gt;{ (sum(t2.b): 2), (sum(t2.b): 2) }&lt;/code&gt; ；但对于聚合提升后的执行计划，则会生成错误的一行结果&lt;code class=&quot;inline&quot;&gt;{ (sum(t2.b): 4) }&lt;/code&gt;。当 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 满足唯一性后，每一行 outer plan 的记录都对应连接结果中的一个分组，所以其聚合结果会和在子查询中的聚合结果一致，这也解释了为什么聚合提升后需要按照 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 做分组。&lt;/li&gt;&lt;li&gt;聚合函数必须满足当输入为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 时输出结果也一定是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 。这是为了在子查询中没有匹配的特殊情况下保证结果的正确性，以上面查询为例，当 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表没有任何记录满足 &lt;code class=&quot;inline&quot;&gt;t2.a = t1.pk&lt;/code&gt; 时，子查询中不管是什么聚合函数都会返回 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 结果，为了保留这种特殊情况，在聚合提升的同时， &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型会被强制改为 left join（改之前可能是 inner join ），所以在这种没有匹配的情况下，&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 输出结果中 inner plan 部分会是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; ，而这个 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 会作为新添加的聚合算子的输入，为了和提升前结果一致，其结果也必须是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;b.&lt;/b&gt;对于根据上述条件判定不能提升的聚合算子，我们再检查这个聚合算子的子节点是否为 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; ，如果是，则将其从 inner plan 中移除并将过滤条件添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中。这种情况下 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 依然会被保留在 inner plan 中，但会将 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 过滤条件中涉及的 inner 表的列添加到聚合算子的 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt;中。比如对于查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select count(*) from t2 where t2.a = t1.a) from t1;&lt;/code&gt;&lt;p&gt;其生成的最初的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7036cbfd4a1bd928df9b68a82ae2c39d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;404&quot; data-rawheight=&quot;402&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7036cbfd4a1bd928df9b68a82ae2c39d&quot; data-watermark-src=&quot;v2-1436664d60cbe58ed392024ac79a0d9d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;因为聚合函数是 &lt;code class=&quot;inline&quot;&gt;count(*)&lt;/code&gt; ，不满足当输入为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 时输出也为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 的条件，所以它不能被提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上，但它可以被改写成：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fff11f20b805a30654aa08f8351c1c3e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;429&quot; data-rawheight=&quot;306&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fff11f20b805a30654aa08f8351c1c3e&quot; data-watermark-src=&quot;v2-59380836f03d5ed5d926d7465b523973&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;注意 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 新加了 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; ，这一步将原本的先做过滤再做聚合转换为了先按照 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; 分组做聚合，再将聚合结果与 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 做连接。 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 提升后 inner plan 已经不再依赖 outer plan 的结果了，整个查询去相关后将会变为：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eca50e9e7bf56e429defc6cf6a266a4c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;449&quot; data-rawheight=&quot;325&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-eca50e9e7bf56e429defc6cf6a266a4c&quot; data-watermark-src=&quot;v2-0d186aabdcfc7d5ecfb758a6cb92d7d3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这是基于规则优化的第二篇文章，后续我们还将介绍更多逻辑优化规则：聚合下推，TopN 下推和 Join Reorder 。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;更多 TiDB 源码阅读系列文章：&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/blog-cn/#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-11-52138596</guid>
<pubDate>Tue, 11 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在量化派风控系统中的应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-10-52046270.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52046270&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ee0165e5af868a1f8bcd156e502cad0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;&lt;b&gt;朱劲松&lt;/b&gt;，量化派研发中心系统架构师，主要参与了基础组件开发、API Gateway 等项目，现在致力于公司风控系统相关业务的架构设计和研发。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;公司简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;量化派（QuantGroup）创办于 2014 年，是数据驱动的科技公司，是国家高新技术企业。量化派以「MOVE THE WORLD WITH DATA, ENLIGHTEN LIFE WITH AI」（数据驱动世界，智能点亮生活）为愿景，利用人工智能、机器学习、大数据技术。为金融、电商、旅游、出行、汽车供应链等多个领域的合作伙伴提供定制化的策略和模型，帮助提升行业效率。量化派已与国内外超过 300 家机构和公司达成深度合作，致力于打造更加有活力的共赢生态，推动经济的可持续发展。&lt;/p&gt;&lt;p&gt;我司从 2017 年年中开始调研 TiDB，并在用户行为数据分析系统中搭建 TiDB 集群进行数据存储，经过一年多的应用和研究，积累了丰富的经验。同时，TiDB 官方推出 2.0 GA 版本，TiDB 愈发成熟，稳定性和查询效率等方面都有很大提升。我们于 2018 年 7 月部署 TiDB 2.0.5 版本，尝试将其应用于风控业务中。风控系统主要是在用户申请放款时，根据风控规则结合模型和用户特征进行实时计算并返回放款结果。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;业务背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;风控系统中用到的数据主要可以分为两部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一类是原始数据，用于分析用户当前的特征指标。&lt;/li&gt;&lt;li&gt;一类是快照数据，用于计算历史指定时间点的特征指标，供模型训练使用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原始数据主要分为三种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;产生自公司内各个产品线的业务系统数据&lt;/li&gt;&lt;li&gt;爬虫组提供的用户联系人、运营商、消费记录等数据&lt;/li&gt;&lt;li&gt;经过处理后的用户特征数据&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于我们的风控策略中用到了大量的模型，包括神经网络模型，评分模型等，这些模型的训练需要依靠大量的历史订单以及相关的用户特征，为了训练出更多精准、优秀的模型，就需要更多维度的特征，此时特征的准确性就直接影响了模型的训练结果，为此我们在回溯每一个订单的用户在指定时间的特征表现时，就需要用到数据快照。&lt;/p&gt;&lt;p&gt;我们可以通过拉链表的方式来实现数据快照功能，简单说就是在每张表中增加三个字段，分别是 new_id、start_time、end_time，每一次记录的更新都会产生一条新的数据，同时变更原有记录的 end_time，以记录数据的变更历史。&lt;/p&gt;&lt;p&gt;通过上面的介绍可以看到，业务数据和爬虫数据本身数据量就很大，再加上需要产生对应的拉链数据，数据量更是成倍增长。假设每条数据自创建后仅变更一次，那拉链表的数据量就已经是原始表的两倍了，而实际生产环境下数据的变更远不止一次。&lt;/p&gt;&lt;p&gt;通过上述的介绍，我们总结风控系统下的数据存储需求应满足以下几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;业务数据&lt;/li&gt;&lt;li&gt;业务数据拉链表&lt;/li&gt;&lt;li&gt;爬虫数据，如联系人信息、运营商数据，消费记录等&lt;/li&gt;&lt;li&gt;爬虫数据拉链表&lt;/li&gt;&lt;li&gt;其他数据，如预处理数据等&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;当前方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;以前方案主要是采用 HBase 进行数据存储。它的水平扩展很好的解决了数据量大的问题。但是在实际使用中，也存在着比较明显的问题，最明显的就是查询的 API 功能性较弱，只能通过 Key 来获取单条数据，或是通过 Scan API 来批量读取，这无疑在特征回溯时增加了额外的开发成本，无法实现代码复用。&lt;/p&gt;&lt;p&gt;在实时计算场景中，为了降低开发成本，对于业务数据的获取则是通过访问线上系统的 MySQL 从库来进行查询；爬虫数据由于统一存放在 HBase 中，计算时需要将用到的数据全量拉取在内存中再进行计算。&lt;/p&gt;&lt;p&gt;在回溯场景中，针对业务特征回溯，通过查询订单时间之前的数据进行特征计算，这种方式对于已经变更的数据是无能为力的，只能通过 HBase 里的数据快照来实现，但无形增加了很多的开发工作。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;TiDB 为我们打开一片新视野&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过上面的介绍，我们知道要构建一个风控系统的实时数仓环境，需要满足下面几个特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高可用，提供健壮、稳定的服务。&lt;/li&gt;&lt;li&gt;支持水平弹性扩展，满足日益增长的数据需求。&lt;/li&gt;&lt;li&gt;性能好，支持高并发。&lt;/li&gt;&lt;li&gt;响应快。&lt;/li&gt;&lt;li&gt;支持标准 SQL，最好是 MySQL 语法和 MySQL 协议，避免回溯时的额外开发。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;可以发现，TiDB 完美契合我们的每个需求。经过 TiDB 在用户行为数据分析系统中的长期使用，我们已经积累了一定的经验，在此过程中 TiDB 官方也给予了长期的技术支持，遇到的问题在沟通时也能够及时的反馈，而且还与我司技术人员进行过多次技术交流及线下分享，在此我们深表感谢。伴随着风控系统需求的持续增长，我们对整体架构进行了新一轮的优化，新的数据接入及存储架构如图 1。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a717388d8270181928386125b5163963_r.jpg&quot; data-caption=&quot;图 1  优化后的架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;545&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a717388d8270181928386125b5163963&quot; data-watermark-src=&quot;v2-8e925539beecb0d3c99975e92584a69b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;通过图 1 可以看到，线上业务系统产生的数据统一存放在 MySQL 中，将这些孤立的数据归集在 TiDB 中，能够提供基于 SQL 的查询服务。通过 binlog 的方式直接从 MySQL 实例进行接入，接入后的数据以两种不同的形式分别存放：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一种是去分库分表后的源数据，降低了实时特征计算的实现及维护成本。&lt;/li&gt;&lt;li&gt;另一种是以拉链数据形式存储实现数据快照功能。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;经过调研，针对第一种场景，可以通过阿里的 otter 或者 TiDB 周边工具 Syncer 来快速实现，但对于第二个需求都没有现成的成熟解决方案。最终，我们基于阿里的 canal 进行客户端的定制化开发，分别按照不同的需求拼装合并 SQL 并写入到不同的 TiDB 集群中；同时还可以按需将部分表的数据进行组装并发送至 Kafka，用于准实时分析场景。&lt;/p&gt;&lt;p&gt;对于来自爬虫组的数据，我们采用直接消费 Kafka 的方式组装 SQL 写入到 TiDB 即可。&lt;/p&gt;&lt;p&gt;在实际是使用中，通过索引等优化，TiDB 完全可以支持线上实时查询的业务需求；在特征回溯时只需要通过增加查询条件就可以获得指定时间的特征结果，大大降低了开发成本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;遇到的问题&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;风控业务中用户特征提取的 SQL 相对都比较复杂，在实际使用中，存在部分 SQL 执行时间比在 MySQL 中耗时高。通过 explain 我们发现，他并没有使用我们创建的索引，而是进行了全表扫描，在进一步分析后还发现 explain 的结果是不确定的。&lt;/p&gt;&lt;p&gt;经过与 TiDB 官方技术人员的沟通，我们进行了删除类似索引、analyze table 等操作，发现问题仍然存在。通过图 2 可以看到完全相同的 SQL 语句，其执行结果的差异性。最后按官方建议，我们采用添加 use index 的方式使其强制走索引，执行时间由 4 分钟变成了 &amp;lt; 1s，暂时解决了业务上的需求。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b456777e7bdb0d4610a70c40aab191d8_r.jpg&quot; data-caption=&quot;图 2  explain 示意图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;408&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b456777e7bdb0d4610a70c40aab191d8&quot; data-watermark-src=&quot;v2-7a247e9dec4e04853609cf215ab3dcc1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;同时 TiDB 技术人员也收集相关信息反馈给了研发人员。在整个问题的处理过程中，TiDB 的技术人员给予了高度的配合和及时的反馈，同时也表现出了很强的专业性，大大减少了问题排查的时间，我们非常感谢。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;目前我们已经搭建两个 TiDB 集群，几十个物理节点，百亿级特征数据，受益于 TiDB 的高可用构架，上线以来一直稳定运行。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上，TiDB 在我们风控业务中的应用才只是开始，部分业务的迁移还有待进一步验证，但是 TiDB 给我们带来的好处不言而喻，为我们在数据存储和数据分析上打开了一片新视野。后续我们会继续加大对 TiDB 的投入，使其更好地服务于在线分析和离线分析等各个场景。我们也希望进一步增加与 PingCAP 团队的交流与合作，进行更深入的应用和研究，为 TiDB 的发展贡献一份力量。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多 TiDB 用户实践：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-10-52046270</guid>
<pubDate>Mon, 10 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB EcoSystem Tools 原理解读（一）：TiDB-Binlog 架构演进与实现原理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-10-51990591.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51990591&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f70e162f6471c2085dcc5b1d71405c89_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Binlog 组件用于收集 TiDB 的 binlog，并提供实时备份和同步功能。该组件在功能上类似于 MySQL 的主从复制，MySQL 的主从复制依赖于记录的 binlog 文件，TiDB-Binlog 组件也是如此，主要的不同点是 TiDB 是分布式的，因此需要收集各个 TiDB 实例产生的 binlog，并按照事务提交的时间排序后才能同步到下游。如果你需要部署 TiDB 集群的从库，或者想订阅 TiDB 数据的变更输出到其他的系统中，TiDB-Binlog 则是必不可少的工具。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;架构演进&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Binlog 这个组件已经发布了 2 年多时间，经历过几次架构演进，去年十月到现在大规模使用的是 Kafka 版本，架构图如下：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-df721303b9040c503599496dcf52d16b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;333&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-df721303b9040c503599496dcf52d16b&quot; data-watermark-src=&quot;v2-5119818836a0f90f06478a754c1d4d02&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Kafka 版本的 TiDB-Binlog 主要包括两个组件：&lt;/p&gt;&lt;p&gt;Pump：一个守护进程，在每个 TiDB 主机的后台运行。其主要功能是实时记录 TiDB 产生的 binlog 并顺序写入 Kafka 中。&lt;/p&gt;&lt;p&gt;Drainer： 从 Kafka 中收集 binlog，并按照 TiDB 中事务的提交顺序转化为指定数据库兼容的 SQL 语句或者指定格式的数据，最后同步到目的数据库或者写到顺序文件。&lt;/p&gt;&lt;p&gt;这个架构的工作原理为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 需要与 Pump 绑定，即 TiDB 实例只能将它生成的 binlog 发送到一个指定的 Pump 中；&lt;/li&gt;&lt;li&gt;Pump 将 binlog 先写到本地文件，再异步地写入到 Kafka；&lt;/li&gt;&lt;li&gt;Drainer 从 Kafka 中读出 binlog，对 binlog 进行排序，对 binlog 解析后生成 SQL 或指定格式的数据再同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;根据用户的反馈，以及我们自己做的一些测试，发现该版本主要存在一些问题。&lt;/p&gt;&lt;p&gt;首先，TiDB 的负载可能不均衡，部分 TiDB 业务较多，产生的 binlog 也比较多，对应的 Pump 的负载高，导致数据同步延迟高。&lt;/p&gt;&lt;p&gt;其次，依赖 Kafka 集群，增加了运维成本；而且 TiDB 产生的单条 binlog 的大小可达 2G（例如批量删除数据、批量写入数据），需要配置 Kafka 的消息大小相关设置，而 Kafka 并不太适合单条数据较大的场景。&lt;/p&gt;&lt;p&gt;最后，Drainer 需要读取 Kafka 中的 binlog、对 binlog 进行排序、解析 binlog，同步数据到下游等工作，可以看出 Drainer 的工作较多，而且 Drainer 是一个单点，所以往往同步数据的瓶颈都在 Drainer。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上这些问题我们很难在已有的框架下进行优化，因此我们对 TiDB-Binlog 进行了重构，最新版本的 TiDB-Binlog 的总体架构如下图所示：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6e5e2bb0245f985f9f7f3b53b2f605c9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;391&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6e5e2bb0245f985f9f7f3b53b2f605c9&quot; data-watermark-src=&quot;v2-742a86727627f4e4ec5e5e9f68583501&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;新版本 TiDB-Binlog 不再使用 Kafka 存储 binlog，仍然保留了 Pump 和 Drainer 两个组件，但是对功能进行了调整：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pump 用于实时记录 TiDB 产生的 binlog，并将 binlog 按照事务的提交时间进行排序，再提供给 Drainer 进行消费。&lt;/li&gt;&lt;li&gt;Drainer 从各个 Pump 中收集 binlog 进行归并，再将 binlog 转化成 SQL 或者指定格式的数据，最终同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该版本的主要优点为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;多个 Pump 形成一个集群，可以水平扩容，各个 Pump 可以均匀地承担业务的压力。&lt;/li&gt;&lt;li&gt;TiDB 通过内置的 Pump Client 将 binlog 分发到各个 Pump，即使有部分 Pump 出现故障也不影响 TiDB 的业务。&lt;/li&gt;&lt;li&gt;Pump 内部实现了简单的 kv 来存储 binlog，方便对 binlog 数据的管理。&lt;/li&gt;&lt;li&gt;原来 Drainer 的 binlog 排序逻辑移到了 Pump 来做，而 Pump 是可扩展的，这样就能提高整体的同步性能。&lt;/li&gt;&lt;li&gt;Drainer 不再需要像原来一样读取一批 binlog 到内存里进行堆排序，只需要依次读取各个 Pump 的 binlog 进行归并排序，这样可以大大节省内存的使用，同时也更容易做内存控制。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;由于该版本最大的特点是多个 Pump 组成了一个集群（cluster），因此该版本命名为 cluster 版本。下面我们以最新的 cluster 版本的架构来介绍 TiDB-Binlog 的实现原理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;工作原理&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;binlog&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;首先我们先介绍一下 TiDB 中的 binlog，TiDB 的事务采用 2pc 算法，一个成功的事务会写两条 binlog，包括一条 Prewrite binlog 和 一条 Commit binlog；如果事务失败，会发一条 Rollback binlog。&lt;/p&gt;&lt;p&gt;binlog 的结构定义为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// Binlog 记录事务中所有的变更，可以用 Binlog 构建 SQL
message Binlog {
    // Binlog 的类型，包括 Prewrite、Commit、Rollback 等
    optional BinlogType  tp = 1 [(gogoproto.nullable) = false];
    
    // Prewrite, Commit 和 Rollback 类型的 binlog 的 start_ts，记录事务开始的 ts
    optional int64  start_ts = 2 [(gogoproto.nullable) = false];
    
    // commit_ts 记录事务结束的 ts，只记录在 commit 类型的 binlog 中
    optional int64  commit_ts = 3 [(gogoproto.nullable) = false];
    
    // prewrite key 只记录在 Prewrite 类型的 binlog 中，
    // 是一个事务的主键，用于查询该事务是否提交
    optional bytes  prewrite_key = 4;
    
    // prewrite_value 记录在 Prewrite 类型的 binlog 中，用于记录每一行数据的改变
    optional bytes  prewrite_value = 5;
    
    // ddl_query 记录 ddl 语句
    optional bytes  ddl_query = 6;
    
    // ddl_job_id 记录 ddl 的 job id
    optional int64  ddl_job_id  = 7 [(gogoproto.nullable) = false];
}&lt;/code&gt;&lt;p&gt;binlog 及相关的数据结构定义见:&lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/binlog.proto&quot;&gt;binlog.proto&lt;/a&gt;&lt;/p&gt;&lt;p&gt;其中 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 为事务开始时的 ts，&lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 为事务提交的 ts。ts 是由物理时间和逻辑时间转化而成的，在 TiDB 中是唯一的，由 PD 来统一提供。在开始一个事务时，TiDB 会请求 PD，获取一个 ts 作为事务的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt;，在事务提交时则再次请求 PD 获取一个 ts 作为 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;。 我们在 Pump 和 Drainer 中就是根据 binlog 的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 来对 binlog 进行排序的。&lt;/p&gt;&lt;p&gt;TiDB 的 binlog 记录为 row 模式，即保存每一行数据的改变。数据的变化记录在  &lt;code class=&quot;inline&quot;&gt;prewrite_value&lt;/code&gt; 字段中，该字段的数据主要由序列化后的 TableMutation 结构的数据组成。TableMutation 的结构如下所示：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// TableMutation 存储表中数据的变化
message TableMutation {
    // 表的 id，唯一标识一个表
    optional int64 table_id = 1 [(gogoproto.nullable) = false];
    
    // 保存插入的每行数据
    repeated bytes inserted_rows = 2;
    
    // 保存修改前和修改后的每行的数据
    repeated bytes updated_rows = 3;
    
    // 已废弃
    repeated int64 deleted_ids = 4;
    
    // 已废弃
    repeated bytes deleted_pks = 5;
    
    // 删除行的数据
    repeated bytes deleted_rows  = 6;
    
    // 记录数据变更的顺序
    repeated MutationType sequence = 7;
}&lt;/code&gt;&lt;p&gt;下面以一个例子来说明 binlog 中是怎么存储数据的变化的。&lt;/p&gt;&lt;p&gt;例如 table 的结构为：&lt;/p&gt;&lt;p&gt;create table &lt;code class=&quot;inline&quot;&gt;test&lt;/code&gt; (&lt;code class=&quot;inline&quot;&gt;id&lt;/code&gt; int, &lt;code class=&quot;inline&quot;&gt;name&lt;/code&gt; varchar(24), primary key &lt;code class=&quot;inline&quot;&gt;id&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;按照顺序执行如下 SQL：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;begin;
insert into test(id, name) values(1, &quot;a&quot;);
insert into test(id, name) values(2, &quot;b&quot;);
update test set name = &quot;c&quot; where id = 1;
update test set name = &quot;d&quot; where id = 2;
delete from test where id = 2;
insert into test(id, name) values(2, &quot;c&quot;);
commit;&lt;/code&gt;&lt;p&gt;则生成的 TableMutation 的数据如下所示：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;inserted_rows:
1, &quot;a&quot;
2, &quot;b&quot;
2, &quot;c&quot;
 
updated_rows:
1, &quot;a&quot;, 1, &quot;c&quot;
2, &quot;b&quot;, 2, &quot;d&quot;
 
deleted_rows:
2, &quot;d&quot;
 
sequence:
Insert, Insert, Update, Update, DeleteRow, Insert&lt;/code&gt;&lt;p&gt;可以从例子中看出，sequence 中保存的数据变更类型的顺序为执行 SQL 的顺序，具体变更的数据内容则保存到了相应的变量中。&lt;/p&gt;&lt;p&gt;Drainer 在把 binlog 数据同步到下游前，就需要把上面的这些数据还原成 SQL，再同步到下游。&lt;/p&gt;&lt;p&gt;另外需要说明的是，TiDB 在写 binlog 时，会同时向 TiKV 发起写数据请求和向 Pump 发送 Prewrite binlog，如果 TiKV 和 Pump 其中一个请求失败，则该事务失败。当 Prewrite 成功后，TiDB 向 TiKV 发起 Commit 消息，并异步地向 Pump 发送一条 Commit binlog。由于 TiDB 是同时向 TiKV 和 Pump 发送请求的，所以只要保证 Pump 处理 Prewrite binlog 请求的时间小于等于 TiKV 执行 Prewrite 的时间，开启 binlog 就不会对事务的延迟造成影响。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Pump Client&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从上面的介绍中我们知道由多个 Pump 组成一个集群，共同承担写 binlog 的请求，那么就需要保证 TiDB 能够将写 binlog 的请求尽可能均匀地分发到各个 Pump，并且需要识别不可用的 Pump，及时获取到新加入集群中 Pump 信息。这部分的工作是在 Pump Client 中实现的。&lt;/p&gt;&lt;p&gt;Pump Client 以包的形式集成在 TiDB 中，代码链接：&lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/v2.1.0/tidb-binlog/pump_client&quot;&gt;pump_client&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Pump Client 维护 Pump 集群的信息，Pump 的信息主要来自于 PD 中保存的 Pump 的状态信息，状态信息的定义如下（代码链接：&lt;a href=&quot;https://github.com/pingcap/tidb-tools/blob/v2.1.0/tidb-binlog/node/node.go&quot;&gt;Status&lt;/a&gt;）：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;type Status struct {
    // Pump/Drainer 实例的唯一标识
    NodeID string `json:&quot;nodeId&quot;`
    
    // Pump/Drainer 的服务地址
    Addr string `json:&quot;host&quot;`
    
    // Pump/Drainer 的状态，值可以为 online、pausing、paused、closing、offline
    State string `json:&quot;state&quot;`
    
    // Pump/Drainer 是否 alive（目前没有使用该字段）
    IsAlive bool `json:&quot;isAlive&quot;`
    
    // Pump的分数，该分数是由节点的负载、磁盘使用率、存储的数据量大小等因素计算得来的，
    // 这样 Pump Client 可以根据分数来选取合适的 Pump 发送 binlog（待实现）
    Score int64 `json:&quot;score&quot;`
    
    // Pump 的标签，可以通过 label 对 TiDB 和 Pump 进行分组，
    // TiDB 只能将 binlog 发送到相同 label 的 Pump（待实现）
    Label *Label `json:&quot;label&quot;`
    
    // Pump： 保存的 binlog 的最大的 commit_ts
    // Drainer：已消费的 binlog 的最大的 commit_ts
    MaxCommitTS int64 `json:&quot;maxCommitTS&quot;`
    
    // 该状态信息的更新时间对应的 ts.
    UpdateTS int64 `json:&quot;updateTS&quot;`
}&lt;/code&gt;&lt;p&gt;Pump Client 根据 Pump 上报到 PD 的信息以及写 binlog 请求的实际情况将 Pump 划分为可用 Pump 与不可用 Pump 两个部分。&lt;/p&gt;&lt;p&gt;划分的方法包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;初始化时从 PD 中获取所有 Pump 的信息，将状态为 online 的 Pump 加入到可用 Pump 列表中，其他 Pump 加入到非可用列表中。&lt;/li&gt;&lt;li&gt;Pump 每隔固定的时间会发送心跳到 PD，并更新自己的状态。Pump Client 监控 PD 中 Pump 上传的状态信息，及时更新内存中维护的 Pump 信息，如果状态由非 online 转换为 online 则将该 Pump 加入到可用 Pump 列表；反之加入到非可用列表中。&lt;/li&gt;&lt;li&gt;在写 binlog 到 Pump 时，如果该 Pump 在重试多次后仍然写 binlog 失败，则把该 Pump 加入到非可用 Pump 列表中。&lt;/li&gt;&lt;li&gt;定时发送探活请求（数据为空的 binlog 写请求）到非可用 Pump 列表中的状态为 online 的 Pump，如果返回成功，则把该 Pump 重新加入到可用 Pump 列表中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过上面的这些措施，Pump Client 就可以及时地更新所维护的 Pump 集群信息，保证将 binlog 发送到可用的 Pump 中。&lt;/p&gt;&lt;p&gt;另外一个问题是，怎么保证 Pump Client 可以将 binlog 写请求均匀地分发到各个 Pump？我们目前提供了几种路由策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;range： 按照顺序依次选取 Pump 发送 binlog，即第一次选取第一个 Pump，第二次选取第二个 Pump…&lt;/li&gt;&lt;li&gt;hash：对 binlog 的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 进行 hash，然后选取 hash 值对应的 Pump。&lt;/li&gt;&lt;li&gt;score：根据 Pump 上报的分数按照加权平均算法选取 Pump 发送 binlog（待实现）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;需要注意的地方是，以上的策略只是针对 Prewrite binlog，对于 Commit binlog，Pump Client 会将它发送到对应的 Prewrite binlog 所选择的 Pump，这样做是因为在 Pump 中需要将包含 Prewrite binlog 和 Commit binlog 的完整 binlog（即执行成功的事务的 binlog）提供给 Drainer，将 Commit binlog 发送到其他 Pump 没有意义。&lt;/p&gt;&lt;p&gt;Pump Client 向 Pump 提交写 binlog 的请求接口为 &lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/pump.proto&quot;&gt;pump.proto&lt;/a&gt; 中的 WriteBinlog，使用 grpc 发送 binlog 请求。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Pump&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Pump 主要用来承担 binlog 的写请求，维护 binlog 数据，并将有序的 binlog 提供给 Drainer。我们将 Pump 抽象成了一个简单的 kv 数据库，key 为 binlog 的 &lt;code class=&quot;inline&quot;&gt;start _ts&lt;/code&gt;（Priwrite binlog） 或者 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;（Commit binlog），value 为 binlog 的元数据，binlog 的数据则存在数据文件中。Drainer 像查数据库一样的来获取所需要的 binlog。&lt;/p&gt;&lt;p&gt;Pump 内置了 leveldb 用于存储 binlog 的元信息。在 Pump 收到 binlog 的写请求时，会首先将 binlog 数据以 append 的形式写到文件中，然后将 binlog 的 ts、类型、数据长度、所保存的文件以及在文件中的位置信息保存在 leveldb 中，如果为 Prewrite binlog，则以 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt;作为 key；如果是 Commit binlog，则以 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 作为 key。&lt;/p&gt;&lt;p&gt;当 Drainer 向 Pump 请求获取指定 ts 之后的 binlog 时，Pump 则查询 leveldb 中大于该 ts 的 binlog 的元数据，如果当前数据为 Prewrite binlog，则必须找到对应的 Commit binlog；如果为 Commit binlog 则继续向前推进。这里有个问题，在 binlog 一节中提到，如果 TiKV 成功写入了数据，并且 Pump 成功接收到了 Prewrite binlog，则该事务就提交成功了，那么如果在 TiDB 发送 Commit binlog 到 Pump 前发生了一些异常（例如 TiDB 异常退出，或者强制终止了 TiDB 进程），导致 Pump 没有接收到 Commit binlog，那么 Pump 中就会一直找不到某些 Prewrite binlog 对应的 Commit binlog。这里我们在 Pump 中做了处理，如果某个 Prewrite binlog 超过了十分钟都没有找到对应的 Commit binlog，则通过 binlog 数据中的 &lt;code class=&quot;inline&quot;&gt;prewrite_key&lt;/code&gt; 去查询 TiKV 该事务是否提交，如果已经提交成功，则 TiKV 会返回该事务的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;；否则 Pump 就丢弃该条 Prewrite binlog。&lt;/p&gt;&lt;p&gt;binlog 元数据中提供了数据存储的文件和位置，可以通过这些信息读取 binlog 文件的指定位置获取到数据。因为 binlog 数据基本上是按顺序写入到文件中的，因此我们只需要顺序地读 binlog 文件即可，这样就保证了不会因为频繁地读取文件而影响 Pump 的性能。最终，Pump 以 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 为排序标准将 binlog 数据传输给 Drainer。Drainer 向 Pump 请求 binlog 数据的接口为 &lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/pump.proto&quot;&gt;pump.proto&lt;/a&gt; 中的 PullBinlogs，以 grpc streaming 的形式传输 binlog 数据。&lt;/p&gt;&lt;p&gt;值得一提的是，Pump 中有一个 fake binlog 机制。Pump 会定时（默认三秒）向本地存储中写入一条数据为空的 binlog，在生成该 binlog 前，会向 PD 中获取一个 ts，作为该 binlog 的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 与 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;，这种 binlog 我们叫作 fake binlog。这样做的原因在 Drainer 中介绍。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Drainer&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Drainer 从各个 Pump 中获取 binlog，归并后按照顺序解析 binlog、生成 SQL 或者指定格式的数据，然后再同步到下游。&lt;/p&gt;&lt;p&gt;既然要从各个 Pump 获取数据，Drainer 就需要维护 Pump 集群的信息，及时获取到新增加的 Pump，并识别出不可用的 Pump，这部分功能与 Pump Client 类似，Drainer 也是通过 PD 中存储的 Pump 的状态信息来维护 Pump 信息。另外需要注意的是，如果新增加了一个 Pump，必须让该 Pump 通知 Drainer 自己上线了，这么做是为了保证不会丢数据。例如：&lt;/p&gt;&lt;p&gt;集群中已经存在 Pump1 和 Pump2，Drainer 读取 Pump1 和 Pump2 的数据并进行归并：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3cd607d2bdbee944445902c45d40e64c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;752&quot; data-rawheight=&quot;499&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3cd607d2bdbee944445902c45d40e64c&quot; data-watermark-src=&quot;v2-1988b173b06ddd67d5b8a6367b3ba589&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Pump1 存储的 binlog 为｛ 1，3，5，7，9 ｝，Pump2 存储的 binlog 为｛2，4，6，10｝。Drainer 从两个 Pump 获取 binlog，假设当前已经读取到了｛1，2，3，4，5，6，7｝这些 binlog，已处理的 binlog 的位置为 7。此时 Pump3 加入集群，从 Pump3 上报自己的上线信息到 PD，到 Drainer 从 PD 中获取到 Pump3 信息需要一定的时间，如果 Pump3 没有通知 Drainer 就直接提供写 binlog 服务，写入了 binlog｛8，12｝，Drainer 在此期间继续读取 Pump1 和 Pump2 的 binlog，假设读取到了 9，之后才识别到了 Pump3 并将 Pump3 加入到归并排序中，此时 Pump3 的 binlog 8 就丢失了。为了避免这种情况，需要让 Pump3 通知 Drainer 自己已经上线，Drainer 收到通知后将 Pump3 加入到归并排序，并返回成功给 Pump3，然后 Pump3 才能提供写 binlog 的服务。&lt;/p&gt;&lt;p&gt;Drainer 通过如上所示的方式对 binlog 进行归并排序，并推进同步的位置。那么可能会存在这种情况：某个 Pump 由于一些特殊的原因一直没有收到 binlog 数据，那么 Drainer 中的归并排序就无法继续下去，正如我们用两条腿走路，其中一只腿不动就不能继续前进。我们使用 Pump 一节中提到的 fake binlog 的机制来避免这种问题，Pump 每隔指定的时间就生成一条 fake binlog，即使某些 Pump 一直没有数据写入，也可以保证归并排序正常向前推进。&lt;/p&gt;&lt;p&gt;Drainer 将所有 Pump 的数据按照 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 进行归并排序后，将 binlog 数据传递给 Drainer 中的数据解析及同步模块。通过上面的 binlog 格式的介绍，我们可以看出 binlog 文件中并没有存储表结构的信息，因此需要在 Drainer 中维护所有库和表的结构信息。在启动 Drainer 时，Drainer 会请求 TiKV，获取到所有历史的 DDL job 的信息，对这些 DDL job 进行过滤，使用 Drainer 启动时指定的 initial-commit-ts（或者 checkpoint 中保存的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;）之前的 DDL 在内存中构建库和表结构信息。这样 Drainer 就有了一份 ts 对应时间点的库和表的快照，在读取到 DDL 类型的 binlog 时，则更新库和表的信息；读取到 DML 类型的 binlog 时，则根据库和表的信息来生成 SQL。&lt;/p&gt;&lt;p&gt;在生成 SQL 之后，就可以同步到下游了。为了提高 Drainer 同步的速度，Drainer 中使用多个协程来执行 SQL。在生成 SQL 时，我们会使用主键／唯一键的值作为该条 SQL 的 key，通过对 key 进行 hash 来将 SQL 发送到对应的协程中。当每个协程收集到了足够多的 SQL，或者超过了一定的时间，则将这一批的 SQL 在一个事务中提交到下游。&lt;/p&gt;&lt;p&gt;但是有些 SQL 是相关的，如果被分到了不同的协程，那 SQL 的执行顺序就不能得到保证，造成数据的不一致。例如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;SQL1: delete from test.test where id = 1;

SQL2: replace into test.test (id, name ) values(1, &quot;a&quot;);&lt;/code&gt;&lt;p&gt;按照顺序执行后表中存在 id ＝ 1 该行数据，如果这两条 SQL 分别分配到了协程 1 和协程 2 中，并且协程 2 先执行了 SQL，则表中不再存在 id ＝ 1 的数据。为了避免这种情况的发生，Drainer 中加入了冲突检测的机制，如果检测出来两条 SQL 存在冲突（修改了同一行数据），则暂时不将后面的 SQL 发送到协程，而是生成一个 Flush 类型的 job 发送到所有的协程， 每个协程在遇到 Flush job 时就会马上执行所缓存的 SQL。接着才会把该条有冲突的 SQL 发送到对应的协程中。下面给出一个例子说明一下冲突检测的机制：&lt;/p&gt;&lt;p&gt;有以下这些 SQL，其中 id 为表的主键：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;SQL1: update itest set id = 4, name = &quot;c&quot;, age = 15 where id = 3;    key: 3, 4

SQL2:  update itest set id = 5, name = &quot;b&quot;, age = 14 where id = 2;   key：5, 2

SQL3：delete from itest where id = 3;                                key: 3&lt;/code&gt;&lt;ol&gt;&lt;li&gt;首先将 SQL1 发送到指定的协程，这时所有的 keys 为［3，4］；&lt;/li&gt;&lt;li&gt;SQL2 的 key［5，2］与 keys 中的［3，4］都没有冲突，将 SQL2 发送到指定的协程，这时 keys 为［3，4，5，2］；&lt;/li&gt;&lt;li&gt;SQL3 的 key［3］与 keys 中的［3］存在冲突，发送 Flush job 到所有协程，SQL1 和 SQL2 被执行，清空 keys；&lt;/li&gt;&lt;li&gt;将 SQL3 发送到指定的协程，同时更新 keys 为［3］。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Drainer 通过以上这些机制来高效地同步数据，并且保证数据的一致。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多技术干货：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/blog-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-10-51990591</guid>
<pubDate>Mon, 10 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在小米的应用实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-03-51472404.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51472404&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-286ddf5e402ba98f9de8bfa95ce5eb57_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;张良，小米 DBA 负责人&lt;br&gt;潘友飞，小米 DBA&lt;br&gt;王必文，小米开发工程师&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;应用场景介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;MIUI 是小米公司旗下基于 Android 系统深度优化、定制、开发的第三方手机操作系统，也是小米的第一个产品。MIUI 在 Android 系统基础上，针对中国用户进行了深度定制，在此之上孕育出了一系列的应用，比如主题商店、小米音乐、应用商店、小米阅读等。     &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c4428dc7c80aea208ba650abba0f8079_r.jpg&quot; data-caption=&quot;图 1  MIUI Android 系统界面图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;595&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c4428dc7c80aea208ba650abba0f8079&quot; data-watermark-src=&quot;v2-acb05a51e9c3630558f4a6880d1ca2c1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;目前 TiDB 主要应用在：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;小米手机桌面负一屏的快递业务&lt;/li&gt;&lt;li&gt;商业广告交易平台素材抽审平台&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;这两个业务场景每天读写量均达到上亿级，上线之后，整个服务稳定运行；接下来我们计划逐步上线更多的业务场景，小米阅读目前正在积极的针对订单系统做迁移测试。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 特点&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 结合了传统的 RDBMS 和 NoSQL 的最佳特性，兼容 MySQL 协议，支持无限的水平扩展，具备强一致性和高可用性。&lt;/p&gt;&lt;p&gt;具有如下的特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高度兼容 MySQL，大多数情况下无需修改代码即可从 MySQL 轻松迁移至 TiDB，即使已经分库分表的 MySQL 集群亦可通过 TiDB 提供的迁移工具进行实时迁移。&lt;/li&gt;&lt;li&gt;水平弹性扩展，通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/li&gt;&lt;li&gt;分布式事务，TiDB 100% 支持标准的 ACID 事务。&lt;/li&gt;&lt;li&gt;真正金融级高可用，相比于传统主从（M-S）复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复（auto-failover），无需人工介入。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 的架构及原理官网有详细介绍（&lt;a href=&quot;https://pingcap.com/&quot;&gt;https://pingcap.com/&lt;/a&gt;），这里不再赘述。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2a500e631bf3062d721ce5cd2cd2c817_r.jpg&quot; data-caption=&quot;图 2  TiDB 基础架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;381&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2a500e631bf3062d721ce5cd2cd2c817&quot; data-watermark-src=&quot;v2-8169cc4855b1c15dc1d79afdb93854a4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;跟绝大数互联网公司一样，小米关系型存储数据库首选 MySQL，单机 2.6T 磁盘。由于小米手机销量的快速上升和 MIUI 负一屏用户量的快速增加，导致负一屏快递业务数据的数据量增长非常快，&lt;b&gt;每天的读写量级均分别达到上亿级别，数据快速增长导致单机出现瓶颈，比如性能明显下降、可用存储空间不断降低、大表 DDL 无法执行等，不得不面临数据库扩展的问题。&lt;/b&gt;比如，我们有一个业务场景（智能终端），需要定时从几千万级的智能终端高频的向数据库写入各种监控及采集数据，MySQL 基于 Binlog 的单线程复制模式，很容易造成从库延迟，并且堆积越来越严重。&lt;/p&gt;&lt;p&gt;&lt;b&gt;对于 MySQL 来讲，最直接的方案就是采用分库分表的水平扩展方式，综合来看并不是最优的方案，比如对于业务来讲，对业务代码的侵入性较大；对于 DBA 来讲提升管理成本，后续需要不断的拆分扩容，即使有中间件也有一定的局限性。&lt;/b&gt;同样是上面的智能终端业务场景，从业务需求看，需要从多个业务维度进行查询，并且业务维度可能随时进行扩展，分表的方案基本不能满足业务的需求。&lt;/p&gt;&lt;p&gt;了解到 TiDB 特点之后，DBA 与业务开发沟通确认当前 MySQL 的使用方式，并与 TiDB 的兼容性做了详细对比，经过业务压测之后，根据压测的结果，决定尝试将数据存储从 MySQL 迁移到 TiDB。经过几个月的线上考验，TiDB 的表现达到预期。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;兼容性对比&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 支持包括跨行事务、JOIN、子查询在内的绝大多数 MySQL 的语法，可以直接使用 MySQL 客户端连接；对于已用 MySQL 的业务来讲，基本可以无缝切换到 TiDB。&lt;/b&gt;&lt;br&gt;二者简单对比如下几方面：&lt;/p&gt;&lt;p&gt;1. 功能支持&lt;/p&gt;&lt;p&gt;TiDB 尚不支持如下几项：&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;增加、删除主键&lt;/li&gt;&lt;li&gt;非 UTF8 字符集&lt;/li&gt;&lt;li&gt;视图（即将支持）、存储过程、触发器、部分内置函数&lt;/li&gt;&lt;li&gt;Event&lt;/li&gt;&lt;li&gt;全文索引、空间索引&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;2. 默认设置&lt;/p&gt;&lt;p&gt;字符集、排序规则、sql_mode、lower_case_table_names 几项默认值不同。&lt;/p&gt;&lt;p&gt;3. 事务&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;TiDB 使用乐观事务模型，提交后注意检查返回值。&lt;/li&gt;&lt;li&gt;TiDB 限制单个事务大小，保持事务尽可能的小。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;4. TiDB 支持绝大多数的 Online DDL。&lt;/p&gt;&lt;p&gt;5. 另，一些 MySQL 语法在 TiDB 中可以解析通过，不会产生任何作用，例如： create table 语句中 engine、partition 选项都是在解析后忽略。&lt;/p&gt;&lt;p&gt;6. 详细信息可以访问官网：https://pingcap.com/docs-cn/sql/mysql-compatibility/ 。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;压测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 目的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过压测 TiDB 了解一下其 OLTP 性能，看是否满足业务要求。&lt;br&gt;&lt;br&gt;&lt;b&gt;2. 机器配置&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d7d319c1480a57e186b79e7bf2cd6ce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;394&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5d7d319c1480a57e186b79e7bf2cd6ce&quot; data-watermark-src=&quot;v2-dbdd0864ecae8948d5ef342c9e5d6c78&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;3. 压测内容以及结果&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt; 标准 Select 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3919bf10c952092538e4df79ee90caa2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;322&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3919bf10c952092538e4df79ee90caa2&quot; data-watermark-src=&quot;v2-1ab2200f75c089f9a9f3cade76c787b4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8d9269cdf30ac3f6bb7c846d4473a248_r.jpg&quot; data-caption=&quot;图 3  标准 Select 压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8d9269cdf30ac3f6bb7c846d4473a248&quot; data-watermark-src=&quot;v2-da683c683c19267c31a5e7fc3f07a732&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;标准 OLTP 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c163a21feec655e19ab64dcb1b07f70a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;357&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c163a21feec655e19ab64dcb1b07f70a&quot; data-watermark-src=&quot;v2-a755b0a83bd7ddba182ba80a6ed66877&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fddec2f978f81acfee3a2d75b2d5ebcb_r.jpg&quot; data-caption=&quot;图 4  标准 OLTP  压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;541&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fddec2f978f81acfee3a2d75b2d5ebcb&quot; data-watermark-src=&quot;v2-847e24943021fd8f04ec9a6c907ff6a8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt; 标准 Insert 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28457ad3a1997ddf3c1f2abb33d16fc4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;333&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-28457ad3a1997ddf3c1f2abb33d16fc4&quot; data-watermark-src=&quot;v2-36a16a1feb93e863164d2d83eb3c472a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-686f7602ce614f8e15fdb0ef388a009a_r.jpg&quot; data-caption=&quot;图 5  标准 Insert 压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-686f7602ce614f8e15fdb0ef388a009a&quot; data-watermark-src=&quot;v2-c687905215da14e1e3976f9a4196762f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;通过压测发现 TiDB 稳定性上与预期稍有差别，不过压测的 Load 会明显高于生产中的业务 Load，参考低 Threads 时 TiDB 的表现，基本可以满足业务对 DB 的性能要求，决定灰度一部分 MySQL 从库读流量体验一下实际效果。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;迁移过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;整个迁移分为 2 大块：数据迁移、流量迁移。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 数据迁移&lt;/b&gt;&lt;/p&gt;&lt;p&gt;数据迁移分为增量数据、存量数据两部分。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于存量数据，可以使用逻辑备份、导入的方式，除了传统的逻辑导入外，官方还提供一款物理导入的工具 TiDB Lightning。&lt;/li&gt;&lt;li&gt;对于增量备份可以使用 TiDB 提供的 Syncer （新版已经更名为 DM - Data Migration）来保证数据同步。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Syncer 结构如图 6，主要依靠各种 Rule 来实现不同的过滤、合并效果，一个同步源对应一个 Syncer 进程，同步 Sharding 数据时则要多个 Syncer 进程。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-facfec2bcf6879a829017e0de8b94a38_r.jpg&quot; data-caption=&quot;图 6  Syncer 结构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;852&quot; data-rawheight=&quot;391&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-facfec2bcf6879a829017e0de8b94a38&quot; data-watermark-src=&quot;v2-fa999818be6d9fcd875987dfa5d194a8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;使用 Syncer 需要注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;做好同步前检查，包含 server-id、log_bin、binlog_format 是否为 ROW、binlog_row_image 是否为 FULL、同步相关用户权限、Binlog 信息等。&lt;/li&gt;&lt;li&gt;使用严格数据检查模式，数据不合法则会停止。数据迁移之前最好针对数据、表结构做检查。&lt;/li&gt;&lt;li&gt;做好监控，TiDB 提供现成的监控方案。&lt;/li&gt;&lt;li&gt;对于已经分片的表同步到同一个 TiDB 集群，要做好预先检查。确认同步场景是否可以用 route-rules 表达，检查分表的唯一键、主键在数据合并后是否冲突等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 流量迁移&lt;/b&gt;&lt;/p&gt;&lt;p&gt;流量切换到 TiDB 分为两部分：读、写流量迁移。每次切换保证灰度过程，观察周期为 1~2 周，做好回滚措施。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;读流量切换到 TiDB，这个过程中回滚比较简单，灰度无问题，则全量切换。&lt;/li&gt;&lt;li&gt;再将写入切换到 TiDB，需要考虑好数据回滚方案或者采用双写的方式（需要断掉 Syncer） 。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;集群状况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 配置&lt;/b&gt;&lt;/p&gt;&lt;p&gt;集群配置采用官方推荐的 7 节点配置，3 个 TiDB 节点，3 个 PD 节点，4 个 TiKV 节点，其中每个 TiDB 与 PD 为一组，共用一台物理机。后续随着业务增长或者新业务接入，再按需添加 TiKV 节点。&lt;br&gt;&lt;br&gt;&lt;b&gt;2. 监控&lt;/b&gt;&lt;/p&gt;&lt;p&gt;监控采用了 TiDB 的提供的监控方案，并且也接入了公司开源的 Falcon，目前整个集群运行比较稳定，监控如图 7。      &lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6288a303380a452fb7c0f8716e97d135_r.jpg&quot; data-caption=&quot;图 7  监控图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;314&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6288a303380a452fb7c0f8716e97d135&quot; data-watermark-src=&quot;v2-729e081c26da54cb02349deb61f1c839&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;遇到的问题、原因及解决办法&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-504342f9d0b3e7799d8dc657180e2083_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;928&quot; data-rawheight=&quot;1638&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-504342f9d0b3e7799d8dc657180e2083&quot; data-watermark-src=&quot;v2-041e4d6ecb39bebe13289f22f6f275b9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;后续和展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前 TiDB 在小米主要提供 OLTP 服务，小米手机负一屏快递业务为使用 TiDB 做了一个良好的开端，而后商业广告也有接入，2 个业务均已上线数月，TiDB 的稳定性经受住了考验，带来了很棒的体验，对于后续大体的规划如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MIUI 生态业务中存在大量的类似场景的业务，后续将会与业务开发积极沟通，从 MySQL 迁移到 TiDB。&lt;/li&gt;&lt;li&gt;针对某些业务场景，以资源合理利用为目标，推出归档集群，利用 Syncer 实现数据归档的功能。&lt;/li&gt;&lt;li&gt;数据分析，结合 TiDB 提供的工具，将支持离线、实时数据分析支持。&lt;/li&gt;&lt;li&gt;将 TiDB 的监控融合到小米公司开源的监控系统 Falcon 中。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;致谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;非常感谢 TiDB 官方在迁移及业务上线期间给予我们的支持，为每一个 TiDB 人专业的精神、及时负责的响应点赞。&lt;br&gt;&lt;br&gt;&lt;br&gt;更多 TiDB 用户实践：&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-03-51472404</guid>
<pubDate>Mon, 03 Dec 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
