<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Tue, 30 Oct 2018 05:22:05 +0800</lastBuildDate>
<item>
<title>TiDB 源码阅读系列文章（二十）Table Partition</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-29-47909702.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47909702&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8606678fbae32d086cdd5e3df15b0beb_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：肖亮亮&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Table Partition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;什么是 Table Partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Table Partition 是指根据一定规则，将数据库中的一张表分解成多个更小的容易管理的部分。从逻辑上看只有一张表，但是底层却是由多个物理分区组成。相信对有关系型数据库使用背景的用户来说可能并不陌生。&lt;/p&gt;&lt;p&gt;TiDB 正在支持分区表这一特性。在 TiDB 中分区表是一个独立的逻辑表，但是底层由多个物理子表组成。物理子表其实就是普通的表，数据按照一定的规则划分到不同的物理子表类内。程序读写的时候操作的还是逻辑表名字，TiDB 服务器自动去操作分区的数据。&lt;br&gt;分区表有什么好处？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;优化器可以使用分区信息做分区裁剪。在语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率。&lt;/li&gt;&lt;li&gt;方便地进行数据生命周期管理。通过创建、删除分区、将过期的数据进行 高效的归档，比使用 Delete 语句删除数据更加优雅，打散写入热点，将一个表的写入分散到多个物理表，使得负载分散开，对于存在 Sequence 类型数据的表来说（比如 Auto Increament ID 或者是 create time 这类的索引）可以显著地提升写入吞吐。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;分区表的限制&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;TiDB 默认一个表最多只能有 1024 个分区 ，默认是不区分表名大小写的。&lt;/li&gt;&lt;li&gt;Range, List, Hash 分区要求分区键必须是 INT 类型，或者通过表达式返回 INT 类型。但 Key 分区的时候，可以使用其他类型的列（BLOB，TEXT 类型除外）作为分区键。&lt;/li&gt;&lt;li&gt;如果分区字段中有主键或者唯一索引的列，那么有主键列和唯一索引的列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列。&lt;/li&gt;&lt;li&gt;TiDB 的分区适用于一个表的所有数据和索引。不能只对表数据分区而不对索引分区，也不能只对索引分区而不对表数据分区，也不能只对表的一部分数据分区。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;常见分区表的类型&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Range 分区：按照分区表达式的范围来划分分区。通常用于对分区键需要按照范围的查询，分区表达式可以为列名或者表达式 ，下面的 employees 表当中 p0, p1, p2, p3 表示 Range 的访问分别是  (min, 1991), [1991, 1996), [1996, 2001), [2001, max) 这样一个范围。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;
CREATE  TABLE employees (
id INT  NOT  NULL,
fname VARCHAR(30),
separated DATE  NOT  NULL
)
    
PARTITION BY RANGE ( YEAR(separated) ) (
PARTITION p0 VALUES LESS THAN (1991),
PARTITION p1 VALUES LESS THAN (1996),
PARTITION p2 VALUES LESS THAN (2001),
PARTITION p3 VALUES LESS THAN MAXVALUE
);&lt;/code&gt;&lt;ul&gt;&lt;li&gt;List 分区：按照 List 中的值分区，主要用于枚举类型，与 Range 分区的区别在于 Range 分区的区间范围值是连续的。&lt;/li&gt;&lt;li&gt;Hash 分区：Hash 分区需要指定分区键和分区个数。通过 Hash 的分区表达式计算得到一个 INT 类型的结果，这个结果再跟分区个数取模得到具体这行数据属于那个分区。通常用于给定分区键的点查询，Hash 分区主要用来分散热点读，确保数据在预先确定个数的分区中尽可能平均分布。&lt;/li&gt;&lt;li&gt;Key 分区：类似 Hash 分区，Hash 分区允许使用用户自定义的表达式，但 Key 分区不允许使用用户自定义的表达式。Hash 仅支持整数分区，而 Key 分区支持除了 Blob 和 Text 的其他类型的列作为分区键。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB Table Partition 的实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本文接下来按照 TiDB 源码的 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/release-2.1&quot;&gt;release-2.1 &lt;/a&gt;分支讲解，部分讲解会在 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;source-code &lt;/a&gt;分支代码，目前只支持 Range 分区所以这里只介绍 Range 类型分区 Table Partition 的源码实现，包括 create table、select 、add partition、insert 、drop partition 这五种语句。&lt;/p&gt;&lt;p&gt;&lt;b&gt;create table&lt;/b&gt;&lt;/p&gt;&lt;p&gt;create table 会重点讲构建 Partition 的这部分，更详细的可以看 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-17/&quot;&gt;TiDB 源码阅读系列文章（十七）DDL 源码解析 &lt;/a&gt;，当用户执行创建分区表的SQL语句，语法解析（Parser）阶段会把 SQL 语句中 Partition 相关信息转换成 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ast/ddl.go&quot;&gt;ast.PartitionOptions &lt;/a&gt;，下文会介绍。接下来会做一系列 Check，分区名在当前的分区表中是否唯一、是否分区 Range 的值保持递增、如果分区键构成为表达式检查表达式里面是否是允许的函数、检查分区键必须是 INT 类型，或者通过表达式返回 INT 类型、检查分区键是否符合一些约束。&lt;/p&gt;&lt;p&gt;解释下分区键，在分区表中用于计算这一行数据属于哪一个分区的列的集合叫做分区键。分区键构成可能是一个字段或多个字段也可以是表达式。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// PartitionOptions specifies the partition options.
type PartitionOptions struct {
Tp          model.PartitionType
Expr        ExprNode
ColumnNames []*ColumnName
Definitions []*PartitionDefinition
}
	​
// PartitionDefinition defines a single partition.
type PartitionDefinition struct {
Name     model.CIStr
LessThan []ExprNode
MaxValue bool
Comment  string
}&lt;/code&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;PartitionOptions&lt;/code&gt; 结构中 Tp 字段表示分区类型， &lt;code class=&quot;inline&quot;&gt;Expr&lt;/code&gt; 字段表示分区键， &lt;code class=&quot;inline&quot;&gt;ColumnNames&lt;/code&gt; 字段表示 Columns 分区，这种类型分区有分为 Range columns 分区和 List columns 分区，这种分区目前先不展开介绍。 &lt;code class=&quot;inline&quot;&gt;PartitionDefinition&lt;/code&gt; 其中 Name 字段表示分区名， &lt;code class=&quot;inline&quot;&gt;LessThan&lt;/code&gt; 表示分区 Range 值， &lt;code class=&quot;inline&quot;&gt;MaxValue&lt;/code&gt; 字段表示 Range 值是否为最大值， &lt;code class=&quot;inline&quot;&gt;Comment&lt;/code&gt; 字段表示分区的描述。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L905&quot;&gt;CreateTable &lt;/a&gt;Partition 部分主要流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;把上文提到语法解析阶段会把 SQL语句中 Partition 相关信息转换成 &lt;code class=&quot;inline&quot;&gt;ast.PartitionOptions&lt;/code&gt; , 然后 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L41&quot;&gt;buildTablePartitionInfo &lt;/a&gt;负责把&lt;code class=&quot;inline&quot;&gt;PartitionOptions&lt;/code&gt; 结构转换 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; ,  即 Partition 的元信息。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L88&quot;&gt;checkPartitionNameUnique &lt;/a&gt;检查分区名是否重复，分表名是不区大小写的。&lt;/li&gt;&lt;li&gt;对于每一分区 Range 值进行 Check， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L469&quot;&gt;checkAddPartitionValue &lt;/a&gt;就是检查新增的 Partition 的 Range 需要比之前所有 Partition 的 Range 都更大。&lt;/li&gt;&lt;li&gt;TiDB 单表最多只能有 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L329&quot;&gt;1024 个分区 &lt;/a&gt;，超过最大分区的限制不会创建成功。&lt;/li&gt;&lt;li&gt;如果分区键构成是一个包含函数的表达式需要检查表达式里面是否是允许的函数 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L107&quot;&gt;checkPartitionFuncValid &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;检查分区键必须是 INT 类型，或者通过表达式返回 INT 类型，同时检查分区键中的字段在表中是否存在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L149&quot;&gt;checkPartitionFuncType &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;如果分区字段中有主键或者唯一索引的列，那么多有主键列和唯一索引列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L348&quot;&gt;checkRangePartitioningKeysConstraints &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;通过以上对 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; 的一系列 check 主要流程就讲完了，需要注意的是我们没有对 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; 的元数据持久化单独存储而是附加在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;Partition 中。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;add partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;add partition 首先需要从 SQL 中解析出来 Partition 的元信息，然后对当前添加的分区会有一些 Check 和限制，主要检查是否是分区表、分区名是已存在、最大分区数限制、是否 Range 值保持递增，最后把 Partition 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L308&quot;&gt;PartitionInfo &lt;/a&gt;追加到 Table 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;中，具体如下:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;检查是否是分区表，若不是分区表则报错提示。&lt;/li&gt;&lt;li&gt;用户的 SQL 语句被解析成将 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ast/ddl.go#L880&quot;&gt;ast.PartitionDefinition &lt;/a&gt;然后 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L2123&quot;&gt;buildPartitionInfo &lt;/a&gt;做的事就是保存表原来已存在的分区信息例如分区类型，分区键，分区具体信息，每个新分区分配一个独立的 PartitionID。&lt;/li&gt;&lt;li&gt;TiDB 默认一个表最多只能有 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L329&quot;&gt;1024 个分区 &lt;/a&gt;，超过最大分区的限制会报错&lt;/li&gt;&lt;li&gt;对于每新增一个分区需要检查 Range 值进行 Check， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L469&quot;&gt;checkAddPartitionValue &lt;/a&gt;简单说就是检查新增的 Partition 的 Range 需要比之前所有 Partition 的 Rrange 都更大。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L88&quot;&gt;checkPartitionNameUnique &lt;/a&gt;检查分区名是否重复，分表名是不区大小写的。&lt;/li&gt;&lt;li&gt;最后把 Partition 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L308&quot;&gt;PartitionInfo &lt;/a&gt;追加到 Table 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;.Partition 中，具体实现在这里 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L459&quot;&gt;updatePartitionInfo &lt;/a&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;drop partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;drop partition 和 drop table 类似，只不过需要先找到对应的 Partition ID，然后删除对应的数据，以及修改对应 Table 的 Partition 元信息，两者区别是如果是 drop table 则删除整个表数据和表的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;元信息，如果是 drop partition 则需删除对应分区数据和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;中的 Partition 元信息，删除分区之前会有一些 Check 具体如下:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只能对分区表做 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L1355&quot;&gt;drop partition 操作 &lt;/a&gt;，若不是分区表则报错提示。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L269&quot;&gt;checkDropTablePartition &lt;/a&gt;检查删除的分区是否存在，TiDB 默认是不能删除所有分区，如果想删除最后一个分区，要用 drop table 代替。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L283&quot;&gt;removePartitionInfo &lt;/a&gt;会把要删除的分区从 Partition 元信息删除掉，删除前会做 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L269&quot;&gt;checkDropTablePartition &lt;/a&gt;的检查。&lt;/li&gt;&lt;li&gt;对分区表数据则需要拿到 PartitionID 根据插入数据时候的编码规则构造出 StartKey 和 EndKey 便能包含对应分区 Range 内所有的数据，然后把这个范围内的数据删除，具体代码实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/delete_range.go#L250&quot;&gt;这里 &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;编码规则：&lt;br&gt;Key： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID_rowID&lt;/code&gt;&lt;br&gt;startKey： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID&lt;/code&gt;&lt;br&gt;endKey： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID + 1&lt;/code&gt; &lt;/li&gt;&lt;li&gt;删除了分区，同时也将删除该分区中的所有数据。如果删除了分区导致分区不能覆盖所有值，那么插入数据的时候会报错。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Select 语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Select 语句重点讲 Select Partition 如何查询的和分区裁剪（Partition Pruning），更详细的可以看 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-6/&quot;&gt;TiDB 源码阅读系列文章（六）Select 语句概览 &lt;/a&gt;。&lt;/p&gt;&lt;p&gt;一条 SQL 语句的处理流程，从 Client 接收数据，MySQL 协议解析和转换，SQL 语法解析，逻辑查询计划和物理查询计划执行，到最后返回结果。那么对于分区表是如何查询的表里的数据的，其实最主要的修改是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L39&quot;&gt;逻辑查询计划 &lt;/a&gt;阶段，举个例子：如果用上文中 employees 表作查询, 在 SQL 语句的处理流程前几个阶段没什么不同，但是在逻辑查询计划阶段， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L46&quot;&gt;rewriteDataSource &lt;/a&gt;将 DataSource 重写了变成 Union All 。每个 Partition id 对应一个 Table Reader。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from employees&lt;/code&gt;&lt;p&gt;等价于：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from (union all
select * from p0 where id &amp;lt; 1991
select * from p1 where id &amp;lt; 1996
select * from p2 where id &amp;lt; 2001
select * from p3 where id &amp;lt; MAXVALUE)&lt;/code&gt;&lt;p&gt;通过观察 &lt;code class=&quot;inline&quot;&gt;EXPLAIN&lt;/code&gt; 的结果可以证实上面的例子，如图 1，最终物理执行计划中有四个 Table Reader 因为 employees 表中有四个分区， &lt;code class=&quot;inline&quot;&gt;Table Reader&lt;/code&gt; 表示在 TiDB 端从 TiKV 端读取， &lt;code class=&quot;inline&quot;&gt;cop task&lt;/code&gt; 是指被下推到 TiKV 端分布式执行的计算任务。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4b7fc3aa7830e5350b4a4ae2b6df58e9_r.jpg&quot; data-caption=&quot;图 1：EXPLAIN 输出&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;379&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4b7fc3aa7830e5350b4a4ae2b6df58e9&quot; data-watermark-src=&quot;v2-f631b41e76235890b1fd41fdaf5157b0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;用户在使用分区表时，往往只需要访问其中部分的分区, 就像程序局部性原理一样，优化器分析 &lt;code class=&quot;inline&quot;&gt;FROM&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;WHERE&lt;/code&gt; 子句来消除不必要的分区，具体还要优化器根据实际的 SQL 语句中所带的条件，避免访问无关分区的优化过程我们称之为分区裁剪（Partition Pruning），具体实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L70&quot;&gt;这里 &lt;/a&gt;，分区裁剪是分区表提供的重要优化手段，通过分区的裁剪，避免访问无关数据，可以加速查询速度。当然用户可以刻意利用分区裁剪的特性在 SQL 加入定位分区的条件，优化查询性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Insert 语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-4/&quot;&gt;Insert 语句 &lt;/a&gt;是怎么样写入 Table Partition ?&lt;/p&gt;&lt;p&gt;其实解释这些问题就可以了：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;普通表和分区表怎么区分？&lt;/li&gt;&lt;li&gt;插入数据应该插入哪个 Partition？&lt;/li&gt;&lt;li&gt;每个 Partition 的 RowKey 怎么编码的和普通表的区别是什么？&lt;/li&gt;&lt;li&gt;怎么将数据插入到相应的 Partition 里面?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;普通 Table 和 Table Partition 也是实现了 Table 的接口，load schema 在初始化 Table 数据结构的时候，如果发现 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 里面没有 Partition 信息，则生成一个普通的 &lt;code class=&quot;inline&quot;&gt;tables.Table&lt;/code&gt; ，普通的 Table 跟以前处理逻辑保持不变，如果 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 里面有 Partition 信息，则会生成一个&lt;code class=&quot;inline&quot;&gt;tables.PartitionedTable&lt;/code&gt; ，它们的区别是 RowKey 的编码方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每个分区有一个独立的 Partition ID，Partition ID 和 Table ID 地位平等，每个 Partition 的 Row 和 index 在编码的时候都使用这个 Partition 的 ID。&lt;/li&gt;&lt;li&gt;下面是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/partition.go#L171&quot;&gt;PartitionRecordKey &lt;/a&gt;和普通表 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/tables.go#L261&quot;&gt;RecordKey &lt;/a&gt;区别。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; (1) 分区表按照规则编码成 Key-Value pair：&lt;/p&gt;&lt;p&gt;        Key: &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID_rowID&lt;/code&gt;&lt;br&gt;        Value: &lt;code class=&quot;inline&quot;&gt;[col1, col2, col3, col4]&lt;/code&gt; &lt;/p&gt;&lt;p&gt;(2) 普通表按照规则编码成 Key-Value pair：&lt;/p&gt;&lt;p&gt;        Key: &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_tableID_rowID&lt;/code&gt;&lt;br&gt;        Value: &lt;code class=&quot;inline&quot;&gt;[col1, col2, col3, col4]&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/partition.go#L177&quot;&gt;locatePartition &lt;/a&gt;操作查询到应该插入哪个 Partition，目前支持 RANGE 分区插入到那个分区主要是通过范围来判断，例如在 employees 表中插入下面的 sql，通过计算范围该条记录会插入到 p3 分区中，接着调用对应 Partition 上面的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/tables.go#L406&quot;&gt;AddRecord &lt;/a&gt;方法，将数据插入到相应的 Partition 里面。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;INSERT INTO employees VALUES (1, &#39;PingCAP TiDB&#39;, &#39;2003-10-15&#39;),&lt;/code&gt;&lt;ul&gt;&lt;li&gt;插入数据时，如果某行数据不属于任何 Partition，则该事务失败，所有操作回滚。如果 Partition 的 Key 算出来是一个 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; ，对于不同的 Partition 类型有不同的处理方式：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;对于 Range Partition：该行数据被插入到最小的那个 Partition&lt;/li&gt;&lt;li&gt;对于 List partition：如果某个 Partition 的 Value List 中有 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; ，该行数据被插入那个 Partition，否则插入失败&lt;/li&gt;&lt;li&gt;对于 Hash 和 Key Partition： &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值视为 0，计算 Partition ID 将数据插入到对应的 Partition&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;在 TiDB 分区表中分区字段插入的值不能大于表中 Range 值最大的上界，否则会报错&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;End&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 目前支持 Range 分区类型，具体以及更细节的可以看 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;这里 &lt;/a&gt;。剩余其它类型的分区类型正在开发中，后面陆续会和大家见面，敬请期待。它们的源码实现读者届时可以自行阅读，流程和文中上述描述类似。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-29-47909702</guid>
<pubDate>Mon, 29 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>我们要做点更酷的事情，来场 Hackathon 怎么样？</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-25-47610023.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47610023&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7370ffeb0809ae9bdc74ea11bbe3c1b1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;i&gt;1024 程序员节，&lt;/i&gt;&lt;br&gt;&lt;i&gt;总把我们标签化为格子衬衫和后退的发际线（很烦啊。。）&lt;/i&gt;&lt;br&gt;&lt;i&gt;发张堆满编程语言名称的图片表示尊重（好吧。。）&lt;/i&gt;&lt;br&gt;&lt;i&gt;我们自己的节日，我们有自己的更酷的玩儿法——&lt;/i&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;报名一场 Hackathon，怎么样？&lt;/i&gt;&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Hackathon2018&lt;/b&gt; &lt;b&gt;将于 12 月 1 - 2 日在 PingCAP 北京总部举办。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本场 Hackathon &lt;b&gt;主题为 TiDB Ecosystem&lt;/b&gt;，参赛团队将在两天一夜的时间里完成一个作品，当然我们也欢迎个人开发者独立报名参赛～ 我司 Co-Founder、首席架构师、技术 VP 等 &lt;b&gt;「PingCAP 大神」将担任导师团，手把手带练，&lt;/b&gt;更邀请了&lt;b&gt;业内大牛评审团&lt;/b&gt; 对每个小组的作品进行打分，获胜队伍将得到&lt;b&gt;丰厚现金奖励和面试直通车&lt;/b&gt;的机会。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;奖项设置&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一等奖（1 支队伍）：   ¥ 60,000 现金奖励 &lt;/p&gt;&lt;p&gt;二等奖（2 支队伍）：每队 ¥ 30,000 现金奖励&lt;/p&gt;&lt;p&gt;三等奖（3 支队伍）：每队 ¥ 10,000 现金奖励&lt;/p&gt;&lt;p&gt;对于现场表现突出的小伙伴，我们也设置了最佳创意奖和最佳贡献奖哦～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;选题方向&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;参赛作品需围绕 TiDB 及其周边生态实现，选题不限。我们有以下选题方向供小伙伴们参考：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 数据可视化，基于异地多机房部署全局视图等&lt;/li&gt;&lt;li&gt;执行计划可视化（explain 的输出生成图，对于 expensive 的步骤在图中高亮，统计信息的图形化展示，比如 histogram）&lt;/li&gt;&lt;li&gt;TiKV 潜在瓶颈分析，cost 量化分析工具（可以结合 wPerf 论文，应用到我们的项目中，发现潜在的环以及锁依赖之类的。甚至对于每个 query 的 cost 进行量化，比如导致的每个线程 cpu circle 消耗、context switch 次数，cache miss 情况等等）&lt;/li&gt;&lt;li&gt;玩转 TiKV，集成其它一些引擎接口，如 Redis&lt;/li&gt;&lt;li&gt;……&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你有更多脑洞大开的选题，想为 TiDB 做一些更好玩儿的周边工具，可以在报名之后与 &lt;b&gt;TiDB Robot（微信号：tidbai）&lt;/b&gt;沟通交流，小伙伴们也可以自由组队报名（每支队伍不超过 3 人），我们会进行审核～另外，暂时没有队伍和选题意向的同学可以在报名后与 TiDB Robot 联系组队。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参考资料&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;源码地址：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/&quot;&gt;https://github.com/pingcap/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/&quot;&gt;https://github.com/tikv/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;官方文档&lt;/b&gt;：&lt;a href=&quot;https://pingcap.com/docs-cn/&quot;&gt;https://pingcap.com/docs-cn/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;技术博客&lt;/b&gt;：&lt;a href=&quot;https://pingcap.com/blog-cn/&quot;&gt;https://pingcap.com/blog-cn/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参赛 Tips&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;比赛时间&lt;/b&gt;：12 月 1 日 10:00  - 12 月 2 日 18:00&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名时间&lt;/b&gt;：即日起至 11 月 23 日 17:00&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名审核&lt;/b&gt;：5 个工作日内反馈审核结果&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名链接&lt;/b&gt;：点击 &lt;a href=&quot;http://nc9hsk15y2xczuor.mikecrm.com/3AarNns&quot;&gt;这里&lt;/a&gt; 报名&lt;br&gt;&lt;br&gt;* 本次大赛诚招志愿者参与活动现场支持。如果你想近距离接触技术大咖，体验大赛氛围，那就联系 TiDB Robot（微信号：tidbai）报名吧～志愿者也可以获得活动定制纪念品哦！&lt;br&gt;* 详细比赛日程请关注近期官方微信公号（ID: PingCAP）推送&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/0zhMVPLEUq8trbY3923B (二维码自动识别)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-25-47610023</guid>
<pubDate>Thu, 25 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>一致性模型</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-23-47445841.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47445841&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c8b584a8186554d98b4bb5442eeeafcd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：唐刘（siddontang）&lt;/p&gt;&lt;p&gt;有时候，在跟一些同学讨论 TiKV 事务模型的时候，我都提到了 Linearizability，也提到了 Snapshot Isolation，以及需要手动 lock 来保证 Serializable Snapshot Isolation，很多时候，当我嘴里面蹦出来这些名词的时候，有些同学就一脸懵逼了。所以我觉得有必要仔细来解释一下，顺带让我自己将所有的 isolation 以及 consistency 这些情况都归纳总结一遍，让自己也理解透彻一点。&lt;/p&gt;&lt;p&gt;幸运的是，业内已经有很多人做了这个事情，譬如在 &lt;a href=&quot;http://www.vldb.org/pvldb/vol7/p181-bailis.pdf&quot;&gt;Highly Available Transactions: Virtues and Limitations&lt;/a&gt; 这篇论文里面，作者就总结了不同模型是否能满足 Highly Available Transactions(HATs)。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-38087fea60b99e9c2e34a151c1cd6a08_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;258&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-38087fea60b99e9c2e34a151c1cd6a08&quot; data-watermark-src=&quot;v2-7ee2fd316c00de39dcf3b26620cce6a0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;图中，红色圆圈里面的模型属于 Unavailable，蓝色的属于 Sticky Available，其余的就是  Highly Available。这里解释下相关的含义：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Unavailable：当出现网络隔离等问题的时候，为了保证数据的一致性，不提供服务。熟悉 CAP 理论的同学应该清楚，这就是典型的 CP 系统了。&lt;/li&gt;&lt;li&gt;Sticky Available：即使一些节点出现问题，在一些还没出现故障的节点，仍然保证可用，但需要保证 client 的操作是一致的。&lt;/li&gt;&lt;li&gt;Highly Available：就是网络全挂掉，在没有出现问题的节点上面，仍然可用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unavailable 比较容易理解，这里在讨论下 Sticky 和 Highly，对于 Highly Available 来说，如果一个 server 挂掉了，client 可以去连接任意的其他 server，如果这时候仍然能获取到结果，那么就是 Highly Available 的。但对于 Sticky 来说，还需要保证 client 操作的一致性，譬如 client 现在 server 1 上面进行了很多操作，这时候 server 1 挂掉了，client 切换到 server 2，但在 server 2 上面看不到 client 之前的操作结果，那么这个系统就不是 Sticky 的。所有能在 Highly Available 系统上面保证的事情一定也能在 Sticky Available 系统上面保证，但反过来就不一定了。&lt;/p&gt;&lt;p&gt;Jepsen 在&lt;a href=&quot;https://jepsen.io/consistency&quot;&gt;官网&lt;/a&gt;上面有一个简化但更好看一点的图&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bb3847f8b9e69b1154cfdf6591af0202_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;438&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bb3847f8b9e69b1154cfdf6591af0202&quot; data-watermark-src=&quot;v2-706a307a3c684b7713aa678335b0baa9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;下面，我会按照 Jepsen 里面的图，对不同的 model 进行解释一下。至于为啥选择 Jepsen 里面的例子，一个是因为 Jepsen 现在是一款主流的测试不同分布式系统一致性的工具，它的测试用例就是测试的是上图提到的模型，我们自然也会关心这些模型。另外一个就是这个模型已经覆盖了大多数场景了，理解了这些，大部分都能游刃有余处理了。&lt;/p&gt;&lt;p&gt;如果大家仔细观察，可以发现，从根节点 Strict Serializable，其实是有两个分支的，一个对应的就是数据库里面的 Isolation（ACID 里面的 I），另一个其实对应的是分布式系统的 Consistency（CAP 里面的 C），在 HATs 里面，叫做 Session Guarantees。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Isolation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;要对 Isolation 有个快速的理解，其实只需要看 &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf&quot;&gt;A Critique of ANSI SQL Isolation Levels&lt;/a&gt; 这篇论文就足够了，里面详细的介绍了数据库实现中遇到的各种各样的 isolation 问题，以及不同的 isolation level 到底能不能解决。&lt;/p&gt;&lt;p&gt;在论文里面，作者详细的列举了多种异常现象，这里大概介绍一下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P0 - Dirty Write&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Dirty Write 就是一个事务，覆盖了另一个之前还未提交事务写入的值。假设现在我们有两个事务，一个事务写入 x = y = 1，而另一个事务写入 x = y = 2，那么最终的结果，我们是希望看到 x 和 y 要不全等于 1，要不全等于 2。但在 Dirty Write 情况下面，可能会出现如下情况：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f9420db92296f248e521212a86bd33d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;310&quot; data-rawheight=&quot;177&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f9420db92296f248e521212a86bd33d1&quot; data-watermark-src=&quot;v2-55a99d7a4bcd04bb2d166e9ac33009a3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;可以看到，最终的值是 x = 2 而 y = 1，已经破坏了数据的一致性了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P1 - Dirty Read&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Dirty Read 出现在一个事务读取到了另一个还未提交事务的修改数据。假设现在我们有一个两个账户，x 和 y，各自有 50 块钱，x 需要给 y 转 40 元钱，那么无论怎样，x + y = 100 这个约束是不能打破的，但在 Dirty Read 下面，可能出现：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a2b718ff1efaf2a194275bfb7ff1bced_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;343&quot; data-rawheight=&quot;172&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a2b718ff1efaf2a194275bfb7ff1bced&quot; data-watermark-src=&quot;v2-e6146413f66d4fe5679ec476ea9633be&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在事务 T2，读取到的 x + y = 60，已经打破了约束条件了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P2 - Fuzzy Read&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Fuzzy Read 也叫做 Non-Repeatable Read，也就是一个还在执行的事务读取到了另一个事务的更新操作，仍然是上面的转账例子：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d827daeddda6b1707d9d721f95a9f1db_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;341&quot; data-rawheight=&quot;166&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d827daeddda6b1707d9d721f95a9f1db&quot; data-watermark-src=&quot;v2-909c1d04a9fe89a09272b083bf22b579&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在 T1 还在运行的过程中，T2 已经完成了转账，但 T1 这时候能读到最新的值，也就是 x + y = 140 了，破坏了约束条件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P3 - Phantom&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Phantom 通常发生在一个事务首先进行了一次按照某个条件的 read 操作，譬如 SQL 里面的 &lt;code class=&quot;inline&quot;&gt;SELECT WHERE P&lt;/code&gt;，然后在这个事务还没结束的时候，另外的事务写入了一个新的满足这个条件的数据，这时候这个新写入的数据就是 Phantom 的了。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2f8f49d023908e37d376c59fe70b83fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;555&quot; data-rawheight=&quot;163&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2f8f49d023908e37d376c59fe70b83fe&quot; data-watermark-src=&quot;v2-c68b450aacc22072b667197b5fca85ab&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;假设现在 T1 按照某个条件读取到了所有雇员 a，b，c，这时候 count 是 3，然后 T2 插入了一个新的雇员 d，同时更新了 count 为 4，但这时候 T1 在读取 count 的时候会得到 4，已经跟之前读取到的 a，b，c 冲突了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P4 - Lost Update&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们有时候也会遇到一种 Lost Update 的问题，如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0ec0ae1e2ab8faf39d3a2ac241805d5a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;275&quot; data-rawheight=&quot;132&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0ec0ae1e2ab8faf39d3a2ac241805d5a&quot; data-watermark-src=&quot;v2-22e2ce89eb33e3b91d492ffa7559d430&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在上面的例子中，我们没有任何 dirty write，因为 T2 在 T1 更新之前已经提交成功，也没有任何 dirty read，因为我们在 write 之后没有任何 read 操作，但是，当整个事务结束之后，T2 的更新其实丢失了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P4C - Cursor Lost Update&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Cursor Lost Update 是上面 Lost Update 的一个变种，跟 SQL 的 cursor 相关。在下面的例子中，RC(x) 表明在 cursor 下面 read x，而 WC(x) 则表明在 cursor 下面写入 x。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41b6ac72eef6631f1a1b519b1bae22ec_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;317&quot; data-rawheight=&quot;134&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-41b6ac72eef6631f1a1b519b1bae22ec&quot; data-watermark-src=&quot;v2-f043051e2c5dcb038d9ca5dac74724c2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果我们允许 T2 在  T1 RC 和 WC 之间写入数据，那么 T2 的更新也会丢失。&lt;/p&gt;&lt;p&gt;&lt;b&gt;A5A - Read Skew&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Read Skew 发生在两个或者多个有完整性约束的数据上面，还是传统的转账例子，需要保证 x + y = 100，那么 T1 就会看到不一致的数据了。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bee88aafb7485a0ee2b236dfbf1fc7fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;342&quot; data-rawheight=&quot;168&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bee88aafb7485a0ee2b236dfbf1fc7fe&quot; data-watermark-src=&quot;v2-1d53742a9442a787055a5570a211b7d7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;A5B - Write Skew&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Write Skew 跟 Read Skew 比较类似，假设 x + y &amp;lt;= 100，T1 和 T2 在执行的时候都发现满足约束，然后 T1 更新了 y，而 T2 更新了 x，然后最终结果打破了约束，如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27784693015ffe46fde0048b629336fc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;339&quot; data-rawheight=&quot;163&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27784693015ffe46fde0048b629336fc&quot; data-watermark-src=&quot;v2-cbbd6978ff401beb981159d476f26e1b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;Isolation Levels&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面我们介绍了不同的异常情况，下面的表格说明了，在不同的隔离级别下面，那些异常情况可能发生：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98b3731ffae0c53321617da70017601a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;630&quot; data-rawheight=&quot;271&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-98b3731ffae0c53321617da70017601a&quot; data-watermark-src=&quot;v2-e879cb84c711e5d3f411752013581b54&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;NP - Not Possible，在该隔离级别下面不可能发生&lt;/li&gt;&lt;li&gt;SP - Sometimes Possible，在该隔离级别下面有时候可能发生&lt;/li&gt;&lt;li&gt;P - Possible，在该隔离级别下面会发生&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;鉴于网上已经对不同的 Isolation Level，尤其是 MySQL 的解释的太多了，这里就简单的解释一下。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Read Uncommitted - 能读到另外事务未提交的修改。&lt;/li&gt;&lt;li&gt;Read Committed - 能读到另外事务已经提交的修改。&lt;/li&gt;&lt;li&gt;Cursor Stability - 使用 cursor 在事务里面引用特定的数据，当一个事务用 cursor 来读取某个数据的时候，这个数据不可能被其他事务更改，除非 cursor 被释放，或者事务提交。&lt;/li&gt;&lt;li&gt;Monotonic Atomic View - 这个级别是 read committed 的增强，提供了一个原子性的约束，当一个在 T1 里面的 write 被另外事务 T2 观察到的时候，T1 里面所有的修改都会被 T2 给观察到。&lt;/li&gt;&lt;li&gt;Repeatable Read - 可重复读，也就是对于某一个数据，即使另外的事务有修改，也会读取到一样的值。&lt;/li&gt;&lt;li&gt;Snapshot - 每个事务都会在各自独立，一致的 snapshot 上面对数据库进行操作。所有修改只有在提交的时候才会对外可见。如果 T1 修改了某个数据，在提交之前另外的事务 T2 修改并提交了，那么 T1 会回滚。&lt;/li&gt;&lt;li&gt;Serializable - 事务按照一定顺序执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另外需要注意，上面提到的 isolation level 都不保证实时约束，如果一个进程 A 完成了一次写入 w，然后另外的进程 B 开始了一次读取 r，r 并不能保证观察到 w 的结果。另外，在不同事务之间，这些 isolation level 也不保证不同进程的顺序。一个进程可能在一次事务里面看到一次写入 w，但可能在后面的事务上面没看到同样的 w。事实上，一个进程甚至可能看不到在这个进程上面之前的写入，如果这些写入都是发生在不同的事务里面。有时候，他们还可能会对事务进行排序，譬如将 write-only 的事务放到所有的 read 事务的后面。&lt;/p&gt;&lt;p&gt;要解决这些问题，我们需要引入顺序约束，这也就是下面 Session Guarantee 要干的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Session Guarantee&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 HATs 论文里面，相关的概念叫做 Session Guarantee，主要是用来保证在一个 session 里面的实时性约束以及客户端的操作顺序。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Writes Follow Reads&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果某个进程读到了一次写入 w1 写入的值 v，然后进行了一次新的写入 w2，那么 w2 写入的值将会在 w1 之后可见。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Monotonic Reads&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程开始了一次读取 r1，然后在开始另一次读取 r2，那么 r2 不可能看到 r1 之前数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Monotonic Writes&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程先进行了一次写入 w1，然后在进行了一次写入 w2，那么所有其他的进程都会观察到 w1 在 w2 之前发生。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Read Your Writes&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程先进行了一次写入 w，然后后面执行了一次读取 r，那么 r 一定会看到 w 的改动。&lt;/p&gt;&lt;p&gt;&lt;b&gt;PRAM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PRAM 就是 Pipeline Random Access Memory，对于单个进程的写操作都被观察到是顺序的，但不同的进程写会观察到不同的顺序。譬如下面这个操作是满足 PRAM 的，但不满足后面说的 Causal。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ff2541b159c86357affa7d99ea5ada82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;310&quot; data-rawheight=&quot;165&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ff2541b159c86357affa7d99ea5ada82&quot; data-watermark-src=&quot;v2-803e3938c3a11c087360711b85e30659&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;Causal&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Causal 确定了有因果关系的操作在所有进程间的一致顺序。譬如下面这个：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-83163b55feb11508651baa8e834f7369_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;162&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-83163b55feb11508651baa8e834f7369&quot; data-watermark-src=&quot;v2-dc8e7b39dd320e689f93f8656748865c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 和 P4 来说，无论是先读到 2，还是先读到 1， 都是没问题的，因为 P1 和 P2 里面的 write 操作并没有因果性，是并行的。但是下面这个：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6b1daeebfd990cecb82687b46d80418c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;360&quot; data-rawheight=&quot;165&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6b1daeebfd990cecb82687b46d80418c&quot; data-watermark-src=&quot;v2-a579e40eaa5a6c55bbc68c02a3a2c596&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;就不满足 Cansal 的一致性要求了，因为对于 P2 来说，在 Write 2 之前，进行了一次 Read 1 的操作，已经确定了 Write 1 会在 Write 2 之前发生，也就是确定了因果关系，所以 P3 打破了这个关系。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Sequential 会保证操作按照一定顺序发生，并且这个顺序会在不同的进程上面都是一致的。一个进程会比另外的进程超前，或者落后，譬如这个进程可能读到了已经是陈旧的数据，但是，如果一个进程 A 从进程 B 读到了某个状态，那么它就不可能在读到 B 之前的状态了。&lt;/p&gt;&lt;p&gt;譬如下面的操作就是满足 Sequential 的：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e254f620c178957e286dedc8b39521c6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;357&quot; data-rawheight=&quot;161&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e254f620c178957e286dedc8b39521c6&quot; data-watermark-src=&quot;v2-a54247f317d5539b43f6f1d43bfe7bc7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 来说，它仍然能读到之前的 stale 状态 1。但下面的就不对了：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0894ed397fa0d0bcbfc5e8ad9a47dfbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;164&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0894ed397fa0d0bcbfc5e8ad9a47dfbf&quot; data-watermark-src=&quot;v2-443516e6968a62e0a43cf08e7d87e6a4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 来说，它已经读到了最新的状态 2，就不可能在读到之前的状态 1 了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Linearizable&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Linearizability 要求所有的操作都是按照一定的顺序原子的发生，而这个顺序可以认为就是跟操作发生的时间一致的。也就是说，如果一个操作 A 在 B 开始之前就结束了，那么 B 只可能在 A 之后才能产生作用。&lt;/p&gt;&lt;p&gt;譬如下面的操作：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3f66bc3e9f6504131cf24a2a940cd94f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;302&quot; data-rawheight=&quot;161&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3f66bc3e9f6504131cf24a2a940cd94f&quot; data-watermark-src=&quot;v2-f876bd93b74fe1b9d004ffd3a171c148&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 和 P4 来说，因为之前已经有新的写入，所以他们只能读到 2，不可能读到 1。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strict Serializable&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;终于来到了 Strict Serializable，大家可以看到，它结合了 serializable 以及 linearizable，也就是说，它会让所有操作按照实时的顺序依次操作，也就是所有的进程会观察到完全一致的顺序，这也是最强的一致性模型了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;好了，最后再来聊聊 &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt;，TiKV 是一个支持分布式事务的 key-value database。对于某个事务，TiKV 会通过 PD 这个服务在事务开始的时候分配一个 start timestamp，以及事务提交的时候分配一个 commit timestamp。因为我们的授时是通过 PD 这个单点服务进行的，所以时间是一定能保证单调递增的，也就是说，我们所有的操作都能跟保证实时有序，也就是满足 Linearizable。&lt;/p&gt;&lt;p&gt;TiKV 采用的是常用的 MVCC 模型，也就是每个 key-value 实际存储的时候，会在 key 上面带一个 timestamp，我们就可以用 timestamp 来生成整个数据库的 snapshot 了，所以 TiKV 是 snapshot isolation 的。既然是 snapshot isolation，那么就会遇到 write skew 问题，所以 TiKV 额外提供了 serializable snapshot isolation，用户需要显示的对要操作的数据进行 lock 操作。&lt;/p&gt;&lt;p&gt;但现在 TiKV 并不支持对 range 加 lock，所以不能完全的防止 phantom，譬如假设最多允许 8 个任务，现在已经有 7 个任务了，我们还可以添加一个任务，但这时候另外一个事务也做了同样的事情，但添加的是不同的任务，这时候就会变成 9 个任务，另外的事务在 scan 的时候就会发现打破了约束。这个也就是 A Critique of ANSI SQL Isolation Levels 里面提到的 sometimes possible。&lt;/p&gt;&lt;p&gt;所以，TiKV 是 snapshot isolation + linearizable。虽然 TiKV 也可以支持 Read Committed，但通常不建议在生产环境中使用，因为 TiKV 的 Read Committed 跟传统的还不太一样，可能会出现能读到一个事务提交到某个节点的数据，但这时候在另外的节点还读不到这个事务提交的数据，毕竟在分布式系统下面，不同节点的事务提交也是有网络延迟的，不可能同时执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;小结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在分布式系统里面，一致性是非常重要的一个概念，理解了它，在自己设计分布式系统的时候，就能充分的考虑到底系统应该提供怎样的一致性模型。譬如对于 TP 数据库来说，就需要有一个比较 strong 的一致性模型，而对于一些不重要的系统，譬如 cache 这些，就可以使用一些比较 weak 的模型。对 TiKV 来说，我们在 Percolator 基础上面，也一直在致力于分布式事务的优化，如果你对这方面感兴趣，欢迎联系我 tl@pingcap.com。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;原文链接：&lt;a href=&quot;https://www.jianshu.com/p/3673e612cce2&quot;&gt;一致性模型&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;延展阅读 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486947&amp;amp;idx=1&amp;amp;sn=28f3fe47d380e0ee991c207251fdd415&amp;amp;chksm=eb162a89dc61a39f47bb4d3f90be9789f4b7f0a23ada7979ee65d21efcb43edd9f5e8254ff2b&amp;amp;scene=21#wechat_redirect&quot;&gt;线性一致性和 Raft&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486842&amp;amp;idx=1&amp;amp;sn=2e21e65010f497693f26cfc344e418fe&amp;amp;chksm=eb162a10dc61a30650269d414de2cfe4eeff08e0d5e9b50834c3353c70850c83b796fd2be364&amp;amp;scene=21#wechat_redirect&quot;&gt;TiKV 是如何存取数据的（上）&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486872&amp;amp;idx=1&amp;amp;sn=1f8d7e88cd92878142a444c2aea8e764&amp;amp;chksm=eb162af2dc61a3e4a6675f86f91e8bd97886b6fd8006662df2f8e58f5190514a192a259beb25&amp;amp;scene=21#wechat_redirect&quot;&gt;TiKV 是如何存储数据的（下）&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-23-47445841</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>线性一致性和 Raft</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-22-47117804.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47117804&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-85e813810e9221590c5a48b11d906dc2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：沈泰宁&lt;/p&gt;&lt;p&gt;在讨论分布式系统时，共识算法（Consensus algorithm）和一致性（Consistency）通常是讨论热点，两者的联系很微妙，很容易搞混。一些常见的误解：使用了 Raft [0] 或者 paxos 的系统都是线性一致的（Linearizability [1]，即强一致），其实不然，共识算法只能提供基础，要实现线性一致还需要在算法之上做出更多的努力。以 TiKV 为例，它的共识算法是 Raft，在 Raft 的保证下，TiKV 提供了满足线性一致性的服务。&lt;/p&gt;&lt;p&gt;本篇文章会讨论一下线性一致性和 Raft，以及 TiKV 针对前者的一些优化。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;线性一致性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;什么是一致性，简单的来说就是评判一个并发系统正确与否的标准。线性一致性是其中一种，CAP [2] 中的 C 一般就指它。什么是线性一致性，或者说怎样才能达到线性一致？在回答这个问题之前先了解一些背景知识。&lt;/p&gt;&lt;p&gt;&lt;b&gt;背景知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了回答上面的问题，我们需要一种表示方法描述分布式系统的行为。分布式系统可以抽象成几个部分:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Client&lt;/li&gt;&lt;li&gt;Server&lt;/li&gt;&lt;li&gt;Events&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Invocation&lt;/li&gt;&lt;li&gt;Response&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Operations&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Read&lt;/li&gt;&lt;li&gt;Write&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;一个分布式系统通常有两种角色，Client 和 Server。Client 通过发起请求来获取 Server 的服务。一次完整请求由两个事件组成，Invocation（以下简称 Inv）和 Response（以下简称 Resp）。一个请求中包含一个 Operation，有两种类型 Read 和 Write，最终会在 Server 上执行。&lt;/p&gt;&lt;p&gt;说了一堆不明所以的概念，现在来看如何用这些表示分布式系统的行为。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c6efe0b1804c98ae04cd2ba866aea6f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;232&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c6efe0b1804c98ae04cd2ba866aea6f9&quot; data-watermark-src=&quot;v2-2dbf1014330a7675f4198926546c6826&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图展示了 Client A 的一个请求从发起到结束的过程。变量 x 的初始值是 1，“x R() A” 是一个事件 Inv 意思是 A 发起了读请求，相应的 “x OK(1) A” 就是事件 Resp，意思是 A 读到了 x 且值为 1，Server 执行读操作（Operation）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;如何达到线性一致&lt;/b&gt;&lt;/p&gt;&lt;p&gt;背景知识介绍完了，怎样才能达到线性一致？这就要求 Server 在执行 Operations 时需要满足以下三点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;瞬间完成（或者原子性）&lt;/li&gt;&lt;li&gt;发生在 Inv 和 Resp 两个事件之间&lt;/li&gt;&lt;li&gt;反映出“最新”的值&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面我举一个例子，用以解释上面三点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;例：&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6465b3ad4bd77c4926e2786c020b5c46_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;799&quot; data-rawheight=&quot;440&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6465b3ad4bd77c4926e2786c020b5c46&quot; data-watermark-src=&quot;v2-83bbf021252b353fec99845780381d71&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;先下结论，上图表示的行为满足线性一致。&lt;/p&gt;&lt;p&gt;对于同一个对象 x，其初始值为 1，客户端 ABCD 并发地进行了请求，按照真实时间（real-time）顺序，各个事件的发生顺序如上图所示。对于任意一次请求都需要一段时间才能完成，例如 A，“x R() A” 到 “x Ok(1) A” 之间的那条线段就代表那次请求花费的时间，而请求中的读操作在 Server 上的执行时间是很短的，相对于整个请求可以认为瞬间，读操作表示为点，并且在该线段上。线性一致性中没有规定读操作发生的时刻，也就说该点可以在线段上的任意位置，可以在中点，也可以在最后，当然在最开始也无妨。&lt;/p&gt;&lt;p&gt;第一点和第二点解释的差不多了，下面说第三点。&lt;/p&gt;&lt;p&gt;反映出“最新”的值？我觉得三点中最难理解就是它了。先不急于对“最新”下定义，来看看上图中 x 所有可能的值，显然只有 1 和 2。四个次请求中只有 B 进行了写请求，改变了 x 的值，我们从 B 着手分析，明确 x 在各个时刻的值。由于不能确定 B 的 W（写操作）在哪个时刻发生，能确定的只有一个区间，因此可以引入&lt;b&gt;上下限&lt;/b&gt;的概念。对于 x=1，它的上下限为&lt;b&gt;开始到事件“x W(2) B”&lt;/b&gt;，在这个范围内所有的读操作必定读到 1。对于 x=2，它的上下限为 &lt;b&gt;事件“x Ok() B”&lt;/b&gt; 到结束，在这个范围内所有的读操作必定读到 2。那么“x W(2) B”到“x Ok() B”这段范围，x 的值是什么？&lt;b&gt;1 或者 2&lt;/b&gt;。由此可以将 x 分为三个阶段，各阶段”最新”的值如下图所示:&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-14f1c87da0956711e7d1325446ea2619_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;996&quot; data-rawheight=&quot;614&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-14f1c87da0956711e7d1325446ea2619&quot; data-watermark-src=&quot;v2-196f91a809357196ccf0e3ef5881865f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;清楚了 x 的变化后理解例子中 A C D 的读到结果就很容易了。&lt;/p&gt;&lt;p&gt;最后返回的 D 读到了 1，看起来是 “stale read”，其实并不是，它仍满足线性一致性。D 请求横跨了三个阶段，而读可能发生在任意时刻，所以 1 或 2 都行。同理，A 读到的值也可以是 2。C 就不太一样了，C 只有读到了 2 才能满足线性一致。因为 “x R() C” 发生在 “x Ok() B” 之后（happen before [3]），可以推出 R 发生在 W 之后，那么 R 一定得读到 W 完成之后的结果：2。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话概括：在分布式系统上实现寄存器语义。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实现线性一致&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如开头所说，一个分布式系统正确实现了共识算法并不意味着能线性一致。共识算法只能保证多个节点对某个对象的状态是一致的，以 Raft 为例，它只能保证不同节点对 Raft Log（以下简称 Log）能达成一致。那么 Log 后面的状态机（state machine）的一致性呢？并没有做详细规定，用户可以自由实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raft&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Raft 是一个强 Leader 的共识算法，只有 Leader 能处理客户端的请求，集群的数据（Log）的流向是从 Leader 流向 Follower。其他的细节在这就不赘述了，网上有很多资料 [4]。&lt;/p&gt;&lt;p&gt;In Practice&lt;/p&gt;&lt;p&gt;以 TiKV 为例，TiKV 内部可分成多个模块，Raft 模块，RocksDB 模块，两者通过 Log 进行交互，整体架构如下图所示，consensus 就是 Raft 模块，state machine 就是 RocksDB 模块。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-18b6d578da3ad19604d89bfc0dbe7641_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;627&quot; data-rawheight=&quot;389&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-18b6d578da3ad19604d89bfc0dbe7641&quot; data-watermark-src=&quot;v2-fcadf6608de8a0d1478f67e3838f0943&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Client 将请求发送到 Leader 后，Leader 将请求作为一个 Proposal 通过 Raft 复制到自身以及 Follower 的 Log 中，然后将其 commit。TiKV 将 commit 的 Log 应用到 RocksDB 上，由于 Input（即 Log）都一样，可推出各个 TiKV 的状态机（即 RocksDB）的状态能达成一致。但实际多个 TiKV 不能保证同时将某一个 Log 应用到 RocksDB 上，也就是说各个节点不能&lt;b&gt;实时&lt;/b&gt;一致，加之 Leader 会在不同节点之间切换，所以 Leader 的状态机也不总有最新的状态。Leader 处理请求时稍有不慎，没有在最新的状态上进行，这会导致整个系统违反线性一致性。&lt;b&gt;好在有一个很简单的解决方法：依次应用 Log，将应用后的结果返回给 Client。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这方法不仅简单还通用，读写请求都可以这样实现。这个方法依据 commit index 对所有请求都做了排序，使得每个请求都能反映出状态机在执行完前一请求后的状态，可以认为 commit 决定了 R/W 事件发生的顺序。Log 是严格全序的（total order），那么自然所有 R/W 也是全序的，将这些 R/W 操作一个一个应用到状态机，所得的结果必定符合线性一致性。这个方法的缺点很明显，性能差，因为所有请求在 Log 那边就被序列化了，无法并发的操作状态机。&lt;br&gt;这样的读简称 LogRead。由于读请求不改变状态机，这个实现就显得有些“重“，不仅有 RPC 开销，还有写 Log 开销。优化的方法大致有两种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;ReadIndex&lt;/li&gt;&lt;li&gt;LeaseRead&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;ReadIndex&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相比于 LogRead，ReadIndex 跳过了 Log，节省了磁盘开销，它能大幅提升读的吞吐，减小延时（但不显著）。Leader 执行 ReadIndex 大致的流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;记录当前的 commit index，称为 ReadIndex&lt;/li&gt;&lt;li&gt;向 Follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是 Leader&lt;/li&gt;&lt;li&gt;等待状态机&lt;b&gt;至少&lt;/b&gt;应用到 ReadIndex 记录的 Log&lt;/li&gt;&lt;li&gt;执行读请求，将结果返回给 Client&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;第 3 点中的“至少”是关键要求，它表明状态机应用到 ReadIndex 之后的状态都能使这个请求满足线性一致，不管过了多久，也不管 Leader 有没有飘走。为什么在 ReadIndex 只有就满足了线性一致性呢？之前 LogRead 的读发生点是 commit index，这个点能使 LogRead 满足线性一致，那显然发生这个点之后的 ReadIndex 也能满足。&lt;/p&gt;&lt;p&gt;&lt;b&gt;LeaseRead&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LeaseRead 与 ReadIndex 类似，但更进一步，不仅省去了 Log，还省去了网络交互。它可以大幅提升读的吞吐也能显著降低延时。基本的思路是 Leader 取一个比 Election Timeout 小的租期，在租期不会发生选举，确保 Leader 不会变，所以可以跳过 ReadIndex 的第二步，也就降低了延时。 LeaseRead 的正确性和时间挂钩，因此时间的实现至关重要，如果漂移严重，这套机制就会有问题。&lt;/p&gt;&lt;p&gt;Wait Free&lt;/p&gt;&lt;p&gt;到此为止 Lease 省去了 ReadIndex 的第二步，实际能再进一步，省去第 3 步。这样的 LeaseRead 在收到请求后会立刻进行读请求，不取 commit index 也不等状态机。由于 Raft 的强 Leader 特性，在租期内的 Client 收到的 Resp 由 Leader 的状态机产生，所以只要状态机满足线性一致，那么在 Lease 内，不管何时发生读都能满足线性一致性。有一点需要注意，只有在 Leader 的状态机应用了当前 term 的第一个 Log 后才能进行 LeaseRead。因为新选举产生的 Leader，它虽然有全部 committed Log，但它的状态机可能落后于之前的 Leader，状态机应用到当前 term 的 Log 就保证了新 Leader 的状态机一定新于旧 Leader，之后肯定不会出现 stale read。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本文粗略地聊了聊线性一致性，以及 TiKV 内部的一些优化。最后留四个问题以便更好地理解本文：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于线性一致中的例子，如果 A 读到了 2，那么 x 的各个阶段是怎样的呢？&lt;/li&gt;&lt;li&gt;对于下图，它符合线性一致吗？（温馨提示：请使用游标卡尺。;-P）&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-58c02cee775ff40a6f89e29afb3e7fe5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;739&quot; data-rawheight=&quot;432&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-58c02cee775ff40a6f89e29afb3e7fe5&quot; data-watermark-src=&quot;v2-165658e7cfbec5e0b5c61f0df93c982a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;3. Leader 的状态机在什么时候没有最新状态？要线性一致性，Raft 该如何解决这问题？&lt;/p&gt;&lt;p&gt;4. FollowerRead 可以由 ReadIndex 实现，那么能由 LeaseRead 实现吗？&lt;/p&gt;&lt;p&gt;如有疑问或想交流，欢迎联系我：&lt;b&gt;shentaining@pingcap.com&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[0].Ongaro, Diego. Consensus: Bridging theory and practice. Diss. Stanford University, 2014.&lt;br&gt;[1].Herlihy, Maurice P., and Jeannette M. Wing. “Linearizability: A correctness condition for concurrent objects.” ACM Transactions on Programming Languages and Systems (TOPLAS) 12.3 (1990): 463-492.&lt;br&gt;[2].Gilbert, Seth, and Nancy Lynch. “Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services.” Acm Sigact News 33.2 (2002): 51-59.&lt;br&gt;[3].Lamport, Leslie. “Time, clocks, and the ordering of events in a distributed system.” Communications of the ACM 21.7 (1978): 558-565.&lt;br&gt;[4].&lt;a href=&quot;https://raft.github.io/&quot;&gt;https://raft.github.io/&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-22-47117804</guid>
<pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>成都 Meetup 预告 |  PingCAP 成都分舵第一次干货分享趴</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-18-47050511.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47050511&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6c06b1218c416abf3c2b2c8b3b3a32fd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;i&gt;&lt;b&gt;Hit!!! &lt;/b&gt;&lt;/i&gt;社区小伙伴期待已久的「西南第一分舵」——&lt;i&gt;&lt;b&gt;成都 Office&lt;/b&gt;&lt;/i&gt;正式成立了！&lt;br&gt;我们将在本周日举办一场 Infra Meetup，欢迎新老朋友过来面基～不管你是想聊一聊技术干货还是想加入分舵，都可以点击文末链接直接报名来现场交流！&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;PingCAP Infra Meetup No.77&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;🚀 时间：2018-10-21  周日  13:00 - 16:00&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;🛸 地点：成都武侯区吉泰路银泰城 17 栋 优客工场&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;🚇 交通：&lt;/b&gt;地铁 1 号线天府三街站 B 口出，步行从吉泰路上的银泰城正门进入。银泰城车库入口在天府四街。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;报名通道&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://www.huodongxing.com/event/8462453900500&quot;&gt;报名：【成都】PingCAP Infra Meetup No.77&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;日程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;13:00 - 13:30  &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;现场签到&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;13:30 - 14:30 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Speaker&lt;/b&gt;&lt;br&gt;&lt;b&gt;申砾&lt;/b&gt;，我司技术副总裁&lt;/li&gt;&lt;li&gt;&lt;b&gt;Talk&lt;/b&gt;&lt;br&gt;Deep Dive into TiDB SQL Layer&lt;/li&gt;&lt;li&gt;&lt;b&gt;Content&lt;/b&gt;&lt;br&gt;本次分享主要涉及 TiDB 和 TiDB SQL 层的架构，带大家深入了解 TiDB SQL 层的优化器和执行引擎。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;14:30 - 14:45 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;茶歇&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;14:45 - 15:30 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Speaker&lt;/b&gt;&lt;br&gt;李银龙，马上消费金融 NewSQL 负责人，原腾讯云运维工程师，原猪八戒 DBA 团队构建者。「马上消费金融股份有限公司是一家经中国银保监会批准，持有消费金融牌照的科技驱动的金融机构，是注册资本金第一大的内资消费金融公司。」&lt;/li&gt;&lt;li&gt;&lt;b&gt;Talk&lt;/b&gt;&lt;br&gt;马上消费金融 TiDB 实践分享&lt;/li&gt;&lt;li&gt;&lt;b&gt;Content&lt;/b&gt;&lt;br&gt;本次我们将分享在消费金融行业爆发式增长背景下，马上消费金融面对的传统 MySQL 关系型数据库瓶颈与 NewSQL 技术探索实践。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;15:30 - 16:00 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;社区圆桌讨论&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;入舵指南&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;成都分舵地址&lt;/b&gt;：天府大道中段 666 号希顿国际中心 C 座&lt;/li&gt;&lt;li&gt;&lt;b&gt;勾搭通道&lt;/b&gt;：hire@pingcap.com&lt;/li&gt;&lt;li&gt;&lt;b&gt;职位信息：&lt;/b&gt;Infrastructure Engineer（包括分布式存储-TiKV、分布式计算-TiDB、分布式调度-PD、商业工具-Tools、SRE、Cloud 等方向）在成都分舵已全面开放，了解更多职位信息：&lt;a href=&quot;https://pingcap.com/recruit-cn/join/&quot;&gt;虚位以待 | PingCAP&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;🧐 报名通道：&lt;a href=&quot;http://www.huodongxing.com/event/8462453900500&quot;&gt;【成都】PingCAP Infra Meetup No.77&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-18-47050511</guid>
<pubDate>Thu, 18 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 是如何存储数据的（下）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-11-46524530.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46524530&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ef0379b7d20b5347e99e16cc5f6bed9e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;u&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486842&amp;amp;idx=1&amp;amp;sn=2e21e65010f497693f26cfc344e418fe&amp;amp;chksm=eb162a10dc61a30650269d414de2cfe4eeff08e0d5e9b50834c3353c70850c83b796fd2be364&amp;amp;scene=21#wechat_redirect&quot;&gt;上篇文章&lt;/a&gt;&lt;/u&gt;中，我们介绍了与 TiKV 处理读写请求相关的基础知识，下面将开始详细的介绍 TiKV 的读写流程。Enjoy~&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;作者：唐刘 siddontang&lt;/p&gt;&lt;h2&gt;&lt;b&gt;RawKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiKV 提供两套 API，一套叫做 RawKV，另一套叫做 TxnKV。TxnKV 对应的就是上面提到的 Percolator，而 RawKV 则不会对事务做任何保证，而且比 TxnKV 简单很多，这里我们先讨论 RawKV。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Write&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-08650031b484425439a68ed74eca75c3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;675&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-08650031b484425439a68ed74eca75c3&quot; data-watermark-src=&quot;v2-1fd4a870d321c5da5c89ad8b1e17ccd3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;当进行写入，譬如 Write a = 1，会进行如下步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Client 找 PD 问 a 所在的 Region&lt;/li&gt;&lt;li&gt;PD 告诉 Region 相关信息，主要是 Leader 所在的 TiKV&lt;/li&gt;&lt;li&gt;Client 将命令发送给 Leader 所在的 TiKV&lt;/li&gt;&lt;li&gt;Leader 接受请求之后执行 Raft 流程&lt;/li&gt;&lt;li&gt;Leader 将 a = 1 Apply 到 KV RocksDB 然后给 Client 返回写入成功&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Read&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-633b8d2972993810a83683ae9a9fbf7f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1212&quot; data-rawheight=&quot;660&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-633b8d2972993810a83683ae9a9fbf7f&quot; data-watermark-src=&quot;v2-2f6356ecb30b884ce25ceec89af1c519&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;对于 Read 来说，也是一样的操作，唯一不同在于 Leader 可以直接提供 Read，不需要走 Raft。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TxnKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Write&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d4e569db6e896428f6d61f59dd10b934_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;649&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d4e569db6e896428f6d61f59dd10b934&quot; data-watermark-src=&quot;v2-8b72f207ab9b77f534c6393acb27b488&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;对于 TxnKV 来说，情况就要复杂的多，不过大部分流程已经在 Percolator 章节进行说明了。这里需要注意的是，因为我们要快速的 seek 到最新的 commit，所以在 RocksDB 里面，我们会先将 TS 使用 bigendian 生成 8 字节的 bytes，然后将这个 bytes 逐位取反，在跟原始的 key 组合存储到 RocksDB 里面，这样就能保证最新的提交存放到前面，seek 的时候就能直接定位了，当然 seek 的时候，也同样会将对应的 TS 按照相同的方式编码处理。&lt;/p&gt;&lt;p&gt;譬如，假设一个 key 现在有两次提交，commitTS 分别为 10 和 12，startTS 则是 9 和 11，那么在 RocksDB 里面，key 的存放顺序则是：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Write CF：

a_12 -&amp;gt; 11
a_10 -&amp;gt; 9

Data CF:

a_11 -&amp;gt; data_11
a_9 -&amp;gt; data_9&lt;/code&gt;&lt;p&gt;另外，还需要注意的是，对于 value 比较小的情况，TiKV 会直接将 value 存放到 Write CF 里面，这样 Read 的时候只要走 Write CF 就行了。在写入的时候，流程如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;PreWrite：

Lock CF: W a -&amp;gt; Lock + Data

Commit:
Lock CF: R a -&amp;gt; Lock + 10 + Data
Lock CF: D a

Write CF: W a_11 -&amp;gt; 10 + Data&lt;/code&gt;&lt;p&gt;对于 TiKV 来说，在 Commit 阶段无论怎样都会读取 Lock 来判断事务冲突，所以我们可以从 Lock 拿到数据，然后再写入到 Write CF 里面。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Read&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-59ca441db4424772565202a0b7b3e5bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;659&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-59ca441db4424772565202a0b7b3e5bb&quot; data-watermark-src=&quot;v2-e3239f73c2c07d91376dd4fd8a766bc1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Read 的流程之前的 Percolator 已经有说明了，这里就不详细解释了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;SQL Key Mapping&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们在 TiKV 上面构建了一个分布式数据库 TiDB，它是一个关系型数据库，所以大家需要关注的是一个关系型的 table 是如何映射到 key-value 上面的。假设我们有如下的表结构：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE t1 {
	id BIGINT PRIMARY KEY,
	name VARCHAR(1024),
	age BIGINT,
	content BLOB,
	UNIQUE(name),
	INDEX(age),
}&lt;/code&gt;&lt;p&gt;上面我们创建了一张表 t1，里面有四个字段，id 是主键，name 是唯一索引，age 是一个索引。那么这个表里面的数据是如何对应到 TiKV 的呢？&lt;/p&gt;&lt;p&gt;在 TiDB 里面，任何一张表都有一个唯一的 ID，譬如这里是 11，任何的索引也有唯一的 ID，上面 name 就是 12，age 就是 13。我们使用前缀 t 和 i 来区分表里面的 data 和 index。对于上面表 t1 来说，假设现在它有两行数据，分别是 (1, “a”, 10, “hello”) 和 (2, “b”, 12, “world”)，在 TiKV 里面，每一行数据会有不同的 key-value 对应。如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;PK
t_11_1 -&amp;gt; (1, “a”, 10, “hello”)
t_11_2 -&amp;gt; (2, “b”, 12, “world”)

Unique Name
i_12_a -&amp;gt; 1
i_12_b -&amp;gt; 2

Index Age
i_13_10_1 -&amp;gt; nil
i_13_12_2 -&amp;gt; nil&lt;/code&gt;&lt;p&gt;因为 PK 具有唯一性，所以我们可以用 t + Table ID + PK 来唯一表示一行数据，value 就是这行数据。对于 Unique 来说，也是具有唯一性的，所以我们用 i + Index ID + name 来表示，而 value 则是对应的 PK。如果两个 name 相同，就会破坏唯一性约束。当我们使用 Unique 来查询的时候，会先找到对应的 PK，然后再通过 PK 找到对应的数据。&lt;/p&gt;&lt;p&gt;对于普通的 Index 来说，不需要唯一性约束，所以我们使用 i + Index ID + age + PK，而 value 为空。因为 PK 一定是唯一的，所以两行数据即使 age 一样，也不会冲突。当我们使用 Index 来查询的时候，会先 seek 到第一个大于等于 i + Index ID + age 这个 key 的数据，然后看前缀是否匹配，如果匹配，则解码出对应的 PK，再从 PK 拿到实际的数据。&lt;/p&gt;&lt;p&gt;TiDB 在操作 TiKV 的时候需要保证操作 keys 的一致性，所以需要使用 TxnKV 模式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结语&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面简单的介绍了下 TiKV 读写数据的流程，还有很多东西并没有覆盖到，譬如错误处理，Percolator 的性能优化这些，如果你对这些感兴趣，可以参与到 TiKV 的开发，欢迎联系我 tl@pingcap.com。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-11-46524530</guid>
<pubDate>Thu, 11 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 是如何存取数据的（上）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-10-46372968.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46372968&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-427f9cc9aaef7836737e7c751e2324ec_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：唐刘 &lt;/blockquote&gt;&lt;p&gt;本文会详细的介绍 TiKV 是如何处理读写请求的，通过该文档，同学们会知道 TiKV 是如何将一个写请求包含的数据更改存储到系统，并且能读出对应的数据的。&lt;/p&gt;&lt;p&gt;本文分为上下两篇，在上篇中，我们将介绍一些基础知识，便于大家去理解后面的流程。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;基础知识&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Raft&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6489ef5e81f554b62f7b3b592ca977b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;432&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6489ef5e81f554b62f7b3b592ca977b3&quot; data-watermark-src=&quot;v2-6199c4b5150737aa28f158a06fa8bb09&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;TiKV 使用 Raft 一致性算法来保证数据的安全，默认提供的是三个副本支持，这三个副本形成了一个 Raft Group。&lt;/p&gt;&lt;p&gt;当 Client 需要写入某个数据的时候，Client 会将操作发送给 Raft Leader，这个在 TiKV 里面我们叫做 Propose，Leader 会将操作编码成一个 entry，写入到自己的 Raft Log 里面，这个我们叫做 Append。&lt;/p&gt;&lt;p&gt;Leader 也会通过 Raft 算法将 entry 复制到其他的 Follower 上面，这个我们叫做 Replicate。Follower 收到这个 entry 之后也会同样进行 Append 操作，顺带告诉 Leader Append 成功。&lt;br&gt;当 Leader 发现这个 entry 已经被大多数节点 Append，就认为这个 entry 已经是 Committed 的了，然后就可以将 entry 里面的操作解码出来，执行并且应用到状态机里面，这个我们叫做 Apply。&lt;/p&gt;&lt;p&gt;在 TiKV 里面，我们提供了 Lease Read，对于 Read 请求，会直接发给 Leader，如果 Leader 确定自己的 lease 没有过期，那么就会直接提供 Read 服务，这样就不用走一次 Raft 了。如果 Leader 发现 lease 过期了，就会强制走一次 Raft 进行续租，然后再提供 Read 服务。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Multi Raft&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-02fcecc350e76cac6d8a35bdc3febfb3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;443&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-02fcecc350e76cac6d8a35bdc3febfb3&quot; data-watermark-src=&quot;v2-cdfe9eadb97a3d203cc670f488af3cff&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;因为一个 Raft Group 处理的数据量有限，所以我们会将数据切分成多个 Raft Group，我们叫做 Region。切分的方式是按照 range 进行切分，也就是我们会将数据的 key 按照字节序进行排序，也就是一个无限的 sorted map，然后将其切分成一段一段（连续）的 key range，每个 key range 当成一个 Region。&lt;/p&gt;&lt;p&gt;两个相邻的 Region 之间不允许出现空洞，也就是前面一个 Region 的 end key 就是后一个 Region 的 start key。Region 的 range 使用的是前闭后开的模式  [start, end)，对于 key start 来说，它就属于这个 Region，但对于 end 来说，它其实属于下一个 Region。&lt;br&gt;TiKV 的 Region 会有最大 size 的限制，当超过这个阈值之后，就会分裂成两个 Region，譬如 [a, b) -&amp;gt; [a, ab) + [ab, b)，当然，如果 Region 里面没有数据，或者只有很少的数据，也会跟相邻的 Region 进行合并，变成一个更大的 Region，譬如 [a, ab) + [ab, b) -&amp;gt; [a, b)。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Percolator&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于同一个 Region 来说，通过 Raft 一致性协议，我们能保证里面的 key 操作的一致性，但如果我们要同时操作多个数据，而这些数据落在不同的 Region 上面，为了保证操作的一致性，我们就需要分布式事务。&lt;/p&gt;&lt;p&gt;譬如我们需要同时将 a = 1，b = 2 修改成功，而 a 和 b 属于不同的 Region，那么当操作结束之后，一定只能出现 a 和 b 要么都修改成功，要么都没有修改成功，不能出现 a 修改了，但 b 没有修改，或者 b 修改了，a 没有修改这样的情况。&lt;/p&gt;&lt;p&gt;最通常的分布式事务的做法就是使用 two-phase commit，也就是俗称的 2PC，但传统的 2PC 需要有一个协调者，而我们也需要有机制来保证协调者的高可用。这里，TiKV 参考了 Google 的 Percolator，对 2PC 进行了优化，来提供分布式事务支持。&lt;/p&gt;&lt;p&gt;Percolator 的原理是比较复杂的，需要关注几点：&lt;/p&gt;&lt;p&gt;首先，Percolator 需要一个服务 timestamp oracle (TSO) 来分配全局的 timestamp，这个 timestamp 是按照时间单调递增的，而且全局唯一。任何事务在开始的时候会先拿一个 start timestamp (startTS)，然后在事务提交的时候会拿一个 commit timestamp (commitTS)。&lt;/p&gt;&lt;p&gt;Percolator 提供三个 column family (CF)，Lock，Data 和 Write，当写入一个 key-value 的时候，会将这个 key 的 lock 放到 Lock CF 里面，会将实际的 value 放到 Data CF 里面，如果这次写入 commit 成功，则会将对应的 commit 信息放到入 Write CF 里面。&lt;/p&gt;&lt;p&gt;Key 在 Data CF 和 Write CF 里面存放的时候，会把对应的时间戳给加到 Key 的后面。在 Data CF 里面，添加的是 startTS，而在 Write CF 里面，则是 commitCF。&lt;/p&gt;&lt;p&gt;假设我们需要写入 a = 1，首先从 TSO 上面拿到一个 startTS，譬如 10，然后我们进入 Percolator 的 PreWrite 阶段，在 Lock 和 Data CF 上面写入数据，如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Lock CF: W a = lock
Data CF: W a_10 = value&lt;/code&gt;&lt;p&gt;后面我们会用 W 表示 Write，R 表示 Read， D 表示 Delete，S 表示 Seek。&lt;/p&gt;&lt;p&gt;当 PreWrite 成功之后，就会进入 Commit 阶段，会从 TSO 拿一个 commitTS，譬如 11，然后写入：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Lock CF: D a
Write CF: W a_11 = 10&lt;/code&gt;&lt;p&gt;当 Commit 成功之后，对于一个 key-value 来说，它就会在 Data CF 和 Write CF 里面都有记录，在 Data CF 里面会记录实际的数据， Write CF 里面则会记录对应的 startTS。&lt;br&gt;当我们要读取数据的时候，也会先从 TSO 拿到一个 startTS，譬如 12，然后进行读：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Lock CF: R a
Write CF: S a_12 -&amp;gt; a_11 = 10
Data CF: R a_10&lt;/code&gt;&lt;p&gt;在 Read 流程里面，首先我们看 Lock CF 里面是否有 lock，如果有，那么读取就失败了。如果没有，我们就会在 Write CF 里面 seek 最新的一个提交版本，这里我们会找到 11，然后拿到对应的 startTS，这里就是 10，然后将 key 和 startTS 组合在 Data CF 里面读取对应的数据。&lt;br&gt;上面只是简单的介绍了下 Percolator 的读写流程，实际会比这个复杂的多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RocksDB&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 会将数据存储到 RocksDB，RocksDB 是一个 key-value 存储系统，所以对于 TiKV 来说，任何的数据都最终会转换成一个或者多个 key-value 存放到 RocksDB 里面。&lt;/p&gt;&lt;p&gt;每个 TiKV 包含两个 RocksDB 实例，一个用于存储 Raft Log，我们后面称为 Raft RocksDB，而另一个则是存放用户实际的数据，我们称为 KV RocksDB。&lt;/p&gt;&lt;p&gt;一个 TiKV 会有多个 Regions，我们在 Raft RocksDB 里面会使用 Region 的 ID 作为 key 的前缀，然后再带上 Raft Log ID 来唯一标识一条 Raft Log。譬如，假设现在有两个 Region，ID 分别为 1，2，那么 Raft Log 在 RocksDB 里面类似如下存放：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;1_1 -&amp;gt; Log {a = 1}
1_2 -&amp;gt; Log {a = 2}
…
1_N -&amp;gt; Log {a = N}
2_1 -&amp;gt; Log {b = 2}
2_2 -&amp;gt; Log {b = 3}
…
2_N -&amp;gt; Log {b = N}&lt;/code&gt;&lt;p&gt;因为我们是按照 range 对 key 进行的切分，那么在 KV RocksDB 里面，我们直接使用 key 来进行保存，类似如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;a -&amp;gt; N
b -&amp;gt; N&lt;/code&gt;&lt;p&gt;里面存放了两个 key，a 和 b，但并没有使用任何前缀进行区分。&lt;/p&gt;&lt;p&gt;RocksDB 支持 Column Family，所以能直接跟 Percolator 里面的 CF 对应，在 TiKV 里面，我们在 RocksDB 使用 Default CF 直接对应 Percolator 的 Data CF，另外使用了相同名字的 Lock 和 Write。&lt;/p&gt;&lt;p&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 会将自己所有的 Region 信息汇报给 PD，这样 PD 就有了整个集群的 Region 信息，当然就有了一张 Region 的路由表，如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-18bf04cad2aa6a7995a78e9518a0d0e1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;484&quot; data-rawheight=&quot;418&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-18bf04cad2aa6a7995a78e9518a0d0e1&quot; data-watermark-src=&quot;v2-fd49907324671035b34aa6b19a1b3f78&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;当 Client 需要操作某一个 key 的数据的时候，它首先会向 PD 问一下这个 key 属于哪一个 Region，譬如对于 key a 来说，PD 知道它属于 Region 1，就会给 Client 返回 Region 1 的相关信息，包括有多少个副本，现在 Leader 是哪一个副本，这个 Leader 副本在哪一个 TiKV 上面。&lt;/p&gt;&lt;p&gt;Client 会将相关的 Region 信息缓存到本地，加速后续的操作，但有可能 Region 的 Raft Leader 变更，或者 Region 出现了分裂，合并，Client 会知道缓存失效，然后重新去 PD 获取最新的信息。&lt;/p&gt;&lt;p&gt;PD 同时也提供全局的授时服务，在 Percolator 事务模型里面，我们知道事务开始以及提交都需要有一个时间戳，这个就是 PD 统一分配的。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;基础知识就介绍到这里，下篇我们将详细的介绍 TiKV 的读写流程～ &lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-10-46372968</guid>
<pubDate>Wed, 10 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>雷神 Thor —— TiDB 自动化运维平台</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-08-46185503.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46185503&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-557ce943fab1b90541ff2d490292bc16_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：瞿锴，同程艺龙资深 DBA &lt;/blockquote&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;背景介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;随着互联网的飞速发展，业务量可能在短短的时间内爆发式地增长，对应的数据量可能快速地从几百 GB 涨到几百个 TB，传统的单机数据库提供的服务，在系统的可扩展性、性价比方面已经不再适用。为了应对大数据量下业务服务访问的性能问题，MySQL 数据库常用的分库、分表方案会随着 MySQL Sharding（分片）的增多，业务访问数据库逻辑会越来越复杂。而且对于某些有多维度查询需求的表，需要引入额外的存储或牺牲性能来满足查询需求，这样会使业务逻辑越来越重，不利于产品的快速迭代。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 的架构&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 作为 PingCAP 旗下开源的分布式数据库产品，具有多副本强一致性的同时能够根据业务需求非常方便的进行弹性伸缩，并且扩缩容期间对上层业务无感知。TiDB 包括三大核心组件：TiDB/TiKV/PD。&lt;/p&gt;&lt;p&gt;TiDB Server：主要负责 SQL 的解析器和优化器，它相当于计算执行层，同时也负责客户端接入和交互。&lt;/p&gt;&lt;p&gt;TiKV Server：是一套分布式的 Key-Value 存储引擎，它承担整个数据库的存储层，数据的水平扩展和多副本高可用特性都是在这一层实现。&lt;/p&gt;&lt;p&gt;PD Server：相当于分布式数据库的大脑，一方面负责收集和维护数据在各个 TiKV 节点的分布情况，另一方面 PD 承担调度器的角色，根据数据分布状况以及各个存储节点的负载来采取合适的调度策略，维持整个系统的平衡与稳定。&lt;/p&gt;&lt;p&gt;上面的这三个组件，每个角色都是一个多节点组成的集群，所以最终 TiDB 的架构看起来是这样的。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dfe437d191248134116e53aea0296e98_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;473&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-dfe437d191248134116e53aea0296e98&quot; data-watermark-src=&quot;v2-6fa8417f9ee498b4632ae6108b2e867e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;由此可见，分布式系统本身的复杂性导致手工部署和运维的成本是比较高的，并且容易出错。传统的自动化部署运维工具如 Puppet / Chef / SaltStack / Ansible 等，由于缺乏状态管理，在节点出现问题时不能及时自动完成故障转移，需要运维人员人工干预。有些则需要写大量的 DSL 甚至与 Shell 脚本一起混合使用，可移植性较差，维护成本比较高。&lt;/p&gt;&lt;p&gt;针对 TiDB 这种复杂的分布式数据库，我们考虑通过对 TiDB 容器化管理，实现以下几个目的：&lt;/p&gt;&lt;p&gt;一、屏蔽底层物理资源&lt;/p&gt;&lt;p&gt;二、提升资源利用率（CPU、内存）&lt;/p&gt;&lt;p&gt;三、提升运维效率&lt;/p&gt;&lt;p&gt;四、精细化管理&lt;/p&gt;&lt;p&gt;因此结合上述需要，我们开发了雷神系统来统一管理和维护 TiDB，其整体架构如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-992b9ecce8d568b33541b73fa9ad11b2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1018&quot; data-rawheight=&quot;511&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-992b9ecce8d568b33541b73fa9ad11b2&quot; data-watermark-src=&quot;v2-dee14b705d51f32695209dbacb771bd1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;从架构图中可以看出此方案是 TiDB 的私有云架构。最下层是容器云，中间一层是开发的容器编排工具，最上面一层针对 TiDB 特性和实际使用中遇到的问题点，进行了针对性开发从而实现了 TiDB 集群实例的统一化管理。下面将逐个介绍各个模块的功能。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;容器调度&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前主流的的容器编排系统 Kuberbetes 曾是我们容器调度的首选解决方案。但 TiDB 作为数据库服务需要将数据库存储到本地磁盘，而 Kuberbetes 对 Local Storage 不支持（目前新的版本已经开始支持）。针对 TiDB 的特性和业务需求，我们决定自己实现一套容器编排系统，具体解决以下问题： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持 LocalStorage，解决数据存储问题&lt;/li&gt;&lt;li&gt;基于 cpuset-cpus 实现 CPU 资源的随机均衡分配&lt;/li&gt;&lt;li&gt;定制化，支持 label，实现特定服务运行在特定宿主机上；宿主机资源限制&lt;/li&gt;&lt;li&gt;容器的主动发现和通知，以便将之前未管理的宿主机接入统一管理&lt;/li&gt;&lt;li&gt;容器的全生命周期的管理&lt;/li&gt;&lt;li&gt;容器异常的修复和通知&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;雷神 Thor 采用了模块化设计，分为控制模块和代理模块，其整体架构如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-28a86e4adc0a0f145c9550a05b5f48ec_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;585&quot; data-rawheight=&quot;623&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-28a86e4adc0a0f145c9550a05b5f48ec&quot; data-watermark-src=&quot;v2-f2782c72ef3ce96e42140b41f3696c4f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;说明：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;控制模块包含了 Allocator，Label，Discover，Manage，Customize。Allocator 主要负责宿主机资源的分配；Label 主要用于标签定制；Customize 主要负责定制化需求； Discover 主要负责容器的发现和异常检测；Manage 主要负责整体的调度和分发。&lt;/li&gt;&lt;li&gt;代理模块主要负责资源检查和信息收集、接受控制端的命令。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;集群管理&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08f65ecac29c104de00d9d3275bb34b4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;530&quot; data-rawheight=&quot;436&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-08f65ecac29c104de00d9d3275bb34b4&quot; data-watermark-src=&quot;v2-572a148e311363b3c8a4f47828df8e6d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;集群管理是整套系统的核心模块之一，包含了 TiDB 集群的日常维护操作，实现了 TiDB 初始化、平滑升级、弹性容量管理、监控的整合、集群的维护、节点的维护等功能。虽然 PingCAP 提供了基于 Ansible 的自动部署方案，但是依然需要填写大量的内容和检查相关机器设定来完成部署。通过此系统只需要将需求按照如何格式提交，即可完成整套集群的部署，部署时间从之前 2 个小时，缩减为 2 分钟左右。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据库管理&lt;/b&gt;&lt;br&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-babcc4bebbf0727ef16051ceeea49e47_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;801&quot; data-rawheight=&quot;431&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-babcc4bebbf0727ef16051ceeea49e47&quot; data-watermark-src=&quot;v2-7688be0215c1f39d4339c06c3e8f2eb7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;数据库管理是日常运维很核心的一块，此模块通过任务完成统计信息更新、过载保护、慢查询分析和 SQL 预警。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.统计信息更新&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 虽然会自动更新统计信息，但需要达到固定的变更百分比，因 TiDB 是作为分片库的合并库，数据量高达几十亿，若依赖自身的统计信息维护，将出现因统计信息不准确而触发的慢查询，故针对此种情况，设计和开发统计信息自动更新，除常规设定外，还可设定例外，避免因统计信息更新时影响业务的正常使用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 过载保护&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过对 SQL 的执行时间和内存的使用情况分析，针对不同的集群可以定制不同的过载保护策略，也可以使用统一的过载保护策略；当触发策略时，会将相关信息通过微信的方式通知相关人员。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 慢查询分析和 SQL 预警&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过 ELK 构建慢查询分析系统，通过 mysql-sniffer、flume、kafka、spark、hadoop 构建 SQL 预警，通过对趋势的分析和预判，为后续自动化容量管理做数据的积累。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据同步&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们尝试将 TiDB 作为所有数据的集合库提供复杂查询，分片集群则提供简单查询，同时由于 TiDB 高度兼容 MySQL 的连接协议为满足复杂的业务查询需求，我们基于 PingCAP 的数据同步工具 Syncer 进行了代码重构，开发了 hamal 同步工具，可以自定义库名和表名，同时新增了同步状态监控，如 TPS、延迟等，如果出现异常，会通过微信告警。从 MySQL 将数据实时同步到 TiDB 来确保数据的一致。该实时同步查询系统架构如下所示：&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8ad16b7b9d86910c4a9b6b7bdf9cde86_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;650&quot; data-rawheight=&quot;469&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8ad16b7b9d86910c4a9b6b7bdf9cde86&quot; data-watermark-src=&quot;v2-30a07f231b88e76c42df5f5bf582f3cb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;Hamal 是伪装成 mysql 从，从 mysql 主上通过主从复制协议来解析成对应的 sql 语句，并经过过滤、改写等步骤，将最终语句在目标库执行的工具。Hamal 主要包含以下特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;position 以及 gtid 模式支持&lt;/li&gt;&lt;li&gt;自动切换主从支持（需要提前配置好主从服务列表）&lt;/li&gt;&lt;li&gt;多目标库支持（多 tidb-server）&lt;/li&gt;&lt;li&gt;binlog 心跳支持&lt;/li&gt;&lt;li&gt;库、表级别过滤，重写支持（用于分片合库）&lt;/li&gt;&lt;li&gt;库表级别额外索引支持&lt;/li&gt;&lt;li&gt;拆解字段支持（额外输出选择某几个字段的小表）&lt;/li&gt;&lt;li&gt;字段过滤支持&lt;/li&gt;&lt;li&gt;智能更新表结构&lt;/li&gt;&lt;li&gt;多线程合并小事务后执行，多种分发策略&lt;/li&gt;&lt;li&gt;纯文本执行模式支持&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Hamal 的内部实现如下：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-73520b9115d50bf2cce7e93b3c229993_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;827&quot; data-rawheight=&quot;419&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-73520b9115d50bf2cce7e93b3c229993&quot; data-watermark-src=&quot;v2-1b352156ecac7dcaa98007b8a141d7c7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt; &lt;br&gt;从架构图中可以看出，通过设定不同的 generators，hamal 支持同步到不同目的库或者其他存储方式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;监控和告警中心&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;监控对于系统的重要性不言而喻。能否有效的告警直接影响着监控的质量，因此监控的核心是监控数据的采集和有效的告警。监控数据主要有三种系统本身的运行状态，例如 CPU、内存、磁盘、网络的使用情况；各种应用的运行状况，例如数据库、容器等，处理网络上发送过来的数据。通过监控项设定和监控例外，可以灵活的定制监控信息的收集。合理、灵活的监控规则可以帮助更快、更精确的定位异常，通过告警策略和告警例外满足不同的告警需求。监控和告警中心的架构图如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4dab4417616d0f38524d33cf8caed068_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;473&quot; data-rawheight=&quot;422&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4dab4417616d0f38524d33cf8caed068&quot; data-watermark-src=&quot;v2-376f9db0cb3f9d9cc931ff24a7e97049&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;其中，监控数据的采集一部分依赖于现有监控系统中的数据，如 zabbix 之类；一部分通过 TiDB 的 API 获取，一部分是开源的收集器，因此导致原始数据存储在不同类型的数据库，通过开发的同步工具，将上述数据同步独立部署的 TiDB 集群，以便后续的数据分析。可视化的实现主要基于 grafana 来完成。告警模块是基于实际的需求，进行开发和实现的，未采用现有的一些开源方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在对 TiDB 的使用过程中，我们按照 1 库 1 集群的方式进行服务部署，这种部署方式可以有效避免不同库的压力不均导致相互影响的问题，同时性能监控精准到库级别，而使用了雷神系统后，能够有效的在单台服务器上对各种服务资源进行快速部署，提升资源利用率的同时避免资源争用带来的问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;系统上线一年以来，已完成公司所有 TiDB 集群从物理机部署到容器化的平稳迁移；管理了数百台机器和数十套 TiDB Cluster，接入应用数百个，承载着几十 T 的数据量，峰值 TPS 数十万；上线前部署一个 TiDB 集群需要花费将近 2 个小时，雷神系统上线后只需要 2 分钟即可部署完成。有效的提升了 DBA 的运维效率和整个 TiDB 服务的可用性。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;未来我们将继续深入，从审计和 SQL 分析方面着手，为业务提供更多的效率提升和稳定性保障。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://mp.weixin.qq.com/s/0cHC8yPzNgKgFUmIWEsJ5g?scene=25#wechat_redirect&quot;&gt;雷神Thor—TIDB自动化运维平台&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-08-46185503</guid>
<pubDate>Mon, 08 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十九）tikv-client（下）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-27-45475314.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45475314&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fa485b637a9f12bf5a48586d49285dfd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：周昱行&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43926052&quot;&gt;上篇文章&lt;/a&gt; 中，我们介绍了数据读写过程中 tikv-client 需要解决的几个具体问题，本文将继续介绍 tikv-client 里的两个主要的模块——负责处理分布式计算的 copIterator 和执行二阶段提交的 twoPhaseCommitter。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;copIterator&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;1.copIterator 是什么&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在介绍 copIterator 的概念之前，我们需要简单回顾一下前面 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot;&gt;TiDB 源码阅读系列文章（六）&lt;/a&gt;中讲过的 distsql 和 coprocessor 的概念以及它们和 SQL 语句的关系。&lt;/p&gt;&lt;p&gt;tikv-server 通过 coprocessor 接口，支持部分 SQL 层的计算能力，大部分只涉及单表数据的常用的算子都可以下推到 tikv-server 上计算，计算下推以后，从存储引擎读取的数据虽然是一样的多，但是通过网络返回的数据会少很多，可以大幅节省序列化和网络传输的开销。&lt;/p&gt;&lt;p&gt;distsql 是位于 SQL 层和 coprocessor 之间的一层抽象，它把下层的 coprocessor 请求封装起来对上层提供一个简单的 &lt;code class=&quot;inline&quot;&gt;Select&lt;/code&gt; 方法。执行一个单表的计算任务。最上层的 SQL 语句可能会包含 &lt;code class=&quot;inline&quot;&gt;JOIN&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;SUBQUERY&lt;/code&gt; 等复杂算子，涉及很多的表，而 distsql 只涉及到单个表的数据。一个 distsql 请求会涉及到多个 region，我们要对涉及到的每一个 region 执行一次 coprocessor 请求。&lt;/p&gt;&lt;p&gt;所以它们的关系是这样的，一个 SQL 语句包含多个 distsql 请求，一个 distsql 请求包含多个 coprocessor 请求。&lt;/p&gt;&lt;p&gt;&lt;b&gt;copIterator 的任务就是实现 distsql 请求，执行所有涉及到的 coprocessor 请求，并依次返回结果。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. 构造 coprocessor task&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一个 distsql 请求需要处理的数据是一个单表上的 index scan 或 table scan，在 Request 包含了转换好的 KeyRange list。接下来，通过 region cache 提供的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/region_cache.go#L138&quot;&gt;LocateKey&lt;/a&gt; 方法，我们可以找到有哪些 region 包含了一个 key range 范围内的数据。&lt;/p&gt;&lt;p&gt;找到所有 KeyRange 包含的所有的 region 以后，我们需要按照 region 的 range 把 key range list 进行切分，让每个 coprocessor task 里的 key range list 不会超过 region 的范围。&lt;/p&gt;&lt;p&gt;构造出了所有 coprocessor task 之后，下一步就是执行这些 task 了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3. copIterator 的执行模式&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了更容易理解 copIterator 的执行模式，我们先从最简单的实现方式开始， 逐步推导到现在的设计。&lt;/p&gt;&lt;p&gt;copIterator 是 &lt;code class=&quot;inline&quot;&gt;kv.Response&lt;/code&gt; 接口的实现，需要实现对应 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L516&quot;&gt;Next&lt;/a&gt; 方法，在上层调用 Next  的时候，返回一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L390:6&quot;&gt;coprocessor response&lt;/a&gt;，上层通过多次调用 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法，获取多个 coprocessor response，直到所有结果获取完。&lt;/p&gt;&lt;p&gt;最简单的实现方式，是在 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法里，执行一个 coprocessor task，返回这个 task 的执行结果。&lt;/p&gt;&lt;p&gt;这个执行方式的一个很大的问题，大量时间耗费在等待 coprocessor 请求返回结果，我们需要改进一下。&lt;/p&gt;&lt;p&gt;coprocessor 请求如果是由 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 触发的，每次调用 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 就必须等待一个 &lt;code class=&quot;inline&quot;&gt;RPC  round trip&lt;/code&gt; 的延迟。我们可以改造成请求在 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 被调用之前触发，这样就能在 Next 被调用的时候，更早拿到结果返回，省掉了阻塞等待的过程。&lt;/p&gt;&lt;p&gt;在 copIterator 创建的时候，我们启动一个后台 worker goroutine 来依次执行所有的 coprocessor task，并把执行结果发送到一个 response channel，这样前台 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法只需要从这个 channel 里  receive 一个 coprocessor response 就可以了。如果这个 task 已经执行完成，&lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法可以直接获取到结果，立即返回。&lt;/p&gt;&lt;p&gt;当所有 coprocessor task 被 work 执行完成的时候，worker 把这个 response channel 关闭，&lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法在 receive channel 的时候发现 channel 已经关闭，就可以返回 &lt;code class=&quot;inline&quot;&gt;nil response&lt;/code&gt;，表示所有结果都处理完成了。&lt;/p&gt;&lt;p&gt;以上的执行方案还是存在一个问题，就是 coprocessor task 只有一个 worker 在执行，没有并行，性能还是不理想。&lt;/p&gt;&lt;p&gt;为了增大并行度，我们可以构造多个 worker 来执行 task，把所有的 task 发送到一个 task channel，多个 worker 从这一个 channel 读取 task，执行完成后，把结果发到 response channel，通过设置 worker 的数量控制并发度。&lt;/p&gt;&lt;p&gt;这样改造以后，就可以充分的并行执行了，但是这样带来一个新的问题，task 是有序的，但是由于多个 worker 并行执行，返回的 response 顺序是乱序的。对于不要求结果有序的 distsql 请求，这个执行模式是可行的，我们使用这个模式来执行。对于要求结果有序的 distsql 请求，就不能满足要求了，我们需要另一种执行模式。&lt;/p&gt;&lt;p&gt;当 worker 执行完一个 task 之后，当前的做法是把 response 发送到一个全局的 channel 里，如果我们给每一个 task 创建一个 channel，把 response 发送到这个 task 自己的 response channel 里，Next 的时候，就可以按照 task 的顺序获取 response，保证结果的有序。&lt;/p&gt;&lt;p&gt;以上就是 copIterator 最终的执行模式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4. copIterator 实现细节&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;理解执行模式之后，我们从源码的角度，分析一遍完整的执行流程。&lt;/p&gt;&lt;p&gt;&lt;b&gt;前台执行流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;前台的执行的第一步是 CopClient 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L82&quot;&gt;Send&lt;/a&gt; 方法。先根据 distsql 请求里的 &lt;code class=&quot;inline&quot;&gt;KeyRanges&lt;/code&gt; &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L238&quot;&gt;构造 coprocessor task&lt;/a&gt;，用构造好的 task 创建 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L88&quot;&gt;copIterator&lt;/a&gt;，然后调用 copIterator 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L438&quot;&gt;open&lt;/a&gt; 方法，启动多个后台 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L452&quot;&gt;worker goroutine&lt;/a&gt;，然后启动一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L454&quot;&gt;sender&lt;/a&gt; 用来把 task 丢进 task channel，最后 copIterator 做为 &lt;code class=&quot;inline&quot;&gt;kv.Reponse&lt;/code&gt; 返回。&lt;/p&gt;&lt;p&gt;前台执行的第二步是多次调用 &lt;code class=&quot;inline&quot;&gt;kv.Response&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 方法，直到获取所有的 response。&lt;/p&gt;&lt;p&gt;copIterator 在 &lt;code class=&quot;inline&quot;&gt;Next&lt;/code&gt; 里会根据结果是否有序，选择相应的执行模式，无序的请求会从 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L526&quot;&gt;全局 channel 里获取结果&lt;/a&gt;，有序的请求会在每一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L537&quot;&gt;task 的 response channel&lt;/a&gt; 里获取结果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;后台执行流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L417&quot;&gt;从 task channel 获取到一个 task&lt;/a&gt; 之后，worker 会执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L424&quot;&gt;handleTask&lt;/a&gt; 来发送 RPC 请求，并处理请求的异常，当 region 分裂的时候，我们需要重新构造 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L572&quot;&gt;新的 task&lt;/a&gt;，并重新发送。对于有序的 distsql 请求，分裂后的多个 task 的执行结果需要发送到旧的 task 的 response channel 里，所以一个 task 的 response channel 可能会返回多个 response，发送完成后需要 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/coprocessor.go#L428&quot;&gt;关闭 task 的 response channel&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;twoPhaseCommitter&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 2PC 简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2PC 是实现分布式事务的一种方式，保证跨越多个网络节点的事务的原子性，不会出现事务只提交一半的问题。&lt;/p&gt;&lt;p&gt;在 TiDB，使用的 2PC 模型是 Google percolator 模型，简单的理解，percolator 模型和传统的 2PC 的区别主要在于消除了事务管理器的单点，把事务状态信息保存在每个 key 上，大幅提高了分布式事务的线性 scale 能力，虽然仍然存在一个 timestamp oracle 的单点，但是因为逻辑非常简单，而且可以 batch 执行，所以并不会成为系统的瓶颈。&lt;/p&gt;&lt;p&gt;关于 percolator 模型的细节，可以参考这篇文章的介绍 &lt;a href=&quot;https://pingcap.com/blog-cn/percolator-and-txn/&quot;&gt;https://pingcap.com/blog-cn/percolator-and-txn/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. 构造 twoPhaseCommitter&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当一个事务准备提交的时候，会创建一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L62&quot;&gt;twoPhaseCommiter&lt;/a&gt;，用来执行分布式的事务。&lt;/p&gt;&lt;p&gt;构造的时候，需要做以下几件事情&lt;/p&gt;&lt;ul&gt;&lt;li&gt; href=&quot;https&lt;code class=&quot;inline&quot;&gt;://github.&lt;/code&gt;com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L91&quot;&amp;gt;从 memBuffer 和 lockedKeys 里收集所有的 key 和 mutation&lt;br&gt;&lt;code class=&quot;inline&quot;&gt;memBuffer&lt;/code&gt; 里的 key 是有序排列的，我们从头遍历 &lt;code class=&quot;inline&quot;&gt;memBuffer&lt;/code&gt; 可以顺序的收集到事务里需要修改的 key，value 长度为 0 的 entry 表示 &lt;code class=&quot;inline&quot;&gt;DELETE&lt;/code&gt; 操作，value 长度大于 0 表示 &lt;code class=&quot;inline&quot;&gt;PUT&lt;/code&gt; 操作，&lt;code class=&quot;inline&quot;&gt;memBuffer&lt;/code&gt; 里的第一个 key 做为事务的 primary key。&lt;code class=&quot;inline&quot;&gt;lockKeys&lt;/code&gt; 里保存的是不需要修改，但需要加读锁的 key，也会做为 mutation 的 &lt;code class=&quot;inline&quot;&gt;LOCK&lt;/code&gt; 操作，写到 TiKV 上。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L132&quot;&gt;计算事务的大小是否超过限制&lt;/a&gt;&lt;br&gt;在收集 mutation 的时候，会统计整个事务的大小，如果超过了最大事务限制，会返回报错。&lt;br&gt;太大的事务可能会让 TiKV 集群压力过大，执行失败并导致集群不可用，所以要对事务的大小做出硬性的限制。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L164&quot;&gt;计算事务的 TTL 时间&lt;/a&gt;&lt;br&gt;如果一个事务的 key 通过 &lt;code class=&quot;inline&quot;&gt;prewrite&lt;/code&gt; 加锁后，事务没有执行完，tidb-server 就挂掉了，这时候集群内其他 tidb-server 是无法读取这个 key 的，如果没有 TTL，就会死锁。设置了 TTL 之后，读请求就可以在 TTL 超时之后执行清锁，然后读取到数据。&lt;br&gt;我们计算一个事务的超时时间需要考虑正常执行一个事务需要花费的时间，如果太短会出现大的事务无法正常执行完的问题，如果太长，会有异常退出导致某个 key 长时间无法访问的问题。所以使用了这样一个算法，TTL 和事务的大小的平方根成正比，并控制在一个最小值和一个最大值之间。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;3. execute&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 twoPhaseCommiter 创建好以后，下一步就是执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L562&quot;&gt;execute&lt;/a&gt; 函数。&lt;/p&gt;&lt;p&gt;在 &lt;code class=&quot;inline&quot;&gt;execute&lt;/code&gt; 函数里，需要在 &lt;code class=&quot;inline&quot;&gt;defer&lt;/code&gt; 函数里执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L572&quot;&gt;cleanupKeys&lt;/a&gt;，在事务没有成功执行的时候，清理掉多余的锁，如果不做这一步操作，残留的锁会让读请求阻塞，直到 TTL 过期才会被清理。第一步会执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L585&quot;&gt;prewriteKeys&lt;/a&gt;，如果成功，会从 PD 获取一个 &lt;code class=&quot;inline&quot;&gt;commitTS&lt;/code&gt; 用来执行 &lt;code class=&quot;inline&quot;&gt;commit&lt;/code&gt; 操作。取到了 &lt;code class=&quot;inline&quot;&gt;commitTS&lt;/code&gt; 之后，还需要做以下验证:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;commitTS&lt;/code&gt; 比 &lt;code class=&quot;inline&quot;&gt;startTS&lt;/code&gt; 大&lt;/li&gt;&lt;li&gt;schema 没有过期&lt;/li&gt;&lt;li&gt;事务的执行时间没有过长&lt;/li&gt;&lt;li&gt;如果没有通过检查，事务会失败报错。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过检查之后，执行最后一步 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L620&quot;&gt;commitKeys&lt;/a&gt;，如果没有错误，事务就提交完成了。&lt;/p&gt;&lt;p&gt;当 &lt;code class=&quot;inline&quot;&gt;commitKeys&lt;/code&gt; 请求遇到了网络超时，那么这个事务是否已经提交是不确定的，这时候不能执行 &lt;code class=&quot;inline&quot;&gt;cleanupKeys&lt;/code&gt; 操作，否则就破坏了事务的一致性。我们对这种情况返回一个特殊的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L625&quot;&gt;undetermined error&lt;/a&gt;，让上层来处理。上层会在遇到这种 error 的时候，把连接断开，而不是返回给用一个执行失败的错误。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L533&quot;&gt;prewriteKeys&lt;/a&gt;,  &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L537&quot;&gt;commitKeys&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L541&quot;&gt;cleanupKeys&lt;/a&gt; 有很多相同的逻辑，需要把 keys 根据 region 分成 batch，然后对每个 batch 执行一次 RPC。&lt;/p&gt;&lt;p&gt;当 RPC 返回 region 过期的错误时，我们需要把这个 region 上的 keys 重新分成 batch，发送 RPC 请求。&lt;/p&gt;&lt;p&gt;这部分逻辑我们把它抽出来，放在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L191&quot;&gt;doActionOnKeys&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L239&quot;&gt;doActionOnBatches&lt;/a&gt; 里，并实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L319&quot;&gt;prewriteSinlgeBatch&lt;/a&gt;，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L421&quot;&gt;commitSingleBatch&lt;/a&gt;，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L497&quot;&gt;cleanupSingleBatch&lt;/a&gt; 函数，用来执行单个 batch 的 RPC 请求。&lt;/p&gt;&lt;p&gt;虽大部分逻辑是相同的，但是不同的请求在执行顺序上有一些不同，在 &lt;code class=&quot;inline&quot;&gt;doActionOnKeys&lt;/code&gt; 里需要特殊的判断和处理。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;prewrite&lt;/code&gt; 分成的多个 batch 需要同步并行的执行。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;commit&lt;/code&gt; 分成的多个 batch 需要先执行第一个 batch，成功后再异步并行执行其他的 batch。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;cleanup&lt;/code&gt; 分成的多个 batch 需要异步并行执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L239:29&quot;&gt;doActionOnBatches&lt;/a&gt; 会开启多个 goroutines 并行的执行多个 batch，如果遇到了 error，会把其他正在执行的 &lt;code class=&quot;inline&quot;&gt;context cancel&lt;/code&gt; 掉，然后返回第一个遇到的 error。&lt;/p&gt;&lt;p&gt;执行 &lt;code class=&quot;inline&quot;&gt;prewriteSingleBatch&lt;/code&gt; 的时候，有可能会遇到 region 分裂错误，这时候 batch 里的 key 就不再是一个 region 上的 key 了，我们会在这里递归的调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.2/store/tikv/2pc.go#L352&quot;&gt;prewriteKeys&lt;/a&gt;，重新走一遍拆分 batch 然后执行 &lt;code class=&quot;inline&quot;&gt;doActionOnBatch&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;prewriteSingleBatch&lt;/code&gt; 的流程。这部分逻辑在 &lt;code class=&quot;inline&quot;&gt;commitSingleBatch&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;cleanupSingleBatch&lt;/code&gt; 里也都有。&lt;/p&gt;&lt;p&gt;twoPhaseCommitter 包含的逻辑只是事务模型的一小部分，主要的逻辑在 tikv-server 端，超出了这篇文章的范围，就不在这里详细讨论了。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多 TiDB 源码阅读系列文章详见：&lt;a href=&quot;http://pingcap.com/blog-cn/#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB&quot;&gt;Blog-cns&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-27-45475314</guid>
<pubDate>Thu, 27 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 集群版本的安全迁移</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-22-45144603.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45144603&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3bc0f0e99ad335bf0ff6992419402210_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：陈书宁&lt;/p&gt;&lt;h2&gt;&lt;b&gt;问题描述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 TiDB 的产品迭代中，不免会碰到一些兼容性问题出现。通常协议上的兼容性 protobuf 已经能帮我们处理的很好，在进行功能开发，性能优化时，通常会保证版本是向后兼容的，但并不保证向前兼容性，因此，当集群中同时有新旧版本节点存在时，旧版本不能兼容新版本的特性，就有可能造成该节点崩溃，影响集群可用性，甚至丢失数据。目前在有不兼容的版本升级时，会要求进行离线升级，但这会影响到服务，我们需要一个适合的机制来进行不停服务的升级。因此我们需要在进行滚动升级时，让这些不能保证整个集群的向后兼容性的功能不被启用。只有在保证集群中所有节点都已经升级完成后，我们才安全的启用这些功能。&lt;/p&gt;&lt;p&gt;常见的当我们对引入新的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt; 的时候，旧版本的 TiKV 并不能识别新的添加的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt;，对于不能认知的 &lt;code class=&quot;inline&quot;&gt;RaftCommand&lt;/code&gt; TiKV 有不同的处理，可能会报错退出或忽略。比如为了支持 Raft Learner, 在 raftpb 里对添加新的 ConfChange 类型。 当 PD 在进行 Region 调度时，会先发送 &lt;code class=&quot;inline&quot;&gt;AddLearner&lt;/code&gt; 到 TiKV 上，接受到这个命令的肯定是这个 Region 的 Leader，在进行一系列检查后，会将该命令 Proposal, 而 Follwer 如果是旧版本的话，在 Apply 这条 Command 就会出错。而在滚动升级时，很有可能存在 Leader 是新版本，Follwer 是老版本的情况。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;引入版本检查机制&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的版本定义是遵循 Semver 的版本规则的。版本格式一般由主版本号（Major），次版本号（Minor），修订号（Patch），版本号递增规则如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;主版本号：当进行了不兼容的 API 修改。&lt;/li&gt;&lt;li&gt;次版本号：当做了向下兼容的功能性新增。&lt;/li&gt;&lt;li&gt;修订号：当做了向下兼容的问题修正。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;先行版本号（PreRelase）及版本编译信息可以加到“主版本号.次版本号.修订号”的后面，作为延伸。比如 TiDB 目前的版本是 2.1.0-beta，先行版号为 beta 版。&lt;/p&gt;&lt;p&gt;在此之前，集群并没有版本的概念，虽然每个组件都有各自的版本信息，但各个节点的各自组件的版本都可以任意的。没有一个管理机制可以管理或查看所有组件的版本信息。为了解决滚动升级过程中存在多个版本的兼容性问题，这里引入集群版本的概念，并由 TiDB 集群的中心节点 PD 来进行管理和检查。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;具体实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1.升级集群&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 PD 中，会设置一个 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 的键值对，对应当前运行集群中 TiKV 节点中最旧的版本。也就是必须要兼容这个版本， 因此不能打开集群中其他新版本的节点的一些不兼容的特性。&lt;/p&gt;&lt;p&gt;在集群启动的时候，每个 TiKV 都需要向 PD 注册，注册时会带上版本信息。当当前 TiKV 的版本低于集群版本的时候，该 TiKV 会注册失败。因为此时集群的版本已经是更高的版本了，而加入旧版本的节点需要对旧版本进行兼容，为了防止已有的特性降级，直接拒绝不兼容的版本加入，目前默认主版本号和此版本号一样则为兼容的版本。&lt;/p&gt;&lt;p&gt;如果 TiKV 的版本高于或等于当前的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 时， TiKV 能够注册成功并成功启动。每次注册都会触发 PD 的一次检查，会检测当前集群中正常运行的 TiKV 的最低版本，并与当前的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 进行比对，如果最低版本比 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 更加新，则将 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 更新。因此每次滚动升级的时候，能够自动更新集群的版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 版本特性的开启&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 很多功能是需要 PD 的参与，目前这些新功能的开启也是通过 PD 进行控制的。在 PD 中，会将每个版本新特性记录下来，在 TiKV 2.0 中，对应有 Raft Leaner， Region Merge。 TiKV 2.1 中有 Batch Split，Joint Consensus 等。这些特性都需要 PD 的参与与控制。比如说 Add Leaner，Region Merge，Joint Consensus 需要 PD 下发调度给 TiKV，Batch Split 则是 TiKV 主动发起并请求 PD 分配新的 Region ID。因此这些功能都是能通过 PD 进行控制的。PD 会通过比对当前的集群版本，选择开启当前集群版本所支持的新特性。从而保证版本的兼容性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 集群回滚&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当升级完成后，如果遇到问题需要进行集群进行回滚时， 需要手动修改集群版本后。PD 提供了 pdctl 可以通过命令手动修改集群的 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;，这时旧版本的 TiKV 就能注册并启动，从而进行回滚。&lt;/p&gt;&lt;p&gt;PD 对 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 是通过 etcd 进行了持久化，在每次 PD 启动的时候，leader 都会从 etcd kv 中加载出 &lt;code class=&quot;inline&quot;&gt;clustrer_version&lt;/code&gt;，然后提供服务。从而保证在 PD leader 切换后 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt; 的一致性。另外 PD 本身的版本可能会小于当前 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;。因此在滚动升级的时候，需要先升级 PD，如果只升级了 TiKV，虽然 &lt;code class=&quot;inline&quot;&gt;cluster_version&lt;/code&gt;已经更新到新的版本的，但 PD 并不能开启新的功能，因为对它来说是不支持的。如果出现这种情况，PD 的日志中会有报警。在升级的时候，最好按 PD，TiKV，TiDB 的顺序逐一对各个组件。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;后续计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面提到的新功能特性一般都是需要 PD 参与的。而有些特性不需要PD的参与，因此需要保证这种特性在 TiKV 之间是可以兼容的，实现的时候可以采用类是 &lt;code class=&quot;inline&quot;&gt;http2 &amp;lt;-&amp;gt; http&lt;/code&gt; 的方式，对请求进行降级装发，保留两套接口等。另为 TiDB 目前是自身保证可以无缝兼容，但与 TiKV 可能存在兼容性问题，往后同样考虑让TiDB 也在 PD上进行注册。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-22-45144603</guid>
<pubDate>Sat, 22 Sep 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
