<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sat, 15 Dec 2018 18:56:41 +0800</lastBuildDate>
<item>
<title>TiDB Lab 诞生记 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-14-52414551.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52414551&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-391401a44b5cfc4f07ef0d321a10fce0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文由&lt;b&gt;红凤凰粉凤凰粉红凤凰队&lt;/b&gt;的成员主笔，他们的项目 &lt;b&gt;TiDB Lab&lt;/b&gt; 在本届 TiDB Hackathon 2018 中获得了二等奖。TiDB Lab 为 TiDB 培训体系增加了一个可以动态观测 TiDB / TiKV / PD 细节的动画教学 Lab，让用户可以一边进行真实操作一边观察组件之间的变化，例如 SQL 的解析，Region 的变更等等，从而生动地理解 TiDB 的工作原理。&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a8ca052d52e744806a93b786c597e257_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;246&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a8ca052d52e744806a93b786c597e257&quot; data-watermark-src=&quot;v2-1698678ea9ba1448ad082b2e043d13e5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;项目简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Lab，全称 TiDB Laboratory&lt;/b&gt;，是一个集 TiDB 集群状态的在线实时可视化与交互式教学的平台。用户可以一边对 TiDB 集群各个组件 TiKV、TiDB、PD 进行各种操作，包括上下线、启动关闭、迁移数据、插入查询数据等，一边在 TiDB Lab 上以动画形式观察操作对集群的影响，例如数据是怎么流动的，Region 副本在什么情况下发生了变更等等。通过 TiDB Lab 这种对操作进行可视化反馈的交互模式，用户可以快速且生动地理解 TiDB 内部原理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.功能&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;实时动态展示 TiDB、TiKV 节点的新增、启动与关闭。&lt;/li&gt;&lt;li&gt;实时动态展示 TiDB 收到 SQL 后，物理算子将具体请求发送给某些 TiKV Region Leader 并获取数据的过程。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本状态的变化，例如新增、删除、分裂。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本内的数据量情况。&lt;/li&gt;&lt;li&gt;浏览集群事件历史（事件指上述四条功能所展示的各项内容）并查看事件的详细情况，包括事件的具体数据内容、SQL Plan 等等。&lt;/li&gt;&lt;li&gt;按 TiDB、TiKV 或 Region 过滤事件历史。&lt;/li&gt;&lt;li&gt;对事件历史进行时间穿梭：回到任意事件发生时刻重新观察当时的集群状态，或按事件单步重放观察集群状态的变化。&lt;/li&gt;&lt;li&gt;在线获取常用运维操作的操作指南。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3.愿景&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们其实为 TiDB Lab 规划了更大的愿景，但由于 Hackathon 时间关系，还来不及实现。我们希望能实现 TiDB Lab + TiDB 生态组件的沙盒，从而在 TiDB Lab 在线平台上直接提供命令执行与 SQL 执行功能。这样用户无需离开平台，无需自行准备机器下载部署，就可以直接在平台上根据提供的操作指南进行各类操作，并能观察操作带来的具体影响，形成操作与反馈的闭环，真正地实现零门槛浏览器在线教学。我们期望平台能提供用户以下的操作流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户获得一个 TiDB Lab 账户并登录（考虑到沙盒是占用实际资源的，需要通过账户许可来限制避免资源快速耗尽）。&lt;/li&gt;&lt;li&gt;用户在 TiDB Lab 上获得若干虚拟机器的访问权限，每个机器处于同一内网并具有独立 IP。这些虚拟机器的实际实现是资源受限的虚拟机或沙盒，因为作为教学实验不需要占用很多资源。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;例如：平台为用户自动分配了 5 个 IP 独立的沙盒的访问权限，地址为 192.168.0.1 ~ 192.168.0.5。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户在平台上进行第零章「架构原理」的学习。平台提供了一个默认的拓扑部署，用户可以在平台提供的在线 SQL Shell 中进行数据的插入、删除、更新等操作。用户通过平台观察到 SQL 是如何对应到 TiKV 存储节点上的，以及数据是怎么切分到不同 Region 的等等。&lt;/li&gt;&lt;li&gt;用户在平台上进行第一章「TiDB 部署」的学习，了解到可以通过 ansible 进行部署。教学样例是一个典型的 TiDB + TiKV 三副本部署。对于这个教学样例，平台告知用户 inventory.ini 具体内容应当写成什么样子。用户可以在平台提供的在线 Terminal 上修改 inventory 文件，并执行部署与集群启动命令。部署和启动均能在平台上实时反馈可视化。&lt;/li&gt;&lt;li&gt;用户继续进行后续章节学习，例如「TiDB 单一服务启动与关闭」。用户在可视化界面上点击某个刚才已经部署出来的节点，可以了解启动或关闭单个 TiDB 的命令。用户可以在平台提供的在线 Terminal 上执行这些命令，尝试启动或关闭单一 TiDB。&lt;/li&gt;&lt;li&gt;用户继续学习基础运维操作，例如「TiKV 扩容」。平台告知 inventory.ini 应当如何进行修改，用户可以根据指南在在线 Terminal 上进行实践，并通过可视化界面观察扩容的过程，例如其他节点上的副本被逐渐搬迁到新节点上。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b00fb1e54cf86282b73e86667a939732_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b00fb1e54cf86282b73e86667a939732&quot; data-watermark-src=&quot;v2-66a6b03f86e2e8fbc23c88126c249016&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;前期准备&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;团队&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们团队有三个人，是一个 PingCAP 同学与 TiDB 社区小伙伴钱同学的混合组队，其中 PingCAP 成员分别来自 TiKV 组与 OLAP 组。我们本着搞事情的想法，团队取名叫「&lt;b&gt;红凤凰粉凤凰粉红凤凰」&lt;/b&gt;，想围观主持人念团队名称（然而机智的主持人小姐姐让我们自报团队名称）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 原计划：干掉 gRPC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;鉴于从报名开始直到 Hackathon 正式开始前几小时，我们都在为原计划做准备，因此值得详细说一下…&lt;/p&gt;&lt;p&gt;我们一开始规划的 Hackathon 项目是换掉 TiKV、TiDB 之间的 RPC 框架 gRPC，原因有几个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一是发现 TiKV、TiDB 中 gRPC 经常占用了大量 CPU，尤其是在请求较多但很简单的 benchmark 场景中经常比 Coprocessor 这块儿还高，这在客户机器 CPU 资源比较少的情况下是性能瓶颈；&lt;/li&gt;&lt;li&gt;二是发现 gRPC 性能一般，在各类常用 RPC 框架的性能测试中 gRPC 经常是垫底水平；&lt;/li&gt;&lt;li&gt;三是 gRPC 主要设计用于用户产品与 Google 服务进行通讯，因而考虑到了包括负载均衡友好、流量控制等方面，但对 TiKV 与 TiDB 这类内部通讯来说这些都是用不上的功能，为此牺牲的性能是无谓的开销；&lt;/li&gt;&lt;li&gt;另外近期 TiKV 内部有一个实验是将多个 RPC 请求 batch 到一起再发送（当然处理时候再拆开一个一个执行原来的 handler），性能可以瞬间提高一倍以上，这也从侧面说明了 gRPC 框架自身开销很大，因为用户侧请求总量是一致的，处理模式也是一致的，唯一的区别就是 RPC 框架批量发送或一条一条发送。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们早在几周前就开始写简单 Echo Server 进行可行性验证和性能测试，是以下三个方面的正交：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;协议：CapnProto RPC、brpc over gRPC、裸写 echo。&lt;/li&gt;&lt;li&gt;服务端：Rust、C++，对应用于 TiKV。&lt;/li&gt;&lt;li&gt;客户端：Golang，对应用于 TiDB。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中 TiKV 侧调研了 Rust 和 C++ 的服务端实现，原因是 Rust 可以通过 binding 方式调用 C++ 服务端。而 TiDB 侧客户端实现不包含 C++ 的原因是 Golang 进行 C / C++ FFI 性能很差，因此可以直接放弃 C/C++ 包装一层 binding 用于 TiDB 的想法。最后测试下来，有以下结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CapnProto：序列化性能很高，但其 RPC 性能没有很突出。最重要的是，CapnProto 的 Golang Client 实现有 bug，并不能稳定地与 Rust 或 C++ 的 Server 进行 RPC 交互。作者回复说这是一个已知缺陷，涉及重构。这对于 Hackathon 来说是一个致命的问题，我们并没有充足的时间解决这个问题，直接导致我们放弃这个方案。&lt;/li&gt;&lt;li&gt;brpc over gRPC：可以实现使用 brpc C++ 客户端 &amp;amp; gRPC Golang 服务端配合（注：brpc 没有 Golang 的实现）。但这个方案本质只是替换服务端的实现，并没有替换协议，并不彻底，我们不是特别喜欢。另外在这个换汤不换药的方案下，测试下来性能的提升有限，且随着 payload 越大会越小。我们最终觉得作为一个 Hackathon 项目如果仅有有限的性能提升（虽然可以在展示的时候掩盖缺陷只展示优点），那么意义不是很大，最终无法用于产品，因而放弃。&lt;/li&gt;&lt;li&gt;裸写：我们三个成员都不是这方面的老司机，裸写大概是写不完的。。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 新计划：做一个用于培训的可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Hackathon 开始的前一个晚上，我们决定推翻重来，于是 brain storm 了几个想法，最后觉得做一个用于教学的可视化比较可行，并且具有比较大的实际意义。另外，这个新项目「集群可视化」相比原项目「换 RPC」来说更适合 Hackathon，主要在于：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;具有图形化界面，容易拿奖可以直观地展现成果。&lt;/li&gt;&lt;li&gt;并不是一个「非零即一」的任务。新项目有很多子功能，可以逐一进行实现，很稳妥，且不像老项目那样只有换完才知道效果。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们继续在这个「可视化」的想法上进行了进一步的思考，想到如果可以做成在线教学的模式，则可以进一步扩展其项目意义，形成一个完整的在线教学体系，因此最终决定了项目的 scope。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;具体实现&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;在 TiKV、TiDB 与 PD 中各个关键路径上将发生的具体「事件」记录下来，并在前端进行一一可视化。&lt;/i&gt;&lt;/blockquote&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;事件&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们将 TiDB Lab 进行可视化所需要的信号称为「事件」，并规划了以下「事件」：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiDB&lt;/li&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiKV&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 启动&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到一条 SQL&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 KvGet 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 PreWrite / Commit 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 Coprocessor 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 创建&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 删除&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 分裂&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Snapshot 复制&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 数据量发生显著变化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，由于时间关系、技术难度和可视化需要，实际实现的是以下事件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 事件：TiDB 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 KvGet 读请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 PreWrite / Commit 写入请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 Coprocessor 读请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 创建（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 删除（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 分裂（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 数据量发生显著变化（通过 Region 心跳实现）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;可视化部分由前端（lab-frontend）和事件收集服务（lab-gateway）组成。&lt;/p&gt;&lt;p&gt;事件收集服务是一个简单的 HTTP Server，各个组件通过 HTTP Post 方式告知事件，事件收集服务将其通过 WebSocket 协议实时发送给前端。事件收集服务非常简单，使用的是 Node.js 开发，基于 ExpressJs 启动 HTTP Server 并基于 SocketIO 实现与浏览器的实时通讯。ExpressJs 收到事件 JSON 后将其通过 SocketIO 进行广播，总代码仅仅十几行。&lt;/p&gt;&lt;p&gt;可视化前端采用 Vue 实现，动画使用 animejs 和 CSS3。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过模板实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一部分事件通过「由事件更新集群状态数据 – 由集群状态通过 Vue 渲染模板」进行可视化。&lt;/p&gt;&lt;p&gt;这类可视化是最简单的，以 TiDB 启动与否为例，TiDB 的启动与否在界面上呈现为一个标签显示为「Started」或「Stopped」，那么就是一个传统的 Vue MVVM 流程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;数据变量 instances.x.online 代表 TiDB x 是否已启动。&lt;/li&gt;&lt;li&gt;收到 TiDB Started 事件后，更新 instances.x.online = true 。&lt;/li&gt;&lt;li&gt;收到 TiDB Stopped 事件后，更新 instances.x.online = false 。&lt;/li&gt;&lt;li&gt;前端模板上，根据 instances.x.online 渲染成 Started 或 Stopped 对应的界面。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这类可视化的动画采用的是 CSS3 动画。由于 Region Peer 位置是由 left, top CSS 属性给出的，因此为其加上 transition，即可实现 Region Peer 在屏幕上显示的位置改变的动画。位置改变会主要发生在分裂时，分裂时 Region 列表中按顺序会新增一个，那么后面各个 Region 都要向后移动（或换到下一行等）。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;244&quot; data-rawheight=&quot;232&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_b.jpg&quot;&gt;&lt;p&gt;最后，使用 Vue Group Transition 功能，即可为 Region Peer 的新增与删除也加入动画效果。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过动画实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另一部分事件并不反应为一个持久化 DOM 的变化，例如 TiDB 收到 SQL 并发请求到某个具体 TiKV 上 Region peer 的事件，在前端展示为一个 TiDB 节点到 TiKV Region Peer 的过渡动画。动画开始前和动画结束后，DOM 没有什么变化，动画是一个临时的可视元素。这类动画通过 animejs 实现。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;748&quot; data-rawheight=&quot;496&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_b.jpg&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_r.gif&quot; data-caption=&quot;这个浮夸的开场动画效果也是 animjs 做的&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;200&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_b.jpg&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间穿梭&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;时间穿梭是一个在目前前端框架中提供的很时髦的功能，我们准备借鉴一波。主要包括：1. 回退到任意一个历史事件发生的时刻展示集群的状态；2. 从当前事件开始往后进行单步可视化重现。&lt;/p&gt;&lt;p&gt;时间穿梭的本质是需要实现两个基础操作：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于单一事件实现正向执行，即事件发生后，更新对应集群数据信息（如果采用「通过模板实现的可视化」），或创建临时动画 DOM（如果采用「通过动画实现的可视化」）。另外允许跳过「通过动画实现的可视化」这一步。&lt;/li&gt;&lt;li&gt;对于单一事件实现反向执行，即撤销这个事件造成的影响。对于「通过模板实现的可视化」，我们需要根据事件内容反向撤销它对集群数据信息的修改。对于「通过动画实现的可视化」，我们什么都不用做。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;时间穿梭的功能可以通过组合这两个基础操作实现。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;回退到任意历史事件发生时刻：若想要前往的事件早于当前呈现的事件，则对于这期间的事件逐一进行反向执行。若想要前往的事件晚于当前呈现的事件，则进行无动画的正向执行。&lt;/li&gt;&lt;li&gt;从当前事件开始单步可视化：执行一次有动画的正向执行。&lt;/li&gt;&lt;li&gt;实时展示新事件：执行一次有动画的正向执行。&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;950&quot; data-rawheight=&quot;572&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;3. TiDB 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的历史事件收集略为 Hack。由于需要过滤任何非用户发起的查询（类似 GC 或者 meta 查询会由背景协程频繁发起打扰使用体验），因此在用户链接入口处添加了 context 标记一路携带到执行层，再修改相应的协程同步数据结构添加需要转发的标记信息。比较麻烦的是类似 Point Get 这样接口允许携带信息非常少的调用，只好将标记位编码进 Key 本身了。Plan 的可视化其实并没有花多少功夫，因为找到 TiDB 本身已经做了类似的功能，我们无非只是将这块代码直接偷来了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. PD 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;原本不少事件希望在 TiKV 端完成侦听，不过显然 KV 一小时编译一次的效率无法满足 Hackathon 中多次试错的需要。因此我们改为在 PD 中侦测心跳和汇报事件。其实并没有什么神秘，在原本 PD 自己检查 Region 变更的代码拆分成 Region 和 Peer 变更：每次 PD 接到 Region 心跳会在 PD Cache 中进行 Version 和 confVer 变更的检测，主要涉及 Peer 的增加和减少等。而 Region Split 会单独由 RegionSplitReport 进行汇报，这里也会做一次 Hook。另外就是每次心跳会检查是否有未上报给 Lab 的 Region 信息，如果有就转换成 Peer 信息进行补发。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. TiKV 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上，原本我们计划了很多 TiKV 事件，但由于开发机器配置不佳，每次修改都要等待一小时进行一次编译，考虑到 Hackathon 上时间紧迫，因此最终大大缩减了 TiKV 上收集的事件数量，改为只收集启动和停止。基本架构是事件发生的时候，事件异步发送给一个 Channel，Channel 的另一端有一个异步的 worker 负责不断处理各个事件并通过 Hyper HTTP Client 发送出去。这个流程其实与 TiKV 中汇报 PD 事件有些类似，只是事件内容和汇报目标不一样。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感悟&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;以下文字 by 马晓宇 @ OLAP Team, PingCAP&lt;/i&gt;&lt;br&gt;&lt;i&gt;“由于比较能调侃干的活相对少一些，所以大王要我来巡山写感悟。”&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;就像队长说的，这个项目原本是希望做一个 bRPC 替换 gRPC 的试验，只是由于种种原因临到 Hackathon 前夜我们才确定这是个大概率翻车的点子。于是乎我们只好在酒店里抓耳挠腮，一边讨论可能的补救措施。&lt;/p&gt;&lt;p&gt;参加过、围观过几次 Hackathon，见过现场 Demo 效果最震撼的一次其实并不是一个技术上最优秀的作品，但是它的确赢得了大奖。那是一个脑洞奇大的点子：用 Kinect 体感加上 DirectX 的全 3D 展示做的网络流量实时展示；程序根据实时数据（举办方提供了实时网络测量统计信息）聚合显示不同粗细的炫酷弧光特效，而演示者则用手势操控地球模型的旋转。&lt;/p&gt;&lt;p&gt;于是在完全不了解大家是否能搞定前端的情况下，我们还是很轻率地决定了要做个重前端的项目。至于展示什么？既然时间不够，那么秀一个需要生产上线的可视化工具，翻车的概率就大的多，不如直接定位为 For Educational Purpose Only。而且大概是过度解读了炫酷对于成功的重要性，因此队长也轻率地决定了这个项目的基调是「极尽浮夸」。就在这样友好且不靠谱的讨论氛围下，这个项目的策划出炉了。&lt;/p&gt;&lt;p&gt;之后就是艰苦卓绝连绵无休的代码过程了。&lt;/p&gt;&lt;p&gt;龙毛二哥，晓峰和胡家属的 &lt;u&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487451&amp;amp;idx=2&amp;amp;sn=5f1ee6e838c3a86556fcd556662112c5&amp;amp;chksm=eb1628b1dc61a1a7e8f4cb82e2bfaab40cbfb27e986f9705f9166d629ff31a812f7ae45b1d73&amp;amp;scene=21#wechat_redirect&quot;&gt;TiNiuB Team&lt;/a&gt;&lt;/u&gt; 就在我们对面开工。讲道理这其实是我个人最喜欢的项目之一。第一天放学的时候，看着他们的进度其实我心里虚得不行：看看别人家的项目，我们的绝对主力还在折腾 TiDB Logo 动画，是我敢怒不敢言的状态。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f2ade45b86ee55be8b85a1198f684000_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;240&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f2ade45b86ee55be8b85a1198f684000&quot; data-watermark-src=&quot;v2-e650574cbe2a265f474817c1ac2c88e2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;原来我想偷一下懒回家睡个觉啥的，就临时改变主意继续配合队长努力干活；钱同学也抱着「TiKV 一天只能 Build 24 次必须珍惜」的态度写着人生的第一个 Rust 项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因此，这里必须鸣谢龙哥他们对我们的鞭策。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;事实证明，误打误撞但又深谋远虑的浮夸战略是有效的。Demo 的时候我很认真地盯着评委：在本项目最浮夸的 Logo 展示环节，大家的眼睛是发着光的，一如目睹了摩西分开红海。我个人认为这个动画 Logo 生生拉高了项目 50% 的评分。&lt;/p&gt;&lt;p&gt;只是具体要说感悟的话（似乎好多年没写感悟了呢），首先这次我们三个组员虽然有两个是 PingCAP 员工，不过由于技能的缺口大于人力，因此负责的任务都不是自己所属的模块：队长是 TiKV 组的但是在写前端，我是 OLAP 组的但是在改 PD 和 DB，钱同学也在做自己从来没写过的 Rust（参赛前一周简单入门了一下），因此其实还蛮有挑战的（笑）。然后偶尔写写自己不熟悉的语言，搞搞自己不熟悉的模块，会有一种别样的新鲜刺激感，这大概就是所谓的路边野花更香吧（嗯）？除此之外，Hackathon 更像一个大型社交活动，满足了猫一样孤僻的程序员群体被隐藏的社交欲，有利于码农的身心发展，因此可以多搞搞 :) 。&lt;/p&gt;&lt;p&gt;回到我们项目本身的话，其实 TiDB 的源码阅读或者其他介绍类文章其实并不能非常直观地帮助一头雾水的初学者理解这个系统。我们做这个项目的最大目的是能降低学习门槛，让所有人能以非常直观，互动的方式近距离理解她。所以希望这个项目能给大家带来方便吧。&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-14-52414551</guid>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>让 TiDB 访问多种数据源 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-14-52413872.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52413872&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4dcc3a79d7a6c17e1fcf431ec2440e84_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自&lt;b&gt;CC 组的兰海同学&lt;/b&gt;，他们的项目&lt;b&gt;《让 TiDB 访问多种数据源》&lt;/b&gt;在本届 TiDB Hackathon 2018 中获得了二等奖。该项目可以让 TiDB 支持多种外部数据源的访问，针对不同数据源的特点会不同的下推工作，使 TiDB 成为一个更加通用的数据库查询优化和计算平台。&lt;/blockquote&gt;&lt;p&gt;我们队伍是由武汉大学在校学生组成。我们选择的课题是让 TiDB 接入若干外部的数据源，使得 TiDB 成为一个更加通用的查询优化和计算平台。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么选这个课题&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;刚开始我们选择课题是 TiDB 执行计划的实时动态可视化。但是填了报名单后，TiDB Robot 回复我们说做可视化的人太多了。我们担心和别人太多冲突，所以咨询了导师的意见，改成了 TiDB 外部数据源访问。这期间也阅读了 F1 Query 和 Calcite 论文，看了东旭哥（PingCAP CTO）在 PingCAP 内部的论文阅读的分享视频。感觉写一个简单 Demo，还是可行的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;系统架构和效果展示&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08d5704dedbfe4ea20221beb6dabcf52_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;449&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-08d5704dedbfe4ea20221beb6dabcf52&quot; data-watermark-src=&quot;v2-c2844ea0dd651526266cace4ae7eac28&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如上图所示，TiDB 通过 RPC 接入多个不同的数据源。TiDB 利用 RPC 发送请求给远端数据源，远端数据源收到请求后，进行查询处理，返回结果。TiDB 拿到返回结果进一步的进行计算处理。&lt;/p&gt;&lt;p&gt;我们通过定义一张系统表 foreign_register(table_name,source_type,rpc_info) 记录一个表上的数据具体来自哪种数据源类型，以及对应的 RPC 连接信息。对于来自 TiKV 的我们不用在这个表中写入，默认的数据就是来自 TiKV。&lt;/p&gt;&lt;p&gt;我们想访问一张 PostgreSQL（后面简称为 PG）上的表：首先，我们在 TiDB 上定义一个表(记为表 a)，然后利用我们 register_foreign(a,postgresql,ip#port#table_name) 注册相关信息。之后我们就可以通过 select * from a 来读取在 PG 上名为 table_name 的表。&lt;/p&gt;&lt;p&gt;我们在设计各个数据源上数据访问时，充分考虑各个数据源自身的特点。将合适的操作下推到具体的数据源来做。例如，PG 本身就是一个完整的数据库系统，我们支持投影、条件、连接下推给 PG 来做。Redis 是一个内存键值数据库，我们考虑到其 Get 以及用正则来匹配键值很快，我们将在 Key 值列的点查询以及模糊匹配查询都推给了 Redis 来做，其他条件查询我们就没有进行下推。&lt;/p&gt;&lt;p&gt;具体的运行效果如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad72ad983498655bfb7831ce6711d645_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;266&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ad72ad983498655bfb7831ce6711d645&quot; data-watermark-src=&quot;v2-413d53a2fae37a96dcf666b492bb54d4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如图所示，我们在远程开了 3 个 RPC Server，负责接收 TiDB 执行过程中的外部表请求，并在内部的系统表中注册三张表，并在 TiDB 本地进行了模式的创建——分别是remotecsv，remoteredis，remotepg，还有一张本地 KV Store 上的 localkv 表。我们对 4 张表进行 Join 操作，效果如图所示，说明如下。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;远程 csv 文件我们不做选择下推，所以可以发现 csv 上的条件还是在 root（即本地）上做。&lt;/li&gt;&lt;li&gt;远程的 PG 表，我们会进行选择下推，所以可以发现 PG 表的 selection 被推到了 PG 上。&lt;/li&gt;&lt;li&gt;远程的 Redis 表，我们也会进行选择下推，同时还可以包括模型查询条件（Like）的下推。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;P.S. 此外，对于 PostgreSQL 源上两个表的 Join 操作，我们也做了Join 的下推，Join 节点也被推送到了 PostgreSQL 来做，具体的图示如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-eed13dc021d5362ada026473113cd194_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;270&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-eed13dc021d5362ada026473113cd194&quot; data-watermark-src=&quot;v2-63305dae95a51fbaa876ecde30b8342f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;如何做的&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于项目偏硬核的，需要充分理解 TiDB 的优化器，执行器等代码细节。所以在比赛前期，我们花了两三天去研读 TiDB 的优化器，执行器代码，弄清楚一个简单的 Select 语句扔进 TiDB 是如何进行逻辑优化，物理优化，以及生成执行器。之前我们对 TiDB 这些细节都不了解，硬着去啃。发现 TiDB 生成完执行器，会调用一个 Open 函数，这个函数还是一个递归调用，最终到 TableReader 才发出数据读取请求，并且已经开始拿返回结果。这个和以前分析的数据库系统还有些不同。前期为了检验我们自己对 TiDB 的执行流程理解的是否清楚，我们尝试着去让 TiDB 读取本地 csv 文件。&lt;/p&gt;&lt;p&gt;比赛正式开始，我们一方面完善 csv，不让其进行条件下推，因为我们远端 RPC 没有处理条件的能力，我们修改了逻辑计划的条件下推规则，遇到数据源是 csv 的，我们拒绝条件下推。另一方面，首先得啃下硬骨头 PostgreSQL。我们考虑了两种方案，第一种是拿到 TiDB 的物理计划后，我们将其转换为 SQL，然后发给 PG；第二种方案我们直接将 TiDB 的物理计划序列化为 PG 的物理计划，发给 PG。我们考虑到第二种方案需要给 PG 本身加接受物理计划的钩子，就果断放弃，不然可能两天都费在改 PG 代码上了。我们首先实现了 select * from pgtable。主要修改了增加 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/distsql/select_result.go#L45&quot;&gt;pgSelectResult&lt;/a&gt; 结构体实现对应的结构体。通过看该结构体以及其对应接口函数，大家就知道如何去读取一个数据源上的数据，以及是如何做投影下推。修改 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/planner/core/logical_plans.go#L314&quot;&gt;Datasource&lt;/a&gt; 数据结构增加对数据源类型，RPC 信息，以及条件字符串，在部分物理计划内，我们也增加相关信息。同时根据数据源信息，在 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/table_reader.go#L132&quot;&gt;（e*TableReaderExecutor）buildResp&lt;/a&gt; 增加对来源是 PG 的表处理。&lt;/p&gt;&lt;p&gt;接着我们开始尝试条件下推：select * from pgtable where … 将 where 推下去。我们发现第一问题：由于我们的注册表里面没有记录外部源数据表的模式信息导致，下推去构建 SQL 的时候根本拿不到外部数据源 PG 上正确的属性名。所以我们暂时保证 TiDB 创建的表模式与 PG 创建的表模式完全一样来解决这个问题。条件下推，我们对条件的转换为字符串在函数 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/expression/expr_to_pb.go#L72&quot;&gt;ExpressionToString&lt;/a&gt; 中，看该函数调用即可明白是如何转换的。当前我们支持等于、大于、小于三种操作符的下推。&lt;/p&gt;&lt;p&gt;很快就到了 1 号下午了，我们主要工作就是进行 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/builder.go#L873&quot;&gt;Join下推&lt;/a&gt; 的工作。Join 下推主要是当我们发现两个 Join 的表都来来自于同一个 PG 实例时，我们就将该 Join 下推给 PG。我们增加一种 Join 执行器：&lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/executor/join.go#L60&quot;&gt;PushDownJoinExec&lt;/a&gt; 。弄完 Join 已经是晚上了。而且中间还遇到几个 Bug，首先，PG 等数据源没有一条结果满足时的边界条件没有进行检查，其次是，在 Join 下推时，某些情况下 Join 条件未必都是在 On 子句，这个时候需要考虑 Where 子句的信息。最后使得连接和条件同时下推没有问题。因为不同表的相同属性需要进行区分。主要难点就是对各个物理计划的结构体中的解析工作。&lt;/p&gt;&lt;p&gt;到了晚上，我们准备开始着手接入 Redis。考虑到 Redis 本身是 KV 型，对于给定 Key 的 Get  以及给定 Key 模式的匹配是很快。我们直接想到对于 Redis，我们允许 Key 值列上的条件下推，让 Redis 来做过滤。因为 Redis 是 API 形式，我们单独定义一个简单请求协议，来区别单值，模糊，以及全库查询三种基本情况，见 &lt;a href=&quot;https://github.com/hailanwhu/tidbhack2018rpcserver/blob/b132bfed58d5b9565250d9584bd6e280bc452ad0/rpcserverforredis.go#L12&quot;&gt;RequestRedis&lt;/a&gt; 定义。Redis 整体也像是 PG 一样的处理，主要没有 Join 下推这一个比较复杂的点。&lt;/p&gt;&lt;p&gt;我们之后又对 Explain 部分进行修改，使得能够打印能够反映我们现在加入外部数据源后算子真实的执行情况，可以见 &lt;a href=&quot;https://github.com/hailanwhu/tidbhackthon2018/blob/b8e06e1c7571121db33efcacc20cb8ab9094f3cf/planner/core/common_plans.go#L525&quot;&gt;explainPlanInRowFormat&lt;/a&gt; 部分代码。之后我们开始进行测试每个数据源上的，以及多个数据源融合起来进行测试。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;不足之处&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;我们很多物理计划都是复用 TiDB 本身的，给物理计划加上很多附属属性。其实最好是将这些物理计划单独抽取出来形成一个，不去复用。&lt;/li&gt;&lt;li&gt;Cost 没有进行细致考虑，例如对于 Join 下推，其实两张 100 万的表进行 Join 可能使得结果成为 1000 万，那么网络传输的代价反而更大了。这些具体算子下推的代价还需要细致的考虑。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;比较痛苦的经历&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;TiDB 不支持 Create Funtion，我们就只好写内置函数，就把另外一个 Parser 模块拖下来，自己修改加上语法，然后在加上自己设计的内置函数。&lt;/li&gt;&lt;li&gt;最痛苦还是两个方面，首先 Golang 语言，我们之前没有用得很多，经常遇到些小问题，例如 interface 的灵活使用等。其次就是涉及的 TiDB 的源码模块很多，从优化器、执行器、内置函数以及各种各样的结构。虽然思路很简单，但是改动的地方都很细节。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;收获&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3071d2d5a54975795c7008af298db4b7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;831&quot; data-rawheight=&quot;616&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3071d2d5a54975795c7008af298db4b7&quot; data-watermark-src=&quot;v2-5e8c67840493cada3362f35c86f6136a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;比赛过程中，看到了非常多优秀选手，以及他们酷炫的作业，感觉还是有很长的路要走。Hackathon 的选手都好厉害，听到大家噼里啪啦敲键盘的声音，似乎自己也不觉得有多累了。人啊，逼一下自己，会感受到自己无穷的力量。通过这次活动，我们最终能够灵活使用 Golang 语言，对 TiDB 整体也有了更深入的认识，希望自己以后能够称为 TiDB 的代码贡献者。&lt;/p&gt;&lt;p&gt;最后非常感谢 PingCAP 这次组织的 Hackathon 活动，感谢导师团、志愿者，以及还有特别感谢导师张建的指导。&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-14-52413872</guid>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十一）基于规则的优化 II</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-11-52138596.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52138596&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-180de02d6f7269fdf95e2294d895caf8_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;在 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;TiDB 源码阅读系列文章（七）基于规则的优化&lt;/a&gt; 一文中，我们介绍了几种 TiDB 中的逻辑优化规则，包括列剪裁，最大最小消除，投影消除，谓词下推和构建节点属性，本篇将继续介绍更多的优化规则：聚合消除、外连接消除和子查询优化。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;聚合消除&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;聚合消除会检查 SQL 查询中 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 语句所使用的列是否具有唯一性属性，如果满足，则会将执行计划中相应的 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 算子替换为 &lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt; 算子。这里的逻辑是当聚合函数按照具有唯一性属性的一列或多列分组时，下层算子输出的每一行都是一个单独的分组，这时就可以将聚合函数展开成具体的参数列或者包含参数列的普通函数表达式，具体的代码实现在 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_aggregation_elimination.go&quot;&gt;rule_aggregation_elimination.go&lt;/a&gt;&lt;/code&gt; 文件中。下面举一些具体的例子。&lt;/p&gt;&lt;p&gt;例一：&lt;/p&gt;&lt;p&gt;下面这个 Query 可以将聚合函数展开成列的查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select max(a) from t group by t.pk;&lt;/code&gt;&lt;p&gt;被等价地改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select a from t;&lt;/code&gt;&lt;p&gt;例二：&lt;/p&gt;&lt;p&gt;下面这个 Query 可以将聚合函数展开为包含参数列的内置函数的查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select count(a) from t group by t.pk;&lt;/code&gt;&lt;p&gt;被等价地改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select if(isnull(a), 0, 1) from t;&lt;/code&gt;&lt;p&gt;这里其实还可以做进一步的优化：如果列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt; 具有 &lt;code class=&quot;inline&quot;&gt;Not Null&lt;/code&gt; 的属性，那么可以将 &lt;code class=&quot;inline&quot;&gt;if(isnull(a), 0, 1)&lt;/code&gt; 直接替换为常量 1（目前 TiDB 还没做这个优化，感兴趣的同学可以来贡献一个 PR）。&lt;/p&gt;&lt;p&gt;另外提一点，对于大部分聚合函数，参数的类型和返回结果的类型一般是不同的，所以在展开聚合函数的时候一般会在参数列上构造 cast 函数做类型转换，展开后的表达式会保存在作为替换 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 算子的 &lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt; 算子中。&lt;/p&gt;&lt;p&gt;这个优化过程中，有一点非常关键，就是如何知道 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 使用的列是否满足唯一性属性，尤其是当聚合算子的下层节点不是 &lt;code class=&quot;inline&quot;&gt;DataSource&lt;/code&gt; 的时候？我们在 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;基于规则的优化&lt;/a&gt; 一文中的“构建节点属性”章节提到过，执行计划中每个算子节点会维护这样一个信息：当前算子的输出会按照哪一列或者哪几列满足唯一性属性。因此，在聚合消除中，我们可以通过查看下层算子保存的这个信息，再结合 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 用到的列判断当前聚合算子是否可以被消除。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;外连接消除&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;不同于 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-7/&quot;&gt;基于规则的优化&lt;/a&gt; 一文中“谓词下推”章节提到的将外连接转换为内连接，这里外连接消除指的是将整个连接操作从查询中移除。&lt;/p&gt;&lt;p&gt;外连接消除需要满足一定条件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;条件 1 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的父亲算子只会用到 &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的 outer plan 所输出的列&lt;/li&gt;&lt;li&gt;条件 2 :&lt;/li&gt;&lt;ul&gt;&lt;li&gt;条件 2.1 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 中的 join key 在 inner plan 的输出结果中满足唯一性属性&lt;/li&gt;&lt;li&gt;条件 2.2 : &lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt; 的父亲算子会对输入的记录去重&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;条件 1 和条件 2 必须同时满足，但条件 2.1 和条件 2.2 只需满足一条即可。&lt;/p&gt;&lt;p&gt;满足条件 1 和 条件 2.1 的一个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.a from t1 left join t2 on t1.b = t2.pk;&lt;/code&gt;&lt;p&gt;可以被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.a from t1;&lt;/code&gt;&lt;p&gt;满足条件 1 和条件 2.2 的一个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select distinct(t1.a) from t1 left join t2 on t1.b = t2.b;&lt;/code&gt;&lt;p&gt;可以被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select distinct(t1.a) from t1;&lt;/code&gt;&lt;p&gt;具体的原理是，对于外连接，outer plan 的每一行记录肯定会在连接的结果集里出现一次或多次，当 outer plan 的行不能找到匹配时，或者只能找到一行匹配时，这行 outer plan 的记录在连接结果中只出现一次；当 outer plan 的行能找到多行匹配时，它会在连接结果中出现多次；那么如果 inner plan 在 join key 上满足唯一性属性，就不可能存在 outer plan 的行能够找到多行匹配，所以这时 outer plan 的每一行都会且仅会在连接结果中出现一次。同时，上层算子只需要 outer plan 的数据，那么外连接可以直接从查询中被去除掉。同理就可以很容易理解当上层算子只需要 outer plan 的去重后结果时，外连接也可以被消除。&lt;/p&gt;&lt;p&gt;这部分优化的具体代码实现在 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_join_elimination.go&quot;&gt;rule_join_elimination.go&lt;/a&gt; 文件中。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;子查询优化 / 去相关&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;子查询分为非相关子查询和相关子查询，例如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;-- 非相关子查询
select * from t1 where t1.a &amp;gt; (select t2.a from t2 limit 1);
-- 相关子查询
select * from t1 where t1.a &amp;gt; (select t2.a from t2 where t2.b &amp;gt; t1.b limit 1);&lt;/code&gt;&lt;p&gt;对于非相关子查询， TiDB 会在 &lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt; 的逻辑中做两类操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;子查询展开&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;即直接执行子查询获得结果，再利用这个结果改写原本包含子查询的表达式；比如上述的非相关子查询，如果其返回的结果为一行记录 “1” ，那么整个查询会被改写为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a &amp;gt; 1;&lt;/code&gt;&lt;p&gt;详细的代码逻辑可以参考&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go&quot;&gt;expression_rewriter.go&lt;/a&gt;中的&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L685&quot;&gt;handleScalarSubquery&lt;/a&gt;和&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L535&quot;&gt;handleExistSubquery&lt;/a&gt;函数。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;子查询转为 Join&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于包含 IN (subquery) 的查询，比如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a in (select t2.a from t2);&lt;/code&gt;&lt;p&gt;会被改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.* from t1 inner join (select distinct(t2.a) as a from t2) as sub on t1.a = sub.a;&lt;/code&gt;&lt;p&gt;如果 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; 满足唯一性属性，根据上面介绍的聚合消除规则，查询会被进一步改写成：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select t1.* from t1 inner join t2 on t1.a = t2.a;&lt;/code&gt;&lt;p&gt;这里选择将子查询转化为 inner join 的 inner plan 而不是执行子查询的原因是：以上述查询为例，子查询的结果集可能会很大，展开子查询需要一次性将 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 的全部数据从 TiKV 返回到 TiDB 中缓存，并作为 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 扫描的过滤条件；如果将子查询转化为 inner join 的 inner plan ，我们可以更灵活地对 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 选择访问方式，比如我们可以对 join 选择 &lt;code class=&quot;inline&quot;&gt;IndexLookUpJoin&lt;/code&gt; 实现方式，那么对于拿到的每一条 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表数据，我们只需拿 &lt;code class=&quot;inline&quot;&gt;t1.a&lt;/code&gt; 作为 range 对 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 做一次索引扫描，如果 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表很小，相比于展开子查询返回 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 全部数据，我们可能总共只需要从 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 返回很少的几条数据。&lt;/p&gt;&lt;p&gt;注意这个转换的结果不一定会比展开子查询更好，其具体情况会受 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 表和 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表数据的影响，如果在上述查询中， &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt;表很大而 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表很小，那么展开子查询再对 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 选择索引扫描可能才是最好的方案，所以现在有参数控制这个转化是否打开，详细的代码可以参考 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go&quot;&gt;expression_rewriter.go&lt;/a&gt; 中的 &lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/expression_rewriter.go#L596&quot;&gt;handleInSubquery&lt;/a&gt; 函数。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;相关子查询&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于相关子查询，TiDB 会在&lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt;中将整个包含相关子查询的表达式转化为&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;算子。&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;算子是一类特殊的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;，特殊之处体现在执行逻辑上：对于 outer plan 返回的每一行记录，取出相关列的具体值传递给子查询，再执行根据子查询生成的 inner plan ，即&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;在执行时只能选择类似 循环嵌套连接的方式，而普通的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;则可以在物理优化阶段根据代价模型选择最合适的执行方式，包括&lt;code class=&quot;inline&quot;&gt;HashJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;MergeJoin&lt;/code&gt;和&lt;code class=&quot;inline&quot;&gt;IndexLookUpJoin&lt;/code&gt;，理论上后者生成的物理执行计划一定会比前者更优，所以在逻辑优化阶段我们会检查是否可以应用“去相关”这一优化规则，试图将&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;转化为等价的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;。其核心思想是将&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的 inner plan 中包含相关列的那些算子提升到&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;之中或之上，在算子提升后如果 inner plan 中不再包含任何的相关列，即不再引用任何 outer plan 中的列，那么&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;就会被转换为普通的&lt;code class=&quot;inline&quot;&gt;LogicalJoin&lt;/code&gt;，这部分代码逻辑实现在&lt;a href=&quot;https://github.com/eurekaka/tidb/blob/logical_rules_reading/planner/core/rule_decorrelate.go&quot;&gt;rule_decorrelate.go&lt;/a&gt;文件中。&lt;/p&gt;&lt;p&gt;具体的算子提升方式分为以下几种情况：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;则将其过滤条件添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中，然后将该 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 从 inner plan 中删除，再递归地对 inner plan 提升算子。&lt;/p&gt;&lt;p&gt;以如下查询为例：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from t1 where t1.a in (select t2.a from t2 where t2.b = t1.b);&lt;/code&gt;&lt;p&gt;其生成的最初执行计划片段会是：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a5982ed5121da2abdc05167506e48e09_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;383&quot; data-rawheight=&quot;277&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a5982ed5121da2abdc05167506e48e09&quot; data-watermark-src=&quot;v2-8a2d9d63bd08f8d908fd93ad370a140d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 提升后会变成如下片段：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-62f3075f148145ff308de465a07ff53b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;404&quot; data-rawheight=&quot;191&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-62f3075f148145ff308de465a07ff53b&quot; data-watermark-src=&quot;v2-62cff68b480f307f9001d16bba429797&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;到此 inner plan 中不再包含相关列，于是 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 会被转换为如下 LogicalJoin ：&lt;/p&gt;&lt;u&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-22d5560a1d5a1513063c1083de52c887_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;418&quot; data-rawheight=&quot;202&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-22d5560a1d5a1513063c1083de52c887&quot; data-watermark-src=&quot;v2-7bb00d9abf4fffa883e0ca832215cc93&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;/u&gt;&lt;p&gt;&lt;b&gt;2.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt;&lt;/b&gt;&lt;br&gt;即要求子查询最多输出一行记录，比如这个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select t2.a from t2 where t2.pk = t1.a) from t1;&lt;/code&gt;&lt;p&gt;因为子查询出现在整个查询的投影项里，所以 &lt;code class=&quot;inline&quot;&gt;expressionRewriter&lt;/code&gt; 在处理子查询时会对其生成的执行计划在根节点上加一个 &lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt; 限制最多产生一行记录，如果在执行时发现下层输出多于一行记录，则会报错。在这个例子中，子查询的过滤条件是 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表的主键上的等值条件，所以子查询肯定最多只会输出一行记录，而这个信息在“构建节点属性”这一步时会被发掘出来并记录在算子节点的 &lt;code class=&quot;inline&quot;&gt;MaxOneRow&lt;/code&gt; 属性中，所以这里的 &lt;code class=&quot;inline&quot;&gt;LogicalMaxOneRow&lt;/code&gt; 节点实际上是冗余的，于是我们可以将其从 inner plan 中移除，然后再递归地对 inner plan 做算子提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;则首先将这个投影算子从 inner plan 中移除，再根据&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的连接类型判断是否需要在&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;之上再加上一个&lt;code class=&quot;inline&quot;&gt;LogicalProjection&lt;/code&gt;，具体来说是：对于非 semi-join 这一类的连接（包括 inner join 和 left join ），inner plan 的输出列会保留在&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt;的结果中，所以这个投影操作需要保留，反之则不需要。最后，再递归地对删除投影后的 inner plan 提升下层算子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.inner plan 的根节点是&lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;a.&lt;/b&gt;首先我们会检查这个聚合算子是否可以被提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上再执行。以如下查询为例：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select sum(t2.b) from t2 where t2.a = t1.pk) from t1;&lt;/code&gt;&lt;p&gt;其最初生成的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3a476443a3930e657a52023f7ec82983_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;427&quot; data-rawheight=&quot;414&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3a476443a3930e657a52023f7ec82983&quot; data-watermark-src=&quot;v2-2f0771f0f6b99f56ef089d61c847b1e0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;将聚合提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 后的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bcbb359b6fc8771c984fe21351e599bc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;385&quot; data-rawheight=&quot;331&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bcbb359b6fc8771c984fe21351e599bc&quot; data-watermark-src=&quot;v2-bab6573026a3946820f9e4b66b465ead&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;即先对 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 做连接，再在连接结果上按照 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 分组后做聚合。这里有两个关键变化：第一是不管提升前 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型是 inner join 还是 left join ，提升后必须被改为 left join ；第二是提升后的聚合新增了 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 的列，即要按照 outer plan 传进 inner plan 中的相关列做分组。这两个变化背后的原因都会在后面进行阐述。因为提升后 inner plan 不再包含相关列，去相关后最终生成的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3af11844da7786a930af425723d7ec90_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;383&quot; data-rawheight=&quot;343&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3af11844da7786a930af425723d7ec90&quot; data-watermark-src=&quot;v2-c2e2c8792666624bcfe5ae7b5c39813c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;聚合提升有很多限定条件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型必须是 inner join 或者 left join 。 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 是根据相关子查询生成的，只可能有 3 类连接类型，除了 inner join 和 left join 外，第三类是 semi join （包括 &lt;code class=&quot;inline&quot;&gt;SemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;LeftOuterSemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;AntiSemiJoin&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;AntiLeftOuterSemiJoin&lt;/code&gt;），具体可以参考 &lt;code class=&quot;inline&quot;&gt;expression_rewriter.go&lt;/code&gt; 中的代码，限于篇幅在这里就不对此做展开了。对于 semi join 类型的 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; ，因为 inner plan 的输出列不会出现在连接的结果中，所以很容易理解我们无法将聚合算子提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 本身不能包含 join condition 。以上面给出的查询为例，可以看到聚合提升后会将子查询中包含相关列的过滤条件 (&lt;code class=&quot;inline&quot;&gt;t2.a = t1.pk&lt;/code&gt;) 添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中，如果 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 本身存在 join condition ，那么聚合提升后聚合算子的输入（连接算子的输出）就会和在子查询中时聚合算子的输入不同，导致聚合算子结果不正确。&lt;/li&gt;&lt;li&gt;子查询中用到的相关列在 outer plan 输出里具有唯一性属性。以上面查询为例，如果 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 不满足唯一性，假设 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 有两条记录满足 &lt;code class=&quot;inline&quot;&gt;t1.pk = 1&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 只有一条记录 &lt;code class=&quot;inline&quot;&gt;{ (t2.a: 1, t2.b: 2) }&lt;/code&gt; ，那么该查询会输出两行结果 &lt;code class=&quot;inline&quot;&gt;{ (sum(t2.b): 2), (sum(t2.b): 2) }&lt;/code&gt; ；但对于聚合提升后的执行计划，则会生成错误的一行结果&lt;code class=&quot;inline&quot;&gt;{ (sum(t2.b): 4) }&lt;/code&gt;。当 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 满足唯一性后，每一行 outer plan 的记录都对应连接结果中的一个分组，所以其聚合结果会和在子查询中的聚合结果一致，这也解释了为什么聚合提升后需要按照 &lt;code class=&quot;inline&quot;&gt;t1.pk&lt;/code&gt; 做分组。&lt;/li&gt;&lt;li&gt;聚合函数必须满足当输入为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 时输出结果也一定是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 。这是为了在子查询中没有匹配的特殊情况下保证结果的正确性，以上面查询为例，当 &lt;code class=&quot;inline&quot;&gt;t2&lt;/code&gt; 表没有任何记录满足 &lt;code class=&quot;inline&quot;&gt;t2.a = t1.pk&lt;/code&gt; 时，子查询中不管是什么聚合函数都会返回 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 结果，为了保留这种特殊情况，在聚合提升的同时， &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的连接类型会被强制改为 left join（改之前可能是 inner join ），所以在这种没有匹配的情况下，&lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 输出结果中 inner plan 部分会是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; ，而这个 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 会作为新添加的聚合算子的输入，为了和提升前结果一致，其结果也必须是 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;b.&lt;/b&gt;对于根据上述条件判定不能提升的聚合算子，我们再检查这个聚合算子的子节点是否为 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; ，如果是，则将其从 inner plan 中移除并将过滤条件添加到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 的 join condition 中。这种情况下 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 依然会被保留在 inner plan 中，但会将 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 过滤条件中涉及的 inner 表的列添加到聚合算子的 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt;中。比如对于查询：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select *, (select count(*) from t2 where t2.a = t1.a) from t1;&lt;/code&gt;&lt;p&gt;其生成的最初的执行计划片段会是：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7036cbfd4a1bd928df9b68a82ae2c39d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;404&quot; data-rawheight=&quot;402&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7036cbfd4a1bd928df9b68a82ae2c39d&quot; data-watermark-src=&quot;v2-1436664d60cbe58ed392024ac79a0d9d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;因为聚合函数是 &lt;code class=&quot;inline&quot;&gt;count(*)&lt;/code&gt; ，不满足当输入为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 时输出也为 &lt;code class=&quot;inline&quot;&gt;null&lt;/code&gt; 的条件，所以它不能被提升到 &lt;code class=&quot;inline&quot;&gt;LogicalApply&lt;/code&gt; 之上，但它可以被改写成：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fff11f20b805a30654aa08f8351c1c3e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;429&quot; data-rawheight=&quot;306&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fff11f20b805a30654aa08f8351c1c3e&quot; data-watermark-src=&quot;v2-59380836f03d5ed5d926d7465b523973&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;注意 &lt;code class=&quot;inline&quot;&gt;LogicalAggregation&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Group By&lt;/code&gt; 新加了 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; ，这一步将原本的先做过滤再做聚合转换为了先按照 &lt;code class=&quot;inline&quot;&gt;t2.a&lt;/code&gt; 分组做聚合，再将聚合结果与 &lt;code class=&quot;inline&quot;&gt;t1&lt;/code&gt; 做连接。 &lt;code class=&quot;inline&quot;&gt;LogicalSelection&lt;/code&gt; 提升后 inner plan 已经不再依赖 outer plan 的结果了，整个查询去相关后将会变为：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eca50e9e7bf56e429defc6cf6a266a4c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;449&quot; data-rawheight=&quot;325&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-eca50e9e7bf56e429defc6cf6a266a4c&quot; data-watermark-src=&quot;v2-0d186aabdcfc7d5ecfb758a6cb92d7d3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这是基于规则优化的第二篇文章，后续我们还将介绍更多逻辑优化规则：聚合下推，TopN 下推和 Join Reorder 。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;更多 TiDB 源码阅读系列文章：&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/blog-cn/#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-11-52138596</guid>
<pubDate>Tue, 11 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在量化派风控系统中的应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-10-52046270.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52046270&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ee0165e5af868a1f8bcd156e502cad0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;&lt;b&gt;朱劲松&lt;/b&gt;，量化派研发中心系统架构师，主要参与了基础组件开发、API Gateway 等项目，现在致力于公司风控系统相关业务的架构设计和研发。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;公司简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;量化派（QuantGroup）创办于 2014 年，是数据驱动的科技公司，是国家高新技术企业。量化派以「MOVE THE WORLD WITH DATA, ENLIGHTEN LIFE WITH AI」（数据驱动世界，智能点亮生活）为愿景，利用人工智能、机器学习、大数据技术。为金融、电商、旅游、出行、汽车供应链等多个领域的合作伙伴提供定制化的策略和模型，帮助提升行业效率。量化派已与国内外超过 300 家机构和公司达成深度合作，致力于打造更加有活力的共赢生态，推动经济的可持续发展。&lt;/p&gt;&lt;p&gt;我司从 2017 年年中开始调研 TiDB，并在用户行为数据分析系统中搭建 TiDB 集群进行数据存储，经过一年多的应用和研究，积累了丰富的经验。同时，TiDB 官方推出 2.0 GA 版本，TiDB 愈发成熟，稳定性和查询效率等方面都有很大提升。我们于 2018 年 7 月部署 TiDB 2.0.5 版本，尝试将其应用于风控业务中。风控系统主要是在用户申请放款时，根据风控规则结合模型和用户特征进行实时计算并返回放款结果。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;业务背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;风控系统中用到的数据主要可以分为两部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一类是原始数据，用于分析用户当前的特征指标。&lt;/li&gt;&lt;li&gt;一类是快照数据，用于计算历史指定时间点的特征指标，供模型训练使用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原始数据主要分为三种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;产生自公司内各个产品线的业务系统数据&lt;/li&gt;&lt;li&gt;爬虫组提供的用户联系人、运营商、消费记录等数据&lt;/li&gt;&lt;li&gt;经过处理后的用户特征数据&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于我们的风控策略中用到了大量的模型，包括神经网络模型，评分模型等，这些模型的训练需要依靠大量的历史订单以及相关的用户特征，为了训练出更多精准、优秀的模型，就需要更多维度的特征，此时特征的准确性就直接影响了模型的训练结果，为此我们在回溯每一个订单的用户在指定时间的特征表现时，就需要用到数据快照。&lt;/p&gt;&lt;p&gt;我们可以通过拉链表的方式来实现数据快照功能，简单说就是在每张表中增加三个字段，分别是 new_id、start_time、end_time，每一次记录的更新都会产生一条新的数据，同时变更原有记录的 end_time，以记录数据的变更历史。&lt;/p&gt;&lt;p&gt;通过上面的介绍可以看到，业务数据和爬虫数据本身数据量就很大，再加上需要产生对应的拉链数据，数据量更是成倍增长。假设每条数据自创建后仅变更一次，那拉链表的数据量就已经是原始表的两倍了，而实际生产环境下数据的变更远不止一次。&lt;/p&gt;&lt;p&gt;通过上述的介绍，我们总结风控系统下的数据存储需求应满足以下几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;业务数据&lt;/li&gt;&lt;li&gt;业务数据拉链表&lt;/li&gt;&lt;li&gt;爬虫数据，如联系人信息、运营商数据，消费记录等&lt;/li&gt;&lt;li&gt;爬虫数据拉链表&lt;/li&gt;&lt;li&gt;其他数据，如预处理数据等&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;当前方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;以前方案主要是采用 HBase 进行数据存储。它的水平扩展很好的解决了数据量大的问题。但是在实际使用中，也存在着比较明显的问题，最明显的就是查询的 API 功能性较弱，只能通过 Key 来获取单条数据，或是通过 Scan API 来批量读取，这无疑在特征回溯时增加了额外的开发成本，无法实现代码复用。&lt;/p&gt;&lt;p&gt;在实时计算场景中，为了降低开发成本，对于业务数据的获取则是通过访问线上系统的 MySQL 从库来进行查询；爬虫数据由于统一存放在 HBase 中，计算时需要将用到的数据全量拉取在内存中再进行计算。&lt;/p&gt;&lt;p&gt;在回溯场景中，针对业务特征回溯，通过查询订单时间之前的数据进行特征计算，这种方式对于已经变更的数据是无能为力的，只能通过 HBase 里的数据快照来实现，但无形增加了很多的开发工作。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;TiDB 为我们打开一片新视野&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过上面的介绍，我们知道要构建一个风控系统的实时数仓环境，需要满足下面几个特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高可用，提供健壮、稳定的服务。&lt;/li&gt;&lt;li&gt;支持水平弹性扩展，满足日益增长的数据需求。&lt;/li&gt;&lt;li&gt;性能好，支持高并发。&lt;/li&gt;&lt;li&gt;响应快。&lt;/li&gt;&lt;li&gt;支持标准 SQL，最好是 MySQL 语法和 MySQL 协议，避免回溯时的额外开发。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;可以发现，TiDB 完美契合我们的每个需求。经过 TiDB 在用户行为数据分析系统中的长期使用，我们已经积累了一定的经验，在此过程中 TiDB 官方也给予了长期的技术支持，遇到的问题在沟通时也能够及时的反馈，而且还与我司技术人员进行过多次技术交流及线下分享，在此我们深表感谢。伴随着风控系统需求的持续增长，我们对整体架构进行了新一轮的优化，新的数据接入及存储架构如图 1。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a717388d8270181928386125b5163963_r.jpg&quot; data-caption=&quot;图 1  优化后的架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;545&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a717388d8270181928386125b5163963&quot; data-watermark-src=&quot;v2-8e925539beecb0d3c99975e92584a69b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;通过图 1 可以看到，线上业务系统产生的数据统一存放在 MySQL 中，将这些孤立的数据归集在 TiDB 中，能够提供基于 SQL 的查询服务。通过 binlog 的方式直接从 MySQL 实例进行接入，接入后的数据以两种不同的形式分别存放：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一种是去分库分表后的源数据，降低了实时特征计算的实现及维护成本。&lt;/li&gt;&lt;li&gt;另一种是以拉链数据形式存储实现数据快照功能。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;经过调研，针对第一种场景，可以通过阿里的 otter 或者 TiDB 周边工具 Syncer 来快速实现，但对于第二个需求都没有现成的成熟解决方案。最终，我们基于阿里的 canal 进行客户端的定制化开发，分别按照不同的需求拼装合并 SQL 并写入到不同的 TiDB 集群中；同时还可以按需将部分表的数据进行组装并发送至 Kafka，用于准实时分析场景。&lt;/p&gt;&lt;p&gt;对于来自爬虫组的数据，我们采用直接消费 Kafka 的方式组装 SQL 写入到 TiDB 即可。&lt;/p&gt;&lt;p&gt;在实际是使用中，通过索引等优化，TiDB 完全可以支持线上实时查询的业务需求；在特征回溯时只需要通过增加查询条件就可以获得指定时间的特征结果，大大降低了开发成本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;遇到的问题&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;风控业务中用户特征提取的 SQL 相对都比较复杂，在实际使用中，存在部分 SQL 执行时间比在 MySQL 中耗时高。通过 explain 我们发现，他并没有使用我们创建的索引，而是进行了全表扫描，在进一步分析后还发现 explain 的结果是不确定的。&lt;/p&gt;&lt;p&gt;经过与 TiDB 官方技术人员的沟通，我们进行了删除类似索引、analyze table 等操作，发现问题仍然存在。通过图 2 可以看到完全相同的 SQL 语句，其执行结果的差异性。最后按官方建议，我们采用添加 use index 的方式使其强制走索引，执行时间由 4 分钟变成了 &amp;lt; 1s，暂时解决了业务上的需求。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b456777e7bdb0d4610a70c40aab191d8_r.jpg&quot; data-caption=&quot;图 2  explain 示意图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;408&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b456777e7bdb0d4610a70c40aab191d8&quot; data-watermark-src=&quot;v2-7a247e9dec4e04853609cf215ab3dcc1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;同时 TiDB 技术人员也收集相关信息反馈给了研发人员。在整个问题的处理过程中，TiDB 的技术人员给予了高度的配合和及时的反馈，同时也表现出了很强的专业性，大大减少了问题排查的时间，我们非常感谢。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;目前我们已经搭建两个 TiDB 集群，几十个物理节点，百亿级特征数据，受益于 TiDB 的高可用构架，上线以来一直稳定运行。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上，TiDB 在我们风控业务中的应用才只是开始，部分业务的迁移还有待进一步验证，但是 TiDB 给我们带来的好处不言而喻，为我们在数据存储和数据分析上打开了一片新视野。后续我们会继续加大对 TiDB 的投入，使其更好地服务于在线分析和离线分析等各个场景。我们也希望进一步增加与 PingCAP 团队的交流与合作，进行更深入的应用和研究，为 TiDB 的发展贡献一份力量。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多 TiDB 用户实践：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-10-52046270</guid>
<pubDate>Mon, 10 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB EcoSystem Tools 原理解读（一）：TiDB-Binlog 架构演进与实现原理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-10-51990591.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51990591&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f70e162f6471c2085dcc5b1d71405c89_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Binlog 组件用于收集 TiDB 的 binlog，并提供实时备份和同步功能。该组件在功能上类似于 MySQL 的主从复制，MySQL 的主从复制依赖于记录的 binlog 文件，TiDB-Binlog 组件也是如此，主要的不同点是 TiDB 是分布式的，因此需要收集各个 TiDB 实例产生的 binlog，并按照事务提交的时间排序后才能同步到下游。如果你需要部署 TiDB 集群的从库，或者想订阅 TiDB 数据的变更输出到其他的系统中，TiDB-Binlog 则是必不可少的工具。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;架构演进&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Binlog 这个组件已经发布了 2 年多时间，经历过几次架构演进，去年十月到现在大规模使用的是 Kafka 版本，架构图如下：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-df721303b9040c503599496dcf52d16b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;333&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-df721303b9040c503599496dcf52d16b&quot; data-watermark-src=&quot;v2-5119818836a0f90f06478a754c1d4d02&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Kafka 版本的 TiDB-Binlog 主要包括两个组件：&lt;/p&gt;&lt;p&gt;Pump：一个守护进程，在每个 TiDB 主机的后台运行。其主要功能是实时记录 TiDB 产生的 binlog 并顺序写入 Kafka 中。&lt;/p&gt;&lt;p&gt;Drainer： 从 Kafka 中收集 binlog，并按照 TiDB 中事务的提交顺序转化为指定数据库兼容的 SQL 语句或者指定格式的数据，最后同步到目的数据库或者写到顺序文件。&lt;/p&gt;&lt;p&gt;这个架构的工作原理为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 需要与 Pump 绑定，即 TiDB 实例只能将它生成的 binlog 发送到一个指定的 Pump 中；&lt;/li&gt;&lt;li&gt;Pump 将 binlog 先写到本地文件，再异步地写入到 Kafka；&lt;/li&gt;&lt;li&gt;Drainer 从 Kafka 中读出 binlog，对 binlog 进行排序，对 binlog 解析后生成 SQL 或指定格式的数据再同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;根据用户的反馈，以及我们自己做的一些测试，发现该版本主要存在一些问题。&lt;/p&gt;&lt;p&gt;首先，TiDB 的负载可能不均衡，部分 TiDB 业务较多，产生的 binlog 也比较多，对应的 Pump 的负载高，导致数据同步延迟高。&lt;/p&gt;&lt;p&gt;其次，依赖 Kafka 集群，增加了运维成本；而且 TiDB 产生的单条 binlog 的大小可达 2G（例如批量删除数据、批量写入数据），需要配置 Kafka 的消息大小相关设置，而 Kafka 并不太适合单条数据较大的场景。&lt;/p&gt;&lt;p&gt;最后，Drainer 需要读取 Kafka 中的 binlog、对 binlog 进行排序、解析 binlog，同步数据到下游等工作，可以看出 Drainer 的工作较多，而且 Drainer 是一个单点，所以往往同步数据的瓶颈都在 Drainer。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上这些问题我们很难在已有的框架下进行优化，因此我们对 TiDB-Binlog 进行了重构，最新版本的 TiDB-Binlog 的总体架构如下图所示：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6e5e2bb0245f985f9f7f3b53b2f605c9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;391&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6e5e2bb0245f985f9f7f3b53b2f605c9&quot; data-watermark-src=&quot;v2-742a86727627f4e4ec5e5e9f68583501&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;新版本 TiDB-Binlog 不再使用 Kafka 存储 binlog，仍然保留了 Pump 和 Drainer 两个组件，但是对功能进行了调整：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pump 用于实时记录 TiDB 产生的 binlog，并将 binlog 按照事务的提交时间进行排序，再提供给 Drainer 进行消费。&lt;/li&gt;&lt;li&gt;Drainer 从各个 Pump 中收集 binlog 进行归并，再将 binlog 转化成 SQL 或者指定格式的数据，最终同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该版本的主要优点为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;多个 Pump 形成一个集群，可以水平扩容，各个 Pump 可以均匀地承担业务的压力。&lt;/li&gt;&lt;li&gt;TiDB 通过内置的 Pump Client 将 binlog 分发到各个 Pump，即使有部分 Pump 出现故障也不影响 TiDB 的业务。&lt;/li&gt;&lt;li&gt;Pump 内部实现了简单的 kv 来存储 binlog，方便对 binlog 数据的管理。&lt;/li&gt;&lt;li&gt;原来 Drainer 的 binlog 排序逻辑移到了 Pump 来做，而 Pump 是可扩展的，这样就能提高整体的同步性能。&lt;/li&gt;&lt;li&gt;Drainer 不再需要像原来一样读取一批 binlog 到内存里进行堆排序，只需要依次读取各个 Pump 的 binlog 进行归并排序，这样可以大大节省内存的使用，同时也更容易做内存控制。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;由于该版本最大的特点是多个 Pump 组成了一个集群（cluster），因此该版本命名为 cluster 版本。下面我们以最新的 cluster 版本的架构来介绍 TiDB-Binlog 的实现原理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;工作原理&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;binlog&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;首先我们先介绍一下 TiDB 中的 binlog，TiDB 的事务采用 2pc 算法，一个成功的事务会写两条 binlog，包括一条 Prewrite binlog 和 一条 Commit binlog；如果事务失败，会发一条 Rollback binlog。&lt;/p&gt;&lt;p&gt;binlog 的结构定义为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// Binlog 记录事务中所有的变更，可以用 Binlog 构建 SQL
message Binlog {
    // Binlog 的类型，包括 Prewrite、Commit、Rollback 等
    optional BinlogType  tp = 1 [(gogoproto.nullable) = false];
    
    // Prewrite, Commit 和 Rollback 类型的 binlog 的 start_ts，记录事务开始的 ts
    optional int64  start_ts = 2 [(gogoproto.nullable) = false];
    
    // commit_ts 记录事务结束的 ts，只记录在 commit 类型的 binlog 中
    optional int64  commit_ts = 3 [(gogoproto.nullable) = false];
    
    // prewrite key 只记录在 Prewrite 类型的 binlog 中，
    // 是一个事务的主键，用于查询该事务是否提交
    optional bytes  prewrite_key = 4;
    
    // prewrite_value 记录在 Prewrite 类型的 binlog 中，用于记录每一行数据的改变
    optional bytes  prewrite_value = 5;
    
    // ddl_query 记录 ddl 语句
    optional bytes  ddl_query = 6;
    
    // ddl_job_id 记录 ddl 的 job id
    optional int64  ddl_job_id  = 7 [(gogoproto.nullable) = false];
}&lt;/code&gt;&lt;p&gt;binlog 及相关的数据结构定义见:&lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/binlog.proto&quot;&gt;binlog.proto&lt;/a&gt;&lt;/p&gt;&lt;p&gt;其中 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 为事务开始时的 ts，&lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 为事务提交的 ts。ts 是由物理时间和逻辑时间转化而成的，在 TiDB 中是唯一的，由 PD 来统一提供。在开始一个事务时，TiDB 会请求 PD，获取一个 ts 作为事务的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt;，在事务提交时则再次请求 PD 获取一个 ts 作为 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;。 我们在 Pump 和 Drainer 中就是根据 binlog 的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 来对 binlog 进行排序的。&lt;/p&gt;&lt;p&gt;TiDB 的 binlog 记录为 row 模式，即保存每一行数据的改变。数据的变化记录在  &lt;code class=&quot;inline&quot;&gt;prewrite_value&lt;/code&gt; 字段中，该字段的数据主要由序列化后的 TableMutation 结构的数据组成。TableMutation 的结构如下所示：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// TableMutation 存储表中数据的变化
message TableMutation {
    // 表的 id，唯一标识一个表
    optional int64 table_id = 1 [(gogoproto.nullable) = false];
    
    // 保存插入的每行数据
    repeated bytes inserted_rows = 2;
    
    // 保存修改前和修改后的每行的数据
    repeated bytes updated_rows = 3;
    
    // 已废弃
    repeated int64 deleted_ids = 4;
    
    // 已废弃
    repeated bytes deleted_pks = 5;
    
    // 删除行的数据
    repeated bytes deleted_rows  = 6;
    
    // 记录数据变更的顺序
    repeated MutationType sequence = 7;
}&lt;/code&gt;&lt;p&gt;下面以一个例子来说明 binlog 中是怎么存储数据的变化的。&lt;/p&gt;&lt;p&gt;例如 table 的结构为：&lt;/p&gt;&lt;p&gt;create table &lt;code class=&quot;inline&quot;&gt;test&lt;/code&gt; (&lt;code class=&quot;inline&quot;&gt;id&lt;/code&gt; int, &lt;code class=&quot;inline&quot;&gt;name&lt;/code&gt; varchar(24), primary key &lt;code class=&quot;inline&quot;&gt;id&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;按照顺序执行如下 SQL：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;begin;
insert into test(id, name) values(1, &quot;a&quot;);
insert into test(id, name) values(2, &quot;b&quot;);
update test set name = &quot;c&quot; where id = 1;
update test set name = &quot;d&quot; where id = 2;
delete from test where id = 2;
insert into test(id, name) values(2, &quot;c&quot;);
commit;&lt;/code&gt;&lt;p&gt;则生成的 TableMutation 的数据如下所示：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;inserted_rows:
1, &quot;a&quot;
2, &quot;b&quot;
2, &quot;c&quot;
 
updated_rows:
1, &quot;a&quot;, 1, &quot;c&quot;
2, &quot;b&quot;, 2, &quot;d&quot;
 
deleted_rows:
2, &quot;d&quot;
 
sequence:
Insert, Insert, Update, Update, DeleteRow, Insert&lt;/code&gt;&lt;p&gt;可以从例子中看出，sequence 中保存的数据变更类型的顺序为执行 SQL 的顺序，具体变更的数据内容则保存到了相应的变量中。&lt;/p&gt;&lt;p&gt;Drainer 在把 binlog 数据同步到下游前，就需要把上面的这些数据还原成 SQL，再同步到下游。&lt;/p&gt;&lt;p&gt;另外需要说明的是，TiDB 在写 binlog 时，会同时向 TiKV 发起写数据请求和向 Pump 发送 Prewrite binlog，如果 TiKV 和 Pump 其中一个请求失败，则该事务失败。当 Prewrite 成功后，TiDB 向 TiKV 发起 Commit 消息，并异步地向 Pump 发送一条 Commit binlog。由于 TiDB 是同时向 TiKV 和 Pump 发送请求的，所以只要保证 Pump 处理 Prewrite binlog 请求的时间小于等于 TiKV 执行 Prewrite 的时间，开启 binlog 就不会对事务的延迟造成影响。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Pump Client&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从上面的介绍中我们知道由多个 Pump 组成一个集群，共同承担写 binlog 的请求，那么就需要保证 TiDB 能够将写 binlog 的请求尽可能均匀地分发到各个 Pump，并且需要识别不可用的 Pump，及时获取到新加入集群中 Pump 信息。这部分的工作是在 Pump Client 中实现的。&lt;/p&gt;&lt;p&gt;Pump Client 以包的形式集成在 TiDB 中，代码链接：&lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/v2.1.0/tidb-binlog/pump_client&quot;&gt;pump_client&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;Pump Client 维护 Pump 集群的信息，Pump 的信息主要来自于 PD 中保存的 Pump 的状态信息，状态信息的定义如下（代码链接：&lt;a href=&quot;https://github.com/pingcap/tidb-tools/blob/v2.1.0/tidb-binlog/node/node.go&quot;&gt;Status&lt;/a&gt;）：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;type Status struct {
    // Pump/Drainer 实例的唯一标识
    NodeID string `json:&quot;nodeId&quot;`
    
    // Pump/Drainer 的服务地址
    Addr string `json:&quot;host&quot;`
    
    // Pump/Drainer 的状态，值可以为 online、pausing、paused、closing、offline
    State string `json:&quot;state&quot;`
    
    // Pump/Drainer 是否 alive（目前没有使用该字段）
    IsAlive bool `json:&quot;isAlive&quot;`
    
    // Pump的分数，该分数是由节点的负载、磁盘使用率、存储的数据量大小等因素计算得来的，
    // 这样 Pump Client 可以根据分数来选取合适的 Pump 发送 binlog（待实现）
    Score int64 `json:&quot;score&quot;`
    
    // Pump 的标签，可以通过 label 对 TiDB 和 Pump 进行分组，
    // TiDB 只能将 binlog 发送到相同 label 的 Pump（待实现）
    Label *Label `json:&quot;label&quot;`
    
    // Pump： 保存的 binlog 的最大的 commit_ts
    // Drainer：已消费的 binlog 的最大的 commit_ts
    MaxCommitTS int64 `json:&quot;maxCommitTS&quot;`
    
    // 该状态信息的更新时间对应的 ts.
    UpdateTS int64 `json:&quot;updateTS&quot;`
}&lt;/code&gt;&lt;p&gt;Pump Client 根据 Pump 上报到 PD 的信息以及写 binlog 请求的实际情况将 Pump 划分为可用 Pump 与不可用 Pump 两个部分。&lt;/p&gt;&lt;p&gt;划分的方法包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;初始化时从 PD 中获取所有 Pump 的信息，将状态为 online 的 Pump 加入到可用 Pump 列表中，其他 Pump 加入到非可用列表中。&lt;/li&gt;&lt;li&gt;Pump 每隔固定的时间会发送心跳到 PD，并更新自己的状态。Pump Client 监控 PD 中 Pump 上传的状态信息，及时更新内存中维护的 Pump 信息，如果状态由非 online 转换为 online 则将该 Pump 加入到可用 Pump 列表；反之加入到非可用列表中。&lt;/li&gt;&lt;li&gt;在写 binlog 到 Pump 时，如果该 Pump 在重试多次后仍然写 binlog 失败，则把该 Pump 加入到非可用 Pump 列表中。&lt;/li&gt;&lt;li&gt;定时发送探活请求（数据为空的 binlog 写请求）到非可用 Pump 列表中的状态为 online 的 Pump，如果返回成功，则把该 Pump 重新加入到可用 Pump 列表中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过上面的这些措施，Pump Client 就可以及时地更新所维护的 Pump 集群信息，保证将 binlog 发送到可用的 Pump 中。&lt;/p&gt;&lt;p&gt;另外一个问题是，怎么保证 Pump Client 可以将 binlog 写请求均匀地分发到各个 Pump？我们目前提供了几种路由策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;range： 按照顺序依次选取 Pump 发送 binlog，即第一次选取第一个 Pump，第二次选取第二个 Pump…&lt;/li&gt;&lt;li&gt;hash：对 binlog 的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 进行 hash，然后选取 hash 值对应的 Pump。&lt;/li&gt;&lt;li&gt;score：根据 Pump 上报的分数按照加权平均算法选取 Pump 发送 binlog（待实现）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;需要注意的地方是，以上的策略只是针对 Prewrite binlog，对于 Commit binlog，Pump Client 会将它发送到对应的 Prewrite binlog 所选择的 Pump，这样做是因为在 Pump 中需要将包含 Prewrite binlog 和 Commit binlog 的完整 binlog（即执行成功的事务的 binlog）提供给 Drainer，将 Commit binlog 发送到其他 Pump 没有意义。&lt;/p&gt;&lt;p&gt;Pump Client 向 Pump 提交写 binlog 的请求接口为 &lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/pump.proto&quot;&gt;pump.proto&lt;/a&gt; 中的 WriteBinlog，使用 grpc 发送 binlog 请求。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Pump&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Pump 主要用来承担 binlog 的写请求，维护 binlog 数据，并将有序的 binlog 提供给 Drainer。我们将 Pump 抽象成了一个简单的 kv 数据库，key 为 binlog 的 &lt;code class=&quot;inline&quot;&gt;start _ts&lt;/code&gt;（Priwrite binlog） 或者 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;（Commit binlog），value 为 binlog 的元数据，binlog 的数据则存在数据文件中。Drainer 像查数据库一样的来获取所需要的 binlog。&lt;/p&gt;&lt;p&gt;Pump 内置了 leveldb 用于存储 binlog 的元信息。在 Pump 收到 binlog 的写请求时，会首先将 binlog 数据以 append 的形式写到文件中，然后将 binlog 的 ts、类型、数据长度、所保存的文件以及在文件中的位置信息保存在 leveldb 中，如果为 Prewrite binlog，则以 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt;作为 key；如果是 Commit binlog，则以 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 作为 key。&lt;/p&gt;&lt;p&gt;当 Drainer 向 Pump 请求获取指定 ts 之后的 binlog 时，Pump 则查询 leveldb 中大于该 ts 的 binlog 的元数据，如果当前数据为 Prewrite binlog，则必须找到对应的 Commit binlog；如果为 Commit binlog 则继续向前推进。这里有个问题，在 binlog 一节中提到，如果 TiKV 成功写入了数据，并且 Pump 成功接收到了 Prewrite binlog，则该事务就提交成功了，那么如果在 TiDB 发送 Commit binlog 到 Pump 前发生了一些异常（例如 TiDB 异常退出，或者强制终止了 TiDB 进程），导致 Pump 没有接收到 Commit binlog，那么 Pump 中就会一直找不到某些 Prewrite binlog 对应的 Commit binlog。这里我们在 Pump 中做了处理，如果某个 Prewrite binlog 超过了十分钟都没有找到对应的 Commit binlog，则通过 binlog 数据中的 &lt;code class=&quot;inline&quot;&gt;prewrite_key&lt;/code&gt; 去查询 TiKV 该事务是否提交，如果已经提交成功，则 TiKV 会返回该事务的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;；否则 Pump 就丢弃该条 Prewrite binlog。&lt;/p&gt;&lt;p&gt;binlog 元数据中提供了数据存储的文件和位置，可以通过这些信息读取 binlog 文件的指定位置获取到数据。因为 binlog 数据基本上是按顺序写入到文件中的，因此我们只需要顺序地读 binlog 文件即可，这样就保证了不会因为频繁地读取文件而影响 Pump 的性能。最终，Pump 以 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 为排序标准将 binlog 数据传输给 Drainer。Drainer 向 Pump 请求 binlog 数据的接口为 &lt;a href=&quot;https://github.com/WangXiangUSTC/tipb/blob/master/proto/binlog/pump.proto&quot;&gt;pump.proto&lt;/a&gt; 中的 PullBinlogs，以 grpc streaming 的形式传输 binlog 数据。&lt;/p&gt;&lt;p&gt;值得一提的是，Pump 中有一个 fake binlog 机制。Pump 会定时（默认三秒）向本地存储中写入一条数据为空的 binlog，在生成该 binlog 前，会向 PD 中获取一个 ts，作为该 binlog 的 &lt;code class=&quot;inline&quot;&gt;start_ts&lt;/code&gt; 与 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;，这种 binlog 我们叫作 fake binlog。这样做的原因在 Drainer 中介绍。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Drainer&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Drainer 从各个 Pump 中获取 binlog，归并后按照顺序解析 binlog、生成 SQL 或者指定格式的数据，然后再同步到下游。&lt;/p&gt;&lt;p&gt;既然要从各个 Pump 获取数据，Drainer 就需要维护 Pump 集群的信息，及时获取到新增加的 Pump，并识别出不可用的 Pump，这部分功能与 Pump Client 类似，Drainer 也是通过 PD 中存储的 Pump 的状态信息来维护 Pump 信息。另外需要注意的是，如果新增加了一个 Pump，必须让该 Pump 通知 Drainer 自己上线了，这么做是为了保证不会丢数据。例如：&lt;/p&gt;&lt;p&gt;集群中已经存在 Pump1 和 Pump2，Drainer 读取 Pump1 和 Pump2 的数据并进行归并：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3cd607d2bdbee944445902c45d40e64c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;752&quot; data-rawheight=&quot;499&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3cd607d2bdbee944445902c45d40e64c&quot; data-watermark-src=&quot;v2-1988b173b06ddd67d5b8a6367b3ba589&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Pump1 存储的 binlog 为｛ 1，3，5，7，9 ｝，Pump2 存储的 binlog 为｛2，4，6，10｝。Drainer 从两个 Pump 获取 binlog，假设当前已经读取到了｛1，2，3，4，5，6，7｝这些 binlog，已处理的 binlog 的位置为 7。此时 Pump3 加入集群，从 Pump3 上报自己的上线信息到 PD，到 Drainer 从 PD 中获取到 Pump3 信息需要一定的时间，如果 Pump3 没有通知 Drainer 就直接提供写 binlog 服务，写入了 binlog｛8，12｝，Drainer 在此期间继续读取 Pump1 和 Pump2 的 binlog，假设读取到了 9，之后才识别到了 Pump3 并将 Pump3 加入到归并排序中，此时 Pump3 的 binlog 8 就丢失了。为了避免这种情况，需要让 Pump3 通知 Drainer 自己已经上线，Drainer 收到通知后将 Pump3 加入到归并排序，并返回成功给 Pump3，然后 Pump3 才能提供写 binlog 的服务。&lt;/p&gt;&lt;p&gt;Drainer 通过如上所示的方式对 binlog 进行归并排序，并推进同步的位置。那么可能会存在这种情况：某个 Pump 由于一些特殊的原因一直没有收到 binlog 数据，那么 Drainer 中的归并排序就无法继续下去，正如我们用两条腿走路，其中一只腿不动就不能继续前进。我们使用 Pump 一节中提到的 fake binlog 的机制来避免这种问题，Pump 每隔指定的时间就生成一条 fake binlog，即使某些 Pump 一直没有数据写入，也可以保证归并排序正常向前推进。&lt;/p&gt;&lt;p&gt;Drainer 将所有 Pump 的数据按照 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt; 进行归并排序后，将 binlog 数据传递给 Drainer 中的数据解析及同步模块。通过上面的 binlog 格式的介绍，我们可以看出 binlog 文件中并没有存储表结构的信息，因此需要在 Drainer 中维护所有库和表的结构信息。在启动 Drainer 时，Drainer 会请求 TiKV，获取到所有历史的 DDL job 的信息，对这些 DDL job 进行过滤，使用 Drainer 启动时指定的 initial-commit-ts（或者 checkpoint 中保存的 &lt;code class=&quot;inline&quot;&gt;commit_ts&lt;/code&gt;）之前的 DDL 在内存中构建库和表结构信息。这样 Drainer 就有了一份 ts 对应时间点的库和表的快照，在读取到 DDL 类型的 binlog 时，则更新库和表的信息；读取到 DML 类型的 binlog 时，则根据库和表的信息来生成 SQL。&lt;/p&gt;&lt;p&gt;在生成 SQL 之后，就可以同步到下游了。为了提高 Drainer 同步的速度，Drainer 中使用多个协程来执行 SQL。在生成 SQL 时，我们会使用主键／唯一键的值作为该条 SQL 的 key，通过对 key 进行 hash 来将 SQL 发送到对应的协程中。当每个协程收集到了足够多的 SQL，或者超过了一定的时间，则将这一批的 SQL 在一个事务中提交到下游。&lt;/p&gt;&lt;p&gt;但是有些 SQL 是相关的，如果被分到了不同的协程，那 SQL 的执行顺序就不能得到保证，造成数据的不一致。例如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;SQL1: delete from test.test where id = 1;

SQL2: replace into test.test (id, name ) values(1, &quot;a&quot;);&lt;/code&gt;&lt;p&gt;按照顺序执行后表中存在 id ＝ 1 该行数据，如果这两条 SQL 分别分配到了协程 1 和协程 2 中，并且协程 2 先执行了 SQL，则表中不再存在 id ＝ 1 的数据。为了避免这种情况的发生，Drainer 中加入了冲突检测的机制，如果检测出来两条 SQL 存在冲突（修改了同一行数据），则暂时不将后面的 SQL 发送到协程，而是生成一个 Flush 类型的 job 发送到所有的协程， 每个协程在遇到 Flush job 时就会马上执行所缓存的 SQL。接着才会把该条有冲突的 SQL 发送到对应的协程中。下面给出一个例子说明一下冲突检测的机制：&lt;/p&gt;&lt;p&gt;有以下这些 SQL，其中 id 为表的主键：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;SQL1: update itest set id = 4, name = &quot;c&quot;, age = 15 where id = 3;    key: 3, 4

SQL2:  update itest set id = 5, name = &quot;b&quot;, age = 14 where id = 2;   key：5, 2

SQL3：delete from itest where id = 3;                                key: 3&lt;/code&gt;&lt;ol&gt;&lt;li&gt;首先将 SQL1 发送到指定的协程，这时所有的 keys 为［3，4］；&lt;/li&gt;&lt;li&gt;SQL2 的 key［5，2］与 keys 中的［3，4］都没有冲突，将 SQL2 发送到指定的协程，这时 keys 为［3，4，5，2］；&lt;/li&gt;&lt;li&gt;SQL3 的 key［3］与 keys 中的［3］存在冲突，发送 Flush job 到所有协程，SQL1 和 SQL2 被执行，清空 keys；&lt;/li&gt;&lt;li&gt;将 SQL3 发送到指定的协程，同时更新 keys 为［3］。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Drainer 通过以上这些机制来高效地同步数据，并且保证数据的一致。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多技术干货：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/blog-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-10-51990591</guid>
<pubDate>Mon, 10 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在小米的应用实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-03-51472404.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51472404&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-286ddf5e402ba98f9de8bfa95ce5eb57_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;张良，小米 DBA 负责人&lt;br&gt;潘友飞，小米 DBA&lt;br&gt;王必文，小米开发工程师&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;应用场景介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;MIUI 是小米公司旗下基于 Android 系统深度优化、定制、开发的第三方手机操作系统，也是小米的第一个产品。MIUI 在 Android 系统基础上，针对中国用户进行了深度定制，在此之上孕育出了一系列的应用，比如主题商店、小米音乐、应用商店、小米阅读等。     &lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c4428dc7c80aea208ba650abba0f8079_r.jpg&quot; data-caption=&quot;图 1  MIUI Android 系统界面图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;595&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c4428dc7c80aea208ba650abba0f8079&quot; data-watermark-src=&quot;v2-acb05a51e9c3630558f4a6880d1ca2c1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;目前 TiDB 主要应用在：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;小米手机桌面负一屏的快递业务&lt;/li&gt;&lt;li&gt;商业广告交易平台素材抽审平台&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;这两个业务场景每天读写量均达到上亿级，上线之后，整个服务稳定运行；接下来我们计划逐步上线更多的业务场景，小米阅读目前正在积极的针对订单系统做迁移测试。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 特点&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 结合了传统的 RDBMS 和 NoSQL 的最佳特性，兼容 MySQL 协议，支持无限的水平扩展，具备强一致性和高可用性。&lt;/p&gt;&lt;p&gt;具有如下的特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高度兼容 MySQL，大多数情况下无需修改代码即可从 MySQL 轻松迁移至 TiDB，即使已经分库分表的 MySQL 集群亦可通过 TiDB 提供的迁移工具进行实时迁移。&lt;/li&gt;&lt;li&gt;水平弹性扩展，通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/li&gt;&lt;li&gt;分布式事务，TiDB 100% 支持标准的 ACID 事务。&lt;/li&gt;&lt;li&gt;真正金融级高可用，相比于传统主从（M-S）复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复（auto-failover），无需人工介入。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 的架构及原理官网有详细介绍（&lt;a href=&quot;https://pingcap.com/&quot;&gt;https://pingcap.com/&lt;/a&gt;），这里不再赘述。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2a500e631bf3062d721ce5cd2cd2c817_r.jpg&quot; data-caption=&quot;图 2  TiDB 基础架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;381&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2a500e631bf3062d721ce5cd2cd2c817&quot; data-watermark-src=&quot;v2-8169cc4855b1c15dc1d79afdb93854a4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;跟绝大数互联网公司一样，小米关系型存储数据库首选 MySQL，单机 2.6T 磁盘。由于小米手机销量的快速上升和 MIUI 负一屏用户量的快速增加，导致负一屏快递业务数据的数据量增长非常快，&lt;b&gt;每天的读写量级均分别达到上亿级别，数据快速增长导致单机出现瓶颈，比如性能明显下降、可用存储空间不断降低、大表 DDL 无法执行等，不得不面临数据库扩展的问题。&lt;/b&gt;比如，我们有一个业务场景（智能终端），需要定时从几千万级的智能终端高频的向数据库写入各种监控及采集数据，MySQL 基于 Binlog 的单线程复制模式，很容易造成从库延迟，并且堆积越来越严重。&lt;/p&gt;&lt;p&gt;&lt;b&gt;对于 MySQL 来讲，最直接的方案就是采用分库分表的水平扩展方式，综合来看并不是最优的方案，比如对于业务来讲，对业务代码的侵入性较大；对于 DBA 来讲提升管理成本，后续需要不断的拆分扩容，即使有中间件也有一定的局限性。&lt;/b&gt;同样是上面的智能终端业务场景，从业务需求看，需要从多个业务维度进行查询，并且业务维度可能随时进行扩展，分表的方案基本不能满足业务的需求。&lt;/p&gt;&lt;p&gt;了解到 TiDB 特点之后，DBA 与业务开发沟通确认当前 MySQL 的使用方式，并与 TiDB 的兼容性做了详细对比，经过业务压测之后，根据压测的结果，决定尝试将数据存储从 MySQL 迁移到 TiDB。经过几个月的线上考验，TiDB 的表现达到预期。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;兼容性对比&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 支持包括跨行事务、JOIN、子查询在内的绝大多数 MySQL 的语法，可以直接使用 MySQL 客户端连接；对于已用 MySQL 的业务来讲，基本可以无缝切换到 TiDB。&lt;/b&gt;&lt;br&gt;二者简单对比如下几方面：&lt;/p&gt;&lt;p&gt;1. 功能支持&lt;/p&gt;&lt;p&gt;TiDB 尚不支持如下几项：&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;增加、删除主键&lt;/li&gt;&lt;li&gt;非 UTF8 字符集&lt;/li&gt;&lt;li&gt;视图（即将支持）、存储过程、触发器、部分内置函数&lt;/li&gt;&lt;li&gt;Event&lt;/li&gt;&lt;li&gt;全文索引、空间索引&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;2. 默认设置&lt;/p&gt;&lt;p&gt;字符集、排序规则、sql_mode、lower_case_table_names 几项默认值不同。&lt;/p&gt;&lt;p&gt;3. 事务&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;TiDB 使用乐观事务模型，提交后注意检查返回值。&lt;/li&gt;&lt;li&gt;TiDB 限制单个事务大小，保持事务尽可能的小。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;4. TiDB 支持绝大多数的 Online DDL。&lt;/p&gt;&lt;p&gt;5. 另，一些 MySQL 语法在 TiDB 中可以解析通过，不会产生任何作用，例如： create table 语句中 engine、partition 选项都是在解析后忽略。&lt;/p&gt;&lt;p&gt;6. 详细信息可以访问官网：https://pingcap.com/docs-cn/sql/mysql-compatibility/ 。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;压测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 目的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过压测 TiDB 了解一下其 OLTP 性能，看是否满足业务要求。&lt;br&gt;&lt;br&gt;&lt;b&gt;2. 机器配置&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d7d319c1480a57e186b79e7bf2cd6ce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;394&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5d7d319c1480a57e186b79e7bf2cd6ce&quot; data-watermark-src=&quot;v2-dbdd0864ecae8948d5ef342c9e5d6c78&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;3. 压测内容以及结果&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt; 标准 Select 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3919bf10c952092538e4df79ee90caa2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;322&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3919bf10c952092538e4df79ee90caa2&quot; data-watermark-src=&quot;v2-1ab2200f75c089f9a9f3cade76c787b4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8d9269cdf30ac3f6bb7c846d4473a248_r.jpg&quot; data-caption=&quot;图 3  标准 Select 压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8d9269cdf30ac3f6bb7c846d4473a248&quot; data-watermark-src=&quot;v2-da683c683c19267c31a5e7fc3f07a732&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;标准 OLTP 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c163a21feec655e19ab64dcb1b07f70a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;357&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c163a21feec655e19ab64dcb1b07f70a&quot; data-watermark-src=&quot;v2-a755b0a83bd7ddba182ba80a6ed66877&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fddec2f978f81acfee3a2d75b2d5ebcb_r.jpg&quot; data-caption=&quot;图 4  标准 OLTP  压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;541&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fddec2f978f81acfee3a2d75b2d5ebcb&quot; data-watermark-src=&quot;v2-847e24943021fd8f04ec9a6c907ff6a8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt; 标准 Insert 压测&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28457ad3a1997ddf3c1f2abb33d16fc4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;333&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-28457ad3a1997ddf3c1f2abb33d16fc4&quot; data-watermark-src=&quot;v2-36a16a1feb93e863164d2d83eb3c472a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-686f7602ce614f8e15fdb0ef388a009a_r.jpg&quot; data-caption=&quot;图 5  标准 Insert 压测图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-686f7602ce614f8e15fdb0ef388a009a&quot; data-watermark-src=&quot;v2-c687905215da14e1e3976f9a4196762f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;通过压测发现 TiDB 稳定性上与预期稍有差别，不过压测的 Load 会明显高于生产中的业务 Load，参考低 Threads 时 TiDB 的表现，基本可以满足业务对 DB 的性能要求，决定灰度一部分 MySQL 从库读流量体验一下实际效果。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;迁移过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;整个迁移分为 2 大块：数据迁移、流量迁移。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 数据迁移&lt;/b&gt;&lt;/p&gt;&lt;p&gt;数据迁移分为增量数据、存量数据两部分。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于存量数据，可以使用逻辑备份、导入的方式，除了传统的逻辑导入外，官方还提供一款物理导入的工具 TiDB Lightning。&lt;/li&gt;&lt;li&gt;对于增量备份可以使用 TiDB 提供的 Syncer （新版已经更名为 DM - Data Migration）来保证数据同步。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Syncer 结构如图 6，主要依靠各种 Rule 来实现不同的过滤、合并效果，一个同步源对应一个 Syncer 进程，同步 Sharding 数据时则要多个 Syncer 进程。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-facfec2bcf6879a829017e0de8b94a38_r.jpg&quot; data-caption=&quot;图 6  Syncer 结构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;852&quot; data-rawheight=&quot;391&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-facfec2bcf6879a829017e0de8b94a38&quot; data-watermark-src=&quot;v2-fa999818be6d9fcd875987dfa5d194a8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;使用 Syncer 需要注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;做好同步前检查，包含 server-id、log_bin、binlog_format 是否为 ROW、binlog_row_image 是否为 FULL、同步相关用户权限、Binlog 信息等。&lt;/li&gt;&lt;li&gt;使用严格数据检查模式，数据不合法则会停止。数据迁移之前最好针对数据、表结构做检查。&lt;/li&gt;&lt;li&gt;做好监控，TiDB 提供现成的监控方案。&lt;/li&gt;&lt;li&gt;对于已经分片的表同步到同一个 TiDB 集群，要做好预先检查。确认同步场景是否可以用 route-rules 表达，检查分表的唯一键、主键在数据合并后是否冲突等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 流量迁移&lt;/b&gt;&lt;/p&gt;&lt;p&gt;流量切换到 TiDB 分为两部分：读、写流量迁移。每次切换保证灰度过程，观察周期为 1~2 周，做好回滚措施。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;读流量切换到 TiDB，这个过程中回滚比较简单，灰度无问题，则全量切换。&lt;/li&gt;&lt;li&gt;再将写入切换到 TiDB，需要考虑好数据回滚方案或者采用双写的方式（需要断掉 Syncer） 。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;集群状况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 配置&lt;/b&gt;&lt;/p&gt;&lt;p&gt;集群配置采用官方推荐的 7 节点配置，3 个 TiDB 节点，3 个 PD 节点，4 个 TiKV 节点，其中每个 TiDB 与 PD 为一组，共用一台物理机。后续随着业务增长或者新业务接入，再按需添加 TiKV 节点。&lt;br&gt;&lt;br&gt;&lt;b&gt;2. 监控&lt;/b&gt;&lt;/p&gt;&lt;p&gt;监控采用了 TiDB 的提供的监控方案，并且也接入了公司开源的 Falcon，目前整个集群运行比较稳定，监控如图 7。      &lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6288a303380a452fb7c0f8716e97d135_r.jpg&quot; data-caption=&quot;图 7  监控图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;314&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6288a303380a452fb7c0f8716e97d135&quot; data-watermark-src=&quot;v2-729e081c26da54cb02349deb61f1c839&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;遇到的问题、原因及解决办法&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-504342f9d0b3e7799d8dc657180e2083_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;928&quot; data-rawheight=&quot;1638&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-504342f9d0b3e7799d8dc657180e2083&quot; data-watermark-src=&quot;v2-041e4d6ecb39bebe13289f22f6f275b9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;后续和展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前 TiDB 在小米主要提供 OLTP 服务，小米手机负一屏快递业务为使用 TiDB 做了一个良好的开端，而后商业广告也有接入，2 个业务均已上线数月，TiDB 的稳定性经受住了考验，带来了很棒的体验，对于后续大体的规划如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MIUI 生态业务中存在大量的类似场景的业务，后续将会与业务开发积极沟通，从 MySQL 迁移到 TiDB。&lt;/li&gt;&lt;li&gt;针对某些业务场景，以资源合理利用为目标，推出归档集群，利用 Syncer 实现数据归档的功能。&lt;/li&gt;&lt;li&gt;数据分析，结合 TiDB 提供的工具，将支持离线、实时数据分析支持。&lt;/li&gt;&lt;li&gt;将 TiDB 的监控融合到小米公司开源的监控系统 Falcon 中。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;致谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;非常感谢 TiDB 官方在迁移及业务上线期间给予我们的支持，为每一个 TiDB 人专业的精神、及时负责的响应点赞。&lt;br&gt;&lt;br&gt;&lt;br&gt;更多 TiDB 用户实践：&lt;/p&gt;&lt;a href=&quot;https://www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-03-51472404</guid>
<pubDate>Mon, 03 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 2.1 GA Release Notes</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-01-51304843.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51304843&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f5709477c7ad0dcb8de138272df091a1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 11 月 30 日，TiDB 发布 2.1 GA 版。相比 2.0 版本，该版本对系统稳定性、性能、兼容性、易用性做了大量改进。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;SQL 优化器&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Index Join&lt;/code&gt; 选择范围，提升执行性能&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Index Join&lt;/code&gt; 外表选择，使用估算的行数较少的表作为外表&lt;/li&gt;&lt;li&gt;扩大 Join Hint &lt;code class=&quot;inline&quot;&gt;TIDB_SMJ&lt;/code&gt; 的作用范围，在没有合适索引可用的情况下也可使用 Merge Join&lt;/li&gt;&lt;li&gt;加强 Join Hint &lt;code class=&quot;inline&quot;&gt;TIDB_INLJ&lt;/code&gt; 的能力，可以指定 Join 中的内表&lt;/li&gt;&lt;li&gt;优化关联子查询，包括下推 Filter 和扩大索引选择范围，部分查询的效率有数量级的提升&lt;/li&gt;&lt;li&gt;支持在 &lt;code class=&quot;inline&quot;&gt;UPDATE&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;DELETE&lt;/code&gt; 语句中使用 Index Hint 和 Join Hint&lt;/li&gt;&lt;li&gt;支持更多函数下推：&lt;code class=&quot;inline&quot;&gt;ABS&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;CEIL&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;FLOOR&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;IS TRUE&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;IS FALSE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;优化内建函数 &lt;code class=&quot;inline&quot;&gt;IF&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;IFNULL&lt;/code&gt; 的常量折叠算法&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;EXPLAIN&lt;/code&gt; 语句输出格式, 使用层级结构表示算子之间的上下游关系&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;SQL 执行引擎&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;重构所有聚合函数，提升 &lt;code class=&quot;inline&quot;&gt;Stream&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;Hash&lt;/code&gt; 聚合算子的执行效率&lt;/li&gt;&lt;li&gt;实现并行 &lt;code class=&quot;inline&quot;&gt;Hash Aggregate&lt;/code&gt; 算子，部分场景下有 350% 的性能提升&lt;/li&gt;&lt;li&gt;实现并行 &lt;code class=&quot;inline&quot;&gt;Project&lt;/code&gt; 算子，部分场景有 74% 的性能提升&lt;/li&gt;&lt;li&gt;并发地读取 &lt;code class=&quot;inline&quot;&gt;Hash Join&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Inner&lt;/code&gt; 表和 &lt;code class=&quot;inline&quot;&gt;Outer&lt;/code&gt; 表的数据，提升执行性能&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;REPLACE INTO&lt;/code&gt; 语句的执行速度，性能提升 10x&lt;/li&gt;&lt;li&gt;优化时间类型的内存占用，时间类型数据的内存使用降低为原来的一半&lt;/li&gt;&lt;li&gt;优化点查的查询性能, Sysbench 点查效率提升 60%&lt;/li&gt;&lt;li&gt;TiDB 插入和更新宽表，性能提升接近 20 倍&lt;/li&gt;&lt;li&gt;支持在配置文件中设置单个查询的内存使用上限&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Hash Join&lt;/code&gt; 的执行过程，当 Join 类型为 &lt;code class=&quot;inline&quot;&gt;Inner Join&lt;/code&gt; 或者 &lt;code class=&quot;inline&quot;&gt;Semi Join&lt;/code&gt; 时，如果内表为空，不再读取外表数据，快速返回结果&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://github.com/pingcap/docs/blob/master/sql/understanding-the-query-execution-plan.md#explain-analyze-output-format&quot;&gt;EXPLAIN ANALYZE&lt;/a&gt;&lt;/code&gt; 语句，用于查看 Query 执行过程中各个算子的运行时间，返回结果行数等运行时统计信息&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;统计信息&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持只在一天中的某个时间段开启统计信息自动 ANALYZE 的功能&lt;/li&gt;&lt;li&gt;支持根据查询的反馈自动更新表的统计信息&lt;/li&gt;&lt;li&gt;支持通过 &lt;code class=&quot;inline&quot;&gt;ANALYZE TABLE WITH BUCKETS&lt;/code&gt; 语句配置直方图中桶的个数&lt;/li&gt;&lt;li&gt;优化等值查询和范围查询混合的情况下使用直方图估算 Row Count 的算法&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;表达式&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持内建函数：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;json_contains&lt;/code&gt; &lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;json_contains_path&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;encode/decode&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Server&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持在单个 tidb-server 实例内部对冲突事务排队，优化事务间冲突频繁的场景下的性能&lt;/li&gt;&lt;li&gt;支持 Server Side Cursor&lt;/li&gt;&lt;li&gt;新增 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/master/docs/tidb_http_api.md&quot;&gt;HTTP 管理接口&lt;/a&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;打散 table 的 regions 在 TiKV 集群中的分布&lt;/li&gt;&lt;li&gt;控制是否打开 &lt;code class=&quot;inline&quot;&gt;general log&lt;/code&gt;&lt;/li&gt;&lt;li&gt;在线修改日志级别&lt;/li&gt;&lt;li&gt;查询 TiDB 集群信息&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/FAQ/#3-3-11-%E5%9C%A8-tidb-%E4%B8%AD-auto-analyze-%E7%9A%84%E8%A7%A6%E5%8F%91%E7%AD%96%E7%95%A5%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84&quot;&gt;添加 auto_analyze_ratio 系统变量控制自动 Analyze 的阈值&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/sql/tidb-specific/#tidb-retry-limit&quot;&gt;添加 tidb_retry_limit 系统变量控制事务自动重试的次数&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/sql/tidb-specific/#tidb-disable-txn-auto-retry&quot;&gt;添加 tidb_disable_txn_auto_retry 系统变量控制事务是否自动重试&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/sql/slow-query/#admin-show-slow-%E5%91%BD%E4%BB%A4&quot;&gt;支持使用 admin show slow 语句来获取慢查询语句&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/sql/tidb-specific/#tidb_slow_log_threshold&quot;&gt;增加环境变量 tidb_slow_log_threshold 动态设置 slow log 的阈值&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs-cn/sql/tidb-specific/#tidb_query_log_max_len&quot;&gt;增加环境变量 tidb_query_log_max_len 动态设置日志中被截断的原始 SQL 语句的长度&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;DDL&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持 Add Index 语句与其他 DDL 语句并行执行，避免耗时的 Add Index 操作阻塞其他操作&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Add Index&lt;/code&gt; 的速度，在某些场景下速度大幅提升&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;select tidb_is_ddl_owner()&lt;/code&gt; 语句，方便判断 TiDB 是否为 &lt;code class=&quot;inline&quot;&gt;DDL Owner&lt;/code&gt;&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;ALTER TABLE FORCE&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;ALTER TABLE RENAME KEY TO&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;Admin Show DDL Jobs&lt;/code&gt; 输出结果中添加表名、库名等信息&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/master/docs/tidb_http_api.md&quot;&gt;支持使用 ddl/owner/resign HTTP 接口释放 DDL Owner 并开启新一轮 DDL Owner 选举&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;兼容性&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持更多 MySQL 语法&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;BIT&lt;/code&gt; 聚合函数支持 &lt;code class=&quot;inline&quot;&gt;ALL&lt;/code&gt; 参数&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;SHOW PRIVILEGES&lt;/code&gt; 语句&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;CHARACTER SET&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;CREATE USER&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;IDENTIFIED WITH&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;LOAD DATA IGNORE LINES&lt;/code&gt; 语句&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;Show ProcessList&lt;/code&gt; 语句返回更准确信息&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;可用性优化&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;引入 TiKV 版本控制机制，支持集群滚动兼容升级&lt;/li&gt;&lt;li&gt;PD 节点间 &lt;a href=&quot;https://github.com/pingcap/pd/blob/5c7b18cf3af91098f07cf46df0b59fbf8c7c5462/conf/config.toml#L22&quot;&gt;开启 Raft PreVote&lt;/a&gt;，避免网络隔离后恢复时产生的重新选举，避免网络隔离后恢复时产生的重新选举&lt;/li&gt;&lt;li&gt;开启 &lt;code class=&quot;inline&quot;&gt;raft learner&lt;/code&gt; 功能，降低调度时出现宕机导致数据不可用的风险&lt;/li&gt;&lt;li&gt;TSO 分配不再受系统时间回退影响&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;Region merge&lt;/code&gt; 功能，减少元数据带来的开销&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;调度器优化&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化 Down Store 的处理流程，加快发生宕机后补副本的速度&lt;/li&gt;&lt;li&gt;优化热点调度器，在流量统计信息抖动时适应性更好&lt;/li&gt;&lt;li&gt;优化 Coordinator 的启动，减少重启 PD 时带来的不必要调度&lt;/li&gt;&lt;li&gt;优化 Balance Scheduler 频繁调度小 Region 的问题&lt;/li&gt;&lt;li&gt;优化 Region merge，调度时考虑 Region 中数据的行数&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/tools/pd-control.md#config-show--set--&quot;&gt;新增一些控制调度策略的开关&lt;/a&gt;&lt;/li&gt;&lt;li&gt;完善&lt;a href=&quot;https://github.com/pingcap/pd/tree/release-2.1/tools/pd-simulator&quot;&gt;调度模拟器&lt;/a&gt;，添加调度场景模拟&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;API 及运维工具&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新增 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51304843/http%3C/code%3Es://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L40&quot;&gt;GetPrevRegion 接口&lt;/a&gt;，用于支持 TiDB reverse scan 功能&lt;/code&gt;&lt;/li&gt;&lt;li&gt;新增 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/%3C/code%3E/github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L54&quot;&gt;BatchSplitRegion 接口&lt;/a&gt;，用于支持 TiKV 快速 Region 分裂&lt;/code&gt;&lt;/li&gt;&lt;li&gt;新增 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51304843/ht%3C/code%3Etps://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L64-L66&quot;&gt;GCSafePoint 接口&lt;/a&gt;，用于支持 TiDB 并发分布式 GC&lt;/code&gt;&lt;/li&gt;&lt;li&gt;新增 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51304843/htt%3C/code%3Eps://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L32&quot;&gt;GetAllStores 接口&lt;/a&gt;，用于支持 TiDB 并发分布式 GC&lt;/code&gt;&lt;/li&gt;&lt;li&gt;pd-ctl 新增：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#operator-show--add--remove&quot;&gt;使用统计信息进行 Region split&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#jq-%E6%A0%BC%E5%BC%8F%E5%8C%96-json-%E8%BE%93%E5%87%BA%E7%A4%BA%E4%BE%8B&quot;&gt;调用 jq 来格式化 JSON 输出&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#region-store-store_id&quot;&gt;查询指定 store 的 Region 信息&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#region-topconfver-limit&quot;&gt;查询按 version 排序的 topN 的 Region 列表&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#region-topsize-limit&quot;&gt;查询按 size 排序的 topN 的 Region 列表&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-control.md#tso&quot;&gt;更精确的 TSO 解码&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/release-2.1/tools/pd-recover.md&quot;&gt;pd-recover&lt;/a&gt; 不再需要提供 max-replica 参数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;监控&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;Filter&lt;/code&gt;相关的监控&lt;/li&gt;&lt;li&gt;新增 etcd Raft 状态机相关监控&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;性能优化&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化处理 Region heartbeat 的性能，减少 heartbeat 带来的内存开销&lt;/li&gt;&lt;li&gt;优化 Region tree 性能&lt;/li&gt;&lt;li&gt;优化计算热点统计的性能问题&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Coprocessor&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新增支持大量内建函数&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/rfcs/blob/master/text/2017-12-22-read-pool.md&quot;&gt;新增 Coprocessor ReadPool，提高请求处理并发度&lt;/a&gt;&lt;/li&gt;&lt;li&gt;修复时间函数解析以及时区相关问题&lt;/li&gt;&lt;li&gt;优化下推聚合计算的内存使用&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Transaction&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化 MVCC 读取逻辑以及内存使用效率，提高扫描操作的性能，Count 全表性能比 2.0 版本提升 1 倍&lt;/li&gt;&lt;li&gt;折叠 MVCC 中连续的 Rollback 记录，保证记录的读取性能&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/rfcs/blob/master/text/2018-08-29-unsafe-destroy-range.md&quot;&gt;新增 UnsafeDestroyRange API 用于在 drop table/index 的情况下快速回收空间&lt;/a&gt;&lt;/li&gt;&lt;li&gt;GC 模块独立出来，减少对正常写入的影响&lt;/li&gt;&lt;li&gt;kv_scan 命令支持设置 upper bound&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Raftstore&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化 snapshot 文件写入流程避免导致 RocksDB stall&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/rfcs/pull/17&quot;&gt;增加 LocalReader 线程专门处理读请求，降低读请求的延迟&lt;/a&gt;&lt;/li&gt;&lt;li&gt;href=&quot;https://github.com/tikv/rfcs/pull/6&quot;&amp;gt;支持 BatchSplit 避免大量写入导致产生特别大的 Region&lt;/li&gt;&lt;li&gt;支持按照统计信息进行 Region Split，减少 IO 开销&lt;/li&gt;&lt;li&gt;支持按照 Key 的数量进行 Region Split，提高索引扫描的并发度&lt;/li&gt;&lt;li&gt;优化部分 Raft 消息处理流程，避免 Region Split 带来不必要的延迟&lt;/li&gt;&lt;li&gt;启用 &lt;code class=&quot;inline&quot;&gt;PreVote&lt;/code&gt; 功能，减少网络隔离对服务的影响&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;存储引擎&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;修复 RocksDB &lt;code class=&quot;inline&quot;&gt;CompactFiles&lt;/code&gt; 的 bug，可能影响 Lightning 导入的数据&lt;/li&gt;&lt;li&gt;升级 RocksDB 到 v5.15，解决 snapshot 文件可能会被写坏的问题&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;IngestExternalFile&lt;/code&gt;，避免 flush 卡住写入的问题&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;tikv-ctl&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/tikv/blob/master/docs/tools/tikv-control.md#ldb-command&quot;&gt;新增 ldb 命令，方便排查 RocksDB 相关问题&lt;/a&gt;&lt;/li&gt;&lt;li&gt;compact 命令支持指定是否 compact bottommost 层的数据&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;全量数据快速导入工具 &lt;a href=&quot;https://pingcap.github.io/docs-cn/tools/lightning-overview-architecture&quot;&gt;TiDB-Lightning&lt;/a&gt;&lt;/li&gt;&lt;li&gt;支持新版本 &lt;a href=&quot;https://pingcap.github.io/docs-cn/tools/tidb-binlog-cluster/&quot;&gt;TiDB-Binlog&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;升级兼容性说明&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;由于新版本存储引擎更新，不支持在升级后回退至 2.0.x 或更旧版本&lt;/li&gt;&lt;li&gt;新版本默认开启 &lt;code class=&quot;inline&quot;&gt;raft learner&lt;/code&gt; 功能，如果从 1.x 版本集群升级至 2.1 版本，须停机升级或者先滚动升级 TiKV，完成后再滚动升级 PD&lt;/li&gt;&lt;li&gt;从 2.0.6 之前的版本升级到 2.1.0 之前，最好确认集群中是否存在正在运行中的 DDL 操作，特别是耗时的 Add Index 操作&lt;/li&gt;&lt;li&gt;因为 2.1 版本启用了并行 DDL，对于早于 2.0.1 版本的集群，无法滚动升级到 2.1，可以选择下面两种方案：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;停机升级，直接从早于 2.0.1 的 TiDB 版本升级到 2.1&lt;/li&gt;&lt;li&gt;先滚动升级到 2.0.1 或者之后的 2.0.x 版本，再滚动升级到 2.1 版本&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-01-51304843</guid>
<pubDate>Sat, 01 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 2.1：Battle-Tested for an Unpredictable World</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-01-51304475.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51304475&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-149ede427e4c2ab2ec61f20036bb4523_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;TiDB 是由 PingCAP 开发的分布式关系型数据库，今天我们很高兴地推出 TiDB 2.1 正式版，提供更丰富的功能、更好的性能以及更高的可靠性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;回顾 2.0 版本&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;今年 4 月份我们发布了 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-2.0-ga-release-detail/&quot;&gt;TiDB 2.0 版本&lt;/a&gt;，提升了稳定性、性能以及可运维性，这个版本在接下来的半年中得到了广泛的关注和使用。&lt;/p&gt;&lt;p&gt;迄今为止 TiDB 已经在 &lt;a href=&quot;https://pingcap.com/cases-cn/&quot;&gt;数百家用户&lt;/a&gt; 的生产环境中稳定运行，涉及互联网、游戏、金融、保险、制造业、银行、证券等多个行业，最大集群包含数百个节点及数百 TB 数据，业务场景包含纯 OLTP、纯 OLAP 以及混合负载。另外，既有使用 TiDB 当做关系数据库的场景，也有只用 TiKV 作为分布式 Key Value 存储的场景。&lt;/p&gt;&lt;p&gt;这几个月，在这些场景中，我们亲历了跨机房容灾需求、亲历了几十万级别的高吞吐业务、亲历了双十一的流量激增、亲历了高并发点查、高并发写入与上百行复杂 SQL 的混合负载、见到过多次的硬件/网络故障、见到过操作系统内核/编译器的 Bug。&lt;/p&gt;&lt;p&gt;简而言之，我们的世界充满了未知，而分布式关系型数据库这样一种应用广泛、功能丰富且非常关键的基础软件，最大的困难就是这些“未知”。在 2.1 版本中，我们引入了不少新的特性来抵御这些未知，适配各种复杂的场景，提升性能和稳定性，帮助我们的用户更好地支撑复杂的业务。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;更全面的 Optimizer&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 2.1 版本中，我们对 TiDB 的 Cost-based Optimizer 做了改进，希望这个优化器能够处理各种复杂的 Query，尽量少的需要人工介入去处理慢 SQL。例如对 Index Join 选择索引、外表的优化，对关联子查询的优化，显著地提升了复杂 SQL 的查询效率。&lt;/p&gt;&lt;p&gt;当然，除了自动的查询优化之外，2.1 也增加了更多的手动干预机制，比如对 Join 算子的 Hint、Update/Delete 语句的 Hint。用户可以在优化器没有指定合适的计划时，手动干预结果或者是用来确保查询计划稳定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;更强大的执行引擎&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 2.1 版本中，我们对部分物理算子的执行效率进行了优化，特别是对 Hash Aggregation 和 Projection 这两个算子进行了并行化改造，另外重构了聚合算子的运行框架，支持向量化计算。&lt;/p&gt;&lt;p&gt;得益于这些优化，在 TPC-H 这种 OLAP 的测试集上，2.1 比 2.0 版本有了显著的性能提升，让 2.1 版本更好的面对 HTAP 应用场景。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raft 新特性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 2.1 版本中，我们引入了 Raft PreVote、Raft Learner、Raft Region Merge 三个新特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PreVote 是在 Raft Group Member 发起投票之前，预先检查是否能被其他成员所支持，以避免集群中被网络隔离的节点重新接入集群中的时候引发性能抖动，提升集群稳定性。2.1 版本已经支持 PreVote 功能，并默认打开。&lt;/li&gt;&lt;li&gt;Learner 是只同步数据不参与投票的 Raft Group Member。在新加副本的时候，首先增加 Learner 副本，以避免添加副本过程中，部分 TiKV 节点故障引发丢失多数副本的情况发生，以提升集群的安全性。2.1 版本已经支持 Learner 功能，并默认打开。&lt;/li&gt;&lt;li&gt;Region Merge 用于将多个过小的 Region 合并为一个大的 Region，降低集群的管理成本，对于长期运行的集群以及数据规模较大的集群的性能、稳定性有帮助。2.1 版本已经支持 Region Merge 功能，尚未默认打开。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些新特性的引入，有助于提升存储集群尤其是大规模集群的稳定性和性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;自动更新统计信息&lt;/b&gt;&lt;/p&gt;&lt;p&gt;统计信息的及时性对查询计划的正确性非常重要。在 2.1 版本中，我们提供了基于 Query Feedback 的动态增量更新机制。&lt;/p&gt;&lt;p&gt;在制定查询计划时，会根据现有的统计信息估算出需要处理的数据量；在执行查询计划时，会统计出真实处理的数据量。TiDB 会根据这两个值之间的差距来更新统计信息，包括直方图和 CM-Sketch。在我们的测试中，对于一个完全没有统计信息的表，经过十轮左右的更新，可以达到统计信息基本稳定的状态。这对于维持正确的查询计划非常重要。&lt;/p&gt;&lt;p&gt;除了动态增量更新之外，我们对自动全量 Analyze 也提供了更多支持，可以通过 &lt;a href=&quot;https://www.pingcap.com/docs-cn/sql/statistics/#%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0&quot;&gt;系统变量&lt;/a&gt; 指定做自动 Analyze 的时间段。&lt;/p&gt;&lt;p&gt;&lt;b&gt;并行 DDL&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 所有的 DDL 操作都是 Online 进行，不过在 2.0 以及之前的版本中，所有的 DDL 操作都是串行执行，即使 DDL 所操作的表之间没有关联。比如在对 A 表 Add Index 时候，想创建一个 B 表，需要等待 Add Index 操作结束。这在一些场景下对用户使用造成了困扰。&lt;br&gt;在 2.1 版本中，我们对 DDL 流程进行拆分，将 Add Index 操作和其他的 DDL 操作的处理分开。由于在 TiDB 的 DDL 操作中，只有 Add Index 操作需要去回填数据，耗时较长，其他的 DDL 操作正常情况下都可以在秒级别完成，所以经过这个拆分，可以保证大多数 DDL 操作能够不需要等待，直接执行。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Explain 和 Explain Analyze&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Explain 对于理解查询计划至关重要，2.1 之前的版本，TiDB 追随 MySQL 的 Explain 输出格式来展示查询计划。但是当 SQL 比较复杂时，MySQL 的格式并不利于展示算子之间的层级关系，不利于用户定位问题。&lt;/p&gt;&lt;p&gt;2.1 版本中，我们使用缩进来展示算子之间的层级关系，对每个算子的详细信息也做了优化，希望整个查询计划一目了然，帮助用户尽快定位问题。&lt;a href=&quot;https://www.pingcap.com/docs/sql/understanding-the-query-execution-plan/&quot;&gt;这篇文档&lt;/a&gt; 可以帮助用户了解 TiDB 的查询计划。&lt;/p&gt;&lt;p&gt;用户除了通过 Explain 语句查看查询计划之外，在 2.1 版本中还可以通过 Explain Analyze 语句查看语句的运行时信息，包括每个算子运行时的处理时间以及处理的数据量。这样可以通过实际的运行结果，拿到更加精确的信息。&lt;/p&gt;&lt;p&gt;&lt;b&gt;热点调度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;热点是分布式系统最大的敌人之一，并且用户的业务场景复杂多变，让热点问题捉摸不定，也是最狡猾的敌人。2.1 版本中，我们一方面增强热点检测能力，尽可能详细地统计系统负载，更快的发现热点；另一方面优化热点调度策略，用尽可能小的代价，尽快地打散热点。同时我们也提供了手动分裂 Region 的接口，让用户在特殊场景下将单点瓶颈手动分裂开，再由 PD 进行负载均衡。&lt;/p&gt;&lt;p&gt;&lt;b&gt;高效的 GC 机制&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2.1 版本对 GC（垃圾回收） 模块进行优化。一方面减少对线上的写入的影响，另一方面加快了空间回收速度。在内部测试场景中，删除一个 1TB 的表，新的 GC 机制能够在 10 秒内回收 99% 左右的空间。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;更好的性能&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;OLTP&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们针对 OLTP 场景中，点查占多数的特点进行了针对性的优化。当通过 Unique Key 或者 Primary Key 进行数据访问时，在优化器和执行引擎中都做了改进，使得语句的执行效率更高，通过 &lt;a href=&quot;https://github.com/pingcap/docs/blob/master/benchmark/sysbench-v3.md&quot;&gt;2.1 和 2.0 版本的 Sysbench 对比&lt;/a&gt; 可以看到，点查性能提升 50%。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-135377cc39aac37746c4852e8761f9a8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;937&quot; data-rawheight=&quot;579&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-135377cc39aac37746c4852e8761f9a8&quot; data-watermark-src=&quot;v2-15ef32935b3deb5c82daa52ef0bca23f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;OLAP&lt;/b&gt;&lt;/p&gt;&lt;p&gt;发布 2.0 的时候，我们同时发布了在 TPC-H Scale 50 的场景中 &lt;a href=&quot;https://github.com/pingcap/docs/blob/master/benchmark/tpch.md&quot;&gt;2.0 和 1.0 的对比结果&lt;/a&gt;。其中大多数 Query 都有数量级的提升，部分 Query 在 1.0 中跑不出结果，在 2.0 中可以顺利运行。不过对于 Query17 和 Query18，运行时间依然很长。&lt;/p&gt;&lt;p&gt;我们在相同的场景下，对 2.1 和 2.0 进行了 &lt;a href=&quot;https://github.com/pingcap/docs/blob/master/benchmark/tpch-v2.md&quot;&gt;对比测试&lt;/a&gt;。从下图可以看到（纵坐标是 Query 的响应时间，越低越好），之前的两个慢 Query 的运行时间大幅缩短，其他的 Query 也有一定程度的提升。这些提升一方面得益于查询优化器以及执行引擎的改进，另一方面 得益于 TiKV 对连续数据扫描的性能优化。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-61efde2ac32bc3c281332b498558a5d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;581&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-61efde2ac32bc3c281332b498558a5d9&quot; data-watermark-src=&quot;v2-9dab87e5d7d6e756217b14e7c7db7918&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;完善的生态工具&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了让用户更方便的使用 TiDB，我们提供了三个工具：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs/tools/lightning/overview-architecture/&quot;&gt;TiDB Lightning&lt;/a&gt; 用于将全量数据导入到 TiDB 中，这个工具可以提升全量数据导入速度，目前内部测试场景中，一小时可以导入 100GB 数据。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs/tools/tidb-binlog-cluster/&quot;&gt;TiDB Binlog&lt;/a&gt; 用于将 TiDB 中的数据更新实时同步到下游系统中，可以用于做主从集群同步或者是将 TiDB 中的数据同步回 MySQL。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/docs/tools/data-migration-overview/&quot;&gt;TiDB DM&lt;/a&gt;（Data-Migration）用于将 MySQL/MariaDB 中的数据通过 Binlog 实时同步到 TiDB 集群中，并且提供 Binlog 数据转换功能，可以将 Binlog 中的表/库名称进行修改，或者是对数据内容本身做修改和裁剪。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上述三个工具可以将 TiDB 和周边的系统打通，既能将数据同步进 TiDB，又可以将数据同步出来。所以无论是迁移、回退还是做数据热备，都有完整的解决方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Open Source Community&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们相信战胜“未知”最好的武器就是社区的力量，基础软件需要坚定地走开源路线。为了让社区更深入的了解 TiDB 的技术细节并且更好地参与到项目中来，我们今年已经完成超过 20 篇源码阅读文章，项目的设计文档（&lt;a href=&quot;https://github.com/pingcap/tidb/wiki/Design-Documents&quot;&gt;TiDB&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/tikv/rfcs&quot;&gt;TiKV&lt;/a&gt;）已经在 GitHub 上面公开出来，项目的开发过程也尽量通过 GitHub Issue/Project 向社区展示。一些 Feature 设计方案的讨论也会通过在线视频会议的方式方便社区参与进来，&lt;a href=&quot;https://github.com/pingcap/community/blob/master/proposals.md&quot;&gt;这里&lt;/a&gt; 可以看到会议安排。&lt;/p&gt;&lt;p&gt;从 TiDB 2.0 版发布到现在的半年多时间，TiDB 开源社区新增了 87 位 Contributor，其中 &lt;a href=&quot;https://github.com/spongedu&quot;&gt;杜川&lt;/a&gt; 成为了 TiDB Committer，他已经贡献了 &lt;a href=&quot;https://github.com/pingcap/tidb/commits?author=spongedu&quot;&gt;76 次 PR&lt;/a&gt;，还有一些活跃的 Contributor 有希望成为下一批 Committer。&lt;/p&gt;&lt;p&gt;在这里我们对社区贡献者表示由衷的感谢，希望更多志同道合的人能加入进来，也希望大家在 TiDB 这个开源社区能够有所收获！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-01-51304475</guid>
<pubDate>Sat, 01 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>捕获和增强原生系统的可观测性来发现错误</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-11-23-50671185.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50671185&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3c69c2bd32515795a24a4c954c0e428d_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：唐刘&lt;/p&gt;&lt;p&gt;在对 TiDB 进行 Chaos 实践的时候，我一直在思考如何更好的发现 TiDB 整个系统的故障。最开始，我们参考的就是 Chaos Engineering 里面的方式，观察系统的稳定状态，注入一个错误，然后看 metrics 上面有啥异常，这样等实际环境中出现类似的 metrics，我们就知道发现了什么故障。&lt;/p&gt;&lt;p&gt;但这套机制其实依赖于如何去注入错误，虽然现在我们已经有了很多种错误注入的方式，但总有一些实际的情况我们没有料到。所以后来我们又考虑了另外的一种方式，也就是直接对 metrics 历史进行学习，如果某一段时间 metrics 出现了不正常的波动，那么我们就能报警。但这个对我们现阶段来说难度还是有点大，只使用了几种策略，对 QPS，Latency 这些进行了学习，并不能很好的定位到具体出了什么样的问题。&lt;/p&gt;&lt;p&gt;所以我一直在思考如何更好的去发现系统的故障。最近，刚好看到了OSDI 2018 一篇 Paper， &lt;a href=&quot;https://www.usenix.org/system/files/osdi18-huang.pdf&quot;&gt;Capturing and Enhancing In Situ System Observability for Failure Detection&lt;/a&gt;，眼睛一亮，觉得这种方式也是可以来实践的。&lt;/p&gt;&lt;p&gt;大家都知道，在生产环境中，故障是无处不在，随时可能发生的，譬如硬件问题，软件自身的 bug，或者运维使用了一个错误的配置这些。虽然多数时候，我们的系统都做了容错保护，但我们还是需要能尽快的发现故障，才好进行故障转移。&lt;/p&gt;&lt;p&gt;但现实世界并没有那么美好，很多时候，故障并不是很明显的，譬如整个进程挂掉，机器坏掉这些，它们处于一种时好时坏的状态，我们通常称为『Gray Failure』，譬如磁盘变慢了，网络时不时丢包。这些故障都非常隐蔽，很难被发现。如果单纯的依赖外部的工具，其实很难检测出来。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8b3291c8c8e60ecbc9c96aad2b353d60_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2000&quot; data-rawheight=&quot;1090&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8b3291c8c8e60ecbc9c96aad2b353d60&quot; data-watermark-src=&quot;v2-068dd0bd73f4851f887d506a77a0019e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上面是作者举得一个 Zookeeper 的例子，client 已经完全不能跟 Leader 进行交互了，但是 Leader 却仍然能够给 Follower 发送心跳，同时也能响应外面 Monitor 发过来的探活命令。&lt;br&gt; 如果从外面的 Monitor 看来，这个 Zookeeper 集群还是正常的，但其实它已经有故障了。而这个故障其实 client 是知道的，所以故障检测的原理很简单，从发起请求的这一端来观察，如果发现有问题，那就是有故障了。而这也是这篇论文的中心思想。&lt;/p&gt;&lt;p&gt;在论文里面，作者认为，任何严重的 Gray Failure 都是能够被观察到的，如果发起请求的这边遇到了错误，自然下一件事情就是将这个错误给汇报出去，这样我们就知道某个地方出现了故障。于是作者开发了 &lt;a href=&quot;https://github.com/ryanphuang/panorama&quot;&gt;Panorama&lt;/a&gt; 这套系统，来对故障进行检测。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;整体架构&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;先来说说 Panorama 一些专业术语。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4f53fe8c77b8a65ddfcfd80caca665fa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;634&quot; data-rawheight=&quot;329&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4f53fe8c77b8a65ddfcfd80caca665fa&quot; data-watermark-src=&quot;v2-87ba341924b4bfd8985d1e250208885b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Panorama 整体结构如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-be3c0f34afb5db8ec6b663f3c9994e06_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1148&quot; data-rawheight=&quot;696&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-be3c0f34afb5db8ec6b663f3c9994e06&quot; data-watermark-src=&quot;v2-7f7ad761ff2cc070637b9ded88f1bc04&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Panorama 通过一些方式，譬如静态分析代码进行代码注入等，将 Observer 跟要观察的 Subject 进行绑定，Observer 会将 Subject 的一些信息记录并且汇报给本地的一个 Local Observation Store（LOS）。本地一个决策引擎就会分析 LOS 里面的数据来判断这个组件的状态。如果多个 LOS 里面都有对某个 Subject 的 observation，那么 LOS 会相互交换，用来让中央的 verdict 更好的去判断这个 component 的状态。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;故障判定&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;而用来判断一个 component 是不是有故障也比较容易，采用的是一种大多数 bounded-look-back 算法。对于一个 subject，它可能会有很多 observations，首先我们会对这些 observations 按照 observer 进行分组，对每组单独进行分析。在每个组里面，Observations 会按照时间从后往前检查，并且按照 context 进行聚合。如果一个被观察的 observation 的 status 跟记录前面相同 context 的 observation status 状态不一样，就继续 loop-back，直到遇到一个新的 status。对于一个 context，如果最后的状态是 unhealthy 或者 healthy 的状态没有达到多数，就会被认为是 unhealthy 的。&lt;/p&gt;&lt;p&gt;通过这种方式，我们在每组里面得到了每个 context 的状态，然后又会在多个组里面进行决策，也就是最常用的大多数原则，哪个状态最多，那么这个 context 对应的状态就是哪一个。这里我们需要额外处理下 PENDING 这个状态，如果当前状态是 HEALTHY 而之前老的状态是 PENDING，那么 PENDING 就会变成 HEALTHY，而如果一直是 PENDING 状态并超过了某个阈值，就会退化成 UNHEALTHY。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Observability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这里再来说说 Observability 的模式。对于分布式系统来说，不同 component 之间的交互并不是同步的，我们会面临如下几种情况：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fb41105f1ecc32a94800ea06a1c8aa53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;992&quot; data-rawheight=&quot;282&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fb41105f1ecc32a94800ea06a1c8aa53&quot; data-watermark-src=&quot;v2-e471aea1424c2115bf962e335f755348&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果两个组件 C1 和 C2 是同步交互，那么当 C1 给 C2 发送请求，我们就完全能在 C1 这一端知道这次请求成功还是失败了，但是对于非同步的情况，我们可能面临一个问题，就是 C1 给 C2 发了请求，但其实这个请求是放到了异步消息队列里面，但 C1 觉得是成功了，可是后面的异步队列却失败了。所以 Panorama 需要有机制能正确处理上面多种情况。&lt;/p&gt;&lt;p&gt;为了能更好的从 component 上面得到有用的 observations，Panorama 会用一个离线工具对代码进行静态分析，发现一些关键的地方，注入钩子，这样就能去汇报 observations 了。&lt;/p&gt;&lt;p&gt;通常运行时错误是非常有用能证明有故障的证据，但是，并不是所有的错误都需要汇报，Panorama 仅仅会关系跨 component 边界产生的错误，因为这也是通过发起请求端能观察到的。Panorama 对于这种跨域的函数调用称为 observation boundaries。对于 Panorama 来说，第一件事情就是定位 observation boundaries。通常有两种 boundaries，进程间交互和线程间交互。进程间交互通常就是 socket I/O，RPC，而线程间则是在一个进程里面跨越线程的调用。这些 Panorama 都需要分析出来。&lt;/p&gt;&lt;p&gt;当定位了 observation boundaries 之后，下一件事情就是确定 observer 和 subject 的标识。譬如对于进程间交互的 boundaries，observer 的标识就可能是这个进程在系统里面的唯一标识，而对于 subject，我们可以用 method 名字，或者是函数的一个参数，类里面的一个字段来标识。&lt;/p&gt;&lt;p&gt;然后我们需要去确定 observation points，也就是观测点。通常这些点就是代码处理异常的地方，另外可能就是一些正常处理返回结果但会对外报错的地方。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3155f69432096b54c29da20ebf74c276_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;964&quot; data-rawheight=&quot;720&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3155f69432096b54c29da20ebf74c276&quot; data-watermark-src=&quot;v2-9a7ca500a7eef87bb3f07c69caf9e79f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上面就是一个简单分析代码得到 observation points 的例子，但这个仍然是同步的，对于 indirection 的，还需要额外处理。&lt;/p&gt;&lt;p&gt;对于异步请求，我们知道，通过发出去之后，会异步的处理结果，所以这里分为了两步，叫做 ob-origin 和 ob-sink。如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-60616ccca52710a6a69b13c602821e09_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;950&quot; data-rawheight=&quot;746&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-60616ccca52710a6a69b13c602821e09&quot; data-watermark-src=&quot;v2-225b009ea1fc63f50a6b03ca46844f98&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 ob-origin，代码分析的时候会先给这个 observation 设置成 PENDING 状态，只有对应的 ob-sink 调用并且返回了正确的结果，才会设置成 HEALTHY。因为 ob-origin 和 ob-sink 是异步的，所以代码分析的时候会加上一个特殊的字段，包含 subject 的标识和 context，这样就能让 ob-origin 和 ob-sink 对应起来。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;小结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面大概介绍了 Panorama 的架构以及一些关键的知识点是如何实现的，简单来说，就是在一些关键代码路径上面注入 hook，然后通过 hook 对外将相关的状态给汇报出去，在外面会有其他的分析程序对拿到的数据进行分析从而判定系统是否在正常工作。它其实跟加 metrics 很像，但 metrics 只能看出哪里出现了问题，对于想更细致定位具体的某一个问题以及它的上下文环境，倒不是特别的方便。这点来说 Panorama 的价值还是挺大的。&lt;/p&gt;&lt;p&gt;Panorama 的代码已经开源，总的来说还是挺简单的，但我没找到核心的代码分析，注入 hook 这些，有点遗憾。但理解了大概原理，其实先强制在代码写死也未尝不可。另一个比较可行的办法就是进行在代码里面把日志添加详细，这样就不用代码注入了，而是在外面写一个程序来分析日志，其实 Panorama 代码里面提供了日志分析的功能，为 Zookeeper 来设计的，但作者自己也说到，分析日志的效果比不上直接在代码里面进行注入。&lt;/p&gt;&lt;p&gt;那对我们来说，有啥可以参考的呢？首先当然是这一套故障检查的理念，既然 Panorama 已经做出来并且能发现故障量，自然我们也可以在 TiDB 里面实施。因为我们已经有在 Go 和 Rust 代码里面使用 fail 来进行错误注入的经验，所以早起手写监控代码也未尝不可，但也可以直接完善日志，提供一个程序来分析日志就成。如果你对这块感兴趣，想把 Panorama 相关的东西应用到 TiDB 中来，欢迎联系我 &lt;a href=&quot;mailto:tl@pingcap.com&quot;&gt;tl@pingcap.com&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.jianshu.com/p/de53ff95d697&quot;&gt;捕获和增强原生系统的可观测性来发现错误&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-11-23-50671185</guid>
<pubDate>Fri, 23 Nov 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>拿奖秘诀泄露，TiDB Hackathon 等你来挑战！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-11-16-50103243.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50103243&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-507a9d1aeff2b774624b5be2cf38c312_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;i&gt;TiDB Hackathon 2018 开启报名已有半月有余，作为本场 Hackathon &lt;b&gt;唯一的“小可爱”——我，TiDB Robot&lt;/b&gt;，这段时间受到了小伙伴们的问题轰炸，包括但不限于怎么组队选题、怎么和队友合作、住哪儿、吃啥……还有求划重点拿大奖的。所以今天我总结了一些常见 QA，&lt;b&gt;分为报名篇 / 选题篇 / 实操篇 / 吃住篇&lt;/b&gt;，顺便也划了一下重点，希望对大家有所帮助～&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;报名篇&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Q：对参赛者本身有什么门槛吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：没有门槛，不限年龄，不限职业，唯一的要求是&lt;b&gt;来现场参赛（北京）&lt;/b&gt;。Hackathon 注重现场的团队配合和竞技氛围，不接受线上参与哦～&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：如何组建一支“梦之队”？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：Hackathon 精髓之一是团队协作，&lt;b&gt;队员配合和角色分配&lt;/b&gt;很重要。谁来选题谁来设计，是否需要前端等等都是组队时要考虑的因素。还有一个容易被忽视的重要角色是&lt;b&gt;演讲人&lt;/b&gt;，两天一夜的比赛最后的决定性时刻就在展示的几分钟，如何条理清晰的把做的东西讲清楚，以及这个项目解决了什么实质性的问题很关键。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：一个人也可以成队报名吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：当然可以，我们非常欢迎技能值满点的小伙伴以个人身份参赛（传说中的“一个人活成一支队伍”），也欢迎暂时没有选题或队友的个人参赛者报名，主办方会协调大家进行赛前组队。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;选题篇&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Q：对选题毫无思路怎么办？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：本场 Hackathon 主题为 &lt;b&gt;TiDB Ecosystem。&lt;/b&gt;如果想参赛但不知从何入手，可以&lt;b&gt;勾搭我（微信号：tidbai）&lt;/b&gt;获取我们的选题参考方向哦～希望大家可以举一反三，think out of the box，做出令人眼前一亮的作品。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：已有选题的队伍可以提前写代码了吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：&lt;b&gt;不可以&lt;/b&gt;。为了公平起见，参赛作品的所有代码必须在 Hackathon 现场完成，前期准备仅限于资料搜集、架构设计、环境配置与测试。我们不就是为了在有限的时间里做最有挑战的事情，对吗？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0f0b9fbaf639866ad88b6d5e46c00085_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;370&quot; data-rawheight=&quot;366&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0f0b9fbaf639866ad88b6d5e46c00085&quot; data-watermark-src=&quot;v2-ba2d9f4a5a6f5e5243169f6ed2a2148f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;实操篇&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Q：当天如何与队友合作呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：指定团队协作平台—— &lt;b&gt;GitHub&lt;/b&gt;。GitHub 上的提交记录，也方便于评审团审核参赛作品的完整度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：还不会使用 GitHub 怎么办？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：五分钟学会使用 GitHub——&lt;/p&gt;&lt;p&gt;    - What is GitHub：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w3jLJU7DT5E&quot;&gt;https://www.youtube.com/watch?v=w3jLJU7DT5E&lt;/a&gt;&lt;/p&gt;&lt;p&gt;    - GitHub Tutorial For Beginners：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0fKg7e37bQE&quot;&gt;https://www.youtube.com/watch?v=0fKg7e37bQE&lt;/a&gt;&lt;/p&gt;&lt;p&gt;    - 知乎 GitHub 话题助攻&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/20070065&quot;&gt;https://www.zhihu.com/question/20070065&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：TiDB 的代码对于新手来说好复杂啊，哪里可以找到更多的参考资料？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：&lt;b&gt;官方文档&lt;/b&gt;（github.com/pingcap/docs-cn）中有详细的安装部署及运用指导，另外，&lt;b&gt;官方微信公众号&lt;/b&gt;中有宝藏哦。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;推荐阅读：&lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27275483&quot;&gt;三篇文章了解 TiDB 技术内幕系列&lt;/a&gt;&lt;/u&gt; |   &lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46524530&quot;&gt;TiKV 是如何存储数据的&lt;/a&gt;&lt;/u&gt;  | &lt;u&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;我们定期举办的 Infra Meetup 常常有 TiDB 相关议题，大家可以在微信公众号（ID: pingcap2015）菜单栏“社区活动-&amp;gt;往期 Meetup”中翻阅视频&amp;amp;文字回顾。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另外悄悄的说，我们&lt;b&gt;官网&lt;/b&gt;的搜索很强大，大家可以搜索关键词找到相关资料。除此之外，也可以求助导师团（&lt;a href=&quot;https://mp.weixin.qq.com/s/C5FbZ7HEG3Sr6l_oqCxNng&quot;&gt;七龙珠导师团介绍在此，大家现场“照图抓人”即可&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：如何更好的展示成果？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：Hackathon 不仅是一场代码技术的比拼，临场发挥和演讲技巧也很重要，最后需要在短短几分钟内让大家明白你做了什么以及这个项目的价值。&lt;/p&gt;&lt;p&gt;我们见过花了大量时间介绍项目背景而正题只进行到一半就草草结束的团队，也见过测试时完美运行的项目在演示时却频频报错、选手满头大汗的场景。所以我们建议：&lt;b&gt;演讲时逻辑清晰，开门见山，在有限的时间里阐述项目的重点，并预留出一部分时间进行程序演示，&lt;/b&gt;而现场跑 demo 是一门&lt;b&gt;玄学&lt;/b&gt;，如果不想冒险可以预先录制一个 Demo 视频。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：奖金好诱人，好想拿奖！求划重点！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：和有趣的人一起做有趣的事很重要，顺便拿个大奖当然就更好啦～本次比赛的评判标准是实用性、完成度和创新性。&lt;b&gt;其中，权重最大的是实用性，我们希望 Hackathon 中产出的项目可以长久的在社区运行下去，因此解决实际问题和提高效率是比较重要的。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;吃住篇&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Q：主办方提供餐饮和住宿吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：我们提供参赛者和志愿者比赛期间的餐饮（正餐包括 12 月 1 日午餐、晚餐，12 月 2 日早餐、午餐），参赛选手可留在比赛场地过夜，如需在场地附近租住宾馆需要自己解决哟～&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q：比赛两天都需要呆在活动场地吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A：如果没有特殊需求请不要离开场地，需要回自己住处过夜的小伙伴需要提前告知工作人员，并于第二天早晨 8 点前返回场地。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;最后也是最最重要的建议：&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;合理规划项目时间，选定主题之后不要轻易改动，要有团队精神，&lt;b&gt;不能轻易抛弃队友。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;一定、一定要&lt;b&gt;慎用 rm-rf/ &lt;/b&gt; （这是往年 Hackathon 真实发生过的“惨案”……）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;赛程重要信息&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;奖项设置&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;一等奖（1 支队伍）：¥ 60,000 现金奖励&lt;/p&gt;&lt;p&gt;二等奖（2 支队伍）：每队 ¥ 30,000 现金奖励&lt;/p&gt;&lt;p&gt;三等奖（3 支队伍）：每队 ¥ 10,000 现金奖励&lt;/p&gt;&lt;p&gt;除此之外还设有最佳创意奖和最佳贡献奖。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名时间&lt;/b&gt;：即日起至 11 月 23 日 17:00&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名审核&lt;/b&gt;：5 个工作日内反馈审核结果&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名链接&lt;/b&gt;：点击【&lt;a href=&quot;http://nc9hsk15y2xczuor.mikecrm.com/3AarNns&quot;&gt;这里&lt;/a&gt;】报名&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-11-16-50103243</guid>
<pubDate>Fri, 16 Nov 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
