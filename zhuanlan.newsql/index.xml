<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Mon, 12 Nov 2018 13:16:46 +0800</lastBuildDate>
<item>
<title>TiDB at 丰巢：尝鲜分布式数据库</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-11-12-49418382.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/49418382&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-53b245933c4c1344ffe14c3caa58a35b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;i&gt;作者：丰巢技术团队&lt;/i&gt;&lt;/p&gt;&lt;p&gt;随着丰巢业务系统快速增长，其核心系统的数据量，早就跨越了亿级别，而且每年增量仍然在飞速发展。整个核心系统随着数据量的压力增长，不但系统架构复杂度急剧增长，数据架构更加复杂，传统的单节点数据库，已经日渐不能满足丰巢的需求，当单表数量上亿的时候，Oracle 还能勉强抗住，而 MySQL 到单表千万级别的时候就难以支撑，需要进行分表分库。为此，一款高性能的分布式数据库，日渐成为刚需。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;思考&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在互联网公司业务量增大之后，并行扩展是最常用、最简单、最实时的手段。例如负载均衡设备拆流量，让海量流量变成每个机器可以承受的少量流量，并且通过集群等方式支撑起来整个业务。于是当数据库扛不住的时候也进行拆分。&lt;/p&gt;&lt;p&gt;但有状态数据和无状态数据不同，当数据进行拆分的时候，会发生数据分区，而整个系统又要高可用状态下进行，于是数据的一致性变成了牺牲品，大量的核对工具在系统之间跑着保证着最终的一致性。在业务上，可能业务同学经常会遇到分过库的同学说，这个需求做不了，那个需求做不了，如果有 sql 经验的业务同学可能会有疑问不就是一条 sql 的事情么，其实这就是分库分表后遗症。&lt;/p&gt;&lt;p&gt;为此，我们需要有个数据库帮我们解决以上问题，它的特性应该是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据强一致：支持完整的 ACID；&lt;/li&gt;&lt;li&gt;不分表分库：无论多少数据我们只管插入不需要关心啥时候扩容，会不会有瓶颈；&lt;/li&gt;&lt;li&gt;数据高可用：当我们某台数据库的少部分机器磁盘或者其他挂了的时候，我们业务上可以无感知，甚至某个城市机房发生灾难的时候还可以持续提供服务，数据不丢失；&lt;/li&gt;&lt;li&gt;复杂 SQL 功能：基本上单库的 SQL，都可以在这个数据库上运行，不需要修改或者些许修改；&lt;/li&gt;&lt;li&gt;高性能：在满足高 QPS 的同时，保证比较低的延时。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;选型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;根据以上期望进行分析，我们分析了目前市面上存在的 NewSQL 分布式数据库，列表如下：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d50aaba4268b4bcffdf1037c9df55fb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;540&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7d50aaba4268b4bcffdf1037c9df55fb&quot; data-watermark-src=&quot;v2-d9d2d9cabfc430fe93fe923f6c3d86c3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;在综合考虑了开源协议，成熟度，可控度，性能，服务支撑等综合因素之后，我们选择了 TiDB，它主要优势如下：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高度兼容 MySQL&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;大多数情况下，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;水平弹性扩展&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松松应对高并发、海量数据场景。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分布式事务 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;金融级别的高可用性&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;相比于传统主从（M-S）复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复（auto-failover），无需人工介入。&lt;/p&gt;&lt;p&gt;基于如上的原因，我们选择了 TiDB，作为丰巢的核心系统的分布式数据库，来取代   Oracle 和 MySQL。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;评估&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 性能测试&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的基准测试，使用的工具是 sysbanch 进行测试，使用了 8 张基础数据为一千万的表，分别测试了 insert，select，oltp 和 delete 脚本得到数据如下，查询的 QPS 达到了惊人的 14 万每秒，而插入也稳定在 1 万 4 每秒。&lt;/p&gt;&lt;p&gt;核心服务器配置：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ea896dbfcc915d8b30eae37f8cd72bdd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;218&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ea896dbfcc915d8b30eae37f8cd72bdd&quot; data-watermark-src=&quot;v2-09156297eb416aaa8ee6cdf3d36ea51f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;测试结果：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-175e029b03244dd0853c16f104c56563_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;476&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-175e029b03244dd0853c16f104c56563&quot; data-watermark-src=&quot;v2-803097652d090110d23cbb070bceb613&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;通过～&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 功能测试&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7698939b1c1a4dd75285e254111a6d9a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;363&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7698939b1c1a4dd75285e254111a6d9a&quot; data-watermark-src=&quot;v2-07bcb8e6f8393100108c878fddf734a1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;通过～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;接入&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;因为是核心系统，安全起见，我们采取了多种方案保证验证项目接入的可靠性，保证不影响业务。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 项目选择&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在寻找第一个接入项目的时候，我们以下面 4 个特征，进行了选择：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-14b43de242d2761cd9c40c9bd9335c9b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;681&quot; data-rawheight=&quot;443&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-14b43de242d2761cd9c40c9bd9335c9b&quot; data-watermark-src=&quot;v2-e0772a0cd62151e113f42132dd3b2a17&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;最终，我们选择了推送服务。因为推送服务是丰巢用来发送取件通知的核心服务，量非常大，但逻辑简单，而且有备选外部推送方案，所以即便万一出现问题，而不会影响用户。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 代码修改&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;因为 TiDB 是完全兼容 MySQL 语法的，所以在这个项目的接入过程中，我们对代码的修改是很细微的。&lt;/b&gt;SQL 基本零改动，主要是外围代码，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;异步接口修改，数据异步化入库&lt;/li&gt;&lt;li&gt;同步接口修改，实现异常熔断&lt;/li&gt;&lt;li&gt;停止内嵌数据迁移代码&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上三点，保证了整个系统在不强依赖于数据库，并且能在高并发的情况下通过异步落库保护数据库不被压垮，并且在数据库发生问题的时候，核心业务可以正常进行下去。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;效果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 查询能力&lt;/b&gt;&lt;/p&gt;&lt;p&gt;接入 TiDB 之后，原先按照时间维度来拆分的十几个分表，变成了一张大表。最明显的变化，是在大数据量下，数据查询能力有了显著的提升。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e6bda36f6bcce06417c6f8f2c7255247_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;464&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e6bda36f6bcce06417c6f8f2c7255247&quot; data-watermark-src=&quot;v2-d7e565cc555d52e9439b5cfd8ccc79bc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;2. 监控能力&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 拥有很完善的监控平台，可以直观的看到容量，以及节点状态：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5ce8bed8502373190e4a9ca35461f2ef_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;220&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5ce8bed8502373190e4a9ca35461f2ef&quot; data-watermark-src=&quot;v2-3866522a4cc9ee73cea13be05dff5712&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;还能了解每个节点负载和 sql 执行的延时：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-88181e69d7396fbd0ecd51bb41aea539_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;175&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;当然还能了解所在机器上的位置，CPU 内存等负载情况：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4d995c9bba949f576d89a4aa9c450bbe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;334&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4d995c9bba949f576d89a4aa9c450bbe&quot; data-watermark-src=&quot;v2-e4cb9a05aab6d255d3cb3239c88b18e6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;网络状态也能清晰的监控到：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5e08520002a2e483e600ed5eb18d0e69_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;342&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5e08520002a2e483e600ed5eb18d0e69&quot; data-watermark-src=&quot;v2-7923663063ef472ffbc3b4dc9a4cc89e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;所有这些能让团队能分析出来有问题的 sql，以及数据库本身的问题。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;小结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 的接入过程，整体还是非常顺利的，由于之前做了很多接入的保障工作，当天切换流量到 TiDB 的过程只用了 10 分钟的时间，在此也要感谢 TiDB 对于 MySQL 语法的兼容性的支持，以及 PingCAP 提供的各种有用的工具。到目前为止，系统的稳定运行了一个多月，很好的满足了丰巢的业务需求。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的改造完成之后，丰巢推送服务对大部分消息进行了落地和查询，截止目前为止，推送服务最大的日落地量已经达到了 5 千万，而如果现在推送服务还使用的还是 MySQL 的方案，就需要上各种的分库分表方案，很多细致的业务就无法或者难以开展。&lt;/p&gt;&lt;p&gt;此次 TiDB 的改造，只是丰巢对于分布式数据技术探索的一小步，未来丰巢会将更多的分布式技术，引入到更多的业务系统，打造更加极致的产品和服务。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-11-12-49418382</guid>
<pubDate>Mon, 12 Nov 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 开源社区指南（上）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-11-09-49032099.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/49032099&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-cd7d9bab60939208425651627bac56f5_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者： &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/ed548ab735221a534cfc14702e4c8638&quot; data-hash=&quot;ed548ab735221a534cfc14702e4c8638&quot; data-hovercard=&quot;p$b$ed548ab735221a534cfc14702e4c8638&quot;&gt;@申砾&lt;/a&gt; &lt;/p&gt;&lt;blockquote&gt;本系列文章旨在帮助社区开发者了解 TiDB 项目的全貌，更好的参与 TiDB 项目开发。大致会分两个角度进行描述：&lt;br&gt;1. 从社区参与者的角度描述如何更好的参与 TiDB 项目开发；&lt;br&gt;2. 从 PingCAP 内部团队的角度展示 TiDB 的开发流程，包括版本规划、开发流程、Roadmap 制定等。&lt;br&gt;希望通过一内一外两条线的描述，读者能在技术之外对 TiDB 有更全面的了解。本篇将聚焦在社区参与者的角度进行描述，也就是“外线”。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;了解 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;参与一个开源项目第一步总是了解它，特别是对 TiDB 这样一个大型的项目，了解的难度比较高，这里列出一些相关资料，帮助 newcomers 从架构设计到工程实现细节都能有所了解：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs#tidb-introduction&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/blog/2016-10-17-how-we-build-tidb/&quot;&gt;How we build TiDB&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://tikv.github.io/deep-dive-tikv/book/&quot;&gt;Deep Dive TiKV (Work-In-Process)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，最高效地熟悉 TiDB 的方式还是使用它，在某些场景下遇到了问题或者是想要新的 feature，去跟踪代码，找到相关的代码逻辑，在这个过程中很容易对相关模块有了解，不少 Contributor 就是这样完成了第一次贡献。&lt;/p&gt;&lt;p&gt;我们还有一系列的 Infra Meetup，大约两周一次，如果方便到现场的同学可以听到这些高质量的 Talk。除了北京之外，其他的城市（上海、广州、成都、杭州）也开始组织 Meetup，方便更多的同学到现场来面基。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;发现可以参与的事情&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对 TiDB 有基本的了解之后，就可以选一个入手点。在 TiDB repo 中我们给一些简单的 issue 标记了 &lt;a href=&quot;https://github.com/pingcap/tidb/issues?q=is%3Aissue+is%3Aopen+label%3A%22for+new+contributors%22&quot;&gt;for-new-contributors &lt;/a&gt;标签，这些 issue 都是我们评估过容易上手的事情，可以以此为切入点。另外我们也会定期举行一些活动，把框架搭好，教程写好，新 Contributor 按照固定的模式即可完成某一特性开发。&lt;/p&gt;&lt;p&gt;当然除了那些标记为 for-new-contributors 的 issue 之外，也可以考虑其他的 issue，标记为 &lt;a href=&quot;https://github.com/pingcap/tidb/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&quot;&gt;help-wanted &lt;/a&gt;标签的 issue 可以优先考虑。除此之外的 issue 可能会比较难解决，需要对 TiDB 有较深入的了解或者是对完成时间有较高的要求，不适合第一次参与的同学。&lt;/p&gt;&lt;p&gt;当然除了现有的 issue 之外，也欢迎将自己发现的问题或者是想要的特性提为新的 issue，然后自投自抢 :) 。&lt;/p&gt;&lt;p&gt;当你已经对 TiDB 有了深入的了解，那么可以尝试从 &lt;a href=&quot;https://github.com/pingcap/docs/blob/master/ROADMAP.md&quot;&gt;Roadmap &lt;/a&gt;上找到感兴趣的事项，和我们讨论一下如何参与。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;讨论方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;找到一个感兴趣的点之后，可以在 issue 中进行讨论，如果是一个小的 bug-fix 或者是小的功能点，可以简单讨论之后开工。即使再简单的问题，也建议先进行讨论，以免出现解决方案有问题或者是对问题的理解出了偏差，做了无用功。&lt;/p&gt;&lt;p&gt;但是如果要做的事情比较大，可以先写一个详细的设计文档，提交到 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/master/docs/design&quot;&gt;docs/design &lt;/a&gt;目录下面，这个目录下有设计模板以及一些已有的设计方案供你参考。一篇好的设计方案要写清楚以下几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;背景知识&lt;/li&gt;&lt;li&gt;解决什么问题&lt;/li&gt;&lt;li&gt;方案详细设计&lt;/li&gt;&lt;li&gt;对方案的解释说明，证明正确性和可行性&lt;/li&gt;&lt;li&gt;和现有系统的兼容性&lt;/li&gt;&lt;li&gt;方案的具体实现&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;用一句话来总结就是写清楚“你做了什么，为什么要做这个，怎么做的，为什么要这样做”。如果对自己的方案不太确定，可以先写一个 Google Doc，share 给我们简单评估一下，再提交 PR。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;提交 PR&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;按照方案完成代码编写后，就可以提交 PR。当然如果开发尚未完成，在某些情况下也可以先提交 PR，比如希望先让社区看一下大致的解决方案，这个时候请将 PR 标记为 WIP。&lt;br&gt;对于 PR 我们有一些要求：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;需要能通过 &lt;code class=&quot;inline&quot;&gt;make dev&lt;/code&gt; 的测试，跑过基本的单元测试；&lt;/li&gt;&lt;li&gt;必须有测试，除非只是改动文档或者是依赖包，其他情况需要有充足的理由说明没有测试的原因；&lt;/li&gt;&lt;li&gt;代码以及注释的质量需要足够高， &lt;a href=&quot;https://github.com/pingcap/community/blob/master/CONTRIBUTING.md#code-style&quot;&gt;这里 &lt;/a&gt;有一些关于编码风格和 commit message 的 guide；&lt;/li&gt;&lt;li&gt;请尽可能详细的填写 PR 的描述，并打上合适的 label。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;对于 PR 的描述，我们提供了一个模板，希望大家能够认真填写，一个好的描述能够加速 PR 的 review 过程。通过这个模板能够向 reviewers 以及社区讲明白：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;这个PR 解决什么问题：相关的问题描述或者是 issue 链接；&lt;/li&gt;&lt;li&gt;如何解决：具体的解决方法，reviewers 会根据这里的描述去看代码变动，所以请将这一段写的尽可能详细且有帮助；&lt;/li&gt;&lt;li&gt;测试的情况；&lt;/li&gt;&lt;li&gt;其他相关信息（如果需要）：benchmark 结果、兼容性问题、是否需要更新文档。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后再说几句测试，正确性是数据库安身立命之本，怎么强调测试都不为过。PR 中的测试不但需要充足，覆盖到所做的变动，还需要足够清晰，通过代码或者注释来表达测试的目的，帮助 reviewer 以及今后可能变动/破坏相关逻辑的人能够容易的理解这段测试。一段完善且清晰的测试也有利于让 reviewer 相信这个 Patch 是正确的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;PR review&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;PR review 的过程就是 reviewer 不断地提出 comment，PR 作者持续解决 comment 的过程。&lt;br&gt;&lt;b&gt;每个 PR 在合并之前都需要至少得到两个 Committer/Maintainer 的 LGTM，一些重要的 PR 需要得到三个，比如对于 DDL 模块的修改，默认都需要得到三个 LGTM。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;Tips：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;提了PR 之后，可以 at 一下相关的同学来 review&lt;/li&gt;&lt;li&gt;Address comment 之后可以 at 一下之前提过 comment 的同学，标准做法是 comment 一下 “ &lt;b&gt;PTAL @xxx&lt;/b&gt; ”，这样我们内部的 Slack 中可以得到通知，相关的同学会受到提醒，让整个流程更紧凑高效。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;与项目维护者之间的交流&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;目前标准的交流渠道是 GitHub issue&lt;/b&gt; ，请大家优先使用这个渠道，我们有专门的同学来维护这个渠道，其他渠道不能保证得到研发同学的及时回复。这也是开源项目的标准做法。&lt;br&gt;无论是遇到 bug、讨论具体某一功能如何做、提一些建议、产品使用中的疑惑，都可以来提 issue。在开发过程中遇到了问题，也可以在相关的 issue 中进行讨论，包括方案的设计、具体实现过程中遇到的问题等等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后请大家注意一点，除了 pingcap/docs-cn 这个 repo 之外，请大家使用英文。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;更进一步&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当你完成上面这些步骤的之后，恭喜你已经跨过第一个门槛，正式进入了 TiDB 开源社区，开始参与 TiDB 项目开发，成为 &lt;b&gt;TiDB Contributor&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;如果想更进一步，深入了解 TiDB 的内部机制，掌握一个分布式数据库的核心模块，并能做出改进，那么可以了解更多的模块，提更多的 PR，进一步向 Committer 发展（ &lt;a href=&quot;https://github.com/pingcap/community/blob/master/become-a-committer.md&quot;&gt;这里 &lt;/a&gt;解释了什么是 Committer）。目前 TiDB 社区的 Committer 还非常少，我们希望今后能出现更多的 Committer 甚至是 Maintainer。&lt;/p&gt;&lt;p&gt;从 Contributor 到 Committer 的门槛比较高，比如今年的新晋 Committer 杜川同学，在成为 Committer 的道路上给 tidb/tikv 项目提交了大约 80 个 PR，并且对一些模块有非常深入的了解。当然，成为 Committer 之后，会有一定的权利，比如对一些 PR 点 LGTM 的权利，参加 PingCAP 内部的技术事项、开发规划讨论的权利，参加定期举办的 TechDay/DevCon 的权利。目前社区中还有几位贡献者正走在从 Contributor 到 Committer 的道路上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-11-09-49032099</guid>
<pubDate>Fri, 09 Nov 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 助力卡思数据视频大数据业务创新</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-11-06-48674946.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/48674946&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-632a19539faeb998a4f082d80f81c175_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;卡思数据是国内领先的视频全网数据开放平台，依托领先的数据挖掘与分析能力，为视频内容创作者在节目创作和用户运营方面提供数据支持，为广告主的广告投放提供数据参考和效果监测，为内容投资提供全面客观的价值评估。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-07b63db2487756269fd6da9db5d051ce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1594&quot; data-rawheight=&quot;753&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-07b63db2487756269fd6da9db5d051ce&quot; data-watermark-src=&quot;v2-69ee6da76a96003dff0ae2094e22aaef&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt; 图 1 卡思数据产品展示图&lt;/p&gt;&lt;h2&gt;&lt;b&gt;业务发展遇到的痛点&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;卡思数据首先通过分布式爬虫系统进行数据抓取，每天新增数据量在 50G - 80G 之间，并且入库时间要求比较短，因此对数据库写入性能要求很高，由于数据增长比较快，对数据库的扩展性也有很高的要求。数据抓取完成后，对数据进行清洗和计算，因为数据量比较大，单表 5 亿 + 条数据，所以对数据库的查询性能要求很高。&lt;/p&gt;&lt;p&gt;起初卡思数据采用的是多个 MySQL 实例和一个 MongoDB 集群，如图 2。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MySQL 存储业务相关数据，直接面向用户，对事务的要求很高，但在海量数据存储方面偏弱，由于单行较大，单表数据超过千万或 10G 性能就会急剧下降。&lt;/li&gt;&lt;li&gt;MongoDB 存储最小单元的数据，MongoDB 有更好的写入性能，保证了每天数据爬取存储速度；对海量数据存储上，MongoDB 内建的分片特性，可以很好的适应大数据量的需求。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-146fd37a7dbce48dd44ca485ffaaf630_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;330&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-146fd37a7dbce48dd44ca485ffaaf630&quot; data-watermark-src=&quot;v2-db7288bb62cc10adb9d3df1ba44def5c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图 2 起初卡思数据架构图 &lt;/p&gt;&lt;p&gt;但是随着业务发展，暴露出一些问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MySQL 在大数据量的场景下，查询性能难以满足要求，并且扩展能力偏弱，如果采用分库分表方式，需要对业务代码进行全面改造，成本非常高。&lt;/li&gt;&lt;li&gt;MongoDB 对复杂事务的不支持，前台业务需要用到数据元及连表查询，当前架构支持的不太友好。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;架构优化&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 需求&lt;/b&gt;&lt;/p&gt;&lt;p&gt;针对我们遇到的问题，我们急需这样一款数据库：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;兼容 MySQL 协议，数据迁移成本和代码改造成本低&lt;/li&gt;&lt;li&gt;插入性能强&lt;/li&gt;&lt;li&gt;大数据量下的实时查询性能强，无需分库分表&lt;/li&gt;&lt;li&gt;水平扩展能力强&lt;/li&gt;&lt;li&gt;稳定性强，产品最好有成熟的案例&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 方案调研&lt;/b&gt;&lt;/p&gt;&lt;p&gt;未选择 TiDB 之前我们调研了几个数据库，Greenplum、HybirdDB for MySQL（PetaData）以及 PolarDB。Greenplum 由于插入性能比较差，并且跟 MySQL 协议有一些不兼容，首先被排除。&lt;/p&gt;&lt;p&gt;HybirdDB for MySQL 是阿里云推出的 HTAP 关系型数据库，我们在试用一段时间发现一些问题：&lt;/p&gt;&lt;p&gt;一是复杂语句导致计算引擎拥堵，阻塞所有业务，经常出现查询超时的情况。&lt;/p&gt;&lt;p&gt;二是连表查询性能低下，网络 I/O 出现瓶颈。举一个常见的关联查询，cd_video 表，2200 万数据，cd_program_video 表，节目和视频的关联表，4700 万数据，在关联字段上都建有索引，如下 SQL：&lt;/p&gt;&lt;p&gt;select &lt;a href=&quot;http://v.id&quot;&gt;v.id&lt;/a&gt;,v.url,v.extra_id,v.title fromcd_video v join cd_program_video pv on &lt;a href=&quot;http://v.id&quot;&gt;v.id&lt;/a&gt; = pv.video_id where program_id =xxx；&lt;/p&gt;&lt;p&gt;当相同查询并发超过一定数量时，就会频繁报数据库计算资源不可用的错误。&lt;/p&gt;&lt;p&gt;三是 DDL 操作比较慢，该字段等操作基本需要几分钟，下发至节点后易出现死锁。&lt;/p&gt;&lt;p&gt;PolarDB 是阿里云新推出新一代关系型数据库，主要思想是计算和存储分离架构，使用共享存储技术。由于写入还是单点写入，插入性能有上限，未来我们的数据采集规模还会进一步提升，这有可能成为一个瓶颈。另外由于只有一个只读实例，在对大表进行并发查询时性能表现一般。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 选择 TiDB&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在经历了痛苦的传统解决方案的折磨以及大量调研及对比后，卡思数据最终选择了 TiDB 作为数据仓库及业务数据库。&lt;/p&gt;&lt;p&gt;TiDB 结合了传统的 RDBMS 和 NoSQL 的最佳特性，高度兼容 MySQL，具备强一致性和高可用性，100% 支持标准的 ACID 事务。由于是 Cloud Native 数据库，可通过并行计算发挥机器性能，在大数量的查询下性能表现良好，并且支持无限的水平扩展，可以很方便的通过加机器解决性能和容量问题。另外提供了非常完善的运维工具，大大减轻数据库的运维工作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;上线 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;卡思数据目前配置了两个 32C64G 的 TiDB、三个 4C16G 的 PD、四个 32C128G 的 TiKV。数据量大约 60 亿条、4TB 左右，每天新增数据量大约 5000 万，单节点 QPS 峰值为 3000 左右。&lt;/p&gt;&lt;p&gt;由于数据迁移不能影响线上业务，卡思数据在保持继续使用原数据架构的前提下，使用 Mydumper、Loader 进行数据迁移，并在首轮数据迁移完成后使用 Syncer 进行增量同步。&lt;/p&gt;&lt;p&gt;卡思数据部署了数据库监控系统（Prometheus/Grafana）来实时监控服务状态，可以非常清晰的查看服务器问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;由于 TiDB 对 MySQL 的高度兼容性，在数据迁移完成后，几乎没有对代码做任何修改，平滑实现了无侵入升级。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前卡思数据的架构如图 3：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fac44545432ecf26d884a7440c333c9f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;794&quot; data-rawheight=&quot;606&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fac44545432ecf26d884a7440c333c9f&quot; data-watermark-src=&quot;v2-75128212ab54ea15dc91b47a7cce596d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图 3 目前卡思数据架构图&lt;/p&gt;&lt;p&gt;查询性能，单表最小 1000 万，最大 8 亿，有比较复杂的连表查询，整体响应延时非常稳定，监控展示如图 4、图 5。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54d64dec60af44a379f55b6ff9b4ab84_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;279&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-54d64dec60af44a379f55b6ff9b4ab84&quot; data-watermark-src=&quot;v2-5c0d79b3b7dfa1bd50565bca3bc4dbe0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图 4 Duration 监控展示图&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0ac9b8d8cd9b6de67cac4a3540531b85_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;866&quot; data-rawheight=&quot;244&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0ac9b8d8cd9b6de67cac4a3540531b85&quot; data-watermark-src=&quot;v2-a700f65ed736c50ffbc6ad1db8695976&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt; 图 5 QPS 监控展示图&lt;/p&gt;&lt;h2&gt;&lt;b&gt;未来展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前的卡思数据已全部迁移至 TiDB，但对 TiDB 的使用还局限在数据存储上，可以说只实现了 OLTP。卡思数据准备深入了解 OLAP，将目前一些需要实时返回的复杂查询、数据分析下推至 TiDB。既减少计算服务的复杂性，又可增加数据的准确性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感谢 PingCAP&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;非常感谢 PingCAP 小伙伴们在数据库上线过程中的大力支持，每次遇到困难都能及时、细心的给予指导，非常的专业和热心。相信 PingCAP 会越来越好，相信 TiDB 会越来越完善，引领 NewSQL 的发展。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者：刘广信，火星文化技术经理&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-11-06-48674946</guid>
<pubDate>Tue, 06 Nov 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>PingCAP University · TiDB DBA 官方培训认证计划正式启动</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-31-48098540.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/48098540&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5ac8300d301ef25f49a6b0a7c71d25c4_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;伴随着产品的成熟，TiDB 在越来越多样化的应用场景中落地。在落地过程中，大家遇到问题会寻求官方的答疑和支持，但由于咨询量很大，我们有时无法及时响应。因此，为了赋能社区，提升社区用户满意度，避免因测试用户过多官方无法及时响应的问题，同时打造活跃的 TiDB 技术社区，培养熟悉分布式系统、能独立运维 TiDB 的一流 NewSQL 人才，我们宣布正式成立 &lt;b&gt;PingCAP University。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PingCAP University 是 PingCAP 官方设立的对企业和个人进行 TiDB 全线产品培训并认证的部门，其培训讲师团队由来自 PingCAP 官方的资深解决方案架构师、顶尖核心技术研发工程师和高级资深 TiDB DBA 组成，拥有丰富且专业的 TiDB 实践经验和培训经验。&lt;/p&gt;&lt;p&gt;&lt;b&gt;PingCAP University 也在今天正式启动 TiDB DBA 官方培训认证计划。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;通过该培训认证计划，大家可以：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;深度理解 TiDB 架构、原理及最佳实践，具备独立部署、运维和调优 TiDB 的能力&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;提升分布式计算和存储领域的技术前沿视野&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;获得来自 PingCAP 官方的专业技术能力认可，提升个人技术竞争力&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-858843cb89a7ec10d45813c33fc89c29_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;577&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-858843cb89a7ec10d45813c33fc89c29&quot; data-watermark-src=&quot;v2-3e1a2a03a88a23033b9da7cb77327d76&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;培训特色&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;理论与实践结合，强调动手能力（实践超过 50%），提供累计 4 个半天实战&lt;/li&gt;&lt;li&gt;课程滚动更新，包含大量前沿技术解读及实践分享&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB DBA 官方培训认证总览&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;初级 TiDB DBA&lt;/b&gt;：PCTA（PingCAP Certified TiDB Associate）培训及认证&lt;/li&gt;&lt;li&gt;&lt;b&gt;高级 TiDB DBA&lt;/b&gt;：PCTP（PingCAP Certified TiDB Professional） 培训及认证&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-092ade55e28a33bfb8fc788fd4eb1575_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1032&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-092ade55e28a33bfb8fc788fd4eb1575&quot; data-watermark-src=&quot;v2-ebfab4252b3e30025bc0343616c46173&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;培训及考试安排&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PCTA：线上培训及考试&lt;/li&gt;&lt;li&gt;PCTP：线下小班集中培训及考试，时间地点由 PingCAP 统一安排&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;垂询及报名方式&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;点击 &lt;a href=&quot;http://pingcaptidb.mikecrm.com/KXCarRw&quot;&gt;这里&lt;/a&gt; 直接填写报名信息&lt;/li&gt;&lt;li&gt;或联系您的客户总监&lt;/li&gt;&lt;li&gt;或发送邮件至 university-cn@pingcap.com 交流&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;P.S. 2018 年 11 月 30 日前报名还有&lt;b&gt;【官方技术支持服务礼包】&lt;/b&gt;赠送～&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-31-48098540</guid>
<pubDate>Wed, 31 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十）Table Partition</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-29-47909702.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47909702&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8606678fbae32d086cdd5e3df15b0beb_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：肖亮亮&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Table Partition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;什么是 Table Partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Table Partition 是指根据一定规则，将数据库中的一张表分解成多个更小的容易管理的部分。从逻辑上看只有一张表，但是底层却是由多个物理分区组成。相信对有关系型数据库使用背景的用户来说可能并不陌生。&lt;/p&gt;&lt;p&gt;TiDB 正在支持分区表这一特性。在 TiDB 中分区表是一个独立的逻辑表，但是底层由多个物理子表组成。物理子表其实就是普通的表，数据按照一定的规则划分到不同的物理子表类内。程序读写的时候操作的还是逻辑表名字，TiDB 服务器自动去操作分区的数据。&lt;br&gt;分区表有什么好处？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;优化器可以使用分区信息做分区裁剪。在语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率。&lt;/li&gt;&lt;li&gt;方便地进行数据生命周期管理。通过创建、删除分区、将过期的数据进行 高效的归档，比使用 Delete 语句删除数据更加优雅，打散写入热点，将一个表的写入分散到多个物理表，使得负载分散开，对于存在 Sequence 类型数据的表来说（比如 Auto Increament ID 或者是 create time 这类的索引）可以显著地提升写入吞吐。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;分区表的限制&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;TiDB 默认一个表最多只能有 1024 个分区 ，默认是不区分表名大小写的。&lt;/li&gt;&lt;li&gt;Range, List, Hash 分区要求分区键必须是 INT 类型，或者通过表达式返回 INT 类型。但 Key 分区的时候，可以使用其他类型的列（BLOB，TEXT 类型除外）作为分区键。&lt;/li&gt;&lt;li&gt;如果分区字段中有主键或者唯一索引的列，那么有主键列和唯一索引的列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列。&lt;/li&gt;&lt;li&gt;TiDB 的分区适用于一个表的所有数据和索引。不能只对表数据分区而不对索引分区，也不能只对索引分区而不对表数据分区，也不能只对表的一部分数据分区。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;常见分区表的类型&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Range 分区：按照分区表达式的范围来划分分区。通常用于对分区键需要按照范围的查询，分区表达式可以为列名或者表达式 ，下面的 employees 表当中 p0, p1, p2, p3 表示 Range 的访问分别是  (min, 1991), [1991, 1996), [1996, 2001), [2001, max) 这样一个范围。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;
CREATE  TABLE employees (
id INT  NOT  NULL,
fname VARCHAR(30),
separated DATE  NOT  NULL
)
    
PARTITION BY RANGE ( YEAR(separated) ) (
PARTITION p0 VALUES LESS THAN (1991),
PARTITION p1 VALUES LESS THAN (1996),
PARTITION p2 VALUES LESS THAN (2001),
PARTITION p3 VALUES LESS THAN MAXVALUE
);&lt;/code&gt;&lt;ul&gt;&lt;li&gt;List 分区：按照 List 中的值分区，主要用于枚举类型，与 Range 分区的区别在于 Range 分区的区间范围值是连续的。&lt;/li&gt;&lt;li&gt;Hash 分区：Hash 分区需要指定分区键和分区个数。通过 Hash 的分区表达式计算得到一个 INT 类型的结果，这个结果再跟分区个数取模得到具体这行数据属于那个分区。通常用于给定分区键的点查询，Hash 分区主要用来分散热点读，确保数据在预先确定个数的分区中尽可能平均分布。&lt;/li&gt;&lt;li&gt;Key 分区：类似 Hash 分区，Hash 分区允许使用用户自定义的表达式，但 Key 分区不允许使用用户自定义的表达式。Hash 仅支持整数分区，而 Key 分区支持除了 Blob 和 Text 的其他类型的列作为分区键。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB Table Partition 的实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本文接下来按照 TiDB 源码的 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/release-2.1&quot;&gt;release-2.1 &lt;/a&gt;分支讲解，部分讲解会在 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;source-code &lt;/a&gt;分支代码，目前只支持 Range 分区所以这里只介绍 Range 类型分区 Table Partition 的源码实现，包括 create table、select 、add partition、insert 、drop partition 这五种语句。&lt;/p&gt;&lt;p&gt;&lt;b&gt;create table&lt;/b&gt;&lt;/p&gt;&lt;p&gt;create table 会重点讲构建 Partition 的这部分，更详细的可以看 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-17/&quot;&gt;TiDB 源码阅读系列文章（十七）DDL 源码解析 &lt;/a&gt;，当用户执行创建分区表的SQL语句，语法解析（Parser）阶段会把 SQL 语句中 Partition 相关信息转换成 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ast/ddl.go&quot;&gt;ast.PartitionOptions &lt;/a&gt;，下文会介绍。接下来会做一系列 Check，分区名在当前的分区表中是否唯一、是否分区 Range 的值保持递增、如果分区键构成为表达式检查表达式里面是否是允许的函数、检查分区键必须是 INT 类型，或者通过表达式返回 INT 类型、检查分区键是否符合一些约束。&lt;/p&gt;&lt;p&gt;解释下分区键，在分区表中用于计算这一行数据属于哪一个分区的列的集合叫做分区键。分区键构成可能是一个字段或多个字段也可以是表达式。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// PartitionOptions specifies the partition options.
type PartitionOptions struct {
Tp          model.PartitionType
Expr        ExprNode
ColumnNames []*ColumnName
Definitions []*PartitionDefinition
}
	​
// PartitionDefinition defines a single partition.
type PartitionDefinition struct {
Name     model.CIStr
LessThan []ExprNode
MaxValue bool
Comment  string
}&lt;/code&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;PartitionOptions&lt;/code&gt; 结构中 Tp 字段表示分区类型， &lt;code class=&quot;inline&quot;&gt;Expr&lt;/code&gt; 字段表示分区键， &lt;code class=&quot;inline&quot;&gt;ColumnNames&lt;/code&gt; 字段表示 Columns 分区，这种类型分区有分为 Range columns 分区和 List columns 分区，这种分区目前先不展开介绍。 &lt;code class=&quot;inline&quot;&gt;PartitionDefinition&lt;/code&gt; 其中 Name 字段表示分区名， &lt;code class=&quot;inline&quot;&gt;LessThan&lt;/code&gt; 表示分区 Range 值， &lt;code class=&quot;inline&quot;&gt;MaxValue&lt;/code&gt; 字段表示 Range 值是否为最大值， &lt;code class=&quot;inline&quot;&gt;Comment&lt;/code&gt; 字段表示分区的描述。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L905&quot;&gt;CreateTable &lt;/a&gt;Partition 部分主要流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;把上文提到语法解析阶段会把 SQL语句中 Partition 相关信息转换成 &lt;code class=&quot;inline&quot;&gt;ast.PartitionOptions&lt;/code&gt; , 然后 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L41&quot;&gt;buildTablePartitionInfo &lt;/a&gt;负责把&lt;code class=&quot;inline&quot;&gt;PartitionOptions&lt;/code&gt; 结构转换 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; ,  即 Partition 的元信息。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L88&quot;&gt;checkPartitionNameUnique &lt;/a&gt;检查分区名是否重复，分表名是不区大小写的。&lt;/li&gt;&lt;li&gt;对于每一分区 Range 值进行 Check， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L469&quot;&gt;checkAddPartitionValue &lt;/a&gt;就是检查新增的 Partition 的 Range 需要比之前所有 Partition 的 Range 都更大。&lt;/li&gt;&lt;li&gt;TiDB 单表最多只能有 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L329&quot;&gt;1024 个分区 &lt;/a&gt;，超过最大分区的限制不会创建成功。&lt;/li&gt;&lt;li&gt;如果分区键构成是一个包含函数的表达式需要检查表达式里面是否是允许的函数 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L107&quot;&gt;checkPartitionFuncValid &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;检查分区键必须是 INT 类型，或者通过表达式返回 INT 类型，同时检查分区键中的字段在表中是否存在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L149&quot;&gt;checkPartitionFuncType &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;如果分区字段中有主键或者唯一索引的列，那么多有主键列和唯一索引列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L348&quot;&gt;checkRangePartitioningKeysConstraints &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;通过以上对 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; 的一系列 check 主要流程就讲完了，需要注意的是我们没有对 &lt;code class=&quot;inline&quot;&gt;PartitionInfo&lt;/code&gt; 的元数据持久化单独存储而是附加在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;Partition 中。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;add partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;add partition 首先需要从 SQL 中解析出来 Partition 的元信息，然后对当前添加的分区会有一些 Check 和限制，主要检查是否是分区表、分区名是已存在、最大分区数限制、是否 Range 值保持递增，最后把 Partition 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L308&quot;&gt;PartitionInfo &lt;/a&gt;追加到 Table 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;中，具体如下:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;检查是否是分区表，若不是分区表则报错提示。&lt;/li&gt;&lt;li&gt;用户的 SQL 语句被解析成将 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ast/ddl.go#L880&quot;&gt;ast.PartitionDefinition &lt;/a&gt;然后 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L2123&quot;&gt;buildPartitionInfo &lt;/a&gt;做的事就是保存表原来已存在的分区信息例如分区类型，分区键，分区具体信息，每个新分区分配一个独立的 PartitionID。&lt;/li&gt;&lt;li&gt;TiDB 默认一个表最多只能有 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L329&quot;&gt;1024 个分区 &lt;/a&gt;，超过最大分区的限制会报错&lt;/li&gt;&lt;li&gt;对于每新增一个分区需要检查 Range 值进行 Check， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L469&quot;&gt;checkAddPartitionValue &lt;/a&gt;简单说就是检查新增的 Partition 的 Range 需要比之前所有 Partition 的 Rrange 都更大。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L88&quot;&gt;checkPartitionNameUnique &lt;/a&gt;检查分区名是否重复，分表名是不区大小写的。&lt;/li&gt;&lt;li&gt;最后把 Partition 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L308&quot;&gt;PartitionInfo &lt;/a&gt;追加到 Table 的元信息 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;.Partition 中，具体实现在这里 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/table.go#L459&quot;&gt;updatePartitionInfo &lt;/a&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;drop partition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;drop partition 和 drop table 类似，只不过需要先找到对应的 Partition ID，然后删除对应的数据，以及修改对应 Table 的 Partition 元信息，两者区别是如果是 drop table 则删除整个表数据和表的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;元信息，如果是 drop partition 则需删除对应分区数据和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/model/model.go#L142&quot;&gt;TableInfo &lt;/a&gt;中的 Partition 元信息，删除分区之前会有一些 Check 具体如下:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只能对分区表做 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/ddl_api.go#L1355&quot;&gt;drop partition 操作 &lt;/a&gt;，若不是分区表则报错提示。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L269&quot;&gt;checkDropTablePartition &lt;/a&gt;检查删除的分区是否存在，TiDB 默认是不能删除所有分区，如果想删除最后一个分区，要用 drop table 代替。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L283&quot;&gt;removePartitionInfo &lt;/a&gt;会把要删除的分区从 Partition 元信息删除掉，删除前会做 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/partition.go#L269&quot;&gt;checkDropTablePartition &lt;/a&gt;的检查。&lt;/li&gt;&lt;li&gt;对分区表数据则需要拿到 PartitionID 根据插入数据时候的编码规则构造出 StartKey 和 EndKey 便能包含对应分区 Range 内所有的数据，然后把这个范围内的数据删除，具体代码实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/ddl/delete_range.go#L250&quot;&gt;这里 &lt;/a&gt;。&lt;/li&gt;&lt;li&gt;编码规则：&lt;br&gt;Key： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID_rowID&lt;/code&gt;&lt;br&gt;startKey： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID&lt;/code&gt;&lt;br&gt;endKey： &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID + 1&lt;/code&gt; &lt;/li&gt;&lt;li&gt;删除了分区，同时也将删除该分区中的所有数据。如果删除了分区导致分区不能覆盖所有值，那么插入数据的时候会报错。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Select 语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Select 语句重点讲 Select Partition 如何查询的和分区裁剪（Partition Pruning），更详细的可以看 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-6/&quot;&gt;TiDB 源码阅读系列文章（六）Select 语句概览 &lt;/a&gt;。&lt;/p&gt;&lt;p&gt;一条 SQL 语句的处理流程，从 Client 接收数据，MySQL 协议解析和转换，SQL 语法解析，逻辑查询计划和物理查询计划执行，到最后返回结果。那么对于分区表是如何查询的表里的数据的，其实最主要的修改是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L39&quot;&gt;逻辑查询计划 &lt;/a&gt;阶段，举个例子：如果用上文中 employees 表作查询, 在 SQL 语句的处理流程前几个阶段没什么不同，但是在逻辑查询计划阶段， &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L46&quot;&gt;rewriteDataSource &lt;/a&gt;将 DataSource 重写了变成 Union All 。每个 Partition id 对应一个 Table Reader。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from employees&lt;/code&gt;&lt;p&gt;等价于：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;select * from (union all
select * from p0 where id &amp;lt; 1991
select * from p1 where id &amp;lt; 1996
select * from p2 where id &amp;lt; 2001
select * from p3 where id &amp;lt; MAXVALUE)&lt;/code&gt;&lt;p&gt;通过观察 &lt;code class=&quot;inline&quot;&gt;EXPLAIN&lt;/code&gt; 的结果可以证实上面的例子，如图 1，最终物理执行计划中有四个 Table Reader 因为 employees 表中有四个分区， &lt;code class=&quot;inline&quot;&gt;Table Reader&lt;/code&gt; 表示在 TiDB 端从 TiKV 端读取， &lt;code class=&quot;inline&quot;&gt;cop task&lt;/code&gt; 是指被下推到 TiKV 端分布式执行的计算任务。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4b7fc3aa7830e5350b4a4ae2b6df58e9_r.jpg&quot; data-caption=&quot;图 1：EXPLAIN 输出&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;379&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4b7fc3aa7830e5350b4a4ae2b6df58e9&quot; data-watermark-src=&quot;v2-f631b41e76235890b1fd41fdaf5157b0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;用户在使用分区表时，往往只需要访问其中部分的分区, 就像程序局部性原理一样，优化器分析 &lt;code class=&quot;inline&quot;&gt;FROM&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;WHERE&lt;/code&gt; 子句来消除不必要的分区，具体还要优化器根据实际的 SQL 语句中所带的条件，避免访问无关分区的优化过程我们称之为分区裁剪（Partition Pruning），具体实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/planner/core/rule_partition_processor.go#L70&quot;&gt;这里 &lt;/a&gt;，分区裁剪是分区表提供的重要优化手段，通过分区的裁剪，避免访问无关数据，可以加速查询速度。当然用户可以刻意利用分区裁剪的特性在 SQL 加入定位分区的条件，优化查询性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Insert 语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-4/&quot;&gt;Insert 语句 &lt;/a&gt;是怎么样写入 Table Partition ?&lt;/p&gt;&lt;p&gt;其实解释这些问题就可以了：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;普通表和分区表怎么区分？&lt;/li&gt;&lt;li&gt;插入数据应该插入哪个 Partition？&lt;/li&gt;&lt;li&gt;每个 Partition 的 RowKey 怎么编码的和普通表的区别是什么？&lt;/li&gt;&lt;li&gt;怎么将数据插入到相应的 Partition 里面?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;普通 Table 和 Table Partition 也是实现了 Table 的接口，load schema 在初始化 Table 数据结构的时候，如果发现 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 里面没有 Partition 信息，则生成一个普通的 &lt;code class=&quot;inline&quot;&gt;tables.Table&lt;/code&gt; ，普通的 Table 跟以前处理逻辑保持不变，如果 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 里面有 Partition 信息，则会生成一个&lt;code class=&quot;inline&quot;&gt;tables.PartitionedTable&lt;/code&gt; ，它们的区别是 RowKey 的编码方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每个分区有一个独立的 Partition ID，Partition ID 和 Table ID 地位平等，每个 Partition 的 Row 和 index 在编码的时候都使用这个 Partition 的 ID。&lt;/li&gt;&lt;li&gt;下面是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/partition.go#L171&quot;&gt;PartitionRecordKey &lt;/a&gt;和普通表 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/tables.go#L261&quot;&gt;RecordKey &lt;/a&gt;区别。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; (1) 分区表按照规则编码成 Key-Value pair：&lt;/p&gt;&lt;p&gt;        Key: &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_partitionID_rowID&lt;/code&gt;&lt;br&gt;        Value: &lt;code class=&quot;inline&quot;&gt;[col1, col2, col3, col4]&lt;/code&gt; &lt;/p&gt;&lt;p&gt;(2) 普通表按照规则编码成 Key-Value pair：&lt;/p&gt;&lt;p&gt;        Key: &lt;code class=&quot;inline&quot;&gt;tablePrefix_rowPrefix_tableID_rowID&lt;/code&gt;&lt;br&gt;        Value: &lt;code class=&quot;inline&quot;&gt;[col1, col2, col3, col4]&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/partition.go#L177&quot;&gt;locatePartition &lt;/a&gt;操作查询到应该插入哪个 Partition，目前支持 RANGE 分区插入到那个分区主要是通过范围来判断，例如在 employees 表中插入下面的 sql，通过计算范围该条记录会插入到 p3 分区中，接着调用对应 Partition 上面的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/release-2.1/table/tables/tables.go#L406&quot;&gt;AddRecord &lt;/a&gt;方法，将数据插入到相应的 Partition 里面。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;INSERT INTO employees VALUES (1, &#39;PingCAP TiDB&#39;, &#39;2003-10-15&#39;),&lt;/code&gt;&lt;ul&gt;&lt;li&gt;插入数据时，如果某行数据不属于任何 Partition，则该事务失败，所有操作回滚。如果 Partition 的 Key 算出来是一个 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; ，对于不同的 Partition 类型有不同的处理方式：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;对于 Range Partition：该行数据被插入到最小的那个 Partition&lt;/li&gt;&lt;li&gt;对于 List partition：如果某个 Partition 的 Value List 中有 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; ，该行数据被插入那个 Partition，否则插入失败&lt;/li&gt;&lt;li&gt;对于 Hash 和 Key Partition： &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值视为 0，计算 Partition ID 将数据插入到对应的 Partition&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;在 TiDB 分区表中分区字段插入的值不能大于表中 Range 值最大的上界，否则会报错&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;End&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 目前支持 Range 分区类型，具体以及更细节的可以看 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;这里 &lt;/a&gt;。剩余其它类型的分区类型正在开发中，后面陆续会和大家见面，敬请期待。它们的源码实现读者届时可以自行阅读，流程和文中上述描述类似。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-29-47909702</guid>
<pubDate>Mon, 29 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>我们要做点更酷的事情，来场 Hackathon 怎么样？</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-25-47610023.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47610023&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7370ffeb0809ae9bdc74ea11bbe3c1b1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;i&gt;1024 程序员节，&lt;/i&gt;&lt;br&gt;&lt;i&gt;总把我们标签化为格子衬衫和后退的发际线（很烦啊。。）&lt;/i&gt;&lt;br&gt;&lt;i&gt;发张堆满编程语言名称的图片表示尊重（好吧。。）&lt;/i&gt;&lt;br&gt;&lt;i&gt;我们自己的节日，我们有自己的更酷的玩儿法——&lt;/i&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;报名一场 Hackathon，怎么样？&lt;/i&gt;&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Hackathon2018&lt;/b&gt; &lt;b&gt;将于 12 月 1 - 2 日在 PingCAP 北京总部举办。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本场 Hackathon &lt;b&gt;主题为 TiDB Ecosystem&lt;/b&gt;，参赛团队将在两天一夜的时间里完成一个作品，当然我们也欢迎个人开发者独立报名参赛～ 我司 Co-Founder、首席架构师、技术 VP 等 &lt;b&gt;「PingCAP 大神」将担任导师团，手把手带练，&lt;/b&gt;更邀请了&lt;b&gt;业内大牛评审团&lt;/b&gt; 对每个小组的作品进行打分，获胜队伍将得到&lt;b&gt;丰厚现金奖励和面试直通车&lt;/b&gt;的机会。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;奖项设置&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一等奖（1 支队伍）：   ¥ 60,000 现金奖励 &lt;/p&gt;&lt;p&gt;二等奖（2 支队伍）：每队 ¥ 30,000 现金奖励&lt;/p&gt;&lt;p&gt;三等奖（3 支队伍）：每队 ¥ 10,000 现金奖励&lt;/p&gt;&lt;p&gt;对于现场表现突出的小伙伴，我们也设置了最佳创意奖和最佳贡献奖哦～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;选题方向&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;参赛作品需围绕 TiDB 及其周边生态实现，选题不限。我们有以下选题方向供小伙伴们参考：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 数据可视化，基于异地多机房部署全局视图等&lt;/li&gt;&lt;li&gt;执行计划可视化（explain 的输出生成图，对于 expensive 的步骤在图中高亮，统计信息的图形化展示，比如 histogram）&lt;/li&gt;&lt;li&gt;TiKV 潜在瓶颈分析，cost 量化分析工具（可以结合 wPerf 论文，应用到我们的项目中，发现潜在的环以及锁依赖之类的。甚至对于每个 query 的 cost 进行量化，比如导致的每个线程 cpu circle 消耗、context switch 次数，cache miss 情况等等）&lt;/li&gt;&lt;li&gt;玩转 TiKV，集成其它一些引擎接口，如 Redis&lt;/li&gt;&lt;li&gt;……&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你有更多脑洞大开的选题，想为 TiDB 做一些更好玩儿的周边工具，可以在报名之后与 &lt;b&gt;TiDB Robot（微信号：tidbai）&lt;/b&gt;沟通交流，小伙伴们也可以自由组队报名（每支队伍不超过 3 人），我们会进行审核～另外，暂时没有队伍和选题意向的同学可以在报名后与 TiDB Robot 联系组队。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参考资料&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;源码地址：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/&quot;&gt;https://github.com/pingcap/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tikv/&quot;&gt;https://github.com/tikv/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;官方文档&lt;/b&gt;：&lt;a href=&quot;https://pingcap.com/docs-cn/&quot;&gt;https://pingcap.com/docs-cn/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;技术博客&lt;/b&gt;：&lt;a href=&quot;https://pingcap.com/blog-cn/&quot;&gt;https://pingcap.com/blog-cn/&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参赛 Tips&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;比赛时间&lt;/b&gt;：12 月 1 日 10:00  - 12 月 2 日 18:00&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名时间&lt;/b&gt;：即日起至 11 月 23 日 17:00&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名审核&lt;/b&gt;：5 个工作日内反馈审核结果&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名链接&lt;/b&gt;：点击 &lt;a href=&quot;http://nc9hsk15y2xczuor.mikecrm.com/3AarNns&quot;&gt;这里&lt;/a&gt; 报名&lt;br&gt;&lt;br&gt;* 本次大赛诚招志愿者参与活动现场支持。如果你想近距离接触技术大咖，体验大赛氛围，那就联系 TiDB Robot（微信号：tidbai）报名吧～志愿者也可以获得活动定制纪念品哦！&lt;br&gt;* 详细比赛日程请关注近期官方微信公号（ID: PingCAP）推送&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/0zhMVPLEUq8trbY3923B (二维码自动识别)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-25-47610023</guid>
<pubDate>Thu, 25 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>一致性模型</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-23-47445841.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47445841&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c8b584a8186554d98b4bb5442eeeafcd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：唐刘（siddontang）&lt;/p&gt;&lt;p&gt;有时候，在跟一些同学讨论 TiKV 事务模型的时候，我都提到了 Linearizability，也提到了 Snapshot Isolation，以及需要手动 lock 来保证 Serializable Snapshot Isolation，很多时候，当我嘴里面蹦出来这些名词的时候，有些同学就一脸懵逼了。所以我觉得有必要仔细来解释一下，顺带让我自己将所有的 isolation 以及 consistency 这些情况都归纳总结一遍，让自己也理解透彻一点。&lt;/p&gt;&lt;p&gt;幸运的是，业内已经有很多人做了这个事情，譬如在 &lt;a href=&quot;http://www.vldb.org/pvldb/vol7/p181-bailis.pdf&quot;&gt;Highly Available Transactions: Virtues and Limitations&lt;/a&gt; 这篇论文里面，作者就总结了不同模型是否能满足 Highly Available Transactions(HATs)。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-38087fea60b99e9c2e34a151c1cd6a08_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;258&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-38087fea60b99e9c2e34a151c1cd6a08&quot; data-watermark-src=&quot;v2-7ee2fd316c00de39dcf3b26620cce6a0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;图中，红色圆圈里面的模型属于 Unavailable，蓝色的属于 Sticky Available，其余的就是  Highly Available。这里解释下相关的含义：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Unavailable：当出现网络隔离等问题的时候，为了保证数据的一致性，不提供服务。熟悉 CAP 理论的同学应该清楚，这就是典型的 CP 系统了。&lt;/li&gt;&lt;li&gt;Sticky Available：即使一些节点出现问题，在一些还没出现故障的节点，仍然保证可用，但需要保证 client 的操作是一致的。&lt;/li&gt;&lt;li&gt;Highly Available：就是网络全挂掉，在没有出现问题的节点上面，仍然可用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unavailable 比较容易理解，这里在讨论下 Sticky 和 Highly，对于 Highly Available 来说，如果一个 server 挂掉了，client 可以去连接任意的其他 server，如果这时候仍然能获取到结果，那么就是 Highly Available 的。但对于 Sticky 来说，还需要保证 client 操作的一致性，譬如 client 现在 server 1 上面进行了很多操作，这时候 server 1 挂掉了，client 切换到 server 2，但在 server 2 上面看不到 client 之前的操作结果，那么这个系统就不是 Sticky 的。所有能在 Highly Available 系统上面保证的事情一定也能在 Sticky Available 系统上面保证，但反过来就不一定了。&lt;/p&gt;&lt;p&gt;Jepsen 在&lt;a href=&quot;https://jepsen.io/consistency&quot;&gt;官网&lt;/a&gt;上面有一个简化但更好看一点的图&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bb3847f8b9e69b1154cfdf6591af0202_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;438&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bb3847f8b9e69b1154cfdf6591af0202&quot; data-watermark-src=&quot;v2-706a307a3c684b7713aa678335b0baa9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;下面，我会按照 Jepsen 里面的图，对不同的 model 进行解释一下。至于为啥选择 Jepsen 里面的例子，一个是因为 Jepsen 现在是一款主流的测试不同分布式系统一致性的工具，它的测试用例就是测试的是上图提到的模型，我们自然也会关心这些模型。另外一个就是这个模型已经覆盖了大多数场景了，理解了这些，大部分都能游刃有余处理了。&lt;/p&gt;&lt;p&gt;如果大家仔细观察，可以发现，从根节点 Strict Serializable，其实是有两个分支的，一个对应的就是数据库里面的 Isolation（ACID 里面的 I），另一个其实对应的是分布式系统的 Consistency（CAP 里面的 C），在 HATs 里面，叫做 Session Guarantees。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Isolation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;要对 Isolation 有个快速的理解，其实只需要看 &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf&quot;&gt;A Critique of ANSI SQL Isolation Levels&lt;/a&gt; 这篇论文就足够了，里面详细的介绍了数据库实现中遇到的各种各样的 isolation 问题，以及不同的 isolation level 到底能不能解决。&lt;/p&gt;&lt;p&gt;在论文里面，作者详细的列举了多种异常现象，这里大概介绍一下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P0 - Dirty Write&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Dirty Write 就是一个事务，覆盖了另一个之前还未提交事务写入的值。假设现在我们有两个事务，一个事务写入 x = y = 1，而另一个事务写入 x = y = 2，那么最终的结果，我们是希望看到 x 和 y 要不全等于 1，要不全等于 2。但在 Dirty Write 情况下面，可能会出现如下情况：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f9420db92296f248e521212a86bd33d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;310&quot; data-rawheight=&quot;177&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f9420db92296f248e521212a86bd33d1&quot; data-watermark-src=&quot;v2-55a99d7a4bcd04bb2d166e9ac33009a3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;可以看到，最终的值是 x = 2 而 y = 1，已经破坏了数据的一致性了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P1 - Dirty Read&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Dirty Read 出现在一个事务读取到了另一个还未提交事务的修改数据。假设现在我们有一个两个账户，x 和 y，各自有 50 块钱，x 需要给 y 转 40 元钱，那么无论怎样，x + y = 100 这个约束是不能打破的，但在 Dirty Read 下面，可能出现：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a2b718ff1efaf2a194275bfb7ff1bced_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;343&quot; data-rawheight=&quot;172&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a2b718ff1efaf2a194275bfb7ff1bced&quot; data-watermark-src=&quot;v2-e6146413f66d4fe5679ec476ea9633be&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在事务 T2，读取到的 x + y = 60，已经打破了约束条件了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P2 - Fuzzy Read&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Fuzzy Read 也叫做 Non-Repeatable Read，也就是一个还在执行的事务读取到了另一个事务的更新操作，仍然是上面的转账例子：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d827daeddda6b1707d9d721f95a9f1db_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;341&quot; data-rawheight=&quot;166&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d827daeddda6b1707d9d721f95a9f1db&quot; data-watermark-src=&quot;v2-909c1d04a9fe89a09272b083bf22b579&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在 T1 还在运行的过程中，T2 已经完成了转账，但 T1 这时候能读到最新的值，也就是 x + y = 140 了，破坏了约束条件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P3 - Phantom&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Phantom 通常发生在一个事务首先进行了一次按照某个条件的 read 操作，譬如 SQL 里面的 &lt;code class=&quot;inline&quot;&gt;SELECT WHERE P&lt;/code&gt;，然后在这个事务还没结束的时候，另外的事务写入了一个新的满足这个条件的数据，这时候这个新写入的数据就是 Phantom 的了。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2f8f49d023908e37d376c59fe70b83fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;555&quot; data-rawheight=&quot;163&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2f8f49d023908e37d376c59fe70b83fe&quot; data-watermark-src=&quot;v2-c68b450aacc22072b667197b5fca85ab&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;假设现在 T1 按照某个条件读取到了所有雇员 a，b，c，这时候 count 是 3，然后 T2 插入了一个新的雇员 d，同时更新了 count 为 4，但这时候 T1 在读取 count 的时候会得到 4，已经跟之前读取到的 a，b，c 冲突了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P4 - Lost Update&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们有时候也会遇到一种 Lost Update 的问题，如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0ec0ae1e2ab8faf39d3a2ac241805d5a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;275&quot; data-rawheight=&quot;132&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0ec0ae1e2ab8faf39d3a2ac241805d5a&quot; data-watermark-src=&quot;v2-22e2ce89eb33e3b91d492ffa7559d430&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在上面的例子中，我们没有任何 dirty write，因为 T2 在 T1 更新之前已经提交成功，也没有任何 dirty read，因为我们在 write 之后没有任何 read 操作，但是，当整个事务结束之后，T2 的更新其实丢失了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;P4C - Cursor Lost Update&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Cursor Lost Update 是上面 Lost Update 的一个变种，跟 SQL 的 cursor 相关。在下面的例子中，RC(x) 表明在 cursor 下面 read x，而 WC(x) 则表明在 cursor 下面写入 x。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41b6ac72eef6631f1a1b519b1bae22ec_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;317&quot; data-rawheight=&quot;134&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-41b6ac72eef6631f1a1b519b1bae22ec&quot; data-watermark-src=&quot;v2-f043051e2c5dcb038d9ca5dac74724c2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果我们允许 T2 在  T1 RC 和 WC 之间写入数据，那么 T2 的更新也会丢失。&lt;/p&gt;&lt;p&gt;&lt;b&gt;A5A - Read Skew&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Read Skew 发生在两个或者多个有完整性约束的数据上面，还是传统的转账例子，需要保证 x + y = 100，那么 T1 就会看到不一致的数据了。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bee88aafb7485a0ee2b236dfbf1fc7fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;342&quot; data-rawheight=&quot;168&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bee88aafb7485a0ee2b236dfbf1fc7fe&quot; data-watermark-src=&quot;v2-1d53742a9442a787055a5570a211b7d7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;A5B - Write Skew&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Write Skew 跟 Read Skew 比较类似，假设 x + y &amp;lt;= 100，T1 和 T2 在执行的时候都发现满足约束，然后 T1 更新了 y，而 T2 更新了 x，然后最终结果打破了约束，如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27784693015ffe46fde0048b629336fc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;339&quot; data-rawheight=&quot;163&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27784693015ffe46fde0048b629336fc&quot; data-watermark-src=&quot;v2-cbbd6978ff401beb981159d476f26e1b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;Isolation Levels&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面我们介绍了不同的异常情况，下面的表格说明了，在不同的隔离级别下面，那些异常情况可能发生：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98b3731ffae0c53321617da70017601a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;630&quot; data-rawheight=&quot;271&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-98b3731ffae0c53321617da70017601a&quot; data-watermark-src=&quot;v2-e879cb84c711e5d3f411752013581b54&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;NP - Not Possible，在该隔离级别下面不可能发生&lt;/li&gt;&lt;li&gt;SP - Sometimes Possible，在该隔离级别下面有时候可能发生&lt;/li&gt;&lt;li&gt;P - Possible，在该隔离级别下面会发生&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;鉴于网上已经对不同的 Isolation Level，尤其是 MySQL 的解释的太多了，这里就简单的解释一下。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Read Uncommitted - 能读到另外事务未提交的修改。&lt;/li&gt;&lt;li&gt;Read Committed - 能读到另外事务已经提交的修改。&lt;/li&gt;&lt;li&gt;Cursor Stability - 使用 cursor 在事务里面引用特定的数据，当一个事务用 cursor 来读取某个数据的时候，这个数据不可能被其他事务更改，除非 cursor 被释放，或者事务提交。&lt;/li&gt;&lt;li&gt;Monotonic Atomic View - 这个级别是 read committed 的增强，提供了一个原子性的约束，当一个在 T1 里面的 write 被另外事务 T2 观察到的时候，T1 里面所有的修改都会被 T2 给观察到。&lt;/li&gt;&lt;li&gt;Repeatable Read - 可重复读，也就是对于某一个数据，即使另外的事务有修改，也会读取到一样的值。&lt;/li&gt;&lt;li&gt;Snapshot - 每个事务都会在各自独立，一致的 snapshot 上面对数据库进行操作。所有修改只有在提交的时候才会对外可见。如果 T1 修改了某个数据，在提交之前另外的事务 T2 修改并提交了，那么 T1 会回滚。&lt;/li&gt;&lt;li&gt;Serializable - 事务按照一定顺序执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另外需要注意，上面提到的 isolation level 都不保证实时约束，如果一个进程 A 完成了一次写入 w，然后另外的进程 B 开始了一次读取 r，r 并不能保证观察到 w 的结果。另外，在不同事务之间，这些 isolation level 也不保证不同进程的顺序。一个进程可能在一次事务里面看到一次写入 w，但可能在后面的事务上面没看到同样的 w。事实上，一个进程甚至可能看不到在这个进程上面之前的写入，如果这些写入都是发生在不同的事务里面。有时候，他们还可能会对事务进行排序，譬如将 write-only 的事务放到所有的 read 事务的后面。&lt;/p&gt;&lt;p&gt;要解决这些问题，我们需要引入顺序约束，这也就是下面 Session Guarantee 要干的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Session Guarantee&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 HATs 论文里面，相关的概念叫做 Session Guarantee，主要是用来保证在一个 session 里面的实时性约束以及客户端的操作顺序。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Writes Follow Reads&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果某个进程读到了一次写入 w1 写入的值 v，然后进行了一次新的写入 w2，那么 w2 写入的值将会在 w1 之后可见。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Monotonic Reads&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程开始了一次读取 r1，然后在开始另一次读取 r2，那么 r2 不可能看到 r1 之前数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Monotonic Writes&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程先进行了一次写入 w1，然后在进行了一次写入 w2，那么所有其他的进程都会观察到 w1 在 w2 之前发生。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Read Your Writes&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个进程先进行了一次写入 w，然后后面执行了一次读取 r，那么 r 一定会看到 w 的改动。&lt;/p&gt;&lt;p&gt;&lt;b&gt;PRAM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PRAM 就是 Pipeline Random Access Memory，对于单个进程的写操作都被观察到是顺序的，但不同的进程写会观察到不同的顺序。譬如下面这个操作是满足 PRAM 的，但不满足后面说的 Causal。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ff2541b159c86357affa7d99ea5ada82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;310&quot; data-rawheight=&quot;165&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ff2541b159c86357affa7d99ea5ada82&quot; data-watermark-src=&quot;v2-803e3938c3a11c087360711b85e30659&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;Causal&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Causal 确定了有因果关系的操作在所有进程间的一致顺序。譬如下面这个：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-83163b55feb11508651baa8e834f7369_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;162&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-83163b55feb11508651baa8e834f7369&quot; data-watermark-src=&quot;v2-dc8e7b39dd320e689f93f8656748865c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 和 P4 来说，无论是先读到 2，还是先读到 1， 都是没问题的，因为 P1 和 P2 里面的 write 操作并没有因果性，是并行的。但是下面这个：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6b1daeebfd990cecb82687b46d80418c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;360&quot; data-rawheight=&quot;165&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6b1daeebfd990cecb82687b46d80418c&quot; data-watermark-src=&quot;v2-a579e40eaa5a6c55bbc68c02a3a2c596&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;就不满足 Cansal 的一致性要求了，因为对于 P2 来说，在 Write 2 之前，进行了一次 Read 1 的操作，已经确定了 Write 1 会在 Write 2 之前发生，也就是确定了因果关系，所以 P3 打破了这个关系。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Sequential 会保证操作按照一定顺序发生，并且这个顺序会在不同的进程上面都是一致的。一个进程会比另外的进程超前，或者落后，譬如这个进程可能读到了已经是陈旧的数据，但是，如果一个进程 A 从进程 B 读到了某个状态，那么它就不可能在读到 B 之前的状态了。&lt;/p&gt;&lt;p&gt;譬如下面的操作就是满足 Sequential 的：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e254f620c178957e286dedc8b39521c6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;357&quot; data-rawheight=&quot;161&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e254f620c178957e286dedc8b39521c6&quot; data-watermark-src=&quot;v2-a54247f317d5539b43f6f1d43bfe7bc7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 来说，它仍然能读到之前的 stale 状态 1。但下面的就不对了：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0894ed397fa0d0bcbfc5e8ad9a47dfbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;164&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0894ed397fa0d0bcbfc5e8ad9a47dfbf&quot; data-watermark-src=&quot;v2-443516e6968a62e0a43cf08e7d87e6a4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 来说，它已经读到了最新的状态 2，就不可能在读到之前的状态 1 了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Linearizable&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Linearizability 要求所有的操作都是按照一定的顺序原子的发生，而这个顺序可以认为就是跟操作发生的时间一致的。也就是说，如果一个操作 A 在 B 开始之前就结束了，那么 B 只可能在 A 之后才能产生作用。&lt;/p&gt;&lt;p&gt;譬如下面的操作：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3f66bc3e9f6504131cf24a2a940cd94f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;302&quot; data-rawheight=&quot;161&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3f66bc3e9f6504131cf24a2a940cd94f&quot; data-watermark-src=&quot;v2-f876bd93b74fe1b9d004ffd3a171c148&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于 P3 和 P4 来说，因为之前已经有新的写入，所以他们只能读到 2，不可能读到 1。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strict Serializable&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;终于来到了 Strict Serializable，大家可以看到，它结合了 serializable 以及 linearizable，也就是说，它会让所有操作按照实时的顺序依次操作，也就是所有的进程会观察到完全一致的顺序，这也是最强的一致性模型了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;好了，最后再来聊聊 &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt;，TiKV 是一个支持分布式事务的 key-value database。对于某个事务，TiKV 会通过 PD 这个服务在事务开始的时候分配一个 start timestamp，以及事务提交的时候分配一个 commit timestamp。因为我们的授时是通过 PD 这个单点服务进行的，所以时间是一定能保证单调递增的，也就是说，我们所有的操作都能跟保证实时有序，也就是满足 Linearizable。&lt;/p&gt;&lt;p&gt;TiKV 采用的是常用的 MVCC 模型，也就是每个 key-value 实际存储的时候，会在 key 上面带一个 timestamp，我们就可以用 timestamp 来生成整个数据库的 snapshot 了，所以 TiKV 是 snapshot isolation 的。既然是 snapshot isolation，那么就会遇到 write skew 问题，所以 TiKV 额外提供了 serializable snapshot isolation，用户需要显示的对要操作的数据进行 lock 操作。&lt;/p&gt;&lt;p&gt;但现在 TiKV 并不支持对 range 加 lock，所以不能完全的防止 phantom，譬如假设最多允许 8 个任务，现在已经有 7 个任务了，我们还可以添加一个任务，但这时候另外一个事务也做了同样的事情，但添加的是不同的任务，这时候就会变成 9 个任务，另外的事务在 scan 的时候就会发现打破了约束。这个也就是 A Critique of ANSI SQL Isolation Levels 里面提到的 sometimes possible。&lt;/p&gt;&lt;p&gt;所以，TiKV 是 snapshot isolation + linearizable。虽然 TiKV 也可以支持 Read Committed，但通常不建议在生产环境中使用，因为 TiKV 的 Read Committed 跟传统的还不太一样，可能会出现能读到一个事务提交到某个节点的数据，但这时候在另外的节点还读不到这个事务提交的数据，毕竟在分布式系统下面，不同节点的事务提交也是有网络延迟的，不可能同时执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;小结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在分布式系统里面，一致性是非常重要的一个概念，理解了它，在自己设计分布式系统的时候，就能充分的考虑到底系统应该提供怎样的一致性模型。譬如对于 TP 数据库来说，就需要有一个比较 strong 的一致性模型，而对于一些不重要的系统，譬如 cache 这些，就可以使用一些比较 weak 的模型。对 TiKV 来说，我们在 Percolator 基础上面，也一直在致力于分布式事务的优化，如果你对这方面感兴趣，欢迎联系我 tl@pingcap.com。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;原文链接：&lt;a href=&quot;https://www.jianshu.com/p/3673e612cce2&quot;&gt;一致性模型&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;延展阅读 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486947&amp;amp;idx=1&amp;amp;sn=28f3fe47d380e0ee991c207251fdd415&amp;amp;chksm=eb162a89dc61a39f47bb4d3f90be9789f4b7f0a23ada7979ee65d21efcb43edd9f5e8254ff2b&amp;amp;scene=21#wechat_redirect&quot;&gt;线性一致性和 Raft&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486842&amp;amp;idx=1&amp;amp;sn=2e21e65010f497693f26cfc344e418fe&amp;amp;chksm=eb162a10dc61a30650269d414de2cfe4eeff08e0d5e9b50834c3353c70850c83b796fd2be364&amp;amp;scene=21#wechat_redirect&quot;&gt;TiKV 是如何存取数据的（上）&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486872&amp;amp;idx=1&amp;amp;sn=1f8d7e88cd92878142a444c2aea8e764&amp;amp;chksm=eb162af2dc61a3e4a6675f86f91e8bd97886b6fd8006662df2f8e58f5190514a192a259beb25&amp;amp;scene=21#wechat_redirect&quot;&gt;TiKV 是如何存储数据的（下）&lt;/a&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-23-47445841</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>线性一致性和 Raft</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-22-47117804.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47117804&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-85e813810e9221590c5a48b11d906dc2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：沈泰宁&lt;/p&gt;&lt;p&gt;在讨论分布式系统时，共识算法（Consensus algorithm）和一致性（Consistency）通常是讨论热点，两者的联系很微妙，很容易搞混。一些常见的误解：使用了 Raft [0] 或者 paxos 的系统都是线性一致的（Linearizability [1]，即强一致），其实不然，共识算法只能提供基础，要实现线性一致还需要在算法之上做出更多的努力。以 TiKV 为例，它的共识算法是 Raft，在 Raft 的保证下，TiKV 提供了满足线性一致性的服务。&lt;/p&gt;&lt;p&gt;本篇文章会讨论一下线性一致性和 Raft，以及 TiKV 针对前者的一些优化。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;线性一致性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;什么是一致性，简单的来说就是评判一个并发系统正确与否的标准。线性一致性是其中一种，CAP [2] 中的 C 一般就指它。什么是线性一致性，或者说怎样才能达到线性一致？在回答这个问题之前先了解一些背景知识。&lt;/p&gt;&lt;p&gt;&lt;b&gt;背景知识&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了回答上面的问题，我们需要一种表示方法描述分布式系统的行为。分布式系统可以抽象成几个部分:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Client&lt;/li&gt;&lt;li&gt;Server&lt;/li&gt;&lt;li&gt;Events&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Invocation&lt;/li&gt;&lt;li&gt;Response&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Operations&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Read&lt;/li&gt;&lt;li&gt;Write&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;一个分布式系统通常有两种角色，Client 和 Server。Client 通过发起请求来获取 Server 的服务。一次完整请求由两个事件组成，Invocation（以下简称 Inv）和 Response（以下简称 Resp）。一个请求中包含一个 Operation，有两种类型 Read 和 Write，最终会在 Server 上执行。&lt;/p&gt;&lt;p&gt;说了一堆不明所以的概念，现在来看如何用这些表示分布式系统的行为。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c6efe0b1804c98ae04cd2ba866aea6f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;232&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c6efe0b1804c98ae04cd2ba866aea6f9&quot; data-watermark-src=&quot;v2-2dbf1014330a7675f4198926546c6826&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图展示了 Client A 的一个请求从发起到结束的过程。变量 x 的初始值是 1，“x R() A” 是一个事件 Inv 意思是 A 发起了读请求，相应的 “x OK(1) A” 就是事件 Resp，意思是 A 读到了 x 且值为 1，Server 执行读操作（Operation）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;如何达到线性一致&lt;/b&gt;&lt;/p&gt;&lt;p&gt;背景知识介绍完了，怎样才能达到线性一致？这就要求 Server 在执行 Operations 时需要满足以下三点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;瞬间完成（或者原子性）&lt;/li&gt;&lt;li&gt;发生在 Inv 和 Resp 两个事件之间&lt;/li&gt;&lt;li&gt;反映出“最新”的值&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面我举一个例子，用以解释上面三点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;例：&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6465b3ad4bd77c4926e2786c020b5c46_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;799&quot; data-rawheight=&quot;440&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6465b3ad4bd77c4926e2786c020b5c46&quot; data-watermark-src=&quot;v2-83bbf021252b353fec99845780381d71&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;先下结论，上图表示的行为满足线性一致。&lt;/p&gt;&lt;p&gt;对于同一个对象 x，其初始值为 1，客户端 ABCD 并发地进行了请求，按照真实时间（real-time）顺序，各个事件的发生顺序如上图所示。对于任意一次请求都需要一段时间才能完成，例如 A，“x R() A” 到 “x Ok(1) A” 之间的那条线段就代表那次请求花费的时间，而请求中的读操作在 Server 上的执行时间是很短的，相对于整个请求可以认为瞬间，读操作表示为点，并且在该线段上。线性一致性中没有规定读操作发生的时刻，也就说该点可以在线段上的任意位置，可以在中点，也可以在最后，当然在最开始也无妨。&lt;/p&gt;&lt;p&gt;第一点和第二点解释的差不多了，下面说第三点。&lt;/p&gt;&lt;p&gt;反映出“最新”的值？我觉得三点中最难理解就是它了。先不急于对“最新”下定义，来看看上图中 x 所有可能的值，显然只有 1 和 2。四个次请求中只有 B 进行了写请求，改变了 x 的值，我们从 B 着手分析，明确 x 在各个时刻的值。由于不能确定 B 的 W（写操作）在哪个时刻发生，能确定的只有一个区间，因此可以引入&lt;b&gt;上下限&lt;/b&gt;的概念。对于 x=1，它的上下限为&lt;b&gt;开始到事件“x W(2) B”&lt;/b&gt;，在这个范围内所有的读操作必定读到 1。对于 x=2，它的上下限为 &lt;b&gt;事件“x Ok() B”&lt;/b&gt; 到结束，在这个范围内所有的读操作必定读到 2。那么“x W(2) B”到“x Ok() B”这段范围，x 的值是什么？&lt;b&gt;1 或者 2&lt;/b&gt;。由此可以将 x 分为三个阶段，各阶段”最新”的值如下图所示:&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-14f1c87da0956711e7d1325446ea2619_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;996&quot; data-rawheight=&quot;614&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-14f1c87da0956711e7d1325446ea2619&quot; data-watermark-src=&quot;v2-196f91a809357196ccf0e3ef5881865f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;清楚了 x 的变化后理解例子中 A C D 的读到结果就很容易了。&lt;/p&gt;&lt;p&gt;最后返回的 D 读到了 1，看起来是 “stale read”，其实并不是，它仍满足线性一致性。D 请求横跨了三个阶段，而读可能发生在任意时刻，所以 1 或 2 都行。同理，A 读到的值也可以是 2。C 就不太一样了，C 只有读到了 2 才能满足线性一致。因为 “x R() C” 发生在 “x Ok() B” 之后（happen before [3]），可以推出 R 发生在 W 之后，那么 R 一定得读到 W 完成之后的结果：2。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话概括：在分布式系统上实现寄存器语义。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实现线性一致&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如开头所说，一个分布式系统正确实现了共识算法并不意味着能线性一致。共识算法只能保证多个节点对某个对象的状态是一致的，以 Raft 为例，它只能保证不同节点对 Raft Log（以下简称 Log）能达成一致。那么 Log 后面的状态机（state machine）的一致性呢？并没有做详细规定，用户可以自由实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raft&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Raft 是一个强 Leader 的共识算法，只有 Leader 能处理客户端的请求，集群的数据（Log）的流向是从 Leader 流向 Follower。其他的细节在这就不赘述了，网上有很多资料 [4]。&lt;/p&gt;&lt;p&gt;In Practice&lt;/p&gt;&lt;p&gt;以 TiKV 为例，TiKV 内部可分成多个模块，Raft 模块，RocksDB 模块，两者通过 Log 进行交互，整体架构如下图所示，consensus 就是 Raft 模块，state machine 就是 RocksDB 模块。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-18b6d578da3ad19604d89bfc0dbe7641_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;627&quot; data-rawheight=&quot;389&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-18b6d578da3ad19604d89bfc0dbe7641&quot; data-watermark-src=&quot;v2-fcadf6608de8a0d1478f67e3838f0943&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Client 将请求发送到 Leader 后，Leader 将请求作为一个 Proposal 通过 Raft 复制到自身以及 Follower 的 Log 中，然后将其 commit。TiKV 将 commit 的 Log 应用到 RocksDB 上，由于 Input（即 Log）都一样，可推出各个 TiKV 的状态机（即 RocksDB）的状态能达成一致。但实际多个 TiKV 不能保证同时将某一个 Log 应用到 RocksDB 上，也就是说各个节点不能&lt;b&gt;实时&lt;/b&gt;一致，加之 Leader 会在不同节点之间切换，所以 Leader 的状态机也不总有最新的状态。Leader 处理请求时稍有不慎，没有在最新的状态上进行，这会导致整个系统违反线性一致性。&lt;b&gt;好在有一个很简单的解决方法：依次应用 Log，将应用后的结果返回给 Client。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这方法不仅简单还通用，读写请求都可以这样实现。这个方法依据 commit index 对所有请求都做了排序，使得每个请求都能反映出状态机在执行完前一请求后的状态，可以认为 commit 决定了 R/W 事件发生的顺序。Log 是严格全序的（total order），那么自然所有 R/W 也是全序的，将这些 R/W 操作一个一个应用到状态机，所得的结果必定符合线性一致性。这个方法的缺点很明显，性能差，因为所有请求在 Log 那边就被序列化了，无法并发的操作状态机。&lt;br&gt;这样的读简称 LogRead。由于读请求不改变状态机，这个实现就显得有些“重“，不仅有 RPC 开销，还有写 Log 开销。优化的方法大致有两种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;ReadIndex&lt;/li&gt;&lt;li&gt;LeaseRead&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;ReadIndex&lt;/b&gt;&lt;/p&gt;&lt;p&gt;相比于 LogRead，ReadIndex 跳过了 Log，节省了磁盘开销，它能大幅提升读的吞吐，减小延时（但不显著）。Leader 执行 ReadIndex 大致的流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;记录当前的 commit index，称为 ReadIndex&lt;/li&gt;&lt;li&gt;向 Follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是 Leader&lt;/li&gt;&lt;li&gt;等待状态机&lt;b&gt;至少&lt;/b&gt;应用到 ReadIndex 记录的 Log&lt;/li&gt;&lt;li&gt;执行读请求，将结果返回给 Client&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;第 3 点中的“至少”是关键要求，它表明状态机应用到 ReadIndex 之后的状态都能使这个请求满足线性一致，不管过了多久，也不管 Leader 有没有飘走。为什么在 ReadIndex 只有就满足了线性一致性呢？之前 LogRead 的读发生点是 commit index，这个点能使 LogRead 满足线性一致，那显然发生这个点之后的 ReadIndex 也能满足。&lt;/p&gt;&lt;p&gt;&lt;b&gt;LeaseRead&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LeaseRead 与 ReadIndex 类似，但更进一步，不仅省去了 Log，还省去了网络交互。它可以大幅提升读的吞吐也能显著降低延时。基本的思路是 Leader 取一个比 Election Timeout 小的租期，在租期不会发生选举，确保 Leader 不会变，所以可以跳过 ReadIndex 的第二步，也就降低了延时。 LeaseRead 的正确性和时间挂钩，因此时间的实现至关重要，如果漂移严重，这套机制就会有问题。&lt;/p&gt;&lt;p&gt;Wait Free&lt;/p&gt;&lt;p&gt;到此为止 Lease 省去了 ReadIndex 的第二步，实际能再进一步，省去第 3 步。这样的 LeaseRead 在收到请求后会立刻进行读请求，不取 commit index 也不等状态机。由于 Raft 的强 Leader 特性，在租期内的 Client 收到的 Resp 由 Leader 的状态机产生，所以只要状态机满足线性一致，那么在 Lease 内，不管何时发生读都能满足线性一致性。有一点需要注意，只有在 Leader 的状态机应用了当前 term 的第一个 Log 后才能进行 LeaseRead。因为新选举产生的 Leader，它虽然有全部 committed Log，但它的状态机可能落后于之前的 Leader，状态机应用到当前 term 的 Log 就保证了新 Leader 的状态机一定新于旧 Leader，之后肯定不会出现 stale read。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本文粗略地聊了聊线性一致性，以及 TiKV 内部的一些优化。最后留四个问题以便更好地理解本文：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于线性一致中的例子，如果 A 读到了 2，那么 x 的各个阶段是怎样的呢？&lt;/li&gt;&lt;li&gt;对于下图，它符合线性一致吗？（温馨提示：请使用游标卡尺。;-P）&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-58c02cee775ff40a6f89e29afb3e7fe5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;739&quot; data-rawheight=&quot;432&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-58c02cee775ff40a6f89e29afb3e7fe5&quot; data-watermark-src=&quot;v2-165658e7cfbec5e0b5c61f0df93c982a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;3. Leader 的状态机在什么时候没有最新状态？要线性一致性，Raft 该如何解决这问题？&lt;/p&gt;&lt;p&gt;4. FollowerRead 可以由 ReadIndex 实现，那么能由 LeaseRead 实现吗？&lt;/p&gt;&lt;p&gt;如有疑问或想交流，欢迎联系我：&lt;b&gt;shentaining@pingcap.com&lt;/b&gt;&lt;/p&gt;&lt;p&gt;[0].Ongaro, Diego. Consensus: Bridging theory and practice. Diss. Stanford University, 2014.&lt;br&gt;[1].Herlihy, Maurice P., and Jeannette M. Wing. “Linearizability: A correctness condition for concurrent objects.” ACM Transactions on Programming Languages and Systems (TOPLAS) 12.3 (1990): 463-492.&lt;br&gt;[2].Gilbert, Seth, and Nancy Lynch. “Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services.” Acm Sigact News 33.2 (2002): 51-59.&lt;br&gt;[3].Lamport, Leslie. “Time, clocks, and the ordering of events in a distributed system.” Communications of the ACM 21.7 (1978): 558-565.&lt;br&gt;[4].&lt;a href=&quot;https://raft.github.io/&quot;&gt;https://raft.github.io/&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-22-47117804</guid>
<pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>成都 Meetup 预告 |  PingCAP 成都分舵第一次干货分享趴</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-18-47050511.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47050511&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6c06b1218c416abf3c2b2c8b3b3a32fd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;i&gt;&lt;b&gt;Hit!!! &lt;/b&gt;&lt;/i&gt;社区小伙伴期待已久的「西南第一分舵」——&lt;i&gt;&lt;b&gt;成都 Office&lt;/b&gt;&lt;/i&gt;正式成立了！&lt;br&gt;我们将在本周日举办一场 Infra Meetup，欢迎新老朋友过来面基～不管你是想聊一聊技术干货还是想加入分舵，都可以点击文末链接直接报名来现场交流！&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;PingCAP Infra Meetup No.77&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;🚀 时间：2018-10-21  周日  13:00 - 16:00&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;🛸 地点：成都武侯区吉泰路银泰城 17 栋 优客工场&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;🚇 交通：&lt;/b&gt;地铁 1 号线天府三街站 B 口出，步行从吉泰路上的银泰城正门进入。银泰城车库入口在天府四街。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;报名通道&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://www.huodongxing.com/event/8462453900500&quot;&gt;报名：【成都】PingCAP Infra Meetup No.77&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;日程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;13:00 - 13:30  &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;现场签到&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;13:30 - 14:30 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Speaker&lt;/b&gt;&lt;br&gt;&lt;b&gt;申砾&lt;/b&gt;，我司技术副总裁&lt;/li&gt;&lt;li&gt;&lt;b&gt;Talk&lt;/b&gt;&lt;br&gt;Deep Dive into TiDB SQL Layer&lt;/li&gt;&lt;li&gt;&lt;b&gt;Content&lt;/b&gt;&lt;br&gt;本次分享主要涉及 TiDB 和 TiDB SQL 层的架构，带大家深入了解 TiDB SQL 层的优化器和执行引擎。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;14:30 - 14:45 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;茶歇&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;14:45 - 15:30 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Speaker&lt;/b&gt;&lt;br&gt;李银龙，马上消费金融 NewSQL 负责人，原腾讯云运维工程师，原猪八戒 DBA 团队构建者。「马上消费金融股份有限公司是一家经中国银保监会批准，持有消费金融牌照的科技驱动的金融机构，是注册资本金第一大的内资消费金融公司。」&lt;/li&gt;&lt;li&gt;&lt;b&gt;Talk&lt;/b&gt;&lt;br&gt;马上消费金融 TiDB 实践分享&lt;/li&gt;&lt;li&gt;&lt;b&gt;Content&lt;/b&gt;&lt;br&gt;本次我们将分享在消费金融行业爆发式增长背景下，马上消费金融面对的传统 MySQL 关系型数据库瓶颈与 NewSQL 技术探索实践。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;15:30 - 16:00 &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;社区圆桌讨论&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;入舵指南&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;成都分舵地址&lt;/b&gt;：天府大道中段 666 号希顿国际中心 C 座&lt;/li&gt;&lt;li&gt;&lt;b&gt;勾搭通道&lt;/b&gt;：hire@pingcap.com&lt;/li&gt;&lt;li&gt;&lt;b&gt;职位信息：&lt;/b&gt;Infrastructure Engineer（包括分布式存储-TiKV、分布式计算-TiDB、分布式调度-PD、商业工具-Tools、SRE、Cloud 等方向）在成都分舵已全面开放，了解更多职位信息：&lt;a href=&quot;https://pingcap.com/recruit-cn/join/&quot;&gt;虚位以待 | PingCAP&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;🧐 报名通道：&lt;a href=&quot;http://www.huodongxing.com/event/8462453900500&quot;&gt;【成都】PingCAP Infra Meetup No.77&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-18-47050511</guid>
<pubDate>Thu, 18 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 是如何存储数据的（下）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-10-11-46524530.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46524530&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ef0379b7d20b5347e99e16cc5f6bed9e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;u&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247486842&amp;amp;idx=1&amp;amp;sn=2e21e65010f497693f26cfc344e418fe&amp;amp;chksm=eb162a10dc61a30650269d414de2cfe4eeff08e0d5e9b50834c3353c70850c83b796fd2be364&amp;amp;scene=21#wechat_redirect&quot;&gt;上篇文章&lt;/a&gt;&lt;/u&gt;中，我们介绍了与 TiKV 处理读写请求相关的基础知识，下面将开始详细的介绍 TiKV 的读写流程。Enjoy~&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;作者：唐刘 siddontang&lt;/p&gt;&lt;h2&gt;&lt;b&gt;RawKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiKV 提供两套 API，一套叫做 RawKV，另一套叫做 TxnKV。TxnKV 对应的就是上面提到的 Percolator，而 RawKV 则不会对事务做任何保证，而且比 TxnKV 简单很多，这里我们先讨论 RawKV。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Write&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-08650031b484425439a68ed74eca75c3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;675&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-08650031b484425439a68ed74eca75c3&quot; data-watermark-src=&quot;v2-1fd4a870d321c5da5c89ad8b1e17ccd3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;当进行写入，譬如 Write a = 1，会进行如下步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Client 找 PD 问 a 所在的 Region&lt;/li&gt;&lt;li&gt;PD 告诉 Region 相关信息，主要是 Leader 所在的 TiKV&lt;/li&gt;&lt;li&gt;Client 将命令发送给 Leader 所在的 TiKV&lt;/li&gt;&lt;li&gt;Leader 接受请求之后执行 Raft 流程&lt;/li&gt;&lt;li&gt;Leader 将 a = 1 Apply 到 KV RocksDB 然后给 Client 返回写入成功&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Read&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-633b8d2972993810a83683ae9a9fbf7f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1212&quot; data-rawheight=&quot;660&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-633b8d2972993810a83683ae9a9fbf7f&quot; data-watermark-src=&quot;v2-2f6356ecb30b884ce25ceec89af1c519&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;对于 Read 来说，也是一样的操作，唯一不同在于 Leader 可以直接提供 Read，不需要走 Raft。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TxnKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Write&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d4e569db6e896428f6d61f59dd10b934_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;649&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d4e569db6e896428f6d61f59dd10b934&quot; data-watermark-src=&quot;v2-8b72f207ab9b77f534c6393acb27b488&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;对于 TxnKV 来说，情况就要复杂的多，不过大部分流程已经在 Percolator 章节进行说明了。这里需要注意的是，因为我们要快速的 seek 到最新的 commit，所以在 RocksDB 里面，我们会先将 TS 使用 bigendian 生成 8 字节的 bytes，然后将这个 bytes 逐位取反，在跟原始的 key 组合存储到 RocksDB 里面，这样就能保证最新的提交存放到前面，seek 的时候就能直接定位了，当然 seek 的时候，也同样会将对应的 TS 按照相同的方式编码处理。&lt;/p&gt;&lt;p&gt;譬如，假设一个 key 现在有两次提交，commitTS 分别为 10 和 12，startTS 则是 9 和 11，那么在 RocksDB 里面，key 的存放顺序则是：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Write CF：

a_12 -&amp;gt; 11
a_10 -&amp;gt; 9

Data CF:

a_11 -&amp;gt; data_11
a_9 -&amp;gt; data_9&lt;/code&gt;&lt;p&gt;另外，还需要注意的是，对于 value 比较小的情况，TiKV 会直接将 value 存放到 Write CF 里面，这样 Read 的时候只要走 Write CF 就行了。在写入的时候，流程如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;PreWrite：

Lock CF: W a -&amp;gt; Lock + Data

Commit:
Lock CF: R a -&amp;gt; Lock + 10 + Data
Lock CF: D a

Write CF: W a_11 -&amp;gt; 10 + Data&lt;/code&gt;&lt;p&gt;对于 TiKV 来说，在 Commit 阶段无论怎样都会读取 Lock 来判断事务冲突，所以我们可以从 Lock 拿到数据，然后再写入到 Write CF 里面。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Read&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-59ca441db4424772565202a0b7b3e5bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;659&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-59ca441db4424772565202a0b7b3e5bb&quot; data-watermark-src=&quot;v2-e3239f73c2c07d91376dd4fd8a766bc1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Read 的流程之前的 Percolator 已经有说明了，这里就不详细解释了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;SQL Key Mapping&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们在 TiKV 上面构建了一个分布式数据库 TiDB，它是一个关系型数据库，所以大家需要关注的是一个关系型的 table 是如何映射到 key-value 上面的。假设我们有如下的表结构：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE t1 {
	id BIGINT PRIMARY KEY,
	name VARCHAR(1024),
	age BIGINT,
	content BLOB,
	UNIQUE(name),
	INDEX(age),
}&lt;/code&gt;&lt;p&gt;上面我们创建了一张表 t1，里面有四个字段，id 是主键，name 是唯一索引，age 是一个索引。那么这个表里面的数据是如何对应到 TiKV 的呢？&lt;/p&gt;&lt;p&gt;在 TiDB 里面，任何一张表都有一个唯一的 ID，譬如这里是 11，任何的索引也有唯一的 ID，上面 name 就是 12，age 就是 13。我们使用前缀 t 和 i 来区分表里面的 data 和 index。对于上面表 t1 来说，假设现在它有两行数据，分别是 (1, “a”, 10, “hello”) 和 (2, “b”, 12, “world”)，在 TiKV 里面，每一行数据会有不同的 key-value 对应。如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;PK
t_11_1 -&amp;gt; (1, “a”, 10, “hello”)
t_11_2 -&amp;gt; (2, “b”, 12, “world”)

Unique Name
i_12_a -&amp;gt; 1
i_12_b -&amp;gt; 2

Index Age
i_13_10_1 -&amp;gt; nil
i_13_12_2 -&amp;gt; nil&lt;/code&gt;&lt;p&gt;因为 PK 具有唯一性，所以我们可以用 t + Table ID + PK 来唯一表示一行数据，value 就是这行数据。对于 Unique 来说，也是具有唯一性的，所以我们用 i + Index ID + name 来表示，而 value 则是对应的 PK。如果两个 name 相同，就会破坏唯一性约束。当我们使用 Unique 来查询的时候，会先找到对应的 PK，然后再通过 PK 找到对应的数据。&lt;/p&gt;&lt;p&gt;对于普通的 Index 来说，不需要唯一性约束，所以我们使用 i + Index ID + age + PK，而 value 为空。因为 PK 一定是唯一的，所以两行数据即使 age 一样，也不会冲突。当我们使用 Index 来查询的时候，会先 seek 到第一个大于等于 i + Index ID + age 这个 key 的数据，然后看前缀是否匹配，如果匹配，则解码出对应的 PK，再从 PK 拿到实际的数据。&lt;/p&gt;&lt;p&gt;TiDB 在操作 TiKV 的时候需要保证操作 keys 的一致性，所以需要使用 TxnKV 模式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结语&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面简单的介绍了下 TiKV 读写数据的流程，还有很多东西并没有覆盖到，譬如错误处理，Percolator 的性能优化这些，如果你对这些感兴趣，可以参与到 TiKV 的开发，欢迎联系我 tl@pingcap.com。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-10-11-46524530</guid>
<pubDate>Thu, 11 Oct 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
