<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sun, 01 Jul 2018 14:42:00 +0800</lastBuildDate>
<item>
<title>TiDB 在特来电的实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-07-01-38760049.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38760049&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a456a06b62972dd2a47920e3097ca383_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;背景介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;特来电新能源有限公司是创业板第一股特锐德（300001）的全资子公司，主要从事新能源汽车充电网的建设、运营及互联网的增值服务。特来电颠覆了传统充电桩的模式，世界首创了电动汽车群智能充电系统，获得 336 项技术专利，以“无桩充电、无电插头、群管群控、模块结构、主动防护、柔性充电”的特点引领世界新能源汽车充电的发展，系统的鉴定结论为：“产品世界首创、技术水平国际领先。主动柔性充电对电池寿命可以延长 30% 左右，电池充电的安全性可以提升 100 倍以上。”&lt;/p&gt;&lt;p&gt;特来电采用互联网思维，依靠国际领先的汽车群智能充电技术和系统，创新电动汽车充电商业模式，建设全国最大的汽车充电网，通过大系统卖电、大平台卖车、大共享租车、大数据修车、大支付金融、大客户电商，打造让客户满意、政府放心的中国最大汽车充电网生态公司，引领充电网、车联网、互联网“三网融合”的新能源互联网。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么研究 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;特来电大数据平台通过开源与自研相结合的方式，目前已经上线多套集群满足不同的业务需求。目前在大数据存储和计算方面主要使用了 HBase、Elasticsearch、Druid、Spark、Flink。大数据技术可谓是百花齐放、百家争鸣，不同的技术都有针对性的场景。结合实际情况，选择合适的技术不是一件容易的事情。&lt;/p&gt;&lt;p&gt;随着接入大数据平台的核心业务的增加，我们在 OLAP 上主要遇到以下痛点问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;随着基于大数据分析计算的深入应用，使用 SQL 进行分析的需求越来越旺盛，但目前已经上线的大数据集群（ HBase、Elasticsearch、Druid、Spark、Flink）对 SQL 的支持度都比较弱。&lt;/li&gt;&lt;li&gt;目前进入大数据集群的数据主要以宽表方式进行，导致在数据归集和后期基础数据放生变化时应用成本较高。&lt;/li&gt;&lt;li&gt;数据仓库业务有些还是基于复杂的 T+1 模式的 ETL 过程，延时较高，不能实时的反映业务变化。&lt;/li&gt;&lt;li&gt;由于每个大数据集群主要针对特定的场景，数据重复存储的情况较多，这就造成了存储成本的增加，同时也会导致数据的不一致性。&lt;/li&gt;&lt;li&gt;目前进入 HDFS / Druid / ES 的数据，在历史数据更新时，成本较高，灵活性降低。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;大数据技术发展迅速，我们也一直希望采用新的技术可以解决我们以上问题，我们关注到目前 NewSQL 技术已经有落地产品，并且不少企业在使用，所以决定在我们平台内尝试引入 NewSQL 技术解决我们的痛点问题。&lt;/p&gt;&lt;p&gt;我们先了解一下 NewSQL。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e825ad65efc06df04addcfca0bf8b9c2_r.jpg&quot; data-caption=&quot;图 1 数据库发展史&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;514&quot; data-rawheight=&quot;301&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e825ad65efc06df04addcfca0bf8b9c2&quot; data-watermark-src=&quot;v2-e75d9e8c5df1bce001a63c594d263f0d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如图 1 所示，数据库的发展经历了 RDBMS、NoSQL 以及现在的 NewSQL，每种不同的技术都有对应的产品，每种数据库的技术背后，都有典型的理论支撑。2003 年 Google GFS 开创了分布式文件系统、2006 年的 BigTable 论文催生了 Hadoop 生态，在 2012 年的 Spanner 和 2013 年的 F1 论文发表后，被业界认为指明了未来关系型数据库的发展。&lt;/p&gt;&lt;p&gt;随着大数据技术的发展，实际上 SQL 和 NoSQL 的界限逐渐模糊，比如现在 HBase  之上有 Phoenix，HiveSQL，SparkSQL 等，也有一些观点认为 NewSQL = SQL + NoSQL。不同的技术都有各自的最佳适应场景，Spanner 和 F1 被认为是第一个 NewSQL 在生产环境提供服务的分布式系统技术，基于该理念的开源产品主要为 CockroachDB、TiDB。结合社区活跃度以及相关案例、技术支持，我们决定 NewSQL  技术上引入 TiDB。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 是 PingCAP 公司受 Google Spanner / F1 论文启发而设计的开源分布式 HTAP 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7ccc03f62389ab498786b59f7fba5bab_r.jpg&quot; data-caption=&quot;图 2 TiDB 架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;827&quot; data-rawheight=&quot;393&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7ccc03f62389ab498786b59f7fba5bab&quot; data-watermark-src=&quot;v2-87924f905d12c2cfb2bd7050e1dd7fbd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;TiDB 具有以下核心特性：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;高度兼容 MySQL —— 无需修改代码即可从 MySQL 轻松迁移至 TiDB&lt;/li&gt;&lt;li&gt;水平弹性扩展 —— 轻松应对高并发、海量数据场景&lt;/li&gt;&lt;li&gt;分布式事务 —— TiDB 100% 支持标准的 ACID 事务&lt;/li&gt;&lt;li&gt;高可用 —— 基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证&lt;/li&gt;&lt;li&gt;一站式 HTAP 解决方案 —— 一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中涉及到的分布式存储和分布式计算，大家可以参考 TiDB 的官方网站，在这里就不再进行论述。&lt;/p&gt;&lt;p&gt;在处理大型复杂的计算时，PingCAP 结合上图说的 TiKV 以及目前大数据生态的 Spark，提供了另外一个开源产品 TiSpark。不得不说这是一个巧妙的设计，充分利用了现在企业已有的 Spark 集群的资源，不需要另外再新建集群。TiSpark 架构以及核心原理简单描述如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-92c0f11228150c5cf56dfdaf5b21ea1f_r.jpg&quot; data-caption=&quot;图 3 TiSpark 架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;617&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-92c0f11228150c5cf56dfdaf5b21ea1f&quot; data-watermark-src=&quot;v2-a7609fdaf0319081318d88b6e9eb21e5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;TiSpark 深度整合了 Spark Catalyst 引擎， 可以对计算提供精确的控制，使 Spark 能够高效的读取 TiKV 中的数据，提供索引支持以实现高速的点查。&lt;/p&gt;&lt;p&gt;通过多种计算下推减少 Spark SQL 需要处理的数据大小，以加速查询；利用 TiDB 的内建的统计信息选择更优的查询计划。&lt;/p&gt;&lt;p&gt;从数据集群的角度看，TiSpark + TiDB 可以让用户无需进行脆弱和难以维护的 ETL，直接在同一个平台进行事务和分析两种工作，简化了系统架构和运维。&lt;/p&gt;&lt;p&gt;除此之外，用户借助 TiSpark 项目可以在 TiDB 上使用 Spark 生态圈提供的多种工具进行数据处理。例如使用 TiSpark 进行数据分析和 ETL；使用 TiKV 作为机器学习的数据源；借助调度系统产生定时报表等等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;目前的应用情况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于很多用户已经部署了生产系统，我们没有在测试上再次投入比较大的精力，经过了简单的性能测试以后，搭建了我们的第一个 TiDB 集群，尝试在我们的业务上进行使用。目前主要用于我们的离线计算，以及部分即系查询场景，后续根据使用情况，逐渐调整我们的集群规模以及增加我们的线上应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 目前的集群配置&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f8f32b0c3aaa1ae9b0fd7d66d1c3af88_r.jpg&quot; data-caption=&quot;图 4 集群配置清单&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;580&quot; data-rawheight=&quot;287&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f8f32b0c3aaa1ae9b0fd7d66d1c3af88&quot; data-watermark-src=&quot;v2-b54aeaeb7444ec84d3049a7031a7cb99&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;2. 规划的应用架构&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-48cc92e111dd0a45da7c85a7a5094f68_r.jpg&quot; data-caption=&quot;图 5 引入 TiDB 以后的应用架构图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;438&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-48cc92e111dd0a45da7c85a7a5094f68&quot; data-watermark-src=&quot;v2-a0e2c64fa8a49061b569454d00c54751&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;基于 TiDB 我们规划了完整的数据流处理逻辑，从数据接入到数据展现，由于 TiDB 高度兼容 MySQL，因此在数据源接入和 UI 展现就有很多成熟的工具可以使用，比如 Flume、Grafana、Saiku 等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 应用简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;a. 充电功率的分时统计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个用户使用特来电的充电桩进行充电时，车辆的 BMS 数据、充电桩数据、环境温度等数据是实时的保存到大数据库中。我们基于采集的用户充电数据，需要按照一定的时间展示全国的充电功率 比如展示过去一天，全国的充电功率变化曲线，每隔 15 分钟或者 30 分钟进行一次汇总。随着我们业务规模的增加，此场景的计算也逐步进行了更新换代。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-341fe1d2cb79d61df04446a1d898fdf8_r.jpg&quot; data-caption=&quot;图 6 充电功率的分时统计&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;653&quot; data-rawheight=&quot;236&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-341fe1d2cb79d61df04446a1d898fdf8&quot; data-watermark-src=&quot;v2-b311c213bc9f7d70206bece78b84f843&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;目前我们单表数据量接近 20 亿，每天的增量接近 800 万左右。使用 TiDB 后，在进行离线计算分析时，我们的业务逻辑转成了直接在我们的离线计算平台通过 SQL 的方式进行定义和维护，极大的提高了维护效率，同时计算速度也得到了大幅提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;b. 充电过程分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面我们讲了，我们已经有了充电过程中的宝贵的海量数据，如何让数据发挥价值，我们基于充电数据进行充电过程的分析就是其中的一个方式，比如分析不同的车型在不同的环境（环境温度、电池特性）下，充电的最大电压和电流的变化情况，以及我们充电桩的需求功率满足度等。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fbce582daa6f5a7cf0083489eeef5ad8_r.jpg&quot; data-caption=&quot;图 7 充电过程分析&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;287&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fbce582daa6f5a7cf0083489eeef5ad8&quot; data-watermark-src=&quot;v2-ce51b393bb4f074ab6d845fbfb635310&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;针对海量的历史数据计算我们使用了 TiSpark 进行计算，直接使用了我们现有的 Spark 集群，在使用 Spark 进行计算时，一开始由于不熟悉 TiSpark，分配的资源比较少，耗时多一些。后来和 TiDB 技术人员交流了解到最佳实践，提升配置和调整部分参数后，性能提升不少。这个场景中我们充分利用了 TiDB 和 TiSpark 进行协同工作，满足了我们的业务需求。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结及问题&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 最佳应用场景&lt;/b&gt;&lt;/p&gt;&lt;p&gt;结合我们的线上验证，我们认为使用 TiDB，主要有以下几个优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;SQL 支持度相对于现有的集群支持度较好，灵活性和功能性大大增强。&lt;/li&gt;&lt;li&gt;可以进行表之间的 join 运算，降低了构造宽边的复杂度以及因此带来的维护成本。&lt;/li&gt;&lt;li&gt;历史数据方便修改。&lt;/li&gt;&lt;li&gt;高度兼容 MySQL 生态下对应的成熟软件较多（开发工具、展现、数据接入）。&lt;/li&gt;&lt;li&gt;基于索引的 SQL 性能在离线计算上基本可以满足我们需求，在即席查询上最适合海量数据下进行多维度的精确查询，类似与 “万里挑一” 的场景。&lt;/li&gt;&lt;li&gt;使用 TiSpark 进行复杂的离线计算，充分利用了现有的集群，数据存储做到了一份，同时也降低了运维成本。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 目前的定位&lt;/b&gt;&lt;/p&gt;&lt;p&gt;结合我们的实际现状，现阶段我们主要用于进行离线计算和部分即席查询的场景，后期随着应用的深入，我们逐步考虑增加更多的应用以及部分 OLTP 场景。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;作者介绍：潘博存，特来电大数据技术研发部架构师，具有 10 多年平台软件设计开发经验，现专注于大数据领域快速读写方向。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-07-01-38760049</guid>
<pubDate>Sun, 01 Jul 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十一）Index Lookup Join</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-27-38572730.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38572730&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1bc90f4fea487a82ecb0a73acbf3b306_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt; 作者：徐怀宇&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;什么是 Index Lookup Join&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Nested Loop Join&lt;/b&gt;&lt;br&gt;在介绍 Index Lookup Join 之前，我们首先看一下什么是 &lt;b&gt;Nested Loop Join（NLJ）&lt;/b&gt;。 NLJ 的具体定义可以参考 &lt;a href=&quot;https://en.wikipedia.org/wiki/Nested_loop_join&quot;&gt;Wikipedia&lt;/a&gt;。NLJ 是最为简单暴力的 Join 算法，其执行过程简述如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;遍历 Outer 表，取一条数据 r；&lt;/li&gt;&lt;li&gt;遍历 Inner 表，对于 Inner 表中的每条数据，与 r 进行 join 操作并输出 join 结果；&lt;/li&gt;&lt;li&gt;重复步骤 1，2 直至遍历完 Outer 表中的所有数据。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;NLJ 算法实现非常简单并且 join 结果的顺序与 Outer 表的数据顺序一致。&lt;br&gt;但是存在性能上的问题：执行过程中，对于每一条 OuterRow，我们都需要对 Inner 表进行一次&lt;b&gt;全表扫&lt;/b&gt;操作，这将消耗大量时间。&lt;br&gt;为了减少对于 Inner 表的全表扫次数，我们可以将上述步骤 1 优化为每次从 Outer 表中读取一个 batch 的数据，优化后的算法即 &lt;b&gt;Block Nested-Loop Join（BNJ）&lt;/b&gt;，BNJ 的具体定义可以参考 &lt;a href=&quot;https://en.wikipedia.org/wiki/Block_nested_loop&quot;&gt;Wikipedia&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Index Lookup Join&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于 BNJ 算法，我们注意到，对于 Outer 表中每个 batch，我们并没有必要对 Inner 表都进行一次全表扫操作，很多时候可以通过索引减少数据读取的代价。&lt;b&gt;Index Lookup Join（ILJ）&lt;/b&gt; 在 BNJ 基础上进行了改进，其执行过程简述如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从 Outer 表中取一批数据，设为 B；&lt;/li&gt;&lt;li&gt;通过 Join Key 以及 B 中的数据构造 Inner 表取值范围，只读取对应取值范围的数据，设为 S；&lt;/li&gt;&lt;li&gt;对 B 中的每一行数据，与 S 中的每一条数据执行 Join 操作并输出结果；&lt;/li&gt;&lt;li&gt;重复步骤 1，2，3，直至遍历完 Outer 表中的所有数据。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB Index Lookup Join 的实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的 ILJ 算子是一个多线程的实现，主要的线程有： Main Thead，Outer Worker，和 Inner Worker：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Outer Worker 一个：&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;按 batch 遍历 Outer 表，并封装对应的 task&lt;/li&gt;&lt;li&gt;将 task 发送给 Inner Worker 和 Main Thread&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;Inner Worker N 个：&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取 Outer Worker 构建的 task&lt;/li&gt;&lt;li&gt;根据 task 中的 Outer 表数据，构建 Inner 表的扫描范围，并构造相应的物理执行算子读取该范围内的 Inner 表数据&lt;/li&gt;&lt;li&gt;对读取的 Inner 表数据创建对应的哈希表并存入 task&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;Main Thread 一个：&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;启动 Outer Worker 及 Inner Workers&lt;/li&gt;&lt;li&gt;读取 Outer Worker 构建的 task，并对每行 Outer 数据在对应的哈希表中 probe&lt;/li&gt;&lt;li&gt;对 probe 到的数据进行 join 并返回执行结果&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;这个算子有如下特点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Join 结果的顺序与 Outer 表的数据顺序一致，这样对上一层算子可以提供顺序保证；&lt;/li&gt;&lt;li&gt;对于 Outer 表中的每个 batch，只在 Inner 表中扫描部分数据，提升单个 batch 的处理效率；&lt;/li&gt;&lt;li&gt;Outer 表的读数据操作，Inner 表的读数据操作，及 Join 操作并行执行，整体上是一个并行+Pipeline 的方式，尽可能提升执行效率。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;执行阶段详述&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 中 ILJ 的执行阶段可划分为如下图所示的 5 步：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bce13c946db6b2f593524843dca98df0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;635&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bce13c946db6b2f593524843dca98df0&quot; data-watermark-src=&quot;v2-4fa27fd9a02423446cc8231976e9f80f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;1. 启动 Outer Worker 及 Inner Workers&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L130&quot;&gt;startWorkers&lt;/a&gt; 函数完成。该函数会 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L138&quot;&gt;启动一个 Outer Worker&lt;/a&gt; 和&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L141&quot;&gt;多个 Inner Worker&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L141&quot;&gt;多个 Inner Worker&lt;/a&gt;。Inner Woker 的数量可以通过 &lt;code class=&quot;inline&quot;&gt;tidb_index_lookup_concurrency&lt;/code&gt; 这个系统变量进行设置，默认为 4。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 读取 Outer 表数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L314&quot;&gt;buildTask&lt;/a&gt; 函数完成。此处主要注意两点：&lt;/p&gt;&lt;p&gt;第一点，对于每次读取的 batch 大小，如果将其设置为固定值，则可能会出现如下问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;若设置的 batch 值&lt;b&gt;较大&lt;/b&gt;，但 Outer 表数据量&lt;b&gt;较小&lt;/b&gt;时。各个 Inner Worker 所需处理的任务量可能会不均匀，出现数据倾斜的情况，导致并发整体性能相对单线程提升有限。&lt;/li&gt;&lt;li&gt;若设置的 batch 值&lt;b&gt;较小&lt;/b&gt;，但 Outer 表数据量&lt;b&gt;较大&lt;/b&gt;时。Inner Worker 处理任务时间短，需要频繁从管道中取任务，CPU 不能被持续高效利用，由此带来大量的线程切换开销。此外, 当 batch 值较小时，同一批 inner 表数据能会被反复读取多次，带来更大的网络开销，对整体性能产生极大影响。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，我们通过指数递增的方式动态控制 batch 的大小（由函数 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L348&quot;&gt;increaseBatchSize&lt;/a&gt; 完成），以避免上述问题，batch size 的最大值由 session 变量 &lt;code class=&quot;inline&quot;&gt;tidb_index_join_batch_size&lt;/code&gt; 控制，默认是 25000。读取到的 batch 存储在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/expression/chunk_executor.go#L225&quot;&gt;lookUpJoinTask.outerResult&lt;/a&gt; 中。&lt;/p&gt;&lt;p&gt;第二点，如果 Outer 表的过滤条件不为空，我们需要对 outerResult 中的数据进行过滤（由函数 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/expression/chunk_executor.go#L225&quot;&gt;VectorizedFilter&lt;/a&gt; 完成）。outerResult 是 Chunk 类型（&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38095421&quot;&gt;Chunk 的介绍请参考 TiDB 源码阅读系列文章（十）&lt;/a&gt;），如果对满足过滤条件的行进行提取并重新构建对象进行存储，会带来不必要的时间和内存开销。&lt;code class=&quot;inline&quot;&gt;VectorizedFilter&lt;/code&gt; 函数通过一个长度与 outerResult 实际数据行数相等的 bool slice 记录 outerResult 中的每一行是否满足过滤条件以避免上述开销。 该 bool slice 存储在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L81&quot;&gt;lookUpJoinTask.outerMatch&lt;/a&gt; 中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Outer Worker 将 task 发送给 Inner Worker 和 Main Thread&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Inner Worker 需要根据 Outer 表每个 batch 的数据，构建 Inner 表的数据扫描范围并读取数据，因此 Outer Worker 需要将 task &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L304&quot;&gt;发送给 Inner Worker&lt;/a&gt;。&lt;br&gt;如前文所述，ILJ 多线程并发执行，且 Join 结果的顺序与 Outer 表的数据顺序一致。 为了实现这一点，Outer Worker 通过管道将 task &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L299&quot;&gt;发送给 Main Thread&lt;/a&gt;，Main Thread 从管道中按序读取 task 并执行 Join 操作，这样便可以实现在多线程并发执行的情况下的保序需求。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Inner Worker 读取 inner 表数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L376&quot;&gt;handleTask&lt;/a&gt; 这个函数完成。handleTask 有如下几个步骤:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L393&quot;&gt;constructDatumLookupKeys&lt;/a&gt; 函数计算 Outer 表对应的 Join Keys 的值，我们可以根据 Join Keys 的值从 Inner 表中仅查询所需要的数据即可，而不用对 Inner 表中的所有数据进行遍历。为了避免对同一个 batch 中相同的 Join Keys 重复查询 Inner 表中的数据，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L447&quot;&gt;sortAndDedupDatumLookUpKeys&lt;/a&gt; 会在查询前对前面计算出的 Join Keys 的值进行去重。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L480&quot;&gt;fetchInnerResult&lt;/a&gt; 函数利用去重后的 Join Keys 构造对 Inner 表进行查询的执行器，并读取数据存储于 &lt;code class=&quot;inline&quot;&gt;task.innerResult&lt;/code&gt; 中。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L502&quot;&gt;buildLookUpMap&lt;/a&gt; 函数对读取的 Inner 数据按照对应的 Join Keys 构建哈希表，存储于 &lt;code class=&quot;inline&quot;&gt;task.lookupMap&lt;/code&gt; 中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上述步骤完成后，Inner Worker 向 &lt;code class=&quot;inline&quot;&gt;task.doneCh&lt;/code&gt; 中发送数据，以唤醒 Main Thread 进行接下来的工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. Main Thread 执行 Join 操作&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L209&quot;&gt;prepareJoinResult&lt;/a&gt; 函数完成。prepareJoinResult 有如下几个步骤：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L216&quot;&gt;getFinishedTask&lt;/a&gt; 从 resultCh 中读取 task，并等待 task.doneCh 发送来的数据，若该 task 没有完成，则阻塞住；&lt;/li&gt;&lt;li&gt;接下来的步骤与 Hash Join类似（参考 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/37773956&quot;&gt;TiDB 源码阅读系列文章（九）&lt;/a&gt;），&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/index_lookup_join.go#L273&quot;&gt;lookUpMatchedInners&lt;/a&gt; 取一行 OuterRow 对应的 Join Key，从 task.lookupMap 中 probe 对应的 Inner 表的数据；&lt;/li&gt;&lt;li&gt;主线程对该 OuterRow，与取出的对应的 InnerRows 执行 Join 操作，写满存储结果的 chk 后返回。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;示例&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE `t` (
`a` int(11) DEFAULT NULL,
`pk` int(11) NOT NULL AUTO_INCREMENT,
PRIMARY KEY (`pk`)
);

CREATE TABLE `s` (
`a` int(11) DEFAULT NULL,
KEY `idx_s_a` (`a`)
);
​
insert into t(`a`) value(1),(1),(1),(4),(4),(5);
insert into s value(1),(2),(3),(4);
​
select /*+ TIDB_INLJ(t) */ * from t left join s on t.a = s.a;&lt;/code&gt;&lt;p&gt;&lt;br&gt;在上例中， &lt;code class=&quot;inline&quot;&gt;t&lt;/code&gt; 为 Outer 表，&lt;code class=&quot;inline&quot;&gt;s&lt;/code&gt; 为 Inner 表。 &lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/tidb-specific.md#tidb_inljt1-t2&quot;&gt;/** TIDN_INLJ */&lt;/a&gt; 可以让优化器尽可能选择 Index Lookup Join 算法。&lt;/p&gt;&lt;p&gt;设 Outer 表读数据 batch 的初始大小为 2 行，Inner Worker 数量为 2。&lt;/p&gt;&lt;p&gt;查询语句的一种可能的执行流程如下图所示，其中由上往下箭头表示时间线：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-351d7c688bdf43506185bd617b99ffae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;521&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-351d7c688bdf43506185bd617b99ffae&quot; data-watermark-src=&quot;v2-2cea494c8abd250e517b7fddae0a0446&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;延展阅读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38095421&quot;&gt;（十）Chunk 和执行框架简介&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37773956&quot;&gt;（九）Hash Join&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36420449&quot;&gt;（八）基于代价的优化&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35511864&quot;&gt;（七）基于规则的优化&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot;&gt;（六）Select 语句概览&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34770765&quot;&gt;（五）TiDB SQL Parser 的实现&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34512827&quot;&gt;（四）Insert 语句概览&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34369624&quot;&gt;（三）SQL 的一生&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34176614&quot;&gt;（二）初识 TiDB 源码&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34109413&quot;&gt;（一）序&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-27-38572730</guid>
<pubDate>Wed, 27 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>SuRF: 一个优化的 Fast Succinct Tries</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-22-38385054.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38385054&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e1b5f6d9e6e925d3093d622e3ccdeb17_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：唐刘&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在前一篇文章中，我简单介绍了 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/38194127&quot;&gt;Succinct Data Structure&lt;/a&gt;，这里我们继续介绍 SuRF。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Fast Succinct Tries&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SuRF 的核心数据结构就是 Fast Succinct Tries（FST），一种空间节省，支持 point 和 range query 的静态 trie。在很多时候，对于一棵树来说，上层的 trie 节点较少，但访问频繁，也就是我们俗称的 hot，而下层的节点则相对的 cold 一点。因此，SuRF 使用了两种数据结构来分别处理 hot 和 cold 节点。在 upper 层上面使用了 LOUDS-Dedense，而在 lower 层上面使用 LOUDS-Sparse。&lt;/p&gt;&lt;p&gt;对于一个 trie 来说，SuRF 会将其编码成：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8e6a70a350de2199f8292dbbd699d34f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;336&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8e6a70a350de2199f8292dbbd699d34f&quot; data-watermark-src=&quot;v2-ddcacf7aecd4f2ccbbc1218bcab37b86&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;对于一次查询来说，首先会在 LOUDS-Dense 上面查找，如果找到了，就直接返回，找不到，就会进入到 LOUDS-Sparse 进行查找。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;LOUDS-Dense&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LOUDS-Dense 对于每个 Node 都使用了三个 256 bit 大小的 bitmap。第一个 bitmap 叫做 D-labels，如果表示这个 node 是否有 label i，如果有，那么第 i bit 位就是 1。譬如上面的例子，Dense 的 label 在 level 1 有 f，s 和 t，那么在第 102（f），115（s） 和 116 （t）bit 位就会设置为 1。大家其实可以看到，具体哪一个 bit 位，就是 ASCII 码的值。&lt;br&gt;第二个 bitmap 是 D-HasChild，如果一个 node 下面还有子节点，那么就将该 label 对应的 bit 在 D-HasChild 里面设置为 1。继续上面的例子，f 和 t 都有子节点，而 s 没有，所以 102 和 116 bit 都会设置为 1。&lt;/p&gt;&lt;p&gt;第三个 bitmap 是 D-IsPrefixKey，这个解释其实有点绕，主要是用来表示一个 prefix 是否也是一个合法的 key。还是上面的例子，我们可以看到，f 这个 node 是有子节点的，所以它是一个 prefix，但同时，f 也是一个 key。在上图中， SuRF 使用了 ‘$’ 这个符号（实际对应的值是 0xFF）来表示这样的情况。&lt;/p&gt;&lt;p&gt;最后一个字节序列就是 D-Values，存储的是固定大小的 value。Value 就是按照 每层 level 的顺序存放的。&lt;/p&gt;&lt;p&gt;如果要进行遍历 LOUDS-Dense，我们可以使用之前提到的 rank 和 select 操作。对于 bit 序列 &lt;code class=&quot;inline&quot;&gt;bs&lt;/code&gt; 来说，我们用 &lt;code class=&quot;inline&quot;&gt;rank1/select1(bs, pos)&lt;/code&gt; 来表示在 &lt;code class=&quot;inline&quot;&gt;bs&lt;/code&gt; 上面 pos 的 rank 和 select 操作。譬如，假设 pos 是 D-Labels 上面的当前 bit pos，如果 &lt;code class=&quot;inline&quot;&gt;D-HasChild[pos] = 1&lt;/code&gt;，那么第一个子节点的 pos 则是 &lt;code class=&quot;inline&quot;&gt;D-ChildNodePos(pos) = 256 x rank1(D-HasChild, pos)&lt;/code&gt;，而父节点则是 &lt;code class=&quot;inline&quot;&gt;ParentNodePos(pos) = 256 x select1(D-HasChild, pos / 256)&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;LOUDS-Sparse&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;不同于 LOUDS-Dense，LOUDS-Sparse 使用了 bytes 或者 bits 序列来编码。第一个 bytes 序列，S-Labels，按照 level order 的方式记录了所有 node 的 label。譬如上图的 &lt;code class=&quot;inline&quot;&gt;rst&lt;/code&gt; 这样的 bytes 顺序，Sparse 仍然使用了 0xFF（也就是上图的 &lt;code class=&quot;inline&quot;&gt;$&lt;/code&gt; 符号）来表示一个 prefix key。因为这样的 0xFF 只会出现在第一个子节点上面，所以是能跟实际的 0xFF label 进行区分的。&lt;br&gt;第二个 bit 序列就是 S-HasChild, 这个跟 D-HasChild 差不多，就不解释了。&lt;/p&gt;&lt;p&gt;第三个 bit 序列 S-LOUDS 用来表示，如果一个 label 是第一个节点，那么对应的 S-LOUDS 就设置为 1，否则为 0。譬如上图第三层，r，p 和 i 都是第一个节点，那么对应的 S-LOUDS 就设置为 1 了。&lt;/p&gt;&lt;p&gt;最后一个 bytes 序列是 S-Values，跟 D-Values 类似，不再解释了。&lt;/p&gt;&lt;p&gt;如果要便利 Sparse，也是通过 rank 和 select 进行，譬如找到第一个子节点 &lt;code class=&quot;inline&quot;&gt;S-ChildNodePos(pos) = select1(S-LOUDS, ranks(S-HasChild, pos) + 1)&lt;/code&gt;，而找到父节点则是 &lt;code class=&quot;inline&quot;&gt;S-ParentNodePos(pos) = select1(S-HasChild, rank1(S-LOUDS, pos) - 1)&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Optimization&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于 SuRF 来说，为了提高查询的速度，一个重要的优化手段就是提高 rank 和 select 执行的效率，在 SuRF 里面，引入了 LookUp Table（LUT）。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a07fb76d8e444f2c7d17b37492b5f7c6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;242&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a07fb76d8e444f2c7d17b37492b5f7c6&quot; data-watermark-src=&quot;v2-8036fccdff18afcfca8931b23c3129f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;对于 rank 来说，会将 bit vector 切分成 B bits 大小的块，每块都使用 32 bits 的字段来预先保存了计算好的到这个 block 的 rank 值。譬如，在上面的例子，第三个就是 7，保存的就是前两个 block 总的 rank 数量。&lt;/p&gt;&lt;p&gt;而对于一个 pos 来说，它的 &lt;code class=&quot;inline&quot;&gt;rank1(pos) = LUT[i / B] + popcount[i / B * B, i]&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;popcount&lt;/code&gt; 是一个 CPU 指令，用来快速的计算某一段区间的 1 的个数。假设我们现在要得到 pos 12 的 rank 值，先通过 &lt;code class=&quot;inline&quot;&gt;LUT[12 / 5] = LUT[2] = 7&lt;/code&gt;，然后得到 range &lt;code class=&quot;inline&quot;&gt;[12 / 5 * 5, 12] = [10, 12]&lt;/code&gt;，使用 &lt;code class=&quot;inline&quot;&gt;popcount&lt;/code&gt; 得到 2，那么 12 的 rank 就是 9。&lt;br&gt;对于 select 来说，也是使用的 LUT 方法，预先记录算好的值。具体到上面，假设将采样的周期设置为 3，那么第三个 LUT 就保存的是 3 x 2，也就是第 6 的 1 的 pos 值，也就是 8。对于一个 pos 来说，&lt;code class=&quot;inline&quot;&gt;select1(i) = LUT[i / S] + (selecting (i - i / S * S)th set bit starting from LUT[i / S] + 1) + 1&lt;/code&gt;。譬如，如果我们要得到 &lt;code class=&quot;inline&quot;&gt;select1(8)&lt;/code&gt;，首先得到 &lt;code class=&quot;inline&quot;&gt;LUT[8 / 3] = LUT[2] = 8&lt;/code&gt;，然后需要从 position &lt;code class=&quot;inline&quot;&gt;LUT[8 / 3] + 1 = 9&lt;/code&gt; 这个位置，得到第 &lt;code class=&quot;inline&quot;&gt;(8 - 8 / 3 * 3) = 2&lt;/code&gt; 个位置的 bit，也就是 1，所以 &lt;code class=&quot;inline&quot;&gt;select1(8)&lt;/code&gt; 就是 10。&lt;/p&gt;&lt;p&gt;当然，SuRF 还有其它很多优化手段，譬如使用 SIMD 来提速 label 的查找，使用 prefetchj 技术等，这里就不说明了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Succinct Range Filter&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于通常的 SuRF 来说，它因为对这个 trie 都进行了编码，所以它是完全精确的，虽然它是一种省空间的数据结构，但很多时候，我们仍然需要能保证在内存里面存储所有的 SuRF 数据，所以我们就需要对 SuRF 进行裁剪，不存储所有的信息，也就是说，我们需要在查询的 False Positive Rate（FPR）和空间上面做一个权衡。&lt;/p&gt;&lt;p&gt;在 SuRF 里面，有几种方式，Basic，Hash，Real 以及 Mixed。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d99c188249d1164000c0c7b3dadf4f0e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;379&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d99c188249d1164000c0c7b3dadf4f0e&quot; data-watermark-src=&quot;v2-d309450eabe0749b86f8173b32218c2b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;Basic 比较简单，就是直接将最后面的叶子层全部砍掉，这样其实是最省空间的，但 FPR 会比较高。Hash 的方式，则是在最底层，保存了这个 key n bits 位的 hash 值，这样能显著减少 point get 的 FPR，但对于 range 操作则没有任何帮助。&lt;/p&gt;&lt;p&gt;为了解决 Hash range 查询的问题，也可以使用 Real 方式，在最后面继续保存 n bits 位的 key 数据。Real 虽然能处理 point get 和 range，但它的 FPR 其实是比 Hash 要高的。所以我们可以使用 Mixed 方式，将 Hash 和 Real 混合在一起使用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Example&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SuRF 的代码已经开源，大家可以自己从 &lt;a href=&quot;https://github.com/efficient/SuRF&quot;&gt;Github&lt;/a&gt; 获取到，使用起来也非常的简单，下面是一个非常简单的例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;vector&amp;lt;string&amp;gt; words = {
 &quot;f&quot;,
 &quot;farther&quot;,
 &quot;fas&quot;
 &quot;trying&quot;
};

SuRF s(words, true, 16, kNone, 0, 0);

cout &amp;lt;&amp;lt; &quot;Find abc &quot; &amp;lt;&amp;lt; s.lookupKey(&quot;abc&quot;) &amp;lt;&amp;lt; endl;
cout &amp;lt;&amp;lt; &quot;Find trying &quot; &amp;lt;&amp;lt; s.lookupKey(&quot;trying&quot;) &amp;lt;&amp;lt; endl; &lt;/code&gt;&lt;p&gt;上面我创建了一个 SuRF，传入了一批 words，使用了 Full Trie 的模式，然后做了两次点查。&lt;br&gt;具体代码，大家可以自己去研究下，代码质量还是很不错的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Epilogue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SuRF 的研究就暂时到这里结束了，对于 Succinct Data Structure，我个人还是觉得很有意思，可以探究的东西挺多的，毕竟如果能把查询索引全放在内存，不走磁盘，性能还是非常不错的。但我个人毕竟水平有限，仅仅限于了解，所以特别希望能跟业界的大牛多多交流。如果你也对这块很感兴趣，欢迎联系我 &lt;a href=&quot;mailto:tl@pingcap.com&quot;&gt;tl@pingcap.com&lt;/a&gt;。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-22-38385054</guid>
<pubDate>Fri, 22 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>十问 TiDB ：关于架构设计的一些思考</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-19-38254903.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38254903&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e56f6abfcfc0cf1957fada9856bfb193_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者： &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hash=&quot;5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hovercard=&quot;p$b$5940b1ec1c21a3538c6cfcf5711a75a6&quot;&gt;@Ed Huang&lt;/a&gt; &lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;“我希望能够把 TiDB 的设计的一些理念能够更好的传达给大家，相信大家理解了背后原因后，就能够把 TiDB 用的更好。”&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;做 TiDB 的缘起是从思考一个问题开始的：为什么在数据库领域有这么多永远也躲不开的坑？ 从 2015 年我们写下第一行代码，3 年以来我们迎面遇到无数个问题，一边思考一边做，尽量用最小的代价来快速奔跑。&lt;/p&gt;&lt;p&gt;作为一个开源项目，TiDB 是我们基础架构工程师和社区一起努力的结果，TiDB 已经发版到 2.0，有了一个比较稳定的形态，大量在生产环境使用的伙伴们。可以负责任的说，我们做的任何决定都经过了非常慎重的思考和实践，是经过内部和社区一起论证产生的结果。它未必是最好的，但是在这个阶段应该是最适合我们的，而且大家也可以看到 TiDB 在快速迭代进化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这篇文章是关于 TiDB 代表性“为什么”的 TOP 10，希望大家在了解了我们这些背后的选择之后，能更加纯熟的使用 TiDB，让它在适合的环境里更好的发挥价值。&lt;/b&gt;这个世界有很多人，感觉大于思想，疑问多于答案。感恩大家保持疑问，我们承诺回馈我们的思考过程，毕竟有时候很多思考也很有意思。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1. 为什么分布式系统并不是银弹&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;其实并没有什么技术是完美和包治百病的，在存储领域更是如此，如果你的数据能够在一个 MySQL 装下并且服务器的压力不大，或者对复杂查询性能要求不高，其实分布式数据库并不是一个特别好的选择。 选用分布式的架构就意味着引入额外的维护成本，而且这个成本对于特别小的业务来说是不太划算的，即使你说需要高可用的能力，那 MySQL 的主从复制 + GTID 的方案可能也基本够用，这不够的话，还有最近引入的 Group Replication。而且 MySQL 的社区足够庞大，你能 Google 找到几乎一切常见问题的答案。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;我们做 TiDB 的初衷并不是想要在小数据量下取代 MySQL，而是尝试去解决基于单机数据库解决不了的一些本质的问题。&lt;/b&gt; &lt;/p&gt;&lt;p&gt;有很多朋友问我选择分布式数据库的一个比较合适的时机是什么？ 我觉得对于每个公司或者每个业务都不太一样，我并不希望一刀切的给个普适的标准（也可能这个标准并不存在），但是有一些事件开始出现的时候：比如是当你发现你的数据库已经到了你每天开始绞尽脑汁思考数据备份迁移扩容，开始隔三差五的想着优化存储空间和复杂的慢查询，或者你开始不自觉的调研数据库中间件方案时，或者人肉在代码里面做 sharding 的时候，这时给自己提个醒，看看 TiDB 是否能够帮助你，我相信大多数时候应该是可以的。 &lt;/p&gt;&lt;p&gt;而且另一方面，选择 TiDB 和选择 MySQL 并不是一刀切的有你没他的过程，我们为了能让 MySQL 的用户尽可能减小迁移和改造成本，做了大量的工具能让整个数据迁移和灰度上线变得平滑，甚至从 TiDB 无缝的迁移回来，而且有些小数据量的业务你仍然可以继续使用 MySQL。&lt;b&gt;所以一开始如果你的业务和数据量还小，大胆放心的用 MySQL 吧，MySQL still rocks，TiDB 在未来等你。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. 为什么是 MySQL&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;和上面提到的一样，并不是 MySQL 不好我们要取代他，而是选择兼容 MySQL 的生态对我们来说是最贴近用户实际场景的选择：&lt;/p&gt;&lt;p&gt;一，MySQL 的社区足够大，有着特别良好的群众基础，作为一个新的数据库来说，如果需要用户去学习一套新的语法，同时伴随很重的业务迁移的话，是很不利于新项目冷启动的。 &lt;/p&gt;&lt;p&gt;二，MySQL 那么长时间积累下来大量的测试用例和各种依赖 MySQL 的第三方框架和工具的测试用例是我们一个很重要的测试资源，特别是在早期，你如何证明你的数据库是对的，MySQL 的测试就是我们的一把尺子。 &lt;/p&gt;&lt;p&gt;三，已经有大量的已有业务正在使用 MySQL，同时也遇到了扩展性的问题，如果放弃这部分有直接痛点的场景和用户，也是不明智的。&lt;/p&gt;&lt;p&gt;另一方面来看，MySQL 自从被 Oracle 收购后，不管是性能还是稳定性这几年都在稳步的提升，甚至在某些场景下，已经开始有替换 Oracle 的能力，从大的发展趋势上来说，是非常健康的，所以跟随着这个健康的社区一起成长，对我们来说也是一个商业上的策略。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3. 为什么 TiDB 的设计中 SQL 层和存储层是分开的&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一个显而易见的原因是对运维的友好性。很多人觉得这个地方稍微有点反直觉，多一个组件不就会增加部署的复杂度吗？&lt;/p&gt;&lt;p&gt;其实在实际生产环境中，运维并不仅仅包含部署。举个例子，如果在 SQL 层发现了一个 BUG 需要紧急的更新，如果所有部件都是耦合在一起的话，你面临的就是一次整个集群的滚动更新，如果分层得当的话，你可能需要的只是更新无状态的 SQL 层，反之亦然。 &lt;/p&gt;&lt;p&gt;另外一个更深层次的原因是成本。存储和 SQL 所依赖的计算资源是不一样的，存储会依赖 IO，而计算对 CPU 以及内存的要求会更高，无需配置 PCIe/NVMe/Optane 等磁盘，而且这两者是不一定对等的，如果全部耦合在一起的话，对于资源调度是不友好的。 对于 TiDB 来说，目标定位是支持 HTAP，即 OLTP 和 OLAP 需要在同一个系统内部完成。显然，不同的 workload 即使对于 SQL 层的物理资源需求也是不一样的，OLAP 请求更多的是吞吐偏好型以及长 query，部分请求会占用大量内存，而 OLTP 面向的是短平快的请求，优化的是延迟和 OPS (operation per second)，在 TiDB 中 SQL 层是无状态的，所以你可以将不同的 workload 定向到不同的物理资源上做到隔离。&lt;b&gt;还是那句话，对调度器友好，同时调度器的升级也不需要把整个集群全部升级一遍。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;另一方面，底层存储使用 KV 对数据进行抽象，是一个更加灵活的选择。&lt;br&gt;一个好处是简单。对于 Scale-out 的需求，对 KV 键值对进行分片的难度远小于对带有复杂的表结构的结构化数据，另外，存储层抽象出来后也可以给计算带来新的选择，比如可以对接其他的计算引擎，和 TiDB SQL 层同时平行使用，TiSpark 就是一个很好的例子。 &lt;/p&gt;&lt;p&gt;从开发角度来说，这个拆分带来的灵活度还体现在可以选择不同的编程语言来开发。对于无状态的计算层来说，我们选择了 Go 这样开发效率极高的语言，而对于存储层项目 TiKV 来说，是更贴近系统底层，对于性能更加敏感，所以我们选择了 Rust，如果所有组件都耦合在一起很难进行这样的按需多语言的开发，对于开发团队而言，也可以实现专业的人干专业的事情，存储引擎的开发者和 SQL 优化器的开发者能够并行的开发。 另外对于分布式系统来说，所有的通信几乎都是 RPC，所以更明确的分层是一个很自然的而且代价不大的选择。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4.&lt;/b&gt; &lt;b&gt;为什么不复用 MySQL 的 SQL 层，而是选择自己重写&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这点是我们和很多友商非常不一样的地方。 目前已有的很多方案，例如 Aurora 之类的，都是直接基于 MySQL 的源码，保留 SQL 层，下面替换存储引擎的方式实现扩展，这个方案有几个好处：一是 SQL 层代码直接复用，确实减轻了一开始的开发负担，二是面向用户这端确实能做到 100% 兼容 MySQL 应用。&lt;/p&gt;&lt;p&gt;但是缺点也很明显，MySQL 已经是一个 20 多年的老项目，设计之初也没考虑分布式的场景，整个 SQL 层并不能很好的利用数据分布的特性生成更优的查询计划，虽然替换底层存储的方案使得存储层看上去能 Scale，但是计算层并没有，在一些比较复杂的 Query 上就能看出来。另外，如果需要真正能够大范围水平扩展的分布式事务，依靠 MySQL 原生的事务机制还是不够的。&lt;/p&gt;&lt;p&gt;自己重写整个 SQL 层一开始看上去很困难，但其实只要想清楚，有很多在现代的应用里使用频度很小的语法，例如存储过程什么的，不去支持就好了，至少从 Parser 这层，工作量并不会很大。 同时优化器这边自己写的好处就是能够更好的与底层的存储配合，另外重写可以选择一些更现代的编程语言和工具，使得开发效率也更高，从长远来看，是个更加省事的选择。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;5. 为什么用 RocksDB 和 Etcd Raft&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;很多工程师都有着一颗造轮子（玩具）的心，我们也是，但是做一个工业级的产品就完全不一样了，目前的环境下，做一个新的数据库，底层的存储数据结构能选的大概就两种：1. B+Tree,  2. LSM-Tree。&lt;/p&gt;&lt;p&gt;但是对于 B+Tree 来说每个写入，都至少要写两次磁盘： 1.  在日志里； 2. 刷新脏页的时候，即使你的写可能就只改动了一个 Byte，这个 Byte 也会放大成一个页的写 （在 MySQL 里默认 InnoDB 的 Page size 是 16K），虽然说 LSM-Tree 也有写放大的问题，但是好处是 LSM-tree 将所有的随机写变成了顺序写（对应的 B+tree 在回刷脏页的时候可能页和页之间并不是连续的）。 另一方面，LSMTree 对压缩更友好，数据存储的格式相比 B+Tree 紧凑得多，Facebook 发表了一些关于 MyRocks 的文章对比在他们的 MySQL 从 InnoDB 切换成 MyRocks （Facebook 基于 RocksDB 的 MySQL 存储引擎）节省了很多的空间。所以 LSM-Tree 是我们的选择。&lt;/p&gt;&lt;p&gt;选择 RocksDB 的出发点是 RocksDB 身后有个庞大且活跃的社区，同时 RocksDB 在 Facebook 已经有了大规模的应用，而且 RocksDB 的接口足够通用，并且相比原始的 LevelDB 暴露了很多参数可以进行针对性的调优。随着对于 RocksDB 理解和使用的不断深入，我们也已经成为 RocksDB 社区最大的使用者和贡献者之一，另外随着 RocksDB 的用户越来越多，这个项目也会变得越来越好，越来越稳定，可以看到在学术界很多基于 LSM-Tree 的改进都是基于 RocksDB 开发的，另外一些硬件厂商，特别是存储设备厂商很多会针对特定存储引擎进行优化，RocksDB 也是他们的首选之一。&lt;/p&gt;&lt;p&gt;反过来，自己开发存储引擎的好处和问题同样明显，一是从开发到产品的周期会很长，而且要保证工业级的稳定性和质量不是一个简单的事情，需要投入大量的人力物力。好处是可以针对自己的 workload 进行定制的设计和优化，由于分布式系统天然的横向扩展性，单机有限的性能提升对比整个集群吞吐其实意义不大，把有限的精力投入到高可用和扩展性上是一个更加经济的选择。 另一方面，RocksDB 作为 LSM-Tree 其实现比工业级的 B+Tree 简单很多（参考对比 InnoDB），从易于掌握和维护方面来说，也是一个更好的选择。 当然，随着我们对存储的理解越来越深刻，发现很多专门针对数据库的优化在 RocksDB 上实现比较困难，这个时候就需要重新设计新的专门的引擎，就像 CPU 也能做图像处理，但远不如 GPU，而 GPU 做机器学习又不如专用的 TPU。&lt;/p&gt;&lt;p&gt;选择 Etcd Raft 的理由也类似。先说说为什么是 Raft，在 TiDB 项目启动的时候，我们其实有过在 MultiPaxos 和 Raft 之间的纠结，后来结论是选择了 Raft。Raft 的算法整体实现起来更加工程化，从论文就能看出来，论文中甚至连 RPC 的结构都描述了出来，是一个对工业实现很友好的算法，而且当时工业界已经有一个经过大量用户考验的开源实现，就是 Etcd。而且 Etcd 更加吸引我们的地方是它对测试的态度，Etcd 将状态机的各个接口都抽象得很好，基本上可以做到与操作系统的 API 分离，极大降低了写单元测试的难度，同时设计了很多 hook 点能够做诸如错误注入等操作，看得出来设计者对于测试的重视程度。&lt;/p&gt;&lt;p&gt;与其自己重新实现一个 Raft，不如借力社区，互相成长。现在我们也是 Etcd 社区的一个活跃的贡献者，一些重大的 Features 例如 Learner 等新特性，都是由我们设计和贡献给 Etcd 的，同时我们还在不断的为 Etcd 修复 Bug。&lt;/p&gt;&lt;p&gt;没有完全复用 Etcd 的主要的原因是我们存储引擎的开发语言使用了 Rust，Etcd 是用 Go 写的，我们需要做的一个工作是将他们的 Raft 用 Rust 语言重写，为了完成这个事情，我们第一步是将 Etcd 的单元测试和集成测试先移植过来了（没错，这个也是选择 Etcd 的一个很重要的原因，有一个测试集作为参照），以免移植过程破坏了正确性。另外一方面，就如同前面所说，和 Etcd 不一样，TiKV 的 Raft 使用的是 Multi-Raft 的模型，同一个集群内会存在海量的互相独立 Raft 组，真正复杂的地方在如何安全和动态的分裂，移动及合并多个 Raft 组，我在我的 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247483881&amp;amp;idx=1&amp;amp;sn=f42cf3e23cd229a7b12cbf5d2116ecf8&amp;amp;scene=21#wechat_redirect&quot;&gt;这篇文章&lt;/a&gt; 里面描述了这个过程。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;6. 为什么有这样的硬件配置要求&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们其实对生产环境硬件的要求还是蛮高的，对于存储节点来说，SSD 或者 NVMe 或者 Optane 是刚需，另外对 CPU 及内存的使用要求也很高，同时对大规模的集群，网络也会有一些要求 （详见我们的官方文档推荐配置的 &lt;a href=&quot;https://pingcap.com/docs-cn/FAQ/&quot;&gt;相关章节&lt;/a&gt; )，其中一个很重要的原因是我们底层的选择了 RocksDB 的实现，对于 LSM Tree 来说因为存在写放大的天然特性，对磁盘吞吐需求会相应的更高，尤其是 RocksDB 还有类似并行 Compaction 等特性。 而且大多数机械磁盘的机器配置倾向于一台机器放更大容量的磁盘（相比 SSD），但是相应的内存却一般来说不会更大，例如 24T 的机械磁盘 + 64G 内存，磁盘存储的数据量看起来更大，但是大量的随机读会转化为磁盘的读，这时候，机械磁盘很容易出现 IO 瓶颈，另一方面，对于灾难恢复和数据迁移来说，也是不太友好的。&lt;/p&gt;&lt;p&gt;另外，TiDB 的各个组件目前使用 gRPC 作为 RPC 框架，gRPC 是依赖 &lt;a href=&quot;https://developers.google.com/web/fundamentals/performance/http2/&quot;&gt;HTTP2&lt;/a&gt; 作为底层协议，相比很多朴素的 RPC 实现，会有一些额外的 CPU 开销。TiKV 内部使用 RocksDB 的方式会伴随大量的 Prefix Scan，这意味着大量的二分查找和字符串比较，这也是和很多传统的离线数据仓库很不一样的 Pattern，这个会是一个 CPU 密集型的操作。在 TiDB 的 SQL 层这端，SQL 是计算密集型的应用这个自然不用说，另外对内存也有一定的需求。由于 TiDB 的 SQL 是一个完整的 SQL 实现，表达力和众多中间件根本不是一个量级，有些算子，比如 Hashjoin，就是会在内存里开辟一块大内存来执行 Join，所以如果你的查询逻辑比较复杂，或者 Join 的一张子表比较大的情况下（偏 OLAP 实时分析业务），对内存的需求也是比较高的，我们并没有像单机数据库的优化器一样，比如 Order by 内存放不下，就退化到磁盘上，我们的哲学是尽可能的使用内存。 如果硬件资源不足，及时的通过拒绝执行和失败通知用户，因为有时候半死不活的系统反而更加可怕。&lt;/p&gt;&lt;p&gt;另外一方面，还有很多用户使用 TiDB 的目的是用于替换线上 OLTP 业务，这类业务对于性能要求是比较高的。 一开始我们并没有在安装阶段严格检查用户的机器配置，结果很多用户在硬件明显没有匹配业务压力的情况下上线，可能一开始没什么问题，但是峰值压力一上来，可能就会造成故障，尽管 TiDB 和 TiKV 对这种情况做了层层的内部限流，但是很多情况也无济于事。 &lt;b&gt;所以我们决定将配置检查作为部署脚本的强制检查，一是减少了很多沟通成本，二是可以让用户在上线时尽可能的减少后顾之忧。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;7. 为什么用 Range-based 的分片策略，而不是 Hash-based&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Hash-based 的问题是实现有序的 Scan API 会比较困难，我们的目标是实现一个标准的关系型数据库，所以会有大量的顺序扫描的操作，比如 Table Scan，Index Scan 等。用 Hash 分片策略的一个问题就是，可能同一个表的数据是不连续的，一个顺序扫描即使几行都可能会跨越不同的机器，所以这个问题上没得选，只能是 Range 分片。 但是 Range 分片可能会造成一些问题，比如频繁读写小表问题以及单点顺序写入的问题。 &lt;b&gt;在这里首先澄清一下，静态分片在我们这样的系统里面是不存在的，&lt;/b&gt;例如传统中间件方案那样简单的将数据分片和物理机一一对应的分片策略会造成：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;动态添加节点后，需要考虑数据重新分布，这里必然需要做动态的数据迁移；&lt;/li&gt;&lt;li&gt;静态分片对于根据 workload 实时调度是不友好的，例如如果数据存在访问热点，系统需要能够快速进行数据迁移以便于将热点分散在不同的物理服务商上。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;回到刚才提到的基于 Range 分片的问题，刚才我说过，对于顺序写入热点的问题确实存在，但也不是不可解。对于大压力的顺序写入的场景大多数是日志或者类似的场景，这类场景的典型特点是读写比悬殊（读 &amp;lt;&amp;lt; 写），几乎没有 Update 和随机删除，针对这种场景，写入压力其实可以通过 Partition Table 解决，&lt;b&gt;这个已经在 TiDB 的开发路线图里面，今年之内会和大家见面。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;另外还有一个频繁读写小表造成的热点问题。这个原因是，在底层，TiDB 的数据调度的最小单位是 Region，也就是一段段按字节序排序的键值 Key-Value Pairs （默认大小 96M），假设如果一个小表，总大小连 96M 都不到，访问还特别频繁，按照目前的机制，如果不强制的手动 Split，调度系统无论将这块数据调度到什么位置，新的位置都会出现热点，所以这个问题本质上是无解的。因此建议如果有类似的访问 pattern，尽可能的将通用的小表放到 Redis 之类的内存缓存中，或者甚至直接放在业务服务的内存里面（反正小）。  &lt;/p&gt;&lt;h2&gt;&lt;b&gt;8. 为什么性能（延迟）不是唯一的评价标准&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;很多朋友问过我，TiDB 能替换 Redis 吗？大家对 Redis 和 TiDB 的喜爱之情我也很能理解，但是很遗憾，&lt;b&gt;TiDB 并不是一个缓存服务，它支持跨行强一致事务，在非易失设备上实现持久化存储，而这些都是有代价的。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;简单来说，写磁盘的 IO 开销 （WAL，持久化），多副本高可用和保证分布式事务必然会牺牲延迟，更不用说做跨数据中心的同步了，在这点上，我认为如果需要很低延迟的响应速度（亚毫秒级）就需要在业务端做缓存了。TiDB 的定位是给业务提供一个可扩展的 The Source of Truth （真相之源），即使业务层的缓存失效，也有一个地方能够提供强一致的数据，而且业务不用关心容量问题。另一方面&lt;b&gt;衡量一个分布式系统更有意义的指标是吞吐，这个观点我在很多文章里已经提到过，提高并发度，如果系统的吞吐能够随着集群机器数量线性提升，而且延迟是稳定的才有意义，而且这样才能有无限的提升空间。&lt;/b&gt;在实际的环境中，单个 TiDB 集群已经有一些用户使用到了百万级别的 QPS，这个在单机架构上是几乎不可能实现的。另外，这几年硬件的进步速度非常快，特别是 IO 相关的创新，比如 NVMe SSD 的普及，还有刚刚商用的持久化内存等新的存储介质。很多时候我们在软件层面上绞尽脑汁甚至牺牲代码的优雅换来一点点性能提升，很可能换块磁盘就能带来成倍的提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们公司内部有一句话：Make it right before making it fast。正确性和可靠性的位置是在性能之前的，毕竟在一个不稳定的系统上谈性能是没有意义的。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;9. 为什么弹性伸缩能力如此重要&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在业务初期，数据量不大，业务流量和压力不大的时候，基本随便什么数据库都能够搞定，但很多时候业务的爆发性增长可能是没有办法预期的，特别是一些 ToC 端的应用。早期的 Twitter 用户一定对时不时的大鲸鱼（服务不可用）深恶痛绝，近一点还有前两年有一段时间爆红的足记 App，很短的时间之内业务和数据量爆发性增长，数据库几乎是所有这些案例中的核心瓶颈。 很多互联网的 DBA 和年轻的架构师会低估重构业务代码带来的隐形成本，在业务早期快速搞定功能和需求是最重要的。想象一下，业务快速增长，服务器每天都因为数据库过载停止服务的时候，DBA 告诉你没办法，先让你重新去把你的业务全改写成分库分表的形式，在代码里到处加 Sharding key，牺牲一切非 Sharding key 的多维关联查询和相关的跨 Shard 的强一致事务，然后数据复制好多份……这种时候是真正的时间等于金钱，决定这个公司生死存亡的时候不是去写业务和功能代码，而是因为基础设施的限制被迫重构，其实是非常不好的。 如果这个时候，有一个方案，能够让你几乎无成本的，不修改业务代码的时候对数据库吞吐进行线性扩展（无脑加机器其实是最便宜的），最关键的是为了业务的进化争取了时间，我相信这个选择其实一点都不难做。&lt;/p&gt;&lt;p&gt;&lt;b&gt;其实做 TiDB 的初心正是如此，我们过去见到了太多类似的血和泪，实在不能忍了，分库分表用各种中间件什么的炫技是很简单，但是我们想的是真正解决很多开发者和 DBA 眼前的燃眉之急。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最近这段时间，有两个用户的例子让我印象很深，也很自豪，一个是 Mobike，一个是转转，前者是 TiDB 的早期用户，他们自己也在数据增长很快的时候就开始使用 TiDB，在快速的发展过程中没有因为数据库的问题掉链子；后者是典型的快速发展的互联网公司，一个 All-in TiDB 的公司，这个早期的技术选型极大的解放了业务开发的生产力，让业务能够更放开手脚去写业务代码，而不是陷入无休止的选择 Sharding key，做读写分离等等和数据库较劲的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;为业务开发提供更加灵活便捷和低成本的智能基础存储服务，是我们做 TiDB 的出发点和落脚点，分布式/高可用/方便灵活的编程接口/智能省心，这些大的方向上也符合未来主流的技术发展趋势。&lt;/b&gt;对于CEO 、 CTO 和架构师这类的管理者而言，在解决眼前问题的同时，跟随大的技术方向，不给未来多变的业务埋坑，公司尽可能快速发展，这个才是核心要去思考的问题。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;10. 如何根据自己的实际情况参考业内的使用案例&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 是一个通用的数据库，甚至希望比一般的数据库更加通用，TiDB 是很早就尝试融合 OLTP 和 OLAP 的边界的数据库产品，我们是最早将 HTAP 这个概念从实验室和论文里带到现实的产品之一。这类通用基础软件面临的一个问题就是我们在早期其实很难去指导垂直行业的用户把 TiDB 用好，毕竟各自领域都有各自的使用场景和特点，TiDB 的开发团队的背景大部分是互联网行业，所以天然的会对互联网领域的架构和场景更熟悉，但是比如在金融，游戏，电商，传统制造业这些行业里其实我们不是专家，不过现在都已经有很多的行业专家和开发者已经能将 TiDB 在各自领域用得很好。&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们的 Blog，公众号，官网等平台会作为一个案例分享的中心，欢迎各位正在使用 TiDB 的用户，将你们的思考和使用经验分享给我们，就像现在已有案例背后的许多公司一样，我们会将你们的经验变成一篇篇的用户案例，通过我们的平台分享给其他的正在选型的公司。&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-19-38254903</guid>
<pubDate>Tue, 19 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>Succinct Data Structure</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-18-38194127.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38194127&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ff9172b4ded7b8341521e52359f5736f_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：唐刘&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;最近看了一篇论文 &lt;a href=&quot;https://db.cs.cmu.edu/papers/2018/mod601-zhangA-hm.pdf&quot;&gt;SuRF: Practical Range Query Filtering with Fast Succinct Tries&lt;/a&gt;，里面提到使用一种新的数据结构 Succinct Range Filter(SuRF) 替换掉了 RocksDB 默认的 Bloom filter， 在一些性能测试上面，尤其是 seek 上面，性能提升了不少，并且也降低了很多 I/O 开销，这一下子就引起了我的兴趣。&lt;/p&gt;&lt;p&gt;大家都知道，RocksDB 里面，为了加速 key 查询的速度，使用了 Bloom filter，但 Bloom filter 只适用于 point get，对于 seek 就无能为力了。虽然 RocksDB 后面引入了 &lt;a href=&quot;https://github.com/facebook/rocksdb/wiki/Prefix-Seek-API-Changes&quot;&gt;prefix seek&lt;/a&gt;，但对于 key 的格式有要求，使用比较受限。为了提高 RocksDB range query 的速度，论文的作者引入了一种省空间的数据结构，也就是 SuRF。&lt;/p&gt;&lt;p&gt;在了解 SuRF 之前，首先要了解掌握的就是 Succinct data structure 相关的知识，这篇文章主要是讲 Succinct data structure 相关的东西，后面再讨论 SuRF 如何实现的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Rank 和 Select&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Succinct data structure 第一次提出，应该是 Guy Jacobson 的论文 &quot;Succinct static data structures&quot;，但实话，我在网上找了半天，都没找到这篇 paper，只是找到了作者另一篇 &lt;a href=&quot;https://www.computer.org/csdl/proceedings/focs/1989/1982/00/063533.pdf&quot;&gt;Space-efficient static trees and graphs&lt;/a&gt;。它的主要思想就是使用非常少量的空间（接近信息编码的下界）来存储数据。你可以认为就是使用了一种非常高效的压缩算法，但不同于压缩，它同时来提供非常高效的查询。&lt;/p&gt;&lt;p&gt;对于 Succinct data structure 来说，我们会将数据按 0 和 1 来编码，所以可以用 bits，而不是 bytes。操作 succinct 数据，通常的就是几个操作函数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;rank1(x)&lt;/code&gt; - 返回在 range &lt;code class=&quot;inline&quot;&gt;[0, x] &lt;/code&gt;里面 1 的个数&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;select1(y)&lt;/code&gt; - 返回第 y 个 1 所在的位置&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面我们只是列举了 rank1 和 select1，对应的也有 rank0 和 select0，这里就不需要解释了。这么说有点过于抽象，这里举一个简单的例子。假设我们有一个 bits 序列 &lt;code class=&quot;inline&quot;&gt;11000001&lt;/code&gt;，那么 rank1 和 select1 可以映射如下：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-72b4ddac338988955659d310baa8fb06_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;627&quot; data-rawheight=&quot;154&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-72b4ddac338988955659d310baa8fb06&quot; data-watermark-src=&quot;v2-e1efc8d4d59f1d646f99911c5336ba78&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;另外，大家可以注意到，rank 和 select 其实是相反的，上面的例子，&lt;code class=&quot;inline&quot;&gt;select1(3) = 7&lt;/code&gt;，然后我们也会发现，&lt;code class=&quot;inline&quot;&gt;rank1(7) = 3&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Level Order Unary Degree Sequence&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面简单介绍了下 Succinct data structure 的 rank 和 select。 在 SuRF 里面，它参考的基础编码方式，是 Level order unary degree sequence(LOUDS)，在 LOUDS 里面，我们会将一颗树，分层依次进行编码。而规则也是非常的简单，如果这个树的节点有 N 个子节点，那么就用 N 个 1 来编码，然后最后加上 0。&lt;/p&gt;&lt;p&gt;假设我们有如下的 tree：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-db87a1aa60b4a0a3faf5e95b4b8d5487_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;628&quot; data-rawheight=&quot;418&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-db87a1aa60b4a0a3faf5e95b4b8d5487&quot; data-watermark-src=&quot;v2-cc129d1610a5eabd1c020d73c88d82e4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;为了计算简单，LOUDS 会加入一个 pseudo root 节点，这里我们变成如下的 tree：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9d23f7729fb2bb3d41fbaaa614c8af52_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;625&quot; data-rawheight=&quot;524&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9d23f7729fb2bb3d41fbaaa614c8af52&quot; data-watermark-src=&quot;v2-ce54c62bc874167f09514a4cd17ff9df&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;然后我们对这个 tree 进行编码，得到：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-82332386cb20c9cdbc75bf0307ccb1e4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;625&quot; data-rawheight=&quot;523&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-82332386cb20c9cdbc75bf0307ccb1e4&quot; data-watermark-src=&quot;v2-3f2e25516bb2a09b69e7d647c26fb76f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;那么生成的 bits 序列为：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-612fed57670c2200ed027168d0da223f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;695&quot; data-rawheight=&quot;229&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-612fed57670c2200ed027168d0da223f&quot; data-watermark-src=&quot;v2-0fa3ee7e129088f6ed765c1545e8d0fc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;那么我们拿到了这一个序列到底有什么用呢？在 LOUDS 里面，我们可以非常方便的进行很多操作，假设我们的 node 就是按照上面的，0，1，2，这样的 number 来标记的，position 对应的就是 bits 里面的 position。我们通常会用两个计算公式来得到 node number 和 position 的对应关系：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;node-num = rank1(i)&lt;/code&gt;：在 position i 得到对应的 node number，譬如 &lt;code class=&quot;inline&quot;&gt;rank1(2) = 2&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;i = select1(node-num)&lt;/code&gt;，根据 node number，知道对应的 position，譬如 &lt;code class=&quot;inline&quot;&gt;select1(2) = 2&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有了上面的公式，我们就能对这个 tree 进行操作了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;first_child(i) = select0(rank1(i)) + 1&lt;/code&gt; - 得到第 i 个位置所在节点的第一个子节点所在的 position&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;last_child(i) = select0(rank1(i) + 1) - 1&lt;/code&gt; - 得到第 i 个位置所在节点的最后一个子节点所在的 position&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;parent(i) = select1(rank0(i))&lt;/code&gt; - 得到第 i 个位置所在节点的父节点所在的 position&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;children(i) = last_child(i) - first_child(i) + 1&lt;/code&gt; - 得到第 i 个位置所在节点的子节点的个数&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;child(i, num) = first_child(i) + num&lt;/code&gt; 得到第 i 个位置所在节点的第 num 个子节点所在的 position，&lt;code class=&quot;inline&quot;&gt;num &amp;gt;= 0&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面这些公式感觉好绕，那么我们来一个简单的例子，以节点 4 为例。从上面的 tree 可以知道，4 的 parent node 是 1，它的第一个子节点是 7，最后一个是 8，总共有两个子节点。&lt;/p&gt;&lt;p&gt;首先我们需要计算节点 4 的位置，根据上面的公式 &lt;code class=&quot;inline&quot;&gt;select1(4)&lt;/code&gt; 我们得到 position 是 4。那么第一个子节点位置就是 &lt;code class=&quot;inline&quot;&gt;first_child(4) = select0(rank1(4)) + 1 = select0(4) + 1 = 9 + 1 = 10&lt;/code&gt;，那么第一个子节点就是 &lt;code class=&quot;inline&quot;&gt;rank1(10) = 7&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;我们再来计算最后一个子节点，根据公式，最后一个就是 &lt;code class=&quot;inline&quot;&gt;last_child(4) = select0(rank1(4) + 1) - 1 = select0(4 + 1) - 1 = 12 - 1 = 11&lt;/code&gt;，那么最后一个子节点就是 &lt;code class=&quot;inline&quot;&gt;rank1(11) = 8&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;再来看看父节点，就是 &lt;code class=&quot;inline&quot;&gt;parent(4) = select1(rank0(4)) = select1(1) = 0&lt;/code&gt;，那么父节点就是 &lt;code class=&quot;inline&quot;&gt;rank1(0) = 1&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Epilogue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;使用 LODUS，我们可以用 bits 方便的编码一棵树，然后用 rank 和 select 操作，就能方便的对 tree 进行遍历，业内已经有很多 paper，能将 rank 和 select 做到 O(1) 的开销，所以速度还是很快的。&lt;/p&gt;&lt;p&gt;但在实际中，如果光用 LODUS，性能还是没法保证的，所以这也是为啥会有 SuRF 的原因，关于 SuRF，后面会在说明。&lt;/p&gt;&lt;p&gt;在数据库领域，Succinct 是一个比较有趣的研究方向，也有很多数据库采用了 succinct 来保存数据，毕竟如果能用更少的空间存放数据，memory 能装的更多，cache 更友好，性能就更好。但现在 succinct 还没有大规模的落地，可以看看后续的发展。如果你对构建新的存储引擎有兴趣，欢迎联系我 &lt;a href=&quot;mailto:tl@pingcap.com&quot;&gt;tl@pingcap.com&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/36781efac8e9&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-18-38194127</guid>
<pubDate>Mon, 18 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十）Chunk 和执行框架简介</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-14-38095421.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38095421&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-23b3db33eab46a4f9f2fa39360197f04_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：@张建&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;什么是 Chunk&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 2.0 中，我们引入了一个叫 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L32&quot;&gt;Chunk&lt;/a&gt; 的数据结构用来在内存中存储内部数据，用于减小内存分配开销、降低内存占用以及实现内存使用量统计/控制，其特点如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;只读&lt;/li&gt;&lt;li&gt;不支持随机写&lt;/li&gt;&lt;li&gt;只支持追加写&lt;/li&gt;&lt;li&gt;列存，同一列的数据连续的在内存中存放&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Chunk 本质上是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L320&quot;&gt;Column&lt;/a&gt; 的集合，它负责连续的在内存中存储同一列的数据，接下来我们看看 Column 的实现。&lt;br&gt;&lt;br&gt;&lt;b&gt;1. Column&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Column 的实现参考了 Apache Arrow，Column 的代码在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L320&quot;&gt;这里&lt;/a&gt;。根据所存储的数据类型，我们有两种 Column：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;定长 Column：存储定长类型的数据，比如 &lt;code class=&quot;inline&quot;&gt;Double&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;Bigint&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;Decimal&lt;/code&gt; 等&lt;/li&gt;&lt;li&gt;变长 Column：存储变长类型的数据，比如 &lt;code class=&quot;inline&quot;&gt;Char&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;Varchar&lt;/code&gt; 等&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;哪些数据类型用定长 Column，哪些数据类型用变长 Column 可以看函数 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L90&quot;&gt;addColumnByFieldType&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;Column 里面的字段非常多，这里先简单介绍一下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;length &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;用来表示这个 Column 有多少行数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;nullCount&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;用来表示这个 Column 中有多少 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;nullBitmap&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;用来存储这个 Column 中每个元素是否是 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt;，需要特殊注意的是我们使用 0 表示 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt;，1 表示非 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt;，和 Apache Arrow 一样。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;data&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;存储具体的数据，不管定长还是变长的 Column，所有的数据都存储在这个 byte slice 中。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;offsets&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;给变长的 Column 使用，存储每个数据在 data 这个 slice 中的偏移量。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;elemBuf&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;给定长的 Column 使用，当需要读或者写一个数据的时候，使用它来辅助 encode 和 decode。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1  追加一个定长的非 NULL 值&lt;/b&gt;&lt;/p&gt;&lt;p&gt;追加一个元素需要根据具体的数据类型调用具体的 append 方法，比如： &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L378&quot;&gt;appendInt64&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L404&quot;&gt;appendString&lt;/a&gt; 等。&lt;br&gt;一个定长类型的 Column 可以用如下图表示:&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0f164200b3563b7433656d3fcd1c5174_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;634&quot; data-rawheight=&quot;156&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0f164200b3563b7433656d3fcd1c5174&quot; data-watermark-src=&quot;v2-0a1ce58f54f86188b4976e81cbdb241d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;我们以 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L378&quot;&gt;appendInt64&lt;/a&gt; 为例来看看如何追加一个定长类型的数据：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;code class=&quot;inline&quot;&gt;unsafe.Pointer&lt;/code&gt; 把要 append 的数据先复制到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L326&quot;&gt;elemBuf&lt;/a&gt; 中；&lt;/li&gt;&lt;li&gt;将 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L326&quot;&gt;elemBuf&lt;/a&gt; 中的数据 append 到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L325&quot;&gt;data&lt;/a&gt; 中；&lt;/li&gt;&lt;li&gt;往 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L323&quot;&gt;nullBitmap&lt;/a&gt; 中 append 一个 1。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面第 1 步在 &lt;code class=&quot;inline&quot;&gt;appendInt64&lt;/code&gt; 这个函数中完成，第 2、3 步在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L372&quot;&gt;finishAppendFixed&lt;/a&gt; 这个函数中完成。其他定长类型元素的追加操作非常相似，感兴趣的同学可以接着看看 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L388&quot;&gt;appendFloat32&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L414&quot;&gt;appendTime&lt;/a&gt; 等函数。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2  追加一个变长的非 NULL 值&lt;/b&gt;&lt;/p&gt;&lt;p&gt;而一个变长的 Column 可以用下图表示：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c01c278418cd2f8f15f2953bd3ef5a76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;196&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c01c278418cd2f8f15f2953bd3ef5a76&quot; data-watermark-src=&quot;v2-18a8be583d750b82fb3b7956c619f0df&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;我们以 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L404&quot;&gt;appendString&lt;/a&gt; 为例来看看如何追加一个变长类型的数据：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;把数据先 append 到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L325&quot;&gt;data&lt;/a&gt; 中；&lt;/li&gt;&lt;li&gt;往 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L323&quot;&gt;nullBitmap&lt;/a&gt; 中 append 一个 1；&lt;/li&gt;&lt;li&gt;往 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L324&quot;&gt;offsets&lt;/a&gt; 中 append 当前 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L325&quot;&gt;data&lt;/a&gt; 的 size 作为下一个元素在 data 中的起始点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面第 1 步在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L404&quot;&gt;appendString&lt;/a&gt; 这个函数中完成，第 2、3 步在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L398&quot;&gt;finishAppendVar&lt;/a&gt; 这个函数中完成。其他边长类型元素的追加操作也是非常相似，感兴趣的同学可以接着看看 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L409&quot;&gt;appendBytes&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L449&quot;&gt;appendJSON&lt;/a&gt; 等函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3  追加一个 NULL 值&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们使用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L362&quot;&gt;appendNull&lt;/a&gt; 函数来向一个 Column 中追加一个 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;往 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L323&quot;&gt;nullBitmap&lt;/a&gt; 中 append 一个 0；&lt;/li&gt;&lt;li&gt;如果是定长 Column，需要往 data 中 append 一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L326&quot;&gt;elemBuf&lt;/a&gt; 长度的数据，用来占位；&lt;/li&gt;&lt;li&gt;如果是变长 Column，不用往 data中 append 数据，而是往 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L324&quot;&gt;offsets&lt;/a&gt; 中 append 当前 data 的 size 作为下一个元素在 data 中的起始点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. Row&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e955b29d2d3f711028fb35eacb4da90a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;301&quot; data-rawheight=&quot;347&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e955b29d2d3f711028fb35eacb4da90a&quot; data-watermark-src=&quot;v2-b3e0dacdc44654138402f51bc6dc3367&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;如上图所示，Chunk 中的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L456&quot;&gt;Row&lt;/a&gt; 是一个逻辑上的概念：Row 中的数据存储在 Chunk 的各个 Column 中，同一个 Row 中的数据在内存中没有连续存储在一起，我们在获取一个 Row 对象的时候也不需要进行数据拷贝。提供 Row 的概念是因为算子运行过程中，大多数情况都是以 Row 为单位访问和操作数据，比如聚合，排序等。 &lt;/p&gt;&lt;p&gt;Row 提供了获取 Chunk 中数据的方法，比如 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L472&quot;&gt;GetInt64&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L496&quot;&gt;GetString&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L563&quot;&gt;GetMyDecimal&lt;/a&gt; 等，前面介绍了往 Column 中 append 数据的方法，获取数据的方法可以由 append 数据的方法反推，代码也比较简单，这里就不再详细介绍了。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;3. 使用&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前 Chunk 这个包只对外暴露了 Chunk, Row 等接口，而没有暴露 Column，所以，写数据调用的是在 Chunk 上实现的对 Column 具体函数的 warpper，比如 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L230&quot;&gt;AppendInt64&lt;/a&gt;；读数据调用的是在 Row 上实现的 Getxxx 函数，比如 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L472&quot;&gt;GetInt64&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;执行框架简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 老执行框架简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在重构前，TiDB 1.0 中使用的执行框架会不断调用 Child 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/executor.go#L191&quot;&gt;Next&lt;/a&gt; 函数获取一个由 Datum 组成的 Row（和刚才介绍的 Chunk Row 是两个数据结构），这种执行方式的特点是：每次函数调用只返回一行数据，且不管是什么类型的数据都用 Datum 这个结构体来封装。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1271c308a21b086e23c50728d83eafc5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;192&quot; data-rawheight=&quot;380&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;这种方法的优点是简单、易用。缺点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果处理的数据量多，那么框架上的函数调用开销将会非常大；&lt;/li&gt;&lt;li&gt;Datum 占用的无效内存太大，内存浪费比较多（存一个 8 字节的整数需要 56 字节）；&lt;/li&gt;&lt;li&gt;Datum 没有重用，golang 的 gc 压力大；&lt;/li&gt;&lt;li&gt;每个 Operator 一次只输出一行数据，要进行更加缓存友好的计算、更充分的利用 CPU 的 pipeline 非常困难；&lt;/li&gt;&lt;li&gt;Datum 中的 interface 类型的数据，统计它的内存使用量比较困难。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 新执行框架简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在重构后，TiDB 2.0 中使用的执行框架会不断调用 Child 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/executor.go#L198&quot;&gt;NextChunk&lt;/a&gt; 函数，获取一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L32&quot;&gt;Chunk&lt;/a&gt; 的数据。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f9ade9cdfe393088d0841f55fd8a02f2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;192&quot; data-rawheight=&quot;380&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;这种执行方式的特点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每次函数调用返回一批数据，数据量由一个叫 &lt;code class=&quot;inline&quot;&gt;tidb_max_chunk_size&lt;/code&gt; 的 session 变量来控制，默认是 1024 行。因为 TiDB 是一个混合 TP 和 AP 的数据库，对于 AP 类型的查询来说，因为计算的数据量大，1024 没啥问题，但是对于 TP 请求来说，计算的数据量可能比较少，直接在一开始就分配 1024 行的内存并不是最佳的实践（ &lt;a href=&quot;https://github.com/pingcap/tidb/issues/6489&quot;&gt;这里&lt;/a&gt; 有个 github issue 讨论这个问题，欢迎感兴趣的同学来讨论和解决）。&lt;/li&gt;&lt;li&gt;Child 把它产出的数据写入到 Parent 传下来的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L32&quot;&gt;Chunk&lt;/a&gt; 中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种执行方式的好处是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;减少了框架上的函数调用开销。比如同样输出 1024 行结果，现在的函数调用次数将会是以前的 1/1024。&lt;/li&gt;&lt;li&gt;内存使用更加高效。Chunk 中的数据组织非常紧凑，存一个 8 字节的整数几乎就只需要 8 字节，没有其他额外的内存开销了。&lt;/li&gt;&lt;li&gt;减轻了 golang 的 gc 压力。Chunk 占用的内存可以不断地重复利用，不用频繁的申请新内存，从而减轻了 golang 的 gc 压力。&lt;/li&gt;&lt;li&gt;查询的执行过程更加缓存友好。如我们之前所说，Chunk 按列来组织数据，在计算的过程中我们也尽量按列来计算，这样既能让一列的数据尽量长时间的待在 Cache 中，减轻 Cache Miss 率，也能充分利用起 CPU 的 pipeline。这一块在后续的源码分析文章中会有详细介绍，这里就不再展开了。&lt;/li&gt;&lt;li&gt;内存监控和控制更加方便。Chunk 中没有使用任何 interface，我们能很方便的直接获取一个 Chunk 当前所占用的内存的大小，具体可以看这个函数：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/chunk/chunk.go#L63&quot;&gt;MemoryUsage&lt;/a&gt;。关于 TiDB 内存控制，我们也会在后续文章中详细介绍，这里也不再展开了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3.  新旧执行框架性能对比&lt;/b&gt;&lt;/p&gt;&lt;p&gt;采用了新的执行框架后，OLAP 类型语句的执行速度、内存使用效率都有极大提升，从 &lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/benchmark/tpch.md&quot;&gt;TPC-H 对比结果&lt;/a&gt; 看，性能有数量级的提升。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-14-38095421</guid>
<pubDate>Thu, 14 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在西山居实时舆情监控系统中的应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-08-37860493.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37860493&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e988d476051a7c80d10ff0607dce7c5c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;公司简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;西山居创建 1995 年初夏，在美丽的海滨小城珠海，西山居工作室孕育而生，一群西山居居士们十年如一日尅勊业业的奋斗。&quot;创造快乐，传递快乐！&quot; 一直是西山居居士们的创作宗旨。西山居以领先的技术作为坚实的基础以独特的本土化产品为玩家提供时尚化服务。在未来，西山居仍以娱乐软件为主导产品，不断进行研发和市场活动，逐步发展成为国内最优秀的集制作、发行于一体的数字化互动娱乐公司。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;业务背景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于公司产品的社交属性都非常强，对相关舆情进行分析与了解就显得很有必要，在此背景下，舆情监控系统应运而生。该系统利用算法组提供的分词算法，对文本进行解析与分类，打上各类标记后再通过计算产生中间结果。舆情系统直接查询这些中间结果，产生各类报表与趋势图，为及时掌握各类舆情趋势提供便利。用户可以自由组合舆情关注点，从而对平台有很严格的实时交互性查询要求，是典型的实时 HTAP 类业务。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;存储技术选型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;舆情系统之前我们曾经实现过一个客服系统，这个系统要求能实时查询，但面对是海量的玩家行为记录。在当时情况下（2016 年），可以选择的对象只有 MyCAT 这类数据库中间件，通过综合压力测试后，我们选定了 KingShard 这一款由公司前同事开发的中间件，KingShard 虽然没有 MyCAT 丰富的周边功能，但它在满足我们业务需求的核心功能上有更快的表现。但正因为有了这一次中间件的使用，我们对中间件有了比较全面的了解，它们在查询优化上有着天生的弱点，无法满足更复杂的查询或者表现极不友好，为此我们还不得不砍掉了客服系统的部分业务功能，所以在那时我已开始寻找更优的技术方案，其中分布式数据库是我们考察的重点方向。&lt;/p&gt;&lt;p&gt;BigTable、GFS、MapReduce 是谷歌在分布式存储与查询领域的探索成果，他们没有公开具体实现代码，但却发布了相应论文，对分布式文件系统、大数据挖掘和 NoSQL 发展起了重大促进作用。开源界根据这一成果开发出对应产品是 HBase、HDFS、Hadoop，这三个产品红极一时，相关周边产品更是百花齐放，很多细分领域都同时出现了多个产品竞争，让整个生态非常繁荣但也变得复杂，提高了我们的学习与使用成本。那么，在一些领域中有没有更加简单、直接、具有较强融合能力的解决方案呢？此时距谷歌这三篇论文发表已近 10 年，谷歌内部早已在尝试融合 NoSQL 和 SQL，并对它们进行了多次更新换代，Spanner、F1 两篇论文便是谷歌在这一方向的探索成果。开源分布式数据库 TiDB 便是受论文启发而设计的 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性，兼容 MySQL，具有支持分布式事务、无限的水平扩展、数据强一致性保证等核心 NewSQL 特性。&lt;/p&gt;&lt;p&gt;当时，舆情系统接入的第一个游戏平均每天入库数据量就已达到 8500 万条，并且还需要支持各种实时交互性查询，显然中间件已不能满足要求，传统的关系型数据库则更加不可能了。考虑到以后还会有其它游戏接入，我们果断选择了分布式数据库。&lt;br&gt;随着互联网经济的发展，数据量跟并发数也在飞速增长，单机数据库已越来越不能满足要求了，为此谷歌、阿里等大厂都有了自研的分布式数据库，但都没有开源，而 MySQL 的 MGR 及相关功能进展的步子太小，TiDB 的出现很好的弥补了市场空白，成为我们的唯一选择。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;服务器配置&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;舆情系统是内部孵化项目，服务器具体如下：&lt;/p&gt;&lt;p&gt;新购物理机器 6 台：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0471101ffcadd2707d98b4f061cd02a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1370&quot; data-rawheight=&quot;388&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0471101ffcadd2707d98b4f061cd02a4&quot; data-watermark-src=&quot;v2-ed9bd01b4abc1cc7c67e762051422fcd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;旧物理机 4 台：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c98f39139902e22f377cbc8fa65e98cd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1370&quot; data-rawheight=&quot;384&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c98f39139902e22f377cbc8fa65e98cd&quot; data-watermark-src=&quot;v2-07918c7995ccde484584a0514d9e4033&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们将对资源使用相对较小的 PD、监控服务分别放在旧物理机上，TiDB、TiKV 和 TiSpark 则分配在新机器上，详细如下：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ea858da41df5768d8cb0ab7d78dc59fa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1384&quot; data-rawheight=&quot;618&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ea858da41df5768d8cb0ab7d78dc59fa&quot; data-watermark-src=&quot;v2-19faf3bf3e3a3066e28420c53c8da2f5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;其中每个 TiKV 分配 CPU 10C / 内存 64G / 硬盘 2T，每个 TiSpark 分配 CPU 20C / 内存 64G。在资源有限情况下，结合数据量及舆情系统的 AP 业务属性，我们设计了这样相对复杂的架构，目的是为了充分利用好服务器资源，让它们能承担更极限的压力，事后有多次历史数据的导入也证明了我们这样设计的必要性，多谢 TiDB 的兄弟全程耐心指导及帮助。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;项目开发过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;得出中间结果是一个非常大的计算过程，所以我们使用了 TiSpark。TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品，它是将 Spark SQL 直接运行在分布式存储引擎 TiKV 上的 OLAP 解决方案。有了 TiSpark 我们可以方便地使用 Spark，而不需要再单独搭建一套 Spark 集群。&lt;/p&gt;&lt;p&gt;从 TiDB 的 1.0 RC 3 版本开始，我们就在金山云上搭建集群来试用与压测。期间经历了多次版本热更，集群也一直很稳定，功能与性能越来越强，所以在舆情系统开始开发时我们果断使用了 TiDB。并且 TiDB 有强烈的市场需求，他们的版本更新非常迅速，在试用期间时发现了一些功能不能满足需要，往往在下一个版本就解决了，这让人非常惊叹。&lt;/p&gt;&lt;p&gt;当前版本未加入实时计算业务，再加上使用了 TiSpark，所以整个架构相对简单，详细如下图：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-954a46f6aa752402dc0626b41bfff0c3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;729&quot; data-rawheight=&quot;1154&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-954a46f6aa752402dc0626b41bfff0c3&quot; data-watermark-src=&quot;v2-750f536d0baf3ca8146f174a36a0703a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;项目上线及使用情况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;舆情系统目前总数据量数 T，已正式上线三个月，期间从未出现过异常，系统平稳、使用效果也非常好。现在每天原始文本数据在 2500 万条以上，通过算法分词后产生的中间结果则每天有 6000 万条左右（日均入库 8500 万条），高峰时段的平均 QPS 在 3K 以上，闲时的平均 QPS 为 300 多一点。根据这样的量级，在一开始评估时设定的目标是：支持最近一个星期的实时交互性查询，但现在已经远远超过我们的预期。目前所有一个月内的时间跨度查询都在 1 秒左右完成，个别复杂的 3 个月的实时交互性查询则需要 2 秒多一点。&lt;/p&gt;&lt;p&gt;可以说 TiDB 给我们的体验远超预期，这样的数据量级及响应，单机版数据库是不可能达到要求的。&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;：邹学，舆情监控系统技术负责人，珠海金山网络游戏科技有限公司（西山居）数据中心架构师，2015 年加入西山居，具有 10 年游戏行业软件开发经验，主要参与了公司的游戏网关设计，数据分析框架底层架构建设等，现专注于实时计算、爬虫、分布式系统方向。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-08-37860493</guid>
<pubDate>Fri, 08 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（九）Hash Join</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-06-06-37773956.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37773956&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-11402cca6b9863dc806402042e81a78c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;TiDB 目前获得了广泛的关注，特别是一些技术爱好者希望能够参与这个项目。由于整个系统的复杂性，很多人并不能很好的理解整个项目。我们希望通过 TiDB 源码阅读系列文章自顶向下，由浅入深，讲述 TiDB 的技术原理以及实现细节，帮助大家掌握这个项目。&lt;br&gt;&lt;br&gt;本文是 TiDB 源码阅读系列文章的第九篇。内文详细介绍了 TiDB Hash Join 的实现以及几种常见的问题，enjoy～&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;什么是 Hash Join&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Hash Join 的基本定义可以参考维基百科：&lt;a href=&quot;https://en.wikipedia.org/wiki/Hash_join&quot;&gt;Hash join&lt;/a&gt;。简单来说，A 表和 B 表的 Hash Join 需要我们选择一个 Inner 表来构造哈希表，然后对 Outer 表的每一行数据都去这个哈希表中查找是否有匹配的数据。&lt;/p&gt;&lt;p&gt;我们不用 “小表” 和 “大表” 这两个术语是因为：对于类似 Left Outer Join 这种 Outer Join 来说，如果我们使用 Hash Join，不管 Left 表相对于 Right 表而言是大表还是小表，我们都只能使用 Right 表充当 Inner 表并在之上建哈希表，使用 Left 表来当 Outer 表，也就是我们的驱动表。使用 Inner 和 Outer 更准确，没有迷惑性。在 Build 阶段，对 Inner 表建哈希表，在 Probe 阶段，对由 Outer 表驱动执行 Join 过程。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Hash Join 实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的 Hash Join 是一个多线程版本的实现，主要任务有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Main Thread，一个，执行下列任务：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取所有的 Inner 表数据；&lt;/li&gt;&lt;li&gt;根据 Inner 表数据构造哈希表；&lt;/li&gt;&lt;li&gt;启动 Outer Fetcher 和 Join Worker 开始后台工作，生成 Join 结果，各个 goroutine 的启动过程由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L1003&quot;&gt;fetchOuterAndProbeHashTable&lt;/a&gt; 这个函数完成；&lt;/li&gt;&lt;li&gt;将 Join Worker 计算出的 Join 结果返回给 &lt;code class=&quot;inline&quot;&gt;NextChunk&lt;/code&gt; 接口的调用方法。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Outer Fetcher，一个，负责读取 Outer 表的数据并分发给各个 Join Worker；&lt;/li&gt;&lt;li&gt;Join Worker，多个，负责查哈希表、Join 匹配的 Inner 和 Outer 表的数据，并把结果传递给 Main Thread。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来我们细致的介绍 Hash Join 的各个阶段。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Main Thread 读 Inner 表数据&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;读 Inner 表数据的过程由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L329&quot;&gt;fetchInnerRows&lt;/a&gt; 这个函数完成。这个过程会不断调用 Child 的 &lt;code class=&quot;inline&quot;&gt;NextChunk&lt;/code&gt; 接口，把每次函数调用所获取的 Chunk 存储到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L348&quot;&gt;innerResult&lt;/a&gt; 这个 List 中供接下来的计算使用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Main Thread 构造哈希表&lt;/b&gt;&lt;/p&gt;&lt;p&gt;构造哈希表的过程由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L1003&quot;&gt;buildHashTableForList&lt;/a&gt; 这个函数完成。&lt;/p&gt;&lt;p&gt;我们这里使用的哈希表（存储在变量 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L52&quot;&gt;hashTable&lt;/a&gt; 中）本质上是一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/util/mvmap/mvmap.go#L118&quot;&gt;MVMap&lt;/a&gt;。MVMap 的 Key 和 Value 都是 &lt;code class=&quot;inline&quot;&gt;[]byte&lt;/code&gt; 类型的数据，和普通 map 不同的是，MVMap 允许一个 Key 拥有多个 Value。这个特性对于 Hash Join 来说非常方便和实用，因为表中同一个 Join Key 可能对应多行数据。&lt;/p&gt;&lt;p&gt;构造哈希表的过程中，我们会遍历 Inner 表的每行数据（上文提到，此时所有的数据都已经存储在了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L348&quot;&gt;innerResult&lt;/a&gt; 中），对每行数据做如下操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;计算该行数据的 Join Key，得到一个 &lt;code class=&quot;inline&quot;&gt;[]byte&lt;/code&gt;，它将作为 MVMap 的 Key；&lt;/li&gt;&lt;li&gt;计算该行数据的位置信息，得到另一个 &lt;code class=&quot;inline&quot;&gt;[]byte&lt;/code&gt;，它将作为 MVMap 的 Value；&lt;/li&gt;&lt;li&gt;将这个 &lt;code class=&quot;inline&quot;&gt;(Key, Value)&lt;/code&gt; 放入 MVMap 中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. Outer Fetcher&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Outer Fetcher 是一个后台 goroutine，他的主要计算逻辑在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L291&quot;&gt;fetchOuterChunks&lt;/a&gt; 这个函数中。&lt;/p&gt;&lt;p&gt;它会不断的读大表的数据，并将获得的 Outer 表的数据分发给各个 Join Worker。这里多线程之间的资源交互可以用下图表示：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a609ee70a5370629e9a35dd2dd110558_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;494&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a609ee70a5370629e9a35dd2dd110558&quot; data-watermark-src=&quot;v2-5a6e2d50f070eab71ce2314ddbfff5f2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;上图中涉及到了两个 channel：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L74&quot;&gt;outerResultChs[i]&lt;/a&gt;：每个 Join Worker 一个，Outer Fetcher 将获取到的 Outer Chunk 写入到这个 channel 中供相应的 Join Worker 使用；&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L73&quot;&gt;outerChkResourceCh&lt;/a&gt;：当 Join Worker 用完了当前的 Outer Chunk 后，它需要把这个 Chunk 以及自己对应的 outerResultChs[i] 的地址一起写入到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L73&quot;&gt;outerChkResourceCh&lt;/a&gt; 这个 channel 中，告诉 Outer Fetcher 两个信息：&lt;br&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;我提供了一个 Chunk 给你，你直接用这个 Chunk 去拉 Outer 数据吧，不用再重新申请内存了；&lt;/li&gt;&lt;li&gt;我的 Outer Chunk 已经用完了，你需要把拉取到的 Outer 数据直接传给我，不要给别人了。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;所以，整体上 Outer Fetcher 的计算逻辑是：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;i. 从 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L73&quot;&gt;outerChkResourceCh&lt;/a&gt; 中获取一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L84&quot;&gt;outerChkResource&lt;/a&gt;，存储在变量 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L307&quot;&gt;outerResource&lt;/a&gt; 中；&lt;/p&gt;&lt;p&gt;ii. 从 Child 拉取数据，将数据写入到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L307&quot;&gt;outerResource&lt;/a&gt; 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L85&quot;&gt;chk&lt;/a&gt; 字段中；&lt;/p&gt;&lt;p&gt;iii. 将这个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L85&quot;&gt;chk&lt;/a&gt; 发给需要 Outer 表的数据的 Join Worker 的 &lt;code class=&quot;inline&quot;&gt;outerResultChs[i]&lt;/code&gt; 中去，这个信息记录在了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L307&quot;&gt;outerResource&lt;/a&gt; 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L86&quot;&gt;dest&lt;/a&gt; 字段中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Join Worker&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个 Join Worker 都是一个后台 goroutine，主要计算逻辑在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L562&quot;&gt;runJoinWorker4Chunk&lt;/a&gt; 这个函数中。Join Worker 的数量由 &lt;code class=&quot;inline&quot;&gt;tidb_hash_join_concurrency&lt;/code&gt;这个 session 变量来控制，默认是 5 个。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1dc6fa88ff141bce8c9ee5bbdcac5204_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;454&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1dc6fa88ff141bce8c9ee5bbdcac5204&quot; data-watermark-src=&quot;v2-35b95e5b599d161d30c941957a01fffb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图中涉及到两个 channel：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L75&quot;&gt;joinChkResourceCh[i]&lt;/a&gt;：每个 Join Worker 一个，用来存 Join 的结果；&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L76&quot;&gt;joinResultCh&lt;/a&gt;：Join Worker 将 Join 的结果 Chunk 以及它的 joinChkResourceCh 地址写入到这个 channel 中，告诉 Main Thread 两件事：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;我计算出了一个 Join 的结果 Chunk 给你，你读到这个数据后可以直接返回给你 Next 函数的调用方；&lt;/li&gt;&lt;li&gt;你用完这个 Chunk 后赶紧还给我，不要给别人，我好继续干活。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;所以，整体上 Join Worker 的计算逻辑是：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;i. 获取一个 Outer Chunk；&lt;/p&gt;&lt;p&gt;ii. 获取一个 Join Chunk Resource；&lt;/p&gt;&lt;p&gt;iii. 查哈希表，将匹配的 Outer Row 和 Inner Rows 写到 Join Chunk 中；&lt;/p&gt;&lt;p&gt;iv. 将写满了的 Join Chunk 发送给 Main Thread。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. Main Thread&lt;/b&gt;&lt;/p&gt;&lt;p&gt;主线程的计算逻辑由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L776&quot;&gt;NextChunk&lt;/a&gt; 这个函数完成。主线程的计算逻辑非常简单：&lt;/p&gt;&lt;p&gt;i. 从 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L76&quot;&gt;joinResultCh&lt;/a&gt; 中获取一个 Join Chunk；&lt;/p&gt;&lt;p&gt;ii. 将调用方传下来的 chk 和 Join Chunk 中的数据交换；&lt;/p&gt;&lt;p&gt;iii. 把 Join Chunk 还给对应的 Join Worker。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hash Join FAQ&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 如何确定 Inner 和 Outer 表？&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Left Outer Join：左表是 Outer 表，右表是 Inner 表；&lt;/li&gt;&lt;li&gt;Right Outer Join：跟 Left Outer Join 相反，右表是 Outer 表，左表是 Inner 表；&lt;/li&gt;&lt;li&gt;Inner Join：优化器估算出的较大表是 Outer 表，较小的表是 Inner 表；&lt;/li&gt;&lt;li&gt;Semi Join、Anti Semi Join、Left Outer Semi Join 或 Anti Left Outer Semi Join：左表是 Outer 表，右表是 Inner 表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. Join Key 中 NULL 值的问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 不等，所以：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在用 Inner 表建 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值的时候会忽略掉 Join Key 中有 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 的数据（代码在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L1022&quot;&gt;这里&lt;/a&gt;）；&lt;/li&gt;&lt;li&gt;当 Outer 表中某行数据的 Join Key 中有 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值的时候我们不会去查哈希表（代码在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L655&quot;&gt;这里&lt;/a&gt;）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. Join 中的 4 种 Filter&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Inner 表上的 Filter&lt;/b&gt;：这种 Filter 目前被优化器推到了 Hash Join Inner 表上面，在 Hash Join 实现的过程中不用考虑这种 Filter 了。推下去的原因是能够尽早的在 coprocessor 上就把不能匹配到的 Inner 表数据给过滤掉，给上层计算减压。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Outer 表上的 Filter&lt;/b&gt;：这种 Filter 的计算目前在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join.go#L711&quot;&gt;join2Chunk&lt;/a&gt; 中，由 Join Worker 进行。当 Join Worker 拿到一个 Outer Chunk 以后需要先计算 Outer Filter，如果通过了 Outer Filter 再去查哈希表。&lt;/li&gt;&lt;li&gt;&lt;b&gt;两个表上的等值条件&lt;/b&gt;：这就是我们说的 Join Key。比如 A 表和 B 表的等值条件是：&lt;code class=&quot;inline&quot;&gt;A.col1=B.col2 and A.col3=B.col4&lt;/code&gt;，那么 A 表和 B 表上的 Join Key 分别是 &lt;code class=&quot;inline&quot;&gt;(col1, col3)&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;(col2, col4)&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;两个表上的非等值条件&lt;/b&gt;：这种 Filter 需要在 Join 的结果集上计算，如果能够过这个 Filter 才认为两行数据能够匹配。这个 Filter 的计算过程交给了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L36&quot;&gt;joinResultGenerator&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. Join 方式的实现&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前 TiDB 支持的 Join 方式有 7 种，我们使用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L36&quot;&gt;joinResultGenerator&lt;/a&gt; 这个接口来定义两行数据的 Join 方式，实现一种具体的 Join 方式需要特殊的去实现 &lt;code class=&quot;inline&quot;&gt;joinResultGenerator&lt;/code&gt; 这个接口，目前有 7 种实现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L212&quot;&gt;semiJoinResultGenerator&lt;/a&gt;：实现了 Semi Join 的链接方式，当一个 Outer Row 和至少一个 Inner Row 匹配时，输出这个 Outer Row。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L278&quot;&gt;antiSemiJoinResultGenerator&lt;/a&gt;：实现了 Anti Semi Join 的链接方式，当 Outer Row 和所有的 Inner Row 都不能匹配时才输出这个 Outer Row。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L342&quot;&gt;leftOuterSemiJoinResultGenerator&lt;/a&gt;：实现了 Left Outer Semi Join 的链接方式，Join 的结果是 Outer Row + 一个布尔值，如果该 Outer Row 能和至少一个 Inner Row 匹配，则输出该 Outer Row + True，否则输出 Outer Row + False。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L415&quot;&gt;antiLeftOuterSemiJoinResultGenerator&lt;/a&gt;：实现了 Anti Left Outer Semi Join 的链接方式，Join 的结果也是 Outer Row + 一个布尔值，不同的是，如果该 Outer Row 不能和任何 Inner Row 匹配上，则输出 Outer Row + True，否则输出 Outer Row + False。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L490&quot;&gt;leftOuterJoinResultGenerator&lt;/a&gt;：实现了 Left Outer Join 的链接方式，如果 Outer Row 不能和任何 Inner Row 匹配，则输出 Outer Row + NULL 填充的 Inner Row，否则输出每个匹配的 Outer Row + Inner Row。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L555&quot;&gt;rightOuterJoinResultGenerator&lt;/a&gt;：实现了 Right Outer Join 的链接方式，如果 Outer Row 不能和 Inner Row 匹配，则输出 NULL 填充的 Inner Row + Outer Row，否则输出每个匹配的 Inner Row + Outer Row。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/join_result_generators.go#L619&quot;&gt;innerJoinResultGenerator&lt;/a&gt;：实现了 Inner Join 的链接方式，如果 Outer Row 不能和 Inner Row 匹配，不输出任何数据，否则根据 Outer Row 是左表还是右表选择性的输出每个匹配的 Inner Row + Outer Row 或者 Outer Row + Inner Row。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;作者：张建&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36420449&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-c57f34d03b8c3bebf2c377a99031e125&quot; data-image-width=&quot;3072&quot; data-image-height=&quot;2048&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（八）基于代价的优化&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35511864&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-dd77ba57ce8a5c40abb710bfafd997ad&quot; data-image-width=&quot;3072&quot; data-image-height=&quot;2048&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（七）基于规则的优化&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-bbeb258c92e952e8b8d3e3459df80b9e&quot; data-image-width=&quot;620&quot; data-image-height=&quot;414&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（六）Select 语句概览&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34770765&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-aff61c8861faa58cf2b7308dfbdac1ac&quot; data-image-width=&quot;1920&quot; data-image-height=&quot;1235&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（五）TiDB SQL Parser 的实现&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34512827&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-9fbb6c63057d03b0febd343f2fcb6bc7&quot; data-image-width=&quot;720&quot; data-image-height=&quot;480&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（四）Insert 语句概览&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34369624&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-4b4bb31438e3462c116811d78a1fb3a9&quot; data-image-width=&quot;900&quot; data-image-height=&quot;620&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（三）SQL 的一生&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34176614&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-22fa6ca9240bb8f54dea4d7eaa0f50f1&quot; data-image-width=&quot;5116&quot; data-image-height=&quot;3411&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（二）初识 TiDB 源码&lt;/a&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34109413&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-248d9e972a06be2d4632a7bfecf3302a&quot; data-image-width=&quot;1400&quot; data-image-height=&quot;940&quot; data-image-size=&quot;180x120&quot;&gt;ZoeyZhai：TiDB 源码阅读系列文章（一）序&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-06-06-37773956</guid>
<pubDate>Wed, 06 Jun 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 分布式数据库在转转公司的应用实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-05-29-37453182.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37453182&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c988119b9678d66dc35c5174708b154c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者&lt;/b&gt;：孙玄，转转公司首席架构师；陈东，转转公司资深工程师；冀浩东，转转公司资深 DBA。&lt;/blockquote&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;公司及业务架构介绍 &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;转转二手交易网 —— 把家里不用的东西卖了变成钱，一个帮你赚钱的网站。由腾讯与 58 集团共同投资。为海量用户提供一个有担保、便捷的二手交易平台。转转是 2015 年 11 月 12 日正式推出的 APP，遵循“用户第一”的核心价值观，以“让资源重新配置，让人与人更信任”为企业愿景，提倡真实个人交易。&lt;br&gt;&lt;br&gt;转转二手交易涵盖手机、3C 数码、母婴用品等三十余个品类。在系统设计上，转转整体架构采用微服务架构，首先按照业务领域模型垂直拆分成用户、商品、交易、搜索、推荐微服务。对每一个功能单元（商品等），继续进行水平拆分，分为商品网关层、商品业务逻辑层、商品数据访问层、商品 DB / Cache，如下图所示： &lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c24443bb85d9c229ae282694b745e2f0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;491&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c24443bb85d9c229ae282694b745e2f0&quot; data-watermark-src=&quot;v2-04954013e54bc3678baa2f29ed9021b8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;项目背景 &lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;面临的问题&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;转转后端业务现阶段主要使用 MySQL 数据库存储数据，还有少部分业务使用 MongoDB。虽然目前情况下使用这两种存储基本可以满足我们的需求，但随着业务的增长，公司的数据规模逐渐变大，为了应对大数据量下业务服务访问的性能问题，MySQL 数据库常用的分库、分表方案会随着 MySQL Sharding（分片）的增多，业务访问数据库逻辑会越来越复杂。而且对于某些有多维度查询需求的表，我们总需要引入额外的存储或牺牲性能来满足我们的查询需求，这样会使业务逻辑越来越重，不利于产品的快速迭代。&lt;/p&gt;&lt;p&gt;&lt;br&gt;从数据库运维角度讲，大数据量的情况下，MySQL 数据库在每次 DDL 都会对运维人员造成很大的工作量，当节点故障后，由于数据量较大，恢复时间较长。但这种 M - S 架构只能通过主从切换并且需要额外的高可用组件来保障高可用，同时在切换过程由于需要确定主库状态、新主库选举、新路由下发等原因，还是会存在短暂的业务访问中断的情况。 &lt;br&gt;综上所述，我们面临的主要问题可归纳为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据量大，如何快速水平扩展存储；&lt;/li&gt;&lt;li&gt;大数据量下，如何快速 DDL；&lt;/li&gt;&lt;li&gt; 分库分表造成业务逻辑非常复杂；&lt;/li&gt;&lt;li&gt;常规 MySQL 主从故障转移会导致业务访问短暂不可用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 为什么选择 TiDB&lt;/b&gt;&lt;/p&gt;&lt;p&gt;针对上章提到的问题，转转基础架构部和 DBA 团队考虑转转业务数据增速，定位简化业务团队数据库使用方案，更好的助力业务发展，决定启动新型存储服务（NewSQL）的选型调研工作。 &lt;br&gt;TiDB 数据库，结合了关系库与 KV 存储的优点，对于使用方，完全可以当做 MySQL 来用，而且不用考虑数据量大了后的分库分表以及为了支持分库分表后的多维度查询而建立的 Mapping 表，可以把精力全部放在业务需求上。所以我们把 TiDB 作为选型的首选对象展开了测试和试用。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;TiDB 测试 &lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;功能测试&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;TiDB 支持绝大多数 MySQL 语法，业务可以将基于 MySQL 的开发，无缝迁移至 TiDB。不过目前 TiDB 不支持部分 MySQL 特性，如：存储过程、自定义函数、触发器等。 &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;2. TiDB 压力测试 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过测试工具模拟不同的场景的请求，对 TiDB 数据库进行压力测试，通过压力测试结果的对比，可以提供 RD 使用 TiDB 的合适业务场景以及 TiDB 的使用建议。&lt;br&gt;此次压力测试，总共使用 6 台物理服务器，其中 3 台 CPU 密集型服务器，用于启动 TiDB - Server、PD 服务；另外 3 台为 IO / CPU 密集型的PCIE 服务器，用于启动 TiKV 服务。&lt;br&gt;使用 sysbench - 1.0.11 测试数据大小为 200G 的 TiDB 集群，在不同场景下 TiDB 的响应时间（95th per）：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3e3dfde0ff2a12b623639848a5ad0d4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;768&quot; data-rawheight=&quot;320&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b3e3dfde0ff2a12b623639848a5ad0d4&quot; data-watermark-src=&quot;v2-6b46c0ca950b78728bf6c191550a13e8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;3. 结果整理&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;顺序扫描的效率是比较高的，连续的行大概率会存储在同一台机器的邻近位置，每次批量的读取和写入的效率会高；&lt;/li&gt;&lt;li&gt;控制并发运行的线程数，会减少请求响应时间，提高数据库的处理性能。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. 场景建议&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;适合线上业务混合读写场景；&lt;/li&gt;&lt;li&gt;适合顺序写的场景，比如：数据归档、操作日志、摊销流水。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;5. TiDB 预上线&lt;/b&gt;&lt;/p&gt;&lt;p&gt;将 TiDB 挂载到线上 MySQL，作为 MySQL 从库同步线上数据，然后业务将部分线上读流量切换到 TiDB，可以对 TiDB 集群是否满足业务访问做好预判。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;业务接入&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;迁移过程&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们第一个接入 TiDB 的业务线是转转消息服务。消息作为转转最重要的基础服务之一，是保证平台上买卖双方有效沟通、促进交易达成的重要组件，其数据量和访问量都非常大。起初我们使用的是 MySQL 数据库，对其所有的业务都做了库的垂直拆分以及表的水平拆分。目前线上有几十 TB 的数据，记录数据达到了几百亿。虽对 MySQL 做了分库分表，但实例已经开始又有偶发的性能问题，需要马上对数据进行二次拆分，而二次拆分的执行成本也比较高，这也是我们首先迁移消息数据库的原因之一。&lt;br&gt;&lt;br&gt;消息服务有几个核心业务表：联系人列表、消息表、系统消息表等等。联系人列表作为整个消息系统的枢纽，承载着巨大的访问压力。业务场景相对其他表最复杂的，也是这个表的实例出现了性能问题，所以我们决定先迁移联系人列表。&lt;br&gt;整个迁移过程分三步：测试（判断 TiDB 是否满足业务场景，性能是否 OK）、同步数据、切流量。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;（1）测试：&lt;/b&gt;首先我们模拟线上的数据和请求对“联系人列表”做了大量功能和性能的验证，而且还将线上的数据和流量引到线下，对数据库做了真实流量的验证，测试结果证明 TiDB 完全满足消息业务的需求。引流工作，我们是通过转转自研的消息队列，将线上数据库的流量引一份到测试环境。测试环境消费消息队列的数据，转换成数据库访问请求发送到 TiDB 测试集群。通过分析线上和测试环境两个数据访问模块的日志可以初步判断 TiDB 数据库是否可以正常处理业务请求。当然仅仅这样是不够的，DBA 同学还需要校验 TiDB 数据的正确性（是否与线上 MySQL 库一致）。验证思路是抽样验证 MySQL 库表记录和 TiDB 的记录 Checksum 值是否一致。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;（2）同步数据：&lt;/b&gt;DBA 同学部署 TiDB 集群作为 MySQL 实例的从库，将 MySQL 实例中的联系人列表（单实例分了 1024 个表）的数据同步到 TiDB 的一张大表中。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;（3）切流量：&lt;/b&gt;切流量分为三步，每两步之间都有一周左右的观察期。&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;第一步将读流量灰度切到 TiDB 上；&lt;/li&gt;&lt;li&gt;第二步断开 TiDB 与 MySQL 的主从同步，业务开双写（同时写 MySQL 和 TiDB，保证两库数据一致）确保业务流量可以随时回滚到 MySQL；&lt;/li&gt;&lt;li&gt; 第三步停止 MySQL 写入，到此业务流量完全切换到 TiDB 数据库上。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;迁移过程中最重要的点就是确保两个数据库数据一致，这样读写流量随时可以切回 MySQL，业务逻辑不受任何影响。数据库双写的方案与上文提到的引流测试类似，使用消息队列引一份写入流量，TiDB 访问模块消费消息队列数据，写库。但仅仅这样是不能保证两个库数据一致的，因为这个方案无法保证两个写库操作的原子性。所以我们需要一个更严谨的方案，转转的消息队列还提供了事务消息的支持，可以保证本地操作和发送消息的原子性。利用这一特性再加上异步补偿策略（离线扫描日志，如果有失败的写入请求，修正数据）保证每个消息都被成功消费且两个库每次写入结果都是一致的，从而保证了 MySQL 与 TiDB 两个库的数据一致。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;2. 遇到问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;按照上述的方案，我们已经将消息所有的业务都切到 TiDB 数据库上。迁移过程中也不都是顺风顺水，也遇到了问题，过程中也得到了 TiDB 官方团队的大力支持。这里主要介绍两个问题：&lt;br&gt;（1）TiDB 作为分布式存储，其锁机制和 MySQL 有很大不同。我们有一个并发量很大，可能同时更新一条记录的场景，我们用了 MySQL 的唯一索引保证了某个 Key 值的唯一性，但如果业务请求使用默认值就会大量命中唯一索引，会造成 N 多请求都去更新统一同一条记录。在 MySQL 场景下，没有性能问题，所以业务上也没做优化。但当我们用这个场景测试 TiDB 时，发现 TiDB 处理不太好，由于其使用的乐观锁，数据库输出大量的重试的日志。业务出现几十秒的请求延迟，造成队列中大量请求被抛弃。PingCAP 的同学建议调整 retry_limit 但也没有完全生效&lt;b&gt;（该 BUG 已经在 2.0 RC 5 已经修复）&lt;/b&gt;，最后业务进行优化（过滤使用默认值的请求）后问题得到解决。&lt;br&gt;（2）第二个问题是运维方面的，DBA 同学按照使用 MySQL 的运维经验，对一个上近 T 的表做了 Truncate操作，操作后，起初数据库表现正常，但几分钟后，开始出现超时，TiKV 负载变高。最后请教 PingCAP 同学分析，定位是操作触发了频繁回收 Region 的 BUG&lt;b&gt;（该 BUG TiDB 2.0 版本已经修复）&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;线上效果对比&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;队列等待情况对比&lt;/b&gt;&lt;br&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9910d3ada2c0de998121ae2a1261e2f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;269&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9910d3ada2c0de998121ae2a1261e2f9&quot; data-watermark-src=&quot;v2-faa65aed80bc72c402c23f7362069205&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-24e15b226b9d9bd147351928613315e0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;291&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-24e15b226b9d9bd147351928613315e0&quot; data-watermark-src=&quot;v2-2c696d3ad7365679866fd8200e8b98f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;使用 TiDB 数据库，业务模块队列请求数基本保持 1 个，MySQL 会有较大抖动。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;2. 请求延迟情况对比&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-191b3bfc39831ba909350ffea7bbd9a6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;260&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-191b3bfc39831ba909350ffea7bbd9a6&quot; data-watermark-src=&quot;v2-81bf0b424f5f7c21414877f359384fbf&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a2825a0b9941df736253a8a8354e4d28_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;270&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a2825a0b9941df736253a8a8354e4d28&quot; data-watermark-src=&quot;v2-6b014833957515ba4677880f0b0c0167&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;使用 TiDB 数据库，整体响应延时非常稳定，不受业务流量高峰影响，但 MySQL 波动很大。 另外在扩展性方面，我们可以通过无缝扩展 TiDB 和 TiKV 实例提升系统的吞吐量，这个特性 MySQL 是不具备的。&lt;br&gt;&lt;br&gt;&lt;b&gt;3. 业务延迟和错误量对比&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ab4fae85f3c3be5e845f13fd318f841f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;272&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ab4fae85f3c3be5e845f13fd318f841f&quot; data-watermark-src=&quot;v2-56c3092084f11cf5e273a3881a0aee77&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-444cc1db272710bb920426ef935c6bde_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;865&quot; data-rawheight=&quot;268&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-444cc1db272710bb920426ef935c6bde&quot; data-watermark-src=&quot;v2-a62fcb7fc8d8d2574b11613d0ec14854&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;接入 TiDB 数据库后业务逻辑层服务接口耗时稳定无抖动，且没有发生丢弃的情况（上图错误大多由数据访问层服务队列堆积发生请求丢弃造成）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 线上规模及后续规划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前转转线上已经接入消息、风控两套 OLTP 以及一套风控 OLAP 集群。 &lt;br&gt;&lt;br&gt;集群架构如下：目前转转线上 TiDB 集群的总容量几百 TB，线上 TiDB 表现很稳定，我们会继续接入更多的业务（留言，评论、搜索、商品、交易等等）。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-af56234a89312189c6933875c63bc657_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;839&quot; data-rawheight=&quot;636&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-af56234a89312189c6933875c63bc657&quot; data-watermark-src=&quot;v2-30e6b6df7f50bad5d71931a3496f8f9a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;1. 后续规划 &lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;多个正在开发的新业务在开发和测试环境中使用 TiDB，线上会直接使用 TiDB；&lt;/li&gt;&lt;li&gt;转转核心的留言、评论、搜索、商品、交易订单库计划迁移到 TiDB，已经开始梳理业务，准备展开测试；&lt;/li&gt;&lt;li&gt;计划在后续 TiDB 的使用中，TiKV 服务器池化，按需分配 TiKV 节点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. TiDB 使用成果 &lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;利用 TiDB 水平扩展特性，避免分库分表带来的问题，使得业务快速迭代；&lt;/li&gt;&lt;li&gt;TiDB 兼容 MySQL 语法和协议，按照目前线上 MySQL 使用规范，可以无缝的迁移过去，无需 RD 做调整，符合预期；&lt;/li&gt;&lt;li&gt;在数据量较大的情况下，TiDB 响应较快，优于 MySQL；&lt;/li&gt;&lt;li&gt;集群出现故障对用户无感知；&lt;/li&gt;&lt;li&gt;TiDB 自带了完善的监控系统，使得运维成本大大降低。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;延展阅读：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37303465&quot;&gt;TiDB 助力客如云餐饮 SaaS 服务&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36995447&quot;&gt;TiDB 在威锐达 WindRDS 远程诊断及运维中心的应用&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36112488&quot;&gt;TiDB 在饿了么归档环境的应用&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-05-29-37453182</guid>
<pubDate>Tue, 29 May 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>性能测试工具的 Coordinated Omission 问题</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-05-25-37304087.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/37304087&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f6f4c63a19fa344ec34c724fd31ed1a7_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：唐刘&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;很早之前就看过 Gil 大神的一篇文章&lt;a href=&quot;http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html&quot;&gt;《Your Load Generator Is Probably Lying To You - Take The Red Pill And Find Out Why》&lt;/a&gt;，里面提到了性能测试工具 coordinated omission 的问题，但当时并没有怎么在意。这几天有人在我们自己的性能测试工具 &lt;a href=&quot;https://github.com/pingcap/go-ycsb/issues/26&quot;&gt;go-ycsb&lt;/a&gt; （https://github.com/pingcap/go-ycsb/issues/26）上面问了这个问题，我才陡然发现，原来我们也有。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;什么是 coordinated omission&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先来说说什么是 coordinated omission。对于绝大多数 benchmark 工具来说，通常都是这样的模型——启动多个线程，每个线程依次的发送 request，接受 response，然后继续下一次的发送。我们记录的 latency 通常就是 response time - request time。这个看起来很 make sense，但实际是有问题的。&lt;/p&gt;&lt;p&gt;一个简单的例子，当我们去 KFC 买炸鸡，然后我们排在了一个队伍后面，前面有 3 个人，开始 2 个人都好快，30 秒搞定，然后第三个墨迹了半天，花了 5 分钟，然后到我了，30 秒搞定。对于我来说，我绝对不会认为我的 latency 是 30 秒，而是会算上排队的时间 2 x 30 + 300，加上服务时间 30 秒，所以我的总的时间耗时是 390 秒。这里不知道大家看到了区别了没有，就是市面上大多数的性能测试工具，其实用的是服务时间，但并没有算上等待时间。&lt;/p&gt;&lt;p&gt;再来看一个例子，假设我们需要性能测试工具按照 10 ops/sec 的频繁发送请求，也就是希望每 100 ms 发送一个。前面 9 个请求，每个都是 50 us 就返回了，但第 10 个请求持续了 1 s，而后面的又是 50 us。可以明显地看到，在 1 s 那里，系统出现了卡顿，但这时候其实只有 1 个请求发上去，并没有很好地对系统进行测试。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;YCSB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于第一个排队的例子，为了更好的计算 latency，YCSB 引入了一个 intended time 的概念，即记录下操作实际的排队时间。它使用了一个 local thread 变量，在 throttle 的时候，记录：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;private void throttleNanos(long startTimeNanos) {
 //throttle the operations
 if (_targetOpsPerMs &amp;gt; 0)
    {
 // delay until next tick
 long deadline = startTimeNanos + _opsdone*_targetOpsTickNs;
        sleepUntil(deadline);
        _measurements.setIntendedStartTimeNs(deadline);
    }
}&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;然后每次操作的时候，使用 intended time 计算排队时间：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;public Status read(String table, String key, Set&amp;lt;String&amp;gt; fields,
                 Map&amp;lt;String, ByteIterator&amp;gt; result) {
 try (final TraceScope span = tracer.newScope(scopeStringRead)) {
 long ist = measurements.getIntendedtartTimeNs();
 long st = System.nanoTime();
        Status res = db.read(table, key, fields, result);
 long en = System.nanoTime();
        measure(&quot;READ&quot;, res, ist, st, en);
        measurements.reportStatus(&quot;READ&quot;, res);
 return res;
    }
}&lt;/code&gt;&lt;p&gt;需要注意，只有 YCSB 开启了 target，intended time 才有作用。&lt;/p&gt;&lt;p&gt;我当初在看 YCSB 代码的时候，一直没搞明白为什么会有两种时间，而且也不知道 intended time 到底是什么，后来重新回顾了 coordinated omission 才清楚。也就是说 YCSB 通过 intended time 来计算排队时间。&lt;/p&gt;&lt;p&gt;但 YCSB 还是没解决上面说的第二个问题，如果系统真的出现了卡主，测试客户端仍然会跟着卡主，因为是同步发送请求的。在网上搜索了一下，看到了一篇 Paper&lt;a href=&quot;http://btw2017.informatik.uni-stuttgart.de/slidesandpapers/E4-11-107/paper_web.pdf&quot;&gt;《Coordinated Omission in NoSQL Database Benchmarking》&lt;/a&gt;，里面提到了将同步改成异步的方式，也就是说，每次的任务是一个 Future，首先根据 target 按照频率发 Future 就行，至于这个 Future 什么时候完成，后面再说。而且因为是异步的，所以并不会卡主后面的请求。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Go YCSB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;那么具体到 go-ycsb，我们如何解决这个问题呢？我现在唯一能想到的就是利用 Go 的 goroutine，按照一定的频率去生成 goroutine，执行测试。当然 Go 自身也会有调度的开销，这里也需要排除。如果要测试的服务出现了卡顿，就会导致大量的 goroutine 没法释放，最终 OOM。虽然这样子看起来比较残暴，但这才是符合预期的。&lt;/p&gt;&lt;p&gt;这个只是一个想法，具体还没做。一个原因是不同于其他语言，Go 的 goroutine 其实天生就能开很多，所以通常我都是上千并发进行测试的，假设我们有 1000 个并发，按照 1 ms 一次的频率，其实也就等同于每个 goroutine 依次发送了。当然，有总比没有好，如果你对这块感兴趣，欢迎给我们提交 PR，或者给我发邮件详细讨论 tl@pingcap.com。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文：&lt;a href=&quot;https://www.jianshu.com/p/bfb2b0f50edd&quot;&gt;性能测试工具的 Coordinated Omission 问题&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-05-25-37304087</guid>
<pubDate>Fri, 25 May 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
