<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Thu, 27 Dec 2018 01:47:48 +0800</lastBuildDate>
<item>
<title>TiDB Ecosystem Tools 原理解读系列（三）TiDB-DM 架构设计与实现原理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-26-53357816.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53357816&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d68d4233f88a929df432cf4e63c5e7f9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张学程&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;TiDB-DM（Data Migration）是用于将数据从 MySQL/MariaDB 迁移到 TiDB 的工具。&lt;/b&gt;该工具既支持以全量备份文件的方式将 MySQL/MariaDB 的数据导入到 TiDB，也支持通过解析执行 MySQL/MariaDB binlog 的方式将数据增量同步到 TiDB。特别地，对于有多个 MySQL/MariaDB 实例的分库分表需要合并后同步到同一个 TiDB 集群的场景，DM 提供了良好的支持。如果你需要从 MySQL/MariaDB 迁移到 TiDB，或者需要将 TiDB 作为 MySQL/MariaDB 的从库，DM 将是一个非常好的选择。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;架构设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DM 是集群模式的，其主要由 DM-master、DM-worker 与 DM-ctl 三个组件组成，能够以多对多的方式将多个上游 MySQL 实例的数据同步到多个下游 TiDB 集群，其架构图如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f997a346fbb15217c2aef4a941abb33_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;601&quot; data-rawheight=&quot;402&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7f997a346fbb15217c2aef4a941abb33&quot; data-watermark-src=&quot;v2-3d3f8c27cc6c1d13685c6faaa2047725&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;DM-master：管理整个 DM 集群，维护集群的拓扑信息，监控各个 DM-worker 实例的运行状态；进行数据同步任务的拆解与分发，监控数据同步任务的执行状态；在进行合库合表的增量数据同步时，协调各 DM-worker 上 DDL 的执行或跳过；提供数据同步任务管理的统一入口。&lt;/li&gt;&lt;li&gt;DM-worker：与上游 MySQL 实例一一对应，执行具体的全量、增量数据同步任务；将上游 MySQL 的 binlog 拉取到本地并持久化保存；根据定义的数据同步任务，将上游 MySQL 数据全量导出成 SQL 文件后导入到下游 TiDB，或解析本地持久化的 binlog 后增量同步到下游 TiDB；编排 DM-master 拆解后的数据同步子任务，监控子任务的运行状态。&lt;/li&gt;&lt;li&gt;DM-ctl：命令行交互工具，通过连接到 DM-master 后，执行 DM 集群的管理与数据同步任务的管理。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;实现原理&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据迁移流程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;单个 DM 集群可以同时运行多个数据同步任务；对于每一个同步任务，可以拆解为多个子任务同时由多个 DM-worker 节点承担，其中每个 DM-worker 节点负责同步来自对应的上游 MySQL 实例的数据。对于单个 DM-worker 节点上的单个数据同步子任务，其数据迁移流程如下，其中上部的数据流向为全量数据迁移、下部的数据流向为增量数据同步：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9682a9ee2fcdbec6a350fe9d64cdad6e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;601&quot; data-rawheight=&quot;260&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9682a9ee2fcdbec6a350fe9d64cdad6e&quot; data-watermark-src=&quot;v2-6958ab643e8ebd83b8dac03d5b748a15&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在每个 DM-worker 节点内部，对于特定的数据同步子任务，主要由 dumper、loader、relay 与 syncer（binlog replication）等数据同步处理单元执行具体的数据同步操作。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于全量数据迁移，DM 首先使用 dumper 单元从上游 MySQL 中将表结构与数据导出成 SQL 文件；然后使用 loader 单元读取这些 SQL 文件并同步到下游 TiDB。&lt;/li&gt;&lt;li&gt;对于增量数据同步，首先使用 relay 单元作为 slave 连接到上游 MySQL 并拉取 binlog 数据后作为 relay log 持久化存储在本地，然后使用 syncer 单元读取这些 relay log 并解析构造成 SQL 语句后同步到下游 TiDB。这个增量同步的过程与 MySQL 的主从复制类似，主要区别在于在 DM 中，本地持久化的 relay log 可以同时供多个不同子任务的 syncer 单元所共用，避免了多个任务需要重复从上游 MySQL 拉取 binlog 的问题。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据迁移并发模型&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为加快数据导入速度，在 DM 中不论是全量数据迁移，还是增量数据同步，都在其中部分阶段使用了并发处理。&lt;/p&gt;&lt;p&gt;对于全量数据迁移，在导出阶段，dumper 单元调用 mydumper 导出工具执行实际的数据导出操作，对应的并发模型可以直接参考 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper 的源码&lt;/a&gt;。在使用 loader 单元执行的导入阶段，对应的并发模型结构如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-500d7c88efa869029888578be09f73bd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;147&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-500d7c88efa869029888578be09f73bd&quot; data-watermark-src=&quot;v2-4289423da132976eb7cfa4a1547e54f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;使用 mydumper 执行导出时，可以通过 &lt;code class=&quot;inline&quot;&gt;--chunk-filesize&lt;/code&gt; 等参数将单个表拆分成多个 SQL 文件，这些 SQL 文件对应的都是上游 MySQL 某一个时刻的静态快照数据，且各 SQL 文件间的数据不存在关联。因此，在使用 loader 单元执行导入时，可以直接在一个 loader 单元内启动多个 worker 工作协程，由各 worker 协程并发、独立地每次读取一个待导入的 SQL 文件进行导入。即 loader 导入阶段，是以 SQL 文件级别粒度并发进行的。在 DM 的任务配置中，对于 loader 单元，其中的 &lt;code class=&quot;inline&quot;&gt;pool-size&lt;/code&gt; 参数即用于控制此处 worker 协程数量。&lt;/p&gt;&lt;p&gt;对于增量数据同步，在从上游拉取 binlog 并持久化到本地的阶段，由于上游 MySQL 上 binlog 的产生与发送是以 stream 形式进行的，因此这部分只能串行处理。在使用 syncer 单元执行的导入阶段，在一定的限制条件下，可以执行并发导入，对应的模型结构如下：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-94c16769b0f7075bc7e30ce81ed1cc66_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;166&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-94c16769b0f7075bc7e30ce81ed1cc66&quot; data-watermark-src=&quot;v2-a192eca1e8e3f9ae33da328e5fc781ff&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;当 syncer 读取与解析本地 relay log 时，与从上游拉取 binlog 类似，是以 stream 形式进行的，因此也只能串行处理。当 syncer 解析出各 binlog event 并构造成待同步的 job 后，则可以根据对应行数据的主键、索引等信息经过 hash 计算后分发到多个不同的待同步 job channel 中；在 channel 的另一端，与各个 channel 对应的 worker 协程并发地从 channel 中取出 job 后同步到下游的 TiDB。即 syncer 导入阶段，是以 binlog event 级别粒度并发进行的。在 DM 的任务配置中，对于 syncer 单元，其中的 &lt;code class=&quot;inline&quot;&gt;worker-count&lt;/code&gt; 参数即用于控制此处 worker 协程数量。&lt;/p&gt;&lt;p&gt;但 syncer 并发同步到下游 TiDB 时，存在一些限制，主要包括：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于 DDL，由于会变更下游的表结构，因此必须确保在旧表结构对应的 DML 都同步完成后，才能进行同步。在 DM 中，当解析 binlog event 得到 DDL 后，会向每一个 job channel 发送一个特殊的 flush job；当各 worker 协程遇到 flush job 时，会立刻向下游 TiDB 同步之前已经取出的所有 job；等各 job channel 中的 job 都同步到下游 TiDB 后，开始同步 DDL；等待 DDL 同步完成后，继续同步后续的 DML。即 DDL 不能与 DML 并发同步，且 DDL 之前与之后的 DML 也不能并发同步。sharding 场景下 DDL 的同步处理见后文。&lt;/li&gt;&lt;li&gt;对于 DML，多条 DML 可能会修改同一行的数据，甚至是主键。如果并发地同步这些 DML，则可能造成同步后数据的不一致。DM 中对于 DML 之间的冲突检测与处理，与 TiDB-Binlog 中的处理类似，具体原理可以阅读 《&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487391&amp;amp;idx=1&amp;amp;sn=3e173b9c634e028824a69f67a506dd11&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB EcoSystem Tools 原理解读（一）TiDB-Binlog 架构演进与实现原理&lt;/a&gt;&lt;/u&gt;》中关于 Drainer 内 SQL 之间冲突检测的讨论。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;合库合表数据同步&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在使用 MySQL 支撑大量数据时，经常会选择使用分库分表的方案。但当将数据同步到 TiDB 后，通常希望逻辑上进行合库合表。DM 为支持合库合表的数据同步，主要实现了以下的一些功能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;table router&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为说明 DM 中 table router（表名路由）功能，先看如下图所示的一个例子：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-43726bdeba8135376cf56c41f422d81e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;291&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-43726bdeba8135376cf56c41f422d81e&quot; data-watermark-src=&quot;v2-4259409243fc8ccb4daaa510365bf859&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，上游有 2 个 MySQL 实例，每个实例有 2 个逻辑库，每个库有 2 个表，总共 8 个表。当同步到下游 TiDB 后，希望所有的这 8 个表最终都合并同步到同一个表中。&lt;/p&gt;&lt;p&gt;但为了能将 8 个来自不同实例、不同库且有不同名的表同步到同一个表中，首先要处理的，就是要能根据某些定义好的规则 ，将来自不同表的数据都路由到下游的同一个表中。在 DM 中，这类规则叫做 router-rules。对于上面的示例，其规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-router-rule:
    schema-pattern: &quot;schema_*&quot;
    table-pattern: &quot;table_*&quot;
    target-schema: &quot;schema&quot;
    target-table: &quot;table&quot;&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;name-of-router-rule&lt;/code&gt;：规则名，用户指定。当有多个上游实例需要使用相同的规则时，可以只定义一条规则，多个不同的实例通过规则名进行引用。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt;：用于匹配上游库（schema）名的模式，支持在尾部使用通配符（*）。这里使用 &lt;code class=&quot;inline&quot;&gt;schema_*&lt;/code&gt; 即可匹配到示例中的两个库名。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt;：用于匹配上游表名的模式，与 &lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; 类似。这里使用 &lt;code class=&quot;inline&quot;&gt;table_*&lt;/code&gt; 即可匹配到示例中的两个表名。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-schema&lt;/code&gt;：目标库名。对于库名、表名匹配的数据，将被路由到这个库中。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-table&lt;/code&gt;：目标表名。对于库名、表名匹配的数据，将被路由到 &lt;code class=&quot;inline&quot;&gt;target-schema&lt;/code&gt; 库下的这个表中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 DM 内部实现上，首先根据 &lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; / &lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt; 构造对应的 trie 结构，并将规则存储在 trie 节点中；当有 SQL 需要同步到下游时，通过使用上游库名、表名查询 trie 即可得到对应的规则，并根据规则替换原 SQL 中的库名、表名；通过向下游 TiDB 执行替换后的 SQL 即完成了根据表名的路由同步。有关 &lt;code class=&quot;inline&quot;&gt;router-rules&lt;/code&gt; 规则的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/table-router&quot;&gt;table-router pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;column mapping&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 table router 功能，已经可以完成基本的合库合表数据同步了。但在数据库中，我们经常会使用自增类型的列作为主键。如果多个上游分表的主键各自独立地自增，将它们合并同步到下游后，就很可能会出现主键冲突，造成数据的不一致。我们可看一个如下的例子：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6b7af10cbcef959e18a3bbc920719989_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;309&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6b7af10cbcef959e18a3bbc920719989&quot; data-watermark-src=&quot;v2-1f6ff66889ddae1a696f4203ba5b9bfe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，上游 4 个需要合并同步到下游的表中，都存在 id 列值为 1 的记录。假设这个 id 列是表的主键。在同步到下游的过程中，由于相关更新操作是以 id 列作为条件来确定需要更新的记录，因此会造成后同步的数据覆盖前面已经同步过的数据，导致部分数据的丢失。&lt;/p&gt;&lt;p&gt;在 DM 中，我们通过 column mapping 功能在数据同步的过程中依据指定规则对相关列的数据进行转换改写来避免数据冲突与丢失。对于上面的示例，其中 MySQL 实例 1 的 column mapping 规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;mapping-rule-of-instance-1:
    schema-pattern: &quot;schema_*&quot;
    table-pattern: &quot;table_*&quot;
    expression: &quot;partition id&quot;
    source-column: &quot;id&quot;
    target-column: &quot;id&quot;
    arguments: [&quot;1&quot;, &quot;schema_&quot;, &quot;table_&quot;]  &lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;mapping-rule-of-instance-1&lt;/code&gt;：规则名，用户指定。由于不同的上游 MySQL 实例需要转换得到不同的值，因此通常每个 MySQL 实例使用一条专有的规则。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;schema-pattern&lt;/code&gt; / &lt;code class=&quot;inline&quot;&gt;table-pattern&lt;/code&gt;：上游库名、表名匹配模式，与 &lt;code class=&quot;inline&quot;&gt;router-rules&lt;/code&gt; 中的对应配置项一致。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;expression&lt;/code&gt;：进行数据转换的表达式名。目前常用的表达式即为 &lt;code class=&quot;inline&quot;&gt;&quot;partition id&quot;&lt;/code&gt;，有关该表达式的具体说明见下文。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;source-column&lt;/code&gt;：转换表达式的输入数据对应的来源列名，&lt;code class=&quot;inline&quot;&gt;&quot;id&quot;&lt;/code&gt; 表示这个表达式将作用于表中名为 id 的列。暂时只支持对单个来源列进行数据转换。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;target-column&lt;/code&gt;：转换表达式的输出数据对应的目标列名，与 &lt;code class=&quot;inline&quot;&gt;source-column&lt;/code&gt; 类似。暂时只支持对单个目标列进行数据转换，且对应的目标列必须已经存在。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;arguments&lt;/code&gt;：转换表达式所依赖的参数。参数个数与含义依具体表达式而定。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;partition id&lt;/code&gt; 是目前主要受支持的转换表达式，其通过为 bigint 类型的值增加二进制前缀来解决来自不同表的数据合并同步后可能产生冲突的问题。&lt;code class=&quot;inline&quot;&gt;partition id&lt;/code&gt; 的 arguments 包括 3 个参数，分别为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;MySQL 实例 ID：标识数据的来源 MySQL 实例，用户自由指定。如 &lt;code class=&quot;inline&quot;&gt;&quot;1&quot;&lt;/code&gt; 表示匹配该规则的数据来自于 MySQL 实例 1，且这个标识将被转换成数值后以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;li&gt;库名前缀：标识数据的来源逻辑库。如 &lt;code class=&quot;inline&quot;&gt;&quot;schema_&quot;&lt;/code&gt; 应用于 &lt;code class=&quot;inline&quot;&gt;schema_2&lt;/code&gt; 逻辑库时，表示去除前缀后剩下的部分（数字 &lt;code class=&quot;inline&quot;&gt;2&lt;/code&gt;）将以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;li&gt;表名前缀：标识数据的来源表。如 &lt;code class=&quot;inline&quot;&gt;&quot;table_&quot;&lt;/code&gt; 应用于 &lt;code class=&quot;inline&quot;&gt;table_3&lt;/code&gt; 表时，表示去除前缀后剩下的部分（数字 &lt;code class=&quot;inline&quot;&gt;3&lt;/code&gt;）将以二进制的形式作为前缀的一部分添加到转换后的值中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;各部分在经过转换后的数值中的二进制分布如下图所示（各部分默认所占用的 bits 位数如图所示）：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77e7308f132193e01dc614ccadb81ebb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;588&quot; data-rawheight=&quot;48&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;假如转换前的原始数据为 &lt;code class=&quot;inline&quot;&gt;123&lt;/code&gt;，且有如上的 arguments 参数设置，则转换后的值为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;1&amp;lt;&amp;lt;(64-1-4) | 2&amp;lt;&amp;lt;(64-1-4-7) | 3&amp;lt;&amp;lt;(64-1-4-7-8) | 123&lt;/code&gt;&lt;p&gt;另外，arguments 中的 3 个参数均可设置为空字符串（&lt;code class=&quot;inline&quot;&gt;&quot;&quot;&lt;/code&gt;），即表示该部分不被添加到转换后的值中，且不占用额外的 bits。比如将其设置为&lt;code class=&quot;inline&quot;&gt;[&quot;1&quot;, &quot;&quot;, &quot;table_&quot;]&lt;/code&gt;，则转换后的值为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;1 &amp;lt;&amp;lt; (64-1-4) | 3&amp;lt;&amp;lt; (64-1-4-8) | 123&lt;/code&gt;&lt;p&gt;有关 column mapping 功能的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/column-mapping&quot;&gt;column-mapping pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;sharding DDL&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 table router 和 column mapping 功能，DML 的合库合表数据同步已经可以正常进行了。但如果在增量数据同步的过程中，上游待合并的分表上执行了 DDL 操作，则可能出现问题。我们先来看一个简化后的在分表上执行 DDL 的例子。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-428a086f546ef3d7ab3b0223f179c2e6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-428a086f546ef3d7ab3b0223f179c2e6&quot; data-watermark-src=&quot;v2-a786537af890679590b690c35f516fd3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在上图的例子中，分表的合库合表简化成了上游只有两个 MySQL 实例，每个实例内只有一个表。假设在开始数据同步时，将两个分表的表结构 schema 的版本记为 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt;，将 DDL 执行完成后的表结构 schema 的版本记为 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;现在，假设数据同步过程中，从两个上游分表收到的 binlog 数据有如下的时序：&lt;/p&gt;&lt;p&gt;1. 开始同步时，从两个分表收到的都是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;2. 在 t1 时刻，收到实例 1 上分表的 DDL。&lt;/p&gt;&lt;p&gt;3. 从 t2 时刻开始，从实例 1 收到的是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML；但从实例 2 收到的仍是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻，收到实例 2 上分表的 DDL。&lt;/p&gt;&lt;p&gt;5. 从 t4 时刻开始，从实例 2 收到的也是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;假设在数据同步过程中，不对分表的 DDL 进行处理。当将实例 1 的 DDL 同步到下游后，下游的表结构会变更成为 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt;。但对于实例 2，在 t2 时刻到 t3 时刻这段时间内收到的仍然是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。当尝试把这些与 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 对应的 DML 同步到下游时，就会由于 DML 与表结构的不一致而发生错误，造成数据无法正确同步。&lt;/p&gt;&lt;p&gt;继续使用上面的例子，来看看我们在 DM 中是如何处理合库合表过程中的 DDL 同步的。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b986329d81a846f97d3f1612a8a61402_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;600&quot; data-rawheight=&quot;277&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b986329d81a846f97d3f1612a8a61402&quot; data-watermark-src=&quot;v2-4decdb25db89abc60ad666693e38a44f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个例子中，DM-worker-1 用于同步来自 MySQL 实例 1 的数据，DM-worker-2 用于同步来自 MySQL 实例 2 的数据，DM-master 用于协调多个 DM-worker 间的 DDL 同步。从 DM-worker-1 收到 DDL 开始，简化后的 DDL 同步流程为：&lt;/p&gt;&lt;p&gt;1. DM-worker-1 在 t1 时刻收到来自 MySQL 实例 1 的 DDL，自身暂停该 DDL 对应任务的 DDL 及 DML 数据同步，并将 DDL 相关信息发送给 DM-master。&lt;/p&gt;&lt;p&gt;2. DM-master 根据 DDL 信息判断需要协调该 DDL 的同步，为该 DDL 创建一个锁，并将 DDL 锁信息发回给 DM-worker-1，同时将 DM-worker-1 标记为这个锁的 owner。&lt;/p&gt;&lt;p&gt;3. DM-worker-2 继续进行 DML 的同步，直到在 t3 时刻收到来自 MySQL 实例 2 的 DDL，自身暂停该 DDL 对应任务的数据同步，并将 DDL 相关信息发送给 DM-master。&lt;/p&gt;&lt;p&gt;4. DM-master 根据 DDL 信息判断该 DDL 对应的锁信息已经存在，直接将对应锁信息发回给 DM-worker-2。&lt;/p&gt;&lt;p&gt;5. DM-master 根据启动任务时的配置信息、上游 MySQL 实例分表信息、部署拓扑信息等，判断得知已经收到了需要合表的所有上游分表的该 DDL，请求 DDL 锁的 owner（DM-worker-1）向下游同步执行该 DDL。&lt;/p&gt;&lt;p&gt;6. DM-worker-1 根据 step 2 时收到的 DDL 锁信息验证 DDL 执行请求；向下游执行 DDL，并将执行结果反馈给 DM-master；若执行 DDL 成功，则自身开始继续同步后续的（从 t2 时刻对应的 binlog 开始的）DML。&lt;/p&gt;&lt;p&gt;7. DM-master 收到来自 owner 执行 DDL 成功的响应，请求在等待该 DDL 锁的所有其他 DM-worker（DM-worker-2）忽略该 DDL ，直接继续同步后续的（从 t4 时刻对应的 binlog 开始的）DML。&lt;/p&gt;&lt;p&gt;根据上面 DM 处理多个 DM-worker 间的 DDL 同步的流程，归纳一下 DM 内处理多个 DM-worker 间 sharding DDL 同步的特点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;根据任务配置与 DM 集群部署拓扑信息，在 DM-master 内建立一个需要协调 DDL 同步的逻辑 sharding group，group 中的成员为处理该任务拆解后各子任务的 DM-worker。&lt;/li&gt;&lt;li&gt;各 DM-worker 在从 binlog event 中获取到 DDL 后，会将 DDL 信息发送给 DM-master。&lt;/li&gt;&lt;li&gt;DM-master 根据来自 DM-worker 的 DDL 信息及 sharding group 信息创建/更新 DDL 锁。&lt;/li&gt;&lt;li&gt;如果 sharding group 的所有成员都收到了某一条 DDL，则表明上游分表在该 DDL 执行前的 DML 都已经同步完成，可以执行 DDL，并继续后续的 DML 同步。&lt;/li&gt;&lt;li&gt;上游分表的 DDL 在经过 table router 转换后，对应需要在下游执行的 DDL 应该一致，因此仅需 DDL 锁的 owner 执行一次即可，其他 DM-worker 可直接忽略对应的 DDL。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从 DM 处理 DM-worker 间 sharding DDL 同步的特点，可以看出该功能存在以下一些限制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游的分表必须以相同的顺序执行（table router 转换后相同的）DDL，比如表 1 先增加列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt; 后再增加列 &lt;code class=&quot;inline&quot;&gt;b&lt;/code&gt;，而表 2 先增加列 &lt;code class=&quot;inline&quot;&gt;b&lt;/code&gt; 后再增加列 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt;，这种不同顺序的 DDL 执行方式是不支持的。&lt;/li&gt;&lt;li&gt;一个逻辑 sharding group 内的所有 DM-worker 对应的上游分表，都应该执行对应的 DDL，比如其中有 DM-worker-2 对应的上游分表未执行 DDL，则其他已执行 DDL 的 DM-worker 都会暂停同步任务，等待 DM-worker-2 收到对应上游的 DDL。&lt;/li&gt;&lt;li&gt;由于已经收到的 DDL 的 DM-worker 会暂停任务以等待其他 DM-worker 收到对应的 DDL，因此数据同步延迟会增加。&lt;/li&gt;&lt;li&gt;增量同步开始时，需要合并的所有上游分表结构必须一致，才能确保来自不同分表的 DML 可以同步到一个确定表结构的下游，也才能确保后续各分表的 DDL 能够正确匹配与同步。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在上面的示例中，每个 DM-worker 对应的上游 MySQL 实例中只有一个需要进行合并的分表。但在实际场景下，一个 MySQL 实例可能有多个分库内的多个分表需要进行合并，比如前面介绍 table router 与 column mapping 功能时的例子。当一个 MySQL 实例中有多个分表需要合并时，sharding DDL 的协调同步过程增加了更多的复杂性。&lt;/p&gt;&lt;p&gt;假设同一个 MySQL 实例中有 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 两个分表需要进行合并，如下图：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d03c0bdab2df38bb7776ba74b13c0a82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;96&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;由于数据来自同一个 MySQL 实例，因此所有数据都是从同一个 binlog 流中获得。在这个例子中，时序如下：&lt;/p&gt;&lt;p&gt;1. 开始同步时，两个分表收到的数据都是 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;2. 在 t1 时刻，收到了 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL。&lt;/p&gt;&lt;p&gt;3. 从 t2 时刻到 t3 时刻，收到的数据同时包含 table_1 schema V2 的 DML 及 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻，收到了 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 DDL。&lt;/p&gt;&lt;p&gt;5. 从 t4 时刻开始，两个分表收到的数据都是 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML。&lt;/p&gt;&lt;p&gt;假设在数据同步过程中不对 DDL 进行特殊处理，当 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL 同步到下游、变更下游表结构后，&lt;code class=&quot;inline&quot;&gt;table_2 schema V1&lt;/code&gt;的 DML 将无法正常同步。因此，在单个 DM-worker 内部，我们也构造了与 DM-master 内类似的逻辑 sharding group，但 group 的成员是同一个上游 MySQL 实例的不同分表。&lt;/p&gt;&lt;p&gt;但 DM-worker 内协调处理 sharding group 的同步不能完全与 DM-master 处理时一致，主要原因包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当收到 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL 时，同步不能暂停，需要继续解析 binlog 才能获得后续 &lt;code class=&quot;inline&quot;&gt;table_2 &lt;/code&gt;的 DDL，即需要从 t2 时刻继续向前解析直到 t3 时刻。&lt;/li&gt;&lt;li&gt;在继续解析 t2 时刻到 t3 时刻的 binlog 的过程中，&lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; 的 DML 不能向下游同步；但在 sharding DDL 同步并执行成功后，这些 DML 需要同步到下游。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;在 DM 中，简化后的 DM-worker 内 sharding DDL 同步流程为：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 在 t1 时刻收到 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 DDL，记录 DDL 信息及此时的 binlog 位置点信息。&lt;/p&gt;&lt;p&gt;2. 继续向前解析 t2 时刻到 t3 时刻的 binlog。&lt;/p&gt;&lt;p&gt;3. 对于属于 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; DML，忽略；对于属于 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V1&lt;/code&gt; DML，正常同步到下游。&lt;/p&gt;&lt;p&gt;4. 在 t3 时刻收到 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 DDL，记录 DDL 信息及此时的 binlog 位置点信息。&lt;/p&gt;&lt;p&gt;5. 根据同步任务配置信息、上游库表信息等，判断该 MySQL 实例上所有分表的 DDL 都已经收到；将 DDL 同步到下游执行、变更下游表结构。&lt;/p&gt;&lt;p&gt;6. 设置新的 binlog 流的解析起始位置点为 step 1 时保存的位置点。&lt;/p&gt;&lt;p&gt;7. 重新开始解析从 t2 时刻到 t3 时刻的 binlog。&lt;/p&gt;&lt;p&gt;8. 对于属于 &lt;code class=&quot;inline&quot;&gt;table_1&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;schema V2&lt;/code&gt; DML，正常同步到下游；对于属于 &lt;code class=&quot;inline&quot;&gt;table_2&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;shema V1&lt;/code&gt; DML，忽略。&lt;/p&gt;&lt;p&gt;9. 解析到达 step 4 时保存的 binlog 位置点，可得知在 step 3 时被忽略的所有 DML 都已经重新同步到下游。&lt;/p&gt;&lt;p&gt;10. 继续从 t4 时刻对应的 binlog 位置点正常同步。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从上面的分析可以知道，DM 在处理 sharding DDL 同步时，主要通过两级 sharding group 来进行协调控制，简化的流程为：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 各 DM-worker 独立地协调对应上游 MySQL 实例内多个分表组成的 sharding group 的 DDL 同步。&lt;/p&gt;&lt;p&gt;2. 当 DM-worker 内所有分表的 DDL 都收到时，向 DM-master 发送 DDL 相关信息。&lt;/p&gt;&lt;p&gt;3. DM-master 根据 DM-worker 发来的 DDL 信息，协调由各 DM-worker 组成的 sharing group 的 DDL 同步。&lt;/p&gt;&lt;p&gt;4. 当 DM-master 收到所有 DM-worker 的 DDL 信息时，请求 DDL lock 的 owner（某个 DM-worker） 执行 DDL。&lt;/p&gt;&lt;p&gt;5. owner 执行 DDL，并将结果反馈给 DM-master；自身开始重新同步在内部协调 DDL 同步过程中被忽略的 DML。&lt;/p&gt;&lt;p&gt;6. 当 DM-master 发现 owner 执行 DDL 成功后，请求其他所有 DM-worker 开始继续同步。&lt;/p&gt;&lt;p&gt;7. 其他所有 DM-worker 各自开始重新同步在内部协调 DDL 同步过程中被忽略的 DML。&lt;/p&gt;&lt;p&gt;8. 所有 DM-worker 在重新同步完成被忽略的 DML 后，继续正常同步。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据同步过滤&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在进行数据同步的过程中，有时可能并不需要将上游所有的数据都同步到下游，这时一般期望能在同步过程中根据某些规则，过滤掉部分不期望同步的数据。在 DM 中，支持 2 种不同级别的同步过滤方式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;库表黑白名单&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DM 在 dumper、loader、syncer 三个处理单元中都支持配置规则只同步/不同步部分库或表。&lt;/p&gt;&lt;p&gt;对于 dumper 单元，其实际调用 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper&lt;/a&gt; 来 dump 上游 MySQL 的数据。比如只期望导出 test 库中的 t1、t2 两个表的数据，则可以为 dumper 单元配置如下规则：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-dump-rule:
    extra-args: &quot;-B test -T t1,t2&quot;&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;name-of-dump-rule&lt;/code&gt;：规则名，用户指定。当有多个上游实例需要使用相同的规则时，可以只定义一条规则，多个不同的实例通过规则名进行引用。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;extra-args&lt;/code&gt;：dumper 单元额外参数。除 dumper 单元中明确定义的配置项外的其他所有 mydumper 配置项都通过此参数传入，格式与使用 mydumper 时一致。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有关 mydumper 对库表黑白名单的支持，可查看 mydumper 的参数及 &lt;a href=&quot;https://github.com/pingcap/mydumper&quot;&gt;mydumper 的源码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;对于 loader 和 syncer 单元，其对应的库表黑白名单规则为 &lt;code class=&quot;inline&quot;&gt;black-white-list&lt;/code&gt;。假设只期望同步 test 库中的 t1、t2 两个表的数据，则可配置如下规则：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-bwl-rule:
    do-tables:
    - db-name: &quot;test&quot;
      tbl-name: &quot;t1&quot;
    - db-name: &quot;test&quot;
      tbl-name: &quot;t2&quot;&lt;/code&gt;&lt;p&gt;示例中只使用了该规则的部分配置项，完整的配置项及各配置项的含义，可阅读该功能对应的用户文档。DM 中该规则与 MySQL 的主从同步过滤规则类似，因此也可参考 &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-rules-db-options.html&quot;&gt;Evaluation of Database-Level Replication and Binary Logging Options&lt;/a&gt; 与 &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-rules-table-options.html&quot;&gt;Evaluation of Table-Level Replication Options&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;对于 loader 单元，在解析 SQL 文件名获得库名表名后，会与配置的黑白名单规则进行匹配，如果匹配结果为不需要同步，则会忽略对应的整个 SQL 文件。对于 syncer 单元，在解析 binlog 获得库名表名后，会与配置的黑白名单规则进行匹配，如果匹配结果为不需要同步，则会忽略对应的（部分） binlog event 数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;binlog event 过滤&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在进行增量数据同步时，有时会期望过滤掉某些特定类型的 binlog event，两个典型的场景包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游执行 &lt;code class=&quot;inline&quot;&gt;TRUNCATE TABLE&lt;/code&gt; 时不希望清空下游表中的数据。&lt;/li&gt;&lt;li&gt;上游分表上执行 &lt;code class=&quot;inline&quot;&gt;DROP TABLE&lt;/code&gt; 时不希望 &lt;code class=&quot;inline&quot;&gt;DROP&lt;/code&gt; 下游合并后的表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 DM 中支持根据 binlog event 的类型进行过滤，对于需要过滤 &lt;code class=&quot;inline&quot;&gt;TRUNCATE TABLE&lt;/code&gt; 与 &lt;code class=&quot;inline&quot;&gt;DROP TABLE&lt;/code&gt; 的场景，可配置规则如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;name-of-filter-rule:
    schema-pattern: &quot;test_*&quot;
    table-pattern: &quot;t_*&quot;
    events: [&quot;truncate table&quot;, &quot;drop table&quot;]
    action: Ignore&lt;/code&gt;&lt;p&gt;规则的匹配模式与 table router、column mapping 类似，具体的配置项可阅读该功能对应的用户文档。&lt;/p&gt;&lt;p&gt;在实现上，当解析 binlog event 获得库名、表名及 binlog event 类型后，与配置的规则进行匹配，并在匹配后依据 action 配置项来决定是否需要进行过滤。有关 binlog event 过滤功能的具体实现，可以阅读 TiDB-Tools 下的 &lt;a href=&quot;https://github.com/pingcap/tidb-tools/tree/master/pkg/binlog-filter&quot;&gt;binlog-filter pkg 源代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/#TiDB-Ecosystem-Tools&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-26-53357816</guid>
<pubDate>Wed, 26 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 助力东南亚领先电商 Shopee 业务升级</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-25-53257583.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53257583&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-03b7f66215d7a0d664732db2fb9d5c0b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;刘春辉，Shopee DBA&lt;br&gt;洪超，Shopee DBA&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;一、业务场景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Shopee（https://shopee.com/）是东南亚和台湾地区领先的电子商务平台，覆盖新加坡、马来西亚、菲律宾、印度尼西亚、泰国、越南和台湾等七个市场。Shopee 母公司 Sea（https://seagroup.com/）为首家在纽约证券交易所上市的东南亚互联网企业。2015 年底上线以来，Shopee 业务规模迅速扩张，逐步成长为区域内发展最为迅猛的电商平台之一：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;截止 2018 年第三季度 Shopee APP 总下载量达到 1.95 亿次，平台卖家数量超过 700 万。&lt;/li&gt;&lt;li&gt;2018 年第一季度和第二季度 GMV 分别为 19 亿美金和 22 亿美金，2018 上半年的 GMV 已达到 2017 全年水平。2018 年第三季度 GMV 达到了创纪录的 27 亿美元, 较 2017 年同期年增长率为 153%。&lt;/li&gt;&lt;li&gt;2018 年双 11 促销日，Shopee 单日订单超过 1100 万，是 2017 年双 11 的 4.5 倍；刚刚过去的双 12 促销日再创新高，实现单日 1200 万订单。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-322edab2ce7fb2f69f744aa4d20709c5_r.jpg&quot; data-caption=&quot;图 1  Shopee 电商平台展示图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;841&quot; data-rawheight=&quot;583&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-322edab2ce7fb2f69f744aa4d20709c5&quot; data-watermark-src=&quot;v2-738d13f716f51b01868c64e5424c4fbe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;我们从 2018 年初开始调研 TiDB，6 月份上线了第一个 TiDB 集群。到目前为止我们已经有两个集群、60 多个节点在线运行，主要用于以下 Shopee 业务领域：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;风控系统：风控日志数据库是我们最早上线的一个 TiDB 集群，稍后详细展开。&lt;/li&gt;&lt;li&gt;审计日志系统：审计日志数据库存储每一个电商订单的支付和物流等状态变化日志。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文将重点展开风控日志数据库选型和上线的过程，后面也会约略提及上线后系统扩容和性能监控状况。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、选型：MySQL 分库分表 vs TiDB&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-38c37f53b5fba1b2f40dd4db3ecfbb7c_r.jpg&quot; data-caption=&quot;图 2  风控日志收集和处理示意图&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;689&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-38c37f53b5fba1b2f40dd4db3ecfbb7c&quot; data-watermark-src=&quot;v2-046de8c9fd315c566c34c8d3d3f52dab&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;风控系统基于大量历史订单以及用户行为日志，以实时和离线两种方式识别平台上的异常行为和欺诈交易。它的重要数据源之一是各种用户行为日志数据。最初我们将其存储于 MySQL 数据库，并按照 USER_ID 把数据均分为 100 个表。随着 Shopee 用户活跃度见长，数据体积开始疯长，到 2017 年底磁盘空间显得十分捉襟见肘了。作为应急措施，我们启用了 InnoDB 表透明压缩将数据体积减半；同时，我们把 MySQL 服务器磁盘空间从 2.5TB 升级到了 6TB。这两个措施为后续迁移 MySQL 数据到 TiDB 多争取了几个月时间。&lt;/p&gt;&lt;p&gt;关于水平扩容的实现方案，当时内部有两种意见：MySQL 分库分表和直接采用 TiDB。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. MySQL 分库分表&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基本思路：按照 USER_ID 重新均分数据（Re-sharding），从现有的 100 个表增加到1000 个甚至 10000 个表，然后将其分散到若干组 MySQL 数据库。&lt;/li&gt;&lt;li&gt;优点：继续使用 MySQL 数据库 ，不论开发团队还是 DBA 团队都驾轻就熟。&lt;/li&gt;&lt;li&gt;缺点：业务代码复杂度高。Shopee 内部若干个系统都在使用该数据库，同时我们还在使用 Golang 和 Python 两种编程语言，每一个系统都要改动代码以支持新的分库分表规则。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 直接采用 TiDB&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基本思路：把数据从 MySQL 搬迁至 TiDB，把 100 个表合并为一个表。&lt;/li&gt;&lt;li&gt;优点：数据库结构和业务逻辑都得以大幅简化。TiDB 会自动实现数据分片，无须客户端手动分表；支持弹性水平扩容，数据量变大之后可以通过添加新的 TiKV 节点实现水平扩展。理想状况下，我们可以把 TiDB 当做一个「无限大的 MySQL」来用，这一点对我们极具诱惑力。&lt;/li&gt;&lt;li&gt;缺点：TiDB 作为新组件首次引入 Shopee 线上系统，我们要做好「踩坑」的准备。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，我们决定采用 TiDB 方案，在 Shopee 内部做「第一个吃螃蟹的人」。风控日志数据库以服务离线系统为主，只有少许在线查询；这个特点使得它适合作为第一个迁移到 TiDB 的数据库。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、上线：先双写，后切换&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们的上线步骤大致如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;应用程序开启双写：日志数据会同时写入 MySQL 和 TiDB。&lt;/li&gt;&lt;li&gt;搬迁旧数据：把旧数据从 MySQL 搬到 TiDB，并完成校验确保新旧数据一致。&lt;/li&gt;&lt;li&gt;迁移只读流量：应用程序把只读流量从 MySQL 逐步迁移至 TiDB（如图 3 所示）。&lt;/li&gt;&lt;li&gt;停止双写：迁移过程至此结束。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b94de1689b47ff8554e67c1139c2bcba_r.jpg&quot; data-caption=&quot;图 3  迁移过程图：保持双写，逐步从读 MySQL 改为读 TiDB&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;609&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b94de1689b47ff8554e67c1139c2bcba&quot; data-watermark-src=&quot;v2-61c865b27080aae5a95c74cda41c7b45&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;双写方式使得我们可以把整个切换过程拖长至几个月时间。这期间开发团队和 DBA 团队有机会逐步熟悉新的 TiDB 集群，并充分对比新旧数据库的表现。理论上，在双写停掉之前，若新的 TiDB 集群遭遇短时间内无法修复的问题，则应用程序有可能快速回退到 MySQL。&lt;/p&gt;&lt;p&gt;除此之外，采用双写方式也让我们有了重构数据库设计的机会。这一次我们就借机按照用户所属地区把风控日志数据分别存入了七个不同的逻辑数据库：rc_sg，rc_my，rc_ph，…，rc_tw。Shopee 用户分布于七个不同地区。迁移到 TiDB 之前，所有日志数据共存于同一个逻辑数据库。按照地区分别存储使得我们能够更为方便地为每个地区的日志定制不同的数据结构。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、硬件配置和水平扩容&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上线之初我们一共从 MySQL 迁移了大约 4TB 数据到 TiDB 上。当时 TiDB 由 14 个节点构成，包括 3 个 PD 节点，3 个 SQL 节点和 8 个 TiKV 节点。服务器硬件配置如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiKV 节点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CPU: 2 * Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 40 cores&lt;/li&gt;&lt;li&gt;内存: 192GB&lt;/li&gt;&lt;li&gt;磁盘: 4 * 960GB Read Intensive SAS SSD Raid 5&lt;/li&gt;&lt;li&gt;网卡: 2 * 10gbps NIC Bonding&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;PD 节点和 SQL 节点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CPU: 2 * Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 40 cores&lt;/li&gt;&lt;li&gt;内存: 64GB&lt;/li&gt;&lt;li&gt;磁盘: 2 * 960GB Read Intensive SAS SSD Raid 1&lt;/li&gt;&lt;li&gt;网卡: 2 * 10gbps NIC Bonding&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;截至目前，系统已经平稳运行了六个多月，数据量增长至 35TB（如图 4 所示），经历了两次扩容后现在集群共包含 42 个节点。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bee608fafd6029e42b6132886a0d8a62_r.jpg&quot; data-caption=&quot;图 4  风控日志 TiDB 数据库存储容量和使用状况&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bee608fafd6029e42b6132886a0d8a62&quot; data-watermark-src=&quot;v2-ff047ea9ed148260290d1a14929b8a46&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;性能&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a5f96a061bb0aeb697528dc3c156ef70_r.jpg&quot; data-caption=&quot;图 5  风控日志 TiDB 数据库 QPS Total 曲线&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;406&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a5f96a061bb0aeb697528dc3c156ef70&quot; data-watermark-src=&quot;v2-bd9a725c717194b816d7a0815a6b79be&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;风控日志数据库的日常 QPS（如图 5 所示）一般低于每秒 20K，在最近的双 12 促销日我们看到峰值一度攀升到了每秒 100K 以上。&lt;/p&gt;&lt;p&gt;&lt;b&gt;尽管数据量较之 6 个月前涨了 8 倍，目前整个集群的查询响应质量仍然良好，大部分时间 pct99 响应时间（如图 6 所示）都小于 60ms。对于以大型复杂 SQL 查询为主的风控系统而言，这个级别的响应时间已经足够好了。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c53d50d8d4eba74de7bcbea4bc1636cc_r.jpg&quot; data-caption=&quot;图 6  风控日志 TiDB 数据库两天 pct99 查询响应时间曲线&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;356&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c53d50d8d4eba74de7bcbea4bc1636cc&quot; data-watermark-src=&quot;v2-68e0d62112838b47540305d8db0dd691&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;五、问题和对策&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;TiDB 的字符串匹配区分大小写（Case Sensitive）。目前尚不支持 Case Insensitive 方式。应用程序做了适配以实现 Case Insensitive 方式的字符串匹配。&lt;/li&gt;&lt;li&gt;TiDB 对于 MySQL 用户授权 SQL 语法的兼容支持尚不完善。例如，目前不支持 SHOW CREATE USER 语法，有时候不得不读取系统表（mysql.user）来查看一个数据库账户的基本信息。&lt;/li&gt;&lt;li&gt;添加 TiKV 节点后需要较长时间才能完成数据再平衡。据我们观察，1TB 数据大约需要 24 个小时才能完成拷贝。因此促销前我们会提前几天扩容和观察数据平衡状况。&lt;/li&gt;&lt;li&gt;TiDB  v1.x 版本以 region 数目为准在各个 TiKV 节点之间平衡数据。不过每个 region 的大小其实不太一致。这个问题导致不同 TiKV 节点的磁盘空间使用率存在明显差异。据说新的 TiDB v2.x 对此已经做了优化，我们未来会尝试在线验证一下。&lt;/li&gt;&lt;li&gt;TiDB v1.x 版本需要定期手动执行 Analyze Table 以确保元信息准确。PingCAP 的同学告诉我们说：当 (Modify_count / Row_count) 大于 0.3 就要手动 Analyze Table 了。v2.x 版本已经支持自动更新元数据了。我们后续会考虑升级到新版本。&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;mysql&amp;gt; show stats_meta where db_name = &#39;aaa_db&#39;  \G
*************************** 1. row ***************************
     Db_name: aaa_db
  Table_name: xxx_tab
 Update_time: 2018-12-16 23:49:02
Modify_count: 166545248
   Row_count: 8568560708
1 row in set (0.00 sec)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;六、未来规划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;过去一年亲密接触之下，我们对 TiDB 的未来充满信心，相信 TiDB 会成为 Shopee 数据库未来实现弹性水平扩容和分布式事务的关键组件。当前我们正在努力让更多 Shopee 业务使用 TiDB。&lt;/p&gt;&lt;p&gt;我们规划把 Shopee 数据从 MySQL 迁移到 TiDB 上的路线是「先 Non-transactional Data（非交易型数据），后 Transactional Data（交易型数据）」。目前线上运行的集群都属于 Non-transactional Data，他们的特点是数据量超大（TB 级别），写入过程中基本不牵涉数据库事务。接下来我们会探索如何把一些 Transactional Data 迁移到 TiDB 上。&lt;/p&gt;&lt;p&gt;MySQL Replica 是另一个工作重点。MySQL Replica 指的是把 TiDB 作为 MySQL 的从库，实现从 MySQL 到 TiDB 实时复制数据。我们最近把订单数据从 MySQL 实时复制到 TiDB。后续来自 BI 系统以及部分对数据实时性要求不那么高的只读查询就可以尝试改为从 TiDB 读取数据了。这一类查询的特点是全表扫描或者扫描整个索引的现象较多，跑在 TiDB 可能比 MySQL 更快。当然，BI 系统也可以借助 TiSpark 绕过 SQL 层直接读取 TiKV 以提升性能。&lt;/p&gt;&lt;p&gt;目前我们基于物理机运行 TiDB 集群，DBA 日常要耗费不少精力去照顾这些服务器的硬件、网络和 OS。我们有计划把 TiDB 搬到 Shopee 内部的容器平台上，并构建一套工具实现自助式资源申请和配置管理，以期把 DBA 从日常运维的琐碎中解放出来。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;七、致谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;感谢 PingCAP 的同学一年来对我们的帮助和支持。每一次我们在微信群里提问，都能快速获得回应。官方架构师同学还不辞辛劳定期和我们跟进，详细了解项目进度和难点，总是能给出非常棒的建议。&lt;/p&gt;&lt;p&gt;PingCAP 的文档非常棒，结构层次完整清晰，细节翔实，英文文档也非常扎实。一路跟着读下来，受益良多。&lt;/p&gt;&lt;p&gt;TiDB 选择了 Golang 和 RocksDB，并坚信 SSD 会在数据库领域取代传统机械硬盘。这些也是 Shopee 技术团队的共识。过去几年间我们陆续把这些技术引入了公司的技术栈，在一线做开发和运维的同学相信都能真切体会到它们为 Shopee 带来的改变。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更多 TiDB 用户实践：&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-25-53257583</guid>
<pubDate>Tue, 25 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-21-52972108.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52972108&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91a6142a4423a2d939a72a9f4837d32c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是&lt;b&gt;矛盾螺旋队&lt;/b&gt;的成员刘玮，他们的项目&lt;b&gt;TiEye&lt;/b&gt;在 TiDB Hackathon 2018 中获得了三等奖。TiEye 是 Region 信息变迁历史可视化工具，通过 PD记录 Region 的Split、Merge、ConfChange、LeaderChange 等信息，可以方便的回溯 Region 某个时间的具体状态，为开发人员提供了方便的可视化展示界面及查询功能。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;TiKV 的 Region&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Region 是 TiKV 的一个数据调度单元，TiKV 将数据按照键值范围划分为很多个 Region，分在集群的多台机器上，通过调度 Region 来实现负载均衡以及数据存储的扩展，同时一个 Region 也是一个 Raft Group，一个 Region 分布在多个 TiKV 实例上（通常是 3 个或者 5 个），通过 Raft 算法保证多副本的强一致性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;动机&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这个项目的灵感是之前在查一些问题的时候想到的，因为我们很多时候需要去知道 Region 在某个时间的状态，这就需要通过日志从杂乱的信息中提取出来有用的信息来复原当时的场景，但实际并不是特别方便高效，尤其是在看多个 Region 之间的关系的时候。因此通过将 Region 信息变化历史可视化，希望能为开发者们在定位问题的时候提供一个方便直观的工具，同时还能通过它来分析 PD 的调度策略，以及调度带来的写放大问题等等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实际方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一开始我们考虑的是通过一个独立的服务去解析 PD 的日志来获取 Region 信息的变化历史，后来讨论后认为这样做不仅依赖于 PD 中的日志格式，造成系统耦合，同时 PD 的 leader 变迁导致日志内容不连续，以及日志中的信息并不是特别充分等问题也增加了开发难度。因此我们最后决定直接修改 PD 的源代码，在每次 PD 变更 Region 的时候，记录下这些信息并持久化。这样既能保证在 PD 切换 leader 后的变化信息的连续性，又提供了更加丰富的历史信息。同时，PD 添加相关的 API，以供前端进行查询。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 回顾&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们的团队由三个人组成，分别是我（刘玮）、周振靖和张博康，都毕业于北京邮电大学。我们在这次 Hackathon 之前就认识，因为大家都在北京，因此交流还是蛮方便的，在开赛大约一周前就确定了这个题目。其实我最初的想法是做一些有关于性能优化的事情，但是在跟队友们交流后还是决定做 Region 历史可视化，其更具有实用性，也更适合在 Hackathon 上来做。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;10:00 比赛正式开始。我们之前已经讨论好了项目的大体架构，因此没有再做过多的讨论就各自开始码代码了。博康负责后端框架以及 PD 相应的修改，我负责后端查询 API，振靖负责前端可视化。&lt;/li&gt;&lt;li&gt;12:15 午餐。休息片刻，继续码代码。&lt;/li&gt;&lt;li&gt;14:00 后端框架大体完成，已经可以在 PD 中收集 Region 相应的状态变化；前端部分已经画出简单的 Region 分裂、合并等示意图。&lt;/li&gt;&lt;li&gt;17:30 完成了最简单的查询逻辑，进行了第一次联调，发现大家对于 Region 状态的展示方式理解不一样，于是再次讨论统一了意见。&lt;/li&gt;&lt;li&gt;18:00 晚餐时间。&lt;/li&gt;&lt;li&gt;19:00 ~ 次日 2:30: 我们基本完成了后端开发，而前端这时还剩比较多的工作量。同时晚上在前端展示，后端查询 API，数据持久化方面都发现了几个 bug，大家一直忙到很晚才一一解决。&lt;/li&gt;&lt;li&gt;次日 9:00 返回赛场，抽签确定 Demo 时间，最终为第四个出场。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7c8a46afd26bf52c3480899fc07e9c44_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7c8a46afd26bf52c3480899fc07e9c44&quot; data-watermark-src=&quot;v2-48b604cf5bc5bd3891d258d8cd0e99f0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;次日 12:00 前端可视化基本完善，为界面做最后的调整。&lt;/li&gt;&lt;li&gt;次日 12:00 ~ 12:30 午餐时间&lt;/li&gt;&lt;li&gt;次日 13:00 ~ 14:00 准备 PPT 和展示录屏&lt;/li&gt;&lt;li&gt;次日 14:30 ~ 18:30 Demo Time（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiEye 架构&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们采取了前后端分离的架构。&lt;/p&gt;&lt;p&gt;前端是 Vue.js 框架，使用 Typescript 语言开发。由于看上去现有的图表库啥的并不能很好地满足我们的需求，所以前端同学决定手撸 SVG。&lt;/p&gt;&lt;p&gt;后端则是 PD 提供的 API。数据存储目前暂时存储在 etcd，将来会考虑其它方案来应对数据规模太大的情况。我们将 Region 的变化分成了以下四种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;LeaderChange：Raft Group 选举（或者是主动移交）了新的 leader&lt;/li&gt;&lt;li&gt;ConfChange：Raft Group 成员变更&lt;/li&gt;&lt;li&gt;Split：当某个 Region 数据超过一定阙值时（或被手动干预时）会分裂成键值范围相邻的两个 Region&lt;/li&gt;&lt;li&gt;Merge：两个键值范围连续的 Region 合并成一个&lt;/li&gt;&lt;li&gt;Bootstrap：一个新的集群中第一个 Region 产生&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在前端的表示方式如图所示：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a09e108c7302afe3c25b0e4ec20ed9e0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;912&quot; data-rawheight=&quot;431&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a09e108c7302afe3c25b0e4ec20ed9e0&quot; data-watermark-src=&quot;v2-05f0cdcb662a552e165ed910254fd6f1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;给 PD 添加的 API 则有如下几种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/list&lt;/code&gt;，GET 方法，返回全部历史。&lt;/li&gt;&lt;li&gt;返回结果:&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;[
   {
       &quot;timestamp&quot;:1544286220000000,
       &quot;leader_store_id&quot;:0,
       &quot;event_type&quot;:&quot;Bootstrap&quot;,
       &quot;Region&quot;:{
           &quot;id&quot;:2,
           &quot;start_key&quot;:&quot;&quot;,
           &quot;end_key&quot;:&quot;&quot;,
           &quot;Region_epoch&quot;:{
               &quot;conf_ver&quot;:1,
               &quot;version&quot;:1
           },

           &quot;peers&quot;:[
               {
                   &quot;id&quot;:3,
                   &quot;store_id&quot;:1
               }
           ]
       },
       &quot;parents&quot;:[],
       &quot;children&quot;:26
   },
   ...
]
&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/Region/{RegionId}&lt;/code&gt;，GET 方法，查询某个 Region 的变化历史，返回结果同上。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/pd/api/v1/history/key/{key}&lt;/code&gt;，GET 方法，查询某个 key 所属 Region 的变化历史，返回结果同上。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上几个 API 均可附加起止时间参数（时间戳），如：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;/pd/api/v1/history/list?start=0&amp;amp;end=1544286229000000&lt;/code&gt;&lt;p&gt;顺便一提，前端部分原先打算作为 PD 的一部分来提供，与 PD 一起构建（于是前端的代码也放进了 PD 的一个单独的文件夹里）。但是后来觉得对于不涉及这些前端代码的开发者来说这样做不太好，所以我们之后会抽时间将这些前端代码放进一个单独的仓库里。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;测试过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;测试的时候我们部署了1个 TiDB，6个 TiKV，3个 PD，通过 sysbench 导入少量数据，最后通过开启 random-merge-scheduler 来进行随机合并 Region。下图是我们的测试过程中的结果展示：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-52b313df099b00a39576a6fc7714cc05_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;449&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-52b313df099b00a39576a6fc7714cc05&quot; data-watermark-src=&quot;v2-eaef3f65ba836a74cdfa28aeae033769&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;此时通过我们的工具还意外发现了一个 bug。&lt;/p&gt;&lt;p&gt;可以在上图看到，在第一个红框处 Region 2 合并进 Region 28，然后第二红框那里已经被 merge 进 Region 28 的 Region 2 莫名其妙地又连接了后面的 Region 40，显然这里是有问题的。经过通过日志确认，这是由于 PD 收到了一个含有过期 Region 2 信息的心跳导致的，追根溯源发现是 TiKV 中的 pd-client 的一个 Bug 导致了在与 PD 重连后会发送一个过期的心跳信息（Bug 地址在 &lt;a href=&quot;https://github.com/tikv/tikv/issues/3868&quot;&gt;https://github.com/tikv/tikv/issues/3868&lt;/a&gt;）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实际运行结果&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;横轴表示时间，纵轴表示 Region 的存储键值的顺序（仅表示顺序，不代表实际的数据量），矩形上的数字表示 Region id，为了便于理解，所有的 Region 的最终状态都会在最后的时间点上展示出来（即使在这个时间点没有发生 Region 的改变）。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2e3dc2e9d527601c390b67262d612a28_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;775&quot; data-rawheight=&quot;504&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2e3dc2e9d527601c390b67262d612a28&quot; data-watermark-src=&quot;v2-5a386471b93335a3eff5a5d593c74de4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;点击右上角可以更改查下的时间范围。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c47ebd4cd54100f5c1a89a0f898d7d1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;747&quot; data-rawheight=&quot;493&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c47ebd4cd54100f5c1a89a0f898d7d1b&quot; data-watermark-src=&quot;v2-c0d25e05977246c27133a4b13df90eef&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;右上角可以设置按照 key 的范围对齐，效果如下图：&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ebf0ac8520a5bf184bebb85558805eb9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;616&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ebf0ac8520a5bf184bebb85558805eb9&quot; data-watermark-src=&quot;v2-64a3bb508b3b6d094bb3952bf4831f40&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;点击任何一个节点，会展示当时 Region 的详细信息。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fa062306dd0f7ac8196cb021e520aa89_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;877&quot; data-rawheight=&quot;589&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fa062306dd0f7ac8196cb021e520aa89&quot; data-watermark-src=&quot;v2-5874a61e30526155791697ea710a4900&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;拖动下方的框可以对局部进行缩放（你也可以通过查询更小的时间范围达到同样的效果）。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bbf63e89becb32ab0bbbdd53164edec3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;849&quot; data-rawheight=&quot;539&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bbf63e89becb32ab0bbbdd53164edec3&quot; data-watermark-src=&quot;v2-1a043712c79c51d425063ce7b51f6785&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Hackathon Demo&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们团队的 Demo 展示是博康负责的。一开始他还担心如果演讲的时候忘词了怎么办，不过最后展示效果很不错，整个 Demo show 进行得非常顺利（P.S. 要是展示时间能多给几分钟就好了）。&lt;/p&gt;&lt;p&gt;在展示中，我们也看见了其他团队的作品也都非常棒。这其中让我最感兴趣的是有个团队做的是以 TiKV 作为数据存储的 etcd。这个选题一开始我也考虑过，因为我在工作中实际已经遇到了这个问题，不过最后和队友商量后还是选择了现在这个题目。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们“矛盾螺旋”团队最终获得了三等奖，这对我来说简直是意外之喜。在演示中，很多别的团队也都做得十分优秀，我们在观看其它团队的演示时几乎都觉得获奖无望了。最后却拿到了三等奖，实在是意料之外。这次我们之所以能够获奖，一方面是选题选得恰到好处，具有一定的实际作用，同时工作量又能保证在 Hackathon 期间完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后感谢我的两位队友，谢谢导师，谢谢评委老师，谢谢 PingCAP 的所有工作人员为这次 Hackathon 所做的努力。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487370&amp;amp;idx=1&amp;amp;sn=72d9d52558e83eb97cd709c67b5a4149&amp;amp;chksm=eb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487370&amp;amp;idx=2&amp;amp;sn=7eb3d41b2b5cf2a8a440b12121796e2d&amp;amp;chksm=eb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7&amp;amp;scene=21#wechat_redirect&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487451&amp;amp;idx=2&amp;amp;sn=5f1ee6e838c3a86556fcd556662112c5&amp;amp;scene=21#wechat_redirect&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487479&amp;amp;idx=1&amp;amp;sn=8a8861419dd22344a021667545005769&amp;amp;scene=21#wechat_redirect&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487479&amp;amp;idx=2&amp;amp;sn=3a601b2ff9100a9797605a825e478c01&amp;amp;scene=21#wechat_redirect&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-21-52972108</guid>
<pubDate>Fri, 21 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十二）Hash Aggregation</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-21-52969666.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52969666&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2e665a8d2aeb4f9ccf89d305b5638fe8_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;聚合算法执行原理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 SQL 中，聚合操作对一组值执行计算，并返回单个值。TiDB 实现了 2 种聚合算法：Hash Aggregation 和 Stream Aggregation。&lt;/p&gt;&lt;p&gt;我们首先以 &lt;code class=&quot;inline&quot;&gt;AVG&lt;/code&gt; 函数为例（案例参考 &lt;a href=&quot;https://stackoverflow.com/questions/1471147/how-do-aggregates-group-by-work-on-sql-server/1471167#1471167&quot;&gt;Stack Overflow&lt;/a&gt;），简述这两种算法的执行原理。&lt;/p&gt;&lt;p&gt;假设表 &lt;code class=&quot;inline&quot;&gt;t&lt;/code&gt; 如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b2dd18d0611f1126eefb231f6ee49292_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1232&quot; data-rawheight=&quot;504&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b2dd18d0611f1126eefb231f6ee49292&quot; data-watermark-src=&quot;v2-8db01296a4397381f572bfd5a329c488&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;SQL:&lt;code class=&quot;inline&quot;&gt;select avg(b) from t group by a&lt;/code&gt;, 要求将表&lt;code class=&quot;inline&quot;&gt;t&lt;/code&gt;的数据按照&lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt;的值分组，对每一组的&lt;code class=&quot;inline&quot;&gt;b&lt;/code&gt;值计算平均值。不管 Hash 还是 Stream 聚合，在&lt;code class=&quot;inline&quot;&gt;AVG&lt;/code&gt;函数的计算过程中，我们都需要维护 2 个中间结果变量&lt;code class=&quot;inline&quot;&gt;sum&lt;/code&gt;和&lt;code class=&quot;inline&quot;&gt;count&lt;/code&gt;。Hash 和 Stream 聚合算法的执行原理如下。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Hash Aggregate 的执行原理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Hash Aggregate 的计算过程中，我们需要维护一个 Hash 表，Hash 表的键为聚合计算的 &lt;code class=&quot;inline&quot;&gt;Group-By&lt;/code&gt; 列，值为聚合函数的中间结果 &lt;code class=&quot;inline&quot;&gt;sum&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;count&lt;/code&gt;。在本例中，键为 &lt;code class=&quot;inline&quot;&gt;列 a&lt;/code&gt; 的值，值为 &lt;code class=&quot;inline&quot;&gt;sum(b)&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;count(b)&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;计算过程中，只需要根据每行输入数据计算出键，在 Hash 表中找到对应值进行更新即可。对本例的执行过程模拟如下。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4c0e089d8cb104047a03624a62c0aa40_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;508&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4c0e089d8cb104047a03624a62c0aa40&quot; data-watermark-src=&quot;v2-a811115327639d12cea5c777a01121dc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;输入数据输入完后，扫描 Hash 表并计算，便可以得到最终结果：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-26730a2dd30e86d15333c71ed9f077a2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;230&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-26730a2dd30e86d15333c71ed9f077a2&quot; data-watermark-src=&quot;v2-99647b66dfb1d3e931cb250e08325d72&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Stream Aggregation 的执行原理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Stream Aggregate 的计算需要保证输入数据&lt;b&gt;按照&lt;/b&gt; &lt;b&gt;&lt;code class=&quot;inline&quot;&gt;Group-By&lt;/code&gt;&lt;/b&gt; &lt;b&gt;列有序&lt;/b&gt;。在计算过程中，每当读到一个新的 Group 的值或所有数据输入完成时，便对前一个 Group 的聚合最终结果进行计算。&lt;/p&gt;&lt;p&gt;对于本例，我们首先对输入数据按照 &lt;code class=&quot;inline&quot;&gt;a&lt;/code&gt; 列进行排序。排序后，本例执行过程模拟如下。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6a3a1877235777488a3eeb310f583388_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1592&quot; data-rawheight=&quot;658&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6a3a1877235777488a3eeb310f583388&quot; data-watermark-src=&quot;v2-ce83f2fe1e9f559028fd5c0552a46552&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;因为 Stream Aggregate 的输入数据需要保证同一个 Group 的数据连续输入，所以 Stream Aggregate 处理完一个 Group 的数据后可以立刻向上返回结果，不用像 Hash Aggregate 一样需要处理完所有数据后才能正确的对外返回结果。当上层算子只需要计算部分结果时，比如 Limit，当获取到需要的行数后，可以提前中断 Stream Aggregate 后续的无用计算。&lt;/p&gt;&lt;p&gt;当 &lt;code class=&quot;inline&quot;&gt;Group-By&lt;/code&gt; 列上存在索引时，由索引读入数据可以保证输入数据按照 &lt;code class=&quot;inline&quot;&gt;Group-By&lt;/code&gt; 列有序，此时同一个 Group 的数据连续输入 Stream Aggregate 算子，可以避免额外的排序操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 聚合函数的计算模式&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于分布式计算的需要，TiDB 对于聚合函数的计算阶段进行划分，相应定义了 5 种计算模式：CompleteMode，FinalMode，Partial1Mode，Partial2Mode，DedupMode。不同的计算模式下，所处理的输入值和输出值会有所差异，如下表所示：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a47f00865b6947802ad23614be6612f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;826&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a47f00865b6947802ad23614be6612f5&quot; data-watermark-src=&quot;v2-e0330d54b46bfda4ffe89a871977b0b6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;以上文提到的 &lt;code class=&quot;inline&quot;&gt;select avg(b) from t group by a&lt;/code&gt; 为例，通过对计算阶段进行划分，可以有多种不同的计算模式的组合，如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CompleteMode&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此时 &lt;code class=&quot;inline&quot;&gt;AVG&lt;/code&gt; 函数的整个计算过程只有一个阶段，如图所示：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-567c7fc043936b972b03a13e10b11da3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;470&quot; data-rawheight=&quot;493&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-567c7fc043936b972b03a13e10b11da3&quot; data-watermark-src=&quot;v2-18d7246583684dfce6c22c1769a8f4e7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;Partial1Mode –&amp;gt; FinalMode&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此时我们将 &lt;code class=&quot;inline&quot;&gt;AVG&lt;/code&gt; 函数的计算过程拆成两个阶段进行，如图所示：&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-df34ce6dfe2919f80ebd4aa950f2480c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;556&quot; data-rawheight=&quot;637&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-df34ce6dfe2919f80ebd4aa950f2480c&quot; data-watermark-src=&quot;v2-638ea8080df76ae1255c6e340518b5ce&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;除了上面的两个例子外，还可能有如下的几种计算方式：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;聚合被下推到 TiKV 上进行计算（Partial1Mode），并返回经过预聚合的中间结果。为了充分利用 TiDB server 所在机器的 CPU 和内存资源，加快 TiDB 层的聚合计算，TiDB 层的聚合函数计算可以这样进行：Partial2Mode –&amp;gt; FinalMode。&lt;/li&gt;&lt;li&gt;当聚合函数需要对参数进行去重，也就是包含 &lt;code class=&quot;inline&quot;&gt;DISTINCT&lt;/code&gt; 属性，且聚合算子因为一些原因不能下推到 TiKV 时，TiDB 层的聚合函数计算可以这样进行：DedupMode –&amp;gt; Partial1Mode –&amp;gt; FinalMode。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;聚合函数分为几个阶段执行， 每个阶段对应的模式是什么，是否要下推到 TiKV，使用 Hash 还是 Stream 聚合算子等都由优化器根据数据分布、估算的计算代价等来决定。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 并行 Hash Aggregation 的实现&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;如何构建 Hash Aggregation 执行器&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/planner/core/logical_plan_builder.go#L95&quot;&gt;构建逻辑执行计划&lt;/a&gt; 时，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0/expression/aggregation/descriptor.go#L49&quot;&gt;NewAggFuncDesc&lt;/a&gt; 将聚合函数的元信息封装为一个 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/descriptor.go#L35-L46&quot;&gt;AggFuncDesc&lt;/a&gt;。 其中 &lt;code class=&quot;inline&quot;&gt;AggFuncDesc.RetTp&lt;/code&gt; 由 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/descriptor.go#L146-L163&quot;&gt;AggFuncDesc.typeInfer&lt;/a&gt; 根据聚合函数类型及参数类型推导而来；&lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode&lt;/code&gt; 统一初始化为 CompleteMode。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go#L487&quot;&gt;构建物理执行计划&lt;/a&gt;时，&lt;code class=&quot;inline&quot;&gt;PhysicalHashAgg&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;PhysicalStreamAgg&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;attach2Task&lt;/code&gt; 方法会根据当前 &lt;code class=&quot;inline&quot;&gt;task&lt;/code&gt; 的类型尝试进行下推聚合计算，如果 &lt;code class=&quot;inline&quot;&gt;task&lt;/code&gt; 类型满足下推的基本要求，比如 &lt;code class=&quot;inline&quot;&gt;copTask&lt;/code&gt;，接着会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go#L380&quot;&gt;newPartialAggregate&lt;/a&gt; 尝试将聚合算子拆成 TiKV 上执行的 Partial 算子和 TiDB 上执行的 &lt;code class=&quot;inline&quot;&gt;Final&lt;/code&gt; 算子，其中 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/expression/aggregation/agg_to_pb.go#L25&quot;&gt;AggFuncToPBExpr&lt;/a&gt; 函数用来判断某个聚合函数是否可以下推。若聚合函数可以下推，则会在 TiKV 中进行预聚合并返回中间结果，因此需要将 TiDB 层执行的 &lt;code class=&quot;inline&quot;&gt;Final&lt;/code&gt; 聚合算子的 &lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode&lt;/code&gt; &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go#L427&quot;&gt;修改为 FinalMode&lt;/a&gt;，并将其 &lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Args&lt;/code&gt; &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/planner/core/task.go#L403-L426&quot;&gt;修改为 TiKV 预聚合后返回的中间结果&lt;/a&gt;，TiKV 层的 Partial 聚合算子的 &lt;code class=&quot;inline&quot;&gt;AggFuncDesc&lt;/code&gt; 也需要作出对应的修改，这里不再详述。若聚合函数不可以下推，则 &lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode&lt;/code&gt; 保持不变。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0/executor/builder.go#L999&quot;&gt;构建 HashAgg 执行器&lt;/a&gt;时，首先检查当前 &lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 算子&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0/executor/builder.go#L1037-L1047&quot;&gt;是否可以并行执行&lt;/a&gt;。目前当且仅当两种情况下 &lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 不可以并行执行：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;存在某个聚合函数参数为 DISTINCT 时。TiDB 暂未实现对 DedupMode 的支持，因此对于含有 &lt;code class=&quot;inline&quot;&gt;DISTINCT&lt;/code&gt; 的情况目前仅能单线程执行。&lt;/li&gt;&lt;li&gt;系统变量 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/tidb-specific.md#tidb_hashagg_partial_concurrency&quot;&gt;tidb_hashagg_partial_concurrency&lt;/a&gt;&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/tidb-specific.md#tidb_hashagg_final_concurrency&quot;&gt;tidb_hashagg_final_concurrency&lt;/a&gt;&lt;/code&gt; 被同时设置为 1 时。这两个系统变量分别用来控制 Hash Aggregation 并行计算时候，TiDB 层聚合计算 partial 和 final 阶段 worker 的并发数。当它们都被设置为 1 时，选择单线程执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;若&lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt;算子可以并行执行，使用&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/builder.go#L1062&quot;&gt;AggFuncDesc.Split&lt;/a&gt;根据&lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode&lt;/code&gt;将 TiDB 层的聚合算子的计算拆分为 partial 和 final 两个阶段，并分别生成对应的&lt;code class=&quot;inline&quot;&gt;AggFuncDesc&lt;/code&gt;，设为&lt;code class=&quot;inline&quot;&gt;partialAggDesc&lt;/code&gt;和&lt;code class=&quot;inline&quot;&gt;finalAggDesc&lt;/code&gt;。若&lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode == CompleteMode&lt;/code&gt;，则将 TiDB 层的计算阶段拆分为&lt;code class=&quot;inline&quot;&gt;Partial1Mode --&amp;gt; FinalMode&lt;/code&gt;；若&lt;code class=&quot;inline&quot;&gt;AggFuncDesc.Mode == FinalMode&lt;/code&gt;，则将 TiDB 层的计算阶段拆分为&lt;code class=&quot;inline&quot;&gt;Partial2Mode --&amp;gt; FinalMode&lt;/code&gt;。进一步的，我们可以根据&lt;code class=&quot;inline&quot;&gt;partialAggDesc&lt;/code&gt;和&lt;code class=&quot;inline&quot;&gt;finalAggDesc&lt;/code&gt;分别&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/builder.go#L1063-L1066&quot;&gt;构造出对应的执行函数&lt;/a&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行 Hash Aggregation 执行过程详述&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 的并行 Hash Aggregation 算子执行过程中的主要线程有：Main Thead，Data Fetcher，Partial Worker，和 Final Worker：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Main Thread 一个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;启动 Input Reader，Partial Workers 及 Final Workers&lt;/li&gt;&lt;li&gt;等待 Final Worker 的执行结果并返回&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Data Fetcher 一个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;按 batch 读取子节点数据并分发给 Partial Worker&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Partial Worker 多个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取 Data Fetcher 发送来的数据，并做预聚合&lt;/li&gt;&lt;li&gt;将预聚合结果根据 Group 值 shuffle 给对应的 Final Worker&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Final Worker 多个：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;读取 PartialWorker 发送来的数据，计算最终结果，发送给 Main Thread&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;Hash Aggregation 的执行阶段可分为如下图所示的 5 步：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-070c20ca9c7b3e1d03ede09d95d20259_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;347&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-070c20ca9c7b3e1d03ede09d95d20259&quot; data-watermark-src=&quot;v2-65f2027935adc477a4bed0fa5f275d34&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;启动 Data Fetcher，Partial Workers 及 Final Workers。&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0&quot;&gt;prepare4Parallel&lt;/a&gt; 函数完成。该函数会启动一个 Data Fetcher，&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L589-L591&quot;&gt;多个 Partial Worker&lt;/a&gt; 以及 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L596-L598&quot;&gt;多个 Final Worker&lt;/a&gt;。Partial Worker 和 Final Worker 的数量可以分别通过 &lt;code class=&quot;inline&quot;&gt;tidb_hashgg_partial_concurrency&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;tidb_hashagg_final_concurrency&lt;/code&gt; 系统变量进行控制，这两个系统变量的默认值都为 4。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.DataFetcher 读取子节点的数据并分发给 Partial Workers。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L535&quot;&gt;fetchChildData&lt;/a&gt; 函数完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.Partial Workers 预聚合计算，及根据 Group Key shuffle 给对应的 Final Workers。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L326&quot;&gt;HashAggPartialWorker.run&lt;/a&gt; 函数完成。该函数调用 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L351&quot;&gt;updatePartialResult&lt;/a&gt; 函数对 DataFetcher 发来数据执行 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L358-L363&quot;&gt;预聚合计算&lt;/a&gt;，并将预聚合结果存储到 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L63&quot;&gt;partialResultMap&lt;/a&gt; 中。其中 &lt;code class=&quot;inline&quot;&gt;partialResultMap&lt;/code&gt; 的 key 为根据 &lt;code class=&quot;inline&quot;&gt;Group-By&lt;/code&gt; 的值 encode 的结果，value 为 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggfuncs/aggfuncs.go#L89&quot;&gt;PartialResult&lt;/a&gt; 类型的数组，数组中的每个元素表示该下标处的聚合函数在对应 Group 中的预聚合结果。&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L370&quot;&gt;shuffleIntermData&lt;/a&gt; 函数完成根据 Group 值 shuffle 给对应的 Final Worker。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.Final Worker 计算最终结果，发送给 Main Thread。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这部分工作由 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L505&quot;&gt;HashAggFinalWorker.run&lt;/a&gt; 函数完成。该函数调用 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L434&quot;&gt;consumeIntermData&lt;/a&gt; 函数 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/aggregate.go#L443&quot;&gt;接收 PartialWorkers 发送来的预聚合结果&lt;/a&gt;，进而 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L459&quot;&gt;合并&lt;/a&gt; 得到最终结果。&lt;a href=&quot;https://github.com/pingcap/tidb/tree/v2.1.0/executor/aggregate.go#L459&quot;&gt;getFinalResult&lt;/a&gt; 函数完成发送最终结果给 Main Thread。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.Main Thread 接收最终结果并返回。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 并行 Hash Aggregation 的性能提升&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;此处以 &lt;a href=&quot;https://github.com/pingcap/tidb-bench/blob/master/tpch/queries/17.sql&quot;&gt;TPC-H query-17&lt;/a&gt; 为例，测试并行 Hash Aggregation 相较于单线程计算时的性能提升。引入并行 Hash Aggregation 前，它的计算瓶颈在 &lt;code class=&quot;inline&quot;&gt;HashAgg_35&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;该查询执行计划如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8967db518d31d2316d040b20a6620605_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;941&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8967db518d31d2316d040b20a6620605&quot; data-watermark-src=&quot;v2-f47abed183d2edbddd6fda0d8ce6098c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在 TiDB 中，使用 &lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/understanding-the-query-execution-plan.md#explain-analyze-%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F&quot;&gt;EXPLAIN ANALYZE&lt;/a&gt; 可以获取 SQL 的执行统计信息。因篇幅原因此处仅贴出 TPC-H query-17 部分算子的 EXPLAIN ANALYZE 结果。&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 单线程计算时：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-db9b78ba3e21565d4e7d5088e639af8f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;查询总执行时间 23 分 24 秒，其中 &lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 执行时间约 17 分 9 秒。&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 并行计算时（此时 TiDB 层 Partial 和 Final 阶段的 worker 数量都设置为 16）：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8be3186bc4261d00467ac4dc110e5b0b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;113&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;总查询时间 8 分 37 秒，其中 &lt;code class=&quot;inline&quot;&gt;HashAgg&lt;/code&gt; 执行时间约 1 分 4 秒。&lt;/p&gt;&lt;p&gt;并行计算时，Hash Aggregation 的计算速度提升约 16 倍。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-21-52969666</guid>
<pubDate>Fri, 21 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>十分钟成为 Contributor 系列 | 支持 AST 还原为 SQL</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-20-52838362.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52838362&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-761b4fe9feb76b36d597aa45ff9e1ad7_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;背景知识&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SQL 语句发送到 TiDB 后首先会经过 parser，从文本 parse 成为 AST（抽象语法树），AST 节点与 SQL 文本结构是一一对应的，我们通过遍历整个 AST 树就可以拼接出一个与 AST 语义相同的 SQL 文本。&lt;/p&gt;&lt;p&gt;对 parser 不熟悉的小伙伴们可以看 &lt;a href=&quot;https://www.pingcap.com/blog-cn/tidb-source-code-reading-5/&quot;&gt;TiDB 源码阅读系列文章（五）TiDB SQL Parser 的实现&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;为了控制 SQL 文本的输出格式，并且为方便未来新功能的加入（例如在 SQL 文本中用 “*” 替代密码），我们引入了 &lt;code class=&quot;inline&quot;&gt;RestoreFlags&lt;/code&gt; 并封装了 &lt;code class=&quot;inline&quot;&gt;RestoreCtx&lt;/code&gt; 结构（&lt;a href=&quot;https://github.com/pingcap/parser/blob/9339d225378fa9b50e1bf8373c2040524b96c6af/ast/util.go#L78&quot;&gt;相关源码&lt;/a&gt;）：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// `RestoreFlags` 中的互斥组:
// [RestoreStringSingleQuotes, RestoreStringDoubleQuotes]
// [RestoreKeyWordUppercase, RestoreKeyWordLowercase]
// [RestoreNameUppercase, RestoreNameLowercase]
// [RestoreNameDoubleQuotes, RestoreNameBackQuotes]
// 靠前的 flag 拥有更高的优先级。
const (
	RestoreStringSingleQuotes RestoreFlags = 1 &amp;lt;&amp;lt; iota
	
	...
)

// RestoreCtx is `Restore` context to hold flags and writer.
type RestoreCtx struct {
	Flags RestoreFlags
	In    io.Writer
}

// WriteKeyWord 用于向 `ctx` 中写入关键字（例如：SELECT）。
// 它的大小写受 `RestoreKeyWordUppercase`，`RestoreKeyWordLowercase` 控制
func (ctx *RestoreCtx) WriteKeyWord(keyWord string) {
	...
}

// WriteString 用于向 `ctx` 中写入字符串。
// 它是否被引号包裹及转义规则受 `RestoreStringSingleQuotes`，`RestoreStringDoubleQuotes`，`RestoreStringEscapeBackslash` 控制。
func (ctx *RestoreCtx) WriteString(str string) {
	...
}

// WriteName 用于向 `ctx` 中写入名称（库名，表名，列名等）。
// 它是否被引号包裹及转义规则受 `RestoreNameUppercase`，`RestoreNameLowercase`，`RestoreNameDoubleQuotes`，`RestoreNameBackQuotes` 控制。
func (ctx *RestoreCtx) WriteName(name string) {
	...
}

// WriteName 用于向 `ctx` 中写入普通文本。
// 它将被直接写入不受 flag 影响。
func (ctx *RestoreCtx) WritePlain(plainText string) {
	...
}

// WriteName 用于向 `ctx` 中写入普通文本。
// 它将被直接写入不受 flag 影响。
func (ctx *RestoreCtx) WritePlainf(format string, a ...interface{}) {
	...
}&lt;/code&gt;&lt;p&gt;我们在 &lt;code class=&quot;inline&quot;&gt;ast.Node&lt;/code&gt; 接口中添加了一个 &lt;code class=&quot;inline&quot;&gt;Restore(ctx *RestoreCtx) error&lt;/code&gt; 函数，这个函数将当前节点对应的 SQL 文本追加至参数 &lt;code class=&quot;inline&quot;&gt;ctx&lt;/code&gt; 中，如果节点无效则返回 &lt;code class=&quot;inline&quot;&gt;error&lt;/code&gt;。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;type Node interface {
    // Restore AST to SQL text and append them to `ctx`.
    // return error when the AST is invalid.
	Restore(ctx *RestoreCtx) error
    
    ...
}&lt;/code&gt;&lt;p&gt;以 SQL 语句 &lt;code class=&quot;inline&quot;&gt;SELECT column0 FROM table0 UNION SELECT column1 FROM table1 WHERE a = 1&lt;/code&gt; 为例，如下图所示，我们通过遍历整个 AST 树，递归调用每个节点的 &lt;code class=&quot;inline&quot;&gt;Restore()&lt;/code&gt; 方法，即可拼接成一个完整的 SQL 文本。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-581ef8a061bce7e7caec3fd7ec15f20c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1612&quot; data-rawheight=&quot;894&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-581ef8a061bce7e7caec3fd7ec15f20c&quot; data-watermark-src=&quot;v2-318bd8674743fbf503978e526a34acb4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;值得注意的是，SQL 文本与 AST 是一个多对一的关系，我们不可能从 AST 结构中还原出与原 SQL 完全一致的文本， 因此我们只要保证还原出的 SQL 文本与原 SQL &lt;b&gt;语义相同&lt;/b&gt; 即可。所谓语义相同，指的是由 AST 还原出的 SQL 文本再被解析为 AST 后，两个 AST 是相等的。&lt;/p&gt;&lt;p&gt;我们已经完成了接口设计和测试框架，具体的&lt;code class=&quot;inline&quot;&gt;Restore()&lt;/code&gt; 函数留空。因此&lt;b&gt;只需要选择一个留空的&lt;/b&gt; &lt;code class=&quot;inline&quot;&gt;&lt;b&gt;Restore()&lt;/b&gt;&lt;/code&gt; &lt;b&gt;函数实现，并添加相应的测试数据，就可以提交一个 PR 了！&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实现&lt;/b&gt; &lt;b&gt;&lt;code class=&quot;inline&quot;&gt;Restore()&lt;/code&gt;&lt;/b&gt; &lt;b&gt;函数的整体流程&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;请先阅读 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/master/docs/design/2018-11-29-ast-to-sql-text.md&quot;&gt;Proposal&lt;/a&gt;、&lt;a href=&quot;https://github.com/pingcap/tidb/issues/8532&quot;&gt;Issue&lt;/a&gt;&lt;/li&gt;&lt;li&gt;在 &lt;a href=&quot;https://github.com/pingcap/tidb/issues/8532&quot;&gt;Issue&lt;/a&gt; 中找到未实现的函数&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;在 &lt;a href=&quot;https://github.com/pingcap/tidb/issues/8532&quot;&gt;Issue-pingcap/tidb#8532&lt;/a&gt; 中找到一个没有被其他贡献者认领的任务，例如 &lt;code class=&quot;inline&quot;&gt;ast/expressions.go: BetweenExpr&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;在 &lt;a href=&quot;https://github.com/pingcap/parser&quot;&gt;pingcap/parser&lt;/a&gt; 中找到任务对应文件 &lt;code class=&quot;inline&quot;&gt;ast/expressions.go&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;在文件中找到 &lt;code class=&quot;inline&quot;&gt;BetweenExpr&lt;/code&gt; 结构的 &lt;code class=&quot;inline&quot;&gt;Restore&lt;/code&gt; 函数：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;// Restore implements Node interface. 
func (n *BetweenExpr) Restore(ctx *RestoreCtx) error {
     return errors.New(&quot;Not implemented&quot;) 
}&lt;/code&gt;&lt;p&gt;3. 实现 &lt;code class=&quot;inline&quot;&gt;Restore()&lt;/code&gt; 函数&lt;br&gt;根据 Node 节点结构和 SQL 语法实现函数功能。&lt;/p&gt;&lt;blockquote&gt;参考 &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/sql-syntax.html&quot;&gt;MySQL 5.7 SQL Statement Syntax&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;4. 写单元测试&lt;br&gt;参考示例在相关文件下添加单元测试。&lt;/p&gt;&lt;p&gt;5. 运行 &lt;code class=&quot;inline&quot;&gt;make test&lt;/code&gt;，确保所有的 test case 都能跑过。&lt;/p&gt;&lt;p&gt;6. 提交 PR&lt;br&gt;PR 标题统一为：&lt;code class=&quot;inline&quot;&gt;parser: implement Restore for XXX&lt;/code&gt;&lt;br&gt;请在 PR 中关联 Issue: &lt;code class=&quot;inline&quot;&gt;pingcap/tidb#8532&lt;/code&gt; &lt;/p&gt;&lt;h2&gt;&lt;b&gt;示例&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这里以&lt;a href=&quot;https://github.com/pingcap/parser/pull/71/files&quot;&gt;实现 BetweenExpr 的 Restore 函数 PR&lt;/a&gt; 为例，进行详细说明：&lt;/p&gt;&lt;p&gt;1. 首先看 &lt;code class=&quot;inline&quot;&gt;ast/expressions.go&lt;/code&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们要实现一个 &lt;code class=&quot;inline&quot;&gt;ast.Node&lt;/code&gt; 结构的 &lt;code class=&quot;inline&quot;&gt;Restore&lt;/code&gt; 函数，首先清楚该结构代表什么短语，例如 &lt;code class=&quot;inline&quot;&gt;BetweenExpr&lt;/code&gt; 代表 &lt;code class=&quot;inline&quot;&gt;expr [NOT] BETWEEN expr AND expr&lt;/code&gt; （参见：&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/comparison-operators.html#operator_between&quot;&gt;MySQL 语法 - 比较函数和运算符&lt;/a&gt;）。&lt;/li&gt;&lt;li&gt;观察 &lt;code class=&quot;inline&quot;&gt;BetweenExpr&lt;/code&gt; 结构：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;// BetweenExpr is for &quot;between and&quot; or &quot;not between and&quot; expression.
type BetweenExpr struct {
    exprNode
    // 被检查的表达式
    Expr ExprNode
    // AND 左侧的表达式
    Left ExprNode
    // AND 右侧的表达式
    Right ExprNode
    // 是否有 NOT 关键字
    Not bool
}&lt;/code&gt;&lt;ul&gt;&lt;li&gt;实现 &lt;code class=&quot;inline&quot;&gt;BetweenExpr&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Restore&lt;/code&gt; 函数：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;// Restore implements Node interface.
func (n *BetweenExpr) Restore(ctx *RestoreCtx) error {
    // 调用 Expr 的 Restore，向 ctx 写入 Expr
    if err := n.Expr.Restore(ctx); err != nil {
        return errors.Annotate(err, &quot;An error occurred while restore BetweenExpr.Expr&quot;)
    }
    // 判断是否有 NOT，并写入相应关键字
    if n.Not {
        ctx.WriteKeyWord(&quot; NOT BETWEEN &quot;)
    } else {
        ctx.WriteKeyWord(&quot; BETWEEN &quot;)
    }
    // 调用 Left 的 Restore
    if err := n.Left.Restore(ctx); err != nil {
        return errors.Annotate(err, &quot;An error occurred while restore BetweenExpr.Left&quot;)
    }
    // 写入 AND 关键字
    ctx.WriteKeyWord(&quot; AND &quot;)
    // 调用 Right 的 Restore
    if err := n.Right.Restore(ctx); err != nil {
        return errors.Annotate(err, &quot;An error occurred while restore BetweenExpr.Right &quot;)
    }
    return nil
}&lt;/code&gt;&lt;p&gt;2. 接下来给函数实现添加单元测试, &lt;code class=&quot;inline&quot;&gt;ast/expressions_test.go&lt;/code&gt;：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// 添加测试函数
func (tc *testExpressionsSuite) TestBetweenExprRestore(c *C) {
    // 测试用例
    testCases := []NodeRestoreTestCase{
        {&quot;b between 1 and 2&quot;, &quot;`b` BETWEEN 1 AND 2&quot;},
        {&quot;b not between 1 and 2&quot;, &quot;`b` NOT BETWEEN 1 AND 2&quot;},
        {&quot;b between a and b&quot;, &quot;`b` BETWEEN `a` AND `b`&quot;},
        {&quot;b between &#39;&#39; and &#39;b&#39;&quot;, &quot;`b` BETWEEN &#39;&#39; AND &#39;b&#39;&quot;},
        {&quot;b between &#39;2018-11-01&#39; and &#39;2018-11-02&#39;&quot;, &quot;`b` BETWEEN &#39;2018-11-01&#39; AND &#39;2018-11-02&#39;&quot;},
    }
    // 为了不依赖父节点实现，通过 extractNodeFunc 抽取待测节点
    extractNodeFunc := func(node Node) Node {
        return node.(*SelectStmt).Fields.Fields[0].Expr
    }
    // Run Test
    RunNodeRestoreTest(c, testCases, &quot;select %s&quot;, extractNodeFunc)
}&lt;/code&gt;&lt;p&gt;&lt;b&gt;至此 &lt;code class=&quot;inline&quot;&gt;BetweenExpr&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Restore&lt;/code&gt; 函数实现完成，可以提交 PR 了。为了更好的理解测试逻辑，下面我们看 &lt;code class=&quot;inline&quot;&gt;RunNodeRestoreTest&lt;/code&gt;：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// 下面是测试逻辑，已经实现好了，不需要 contributor 实现
func RunNodeRestoreTest(c *C, nodeTestCases []NodeRestoreTestCase, template string, extractNodeFunc func(node Node) Node) {
    parser := parser.New()
    for _, testCase := range nodeTestCases {
        // 通过 template 将测试用例拼接为完整的 SQL
        sourceSQL := fmt.Sprintf(template, testCase.sourceSQL)
        expectSQL := fmt.Sprintf(template, testCase.expectSQL)
        stmt, err := parser.ParseOneStmt(sourceSQL, &quot;&quot;, &quot;&quot;)
        comment := Commentf(&quot;source %#v&quot;, testCase)
        c.Assert(err, IsNil, comment)
        var sb strings.Builder
        // 抽取指定节点并调用其 Restore 函数
        err = extractNodeFunc(stmt).Restore(NewRestoreCtx(DefaultRestoreFlags, &amp;amp;sb))
        c.Assert(err, IsNil, comment)
        // 通过 template 将 restore 结果拼接为完整的 SQL
        restoreSql := fmt.Sprintf(template, sb.String())
        comment = Commentf(&quot;source %#v; restore %v&quot;, testCase, restoreSql)
        // 测试 restore 结果与预期一致
        c.Assert(restoreSql, Equals, expectSQL, comment)
        stmt2, err := parser.ParseOneStmt(restoreSql, &quot;&quot;, &quot;&quot;)
        c.Assert(err, IsNil, comment)
        CleanNodeText(stmt)
        CleanNodeText(stmt2)
        // 测试解析的 stmt 与原 stmt 一致
        c.Assert(stmt2, DeepEquals, stmt, comment)
    }
}&lt;/code&gt;&lt;p&gt;&lt;b&gt;不过对于 &lt;code class=&quot;inline&quot;&gt;ast.StmtNode&lt;/code&gt;（例如：&lt;code class=&quot;inline&quot;&gt;ast.SelectStmt&lt;/code&gt;）测试方法有些不一样， 由于这类节点可以还原为一个完整的 SQL，因此直接在 &lt;code class=&quot;inline&quot;&gt;parser_test.go&lt;/code&gt; 中测试。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面以&lt;a href=&quot;https://github.com/pingcap/parser/pull/62/files&quot;&gt;实现 UseStmt 的 Restore 函数 PR&lt;/a&gt; 为例，对测试进行说明：&lt;/p&gt;&lt;p&gt;1. &lt;code class=&quot;inline&quot;&gt;Restore&lt;/code&gt; 函数实现过程略。&lt;/p&gt;&lt;p&gt;2. 给函数实现添加单元测试，参见 &lt;code class=&quot;inline&quot;&gt;parser_test.go&lt;/code&gt;：&lt;/p&gt;&lt;p&gt;在这个示例中，只添加了几行测试数据就完成了测试：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// 添加 testCase 结构的测试数据
{&quot;use `select`&quot;, true, &quot;USE `select`&quot;},
{&quot;use `sel``ect`&quot;, true, &quot;USE `sel``ect`&quot;},
{&quot;use select&quot;, false, &quot;USE `select`&quot;},&lt;/code&gt;&lt;p&gt;我们看 &lt;code class=&quot;inline&quot;&gt;testCase&lt;/code&gt; 结构声明：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;type testCase struct {
    // 原 SQL
    src     string
    // 是否能被正确 parse
    ok      bool
    // 预期的 restore SQL
    restore string
}&lt;/code&gt;&lt;p&gt;测试代码会判断原 SQL parse 出 AST 后再还原的 SQL 是否与预期的 restore SQL 相等，具体的测试逻辑在 &lt;code class=&quot;inline&quot;&gt;parser_test.go&lt;/code&gt; 中 &lt;code class=&quot;inline&quot;&gt;RunTest()&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;RunRestoreTest()&lt;/code&gt; 函数，逻辑与前例类似，此处不再赘述。&lt;/p&gt;&lt;p&gt;&lt;b&gt;加入 TiDB Contributor Club，无门槛参与开源项目，改变世界从这里开始吧！😎&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;&lt;i&gt;社区大事件 &lt;/i&gt;&lt;/b&gt;&lt;br&gt;年度最高规格的 TiDB 技术大会——TiDB DevCon 2019 将于 2019 年 1 月 19 日在北京举办。欢迎大家报名参加！活动详情👇&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487497&amp;amp;idx=1&amp;amp;sn=9e00bd4cb4ace5c39c266e38338a77ef&amp;amp;chksm=eb163763dc61be753518db2154b9ef36379acedbb30e2789fd6d5f37f538bf906905aac00f83&amp;amp;scene=21#wechat_redirect&quot;&gt;br/&amp;gt;TiDB DevCon 2019 报名开启&lt;/a&gt;&lt;br&gt;* TiDB Committers &amp;amp; Contributors 可以享受【社区贡献者注册】&lt;b&gt;免费&lt;/b&gt;权益。&lt;br&gt;* TiDB 社区贡献者即 GitHub 上以下 15 个 Repositories  的代码贡献者：&lt;br&gt;docs-cn | docs | grpc-rs | jepsen | parser | pd | tidb-ansible | tidb-bench | tidb-docker-compose | tidb-operator | tidb | tidb-tools | tikv | tipb | tispark&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-20-52838362</guid>
<pubDate>Thu, 20 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Ecosystem Tools 原理解读之 TiDB-Lightning Toolset</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-18-52699499.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52699499&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8b0bba75055cd143356e76a82064b113_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB-Lightning Toolset 是一套快速全量导入 SQL dump 文件到 TiDB 集群的工具集，自 2.1.0 版本起随 TiDB 发布，速度可达到传统执行 SQL 导入方式的至少 3 倍、大约每小时 100 GB，适合在上线前用作迁移现有的大型数据库到全新的 TiDB 集群。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 从 2017 年开始提供全量导入工具 &lt;a href=&quot;https://pingcap.com/docs-cn/tools/loader/&quot;&gt;Loader&lt;/a&gt;，它以多线程操作、错误重试、断点续传以及修改一些 TiDB 专属配置来提升数据导入速度。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-23ac26045113e913212b950ab743d557_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;835&quot; data-rawheight=&quot;458&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-23ac26045113e913212b950ab743d557&quot; data-watermark-src=&quot;v2-9abe9f1df7771b4219e03c60f6924dad&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;然而，当我们全新初始化一个 TiDB 集群时，Loader 这种逐条 INSERT 指令在线上执行的方式从根本上是无法尽用性能的。原因在于 SQL 层的操作有太强的保证了。在整个导入过程中，TiDB 需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;保证 ACID 特性，需要执行完整的事务流程。&lt;/li&gt;&lt;li&gt;保证各个 TiKV 服务器数据量平衡及有足够的副本，在数据增长的时候需要不断的分裂、调度 Regions。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些动作确保 TiDB 整段导入的期间是稳定的，但在导入完毕前我们根本不会对外提供服务，这些保证就变成多此一举了。此外，多线程的线上导入也代表资料是乱序插入的，新的数据范围会与旧的重叠。TiKV 要求储存的数据是有序的，大量的乱序写入会令 TiKV 要不断地移动原有的数据（这称为 Compaction），这也会拖慢写入过程。&lt;/p&gt;&lt;p&gt;TiKV 是使用 RocksDB 以 KV 对的形式储存数据，这些数据会压缩成一个个 SST 格式文件。TiDB-Lightning Toolset使用新的思路，绕过SQL层，在线下将整个 SQL dump 转化为 KV 对、生成排好序的 SST 文件，然后直接用 &lt;a href=&quot;https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files&quot;&gt;Ingestion&lt;/a&gt; 推送到 RocksDB 里面。这样批量处理的方法略过 ACID 和线上排序等耗时步骤，让我们提升最终的速度。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;架构&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-55cd8f3a33a4f958cd86d036b5a39dde_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;854&quot; data-rawheight=&quot;625&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-55cd8f3a33a4f958cd86d036b5a39dde&quot; data-watermark-src=&quot;v2-53068d757bfdd30eea0a8589478bf276&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;TiDB-Lightning Toolset 包含两个组件：tidb-lightning 和 tikv-importer。Lightning 负责解析 SQL 成为 KV 对，而 Importer 负责将 KV 对排序与调度、上传到 TiKV 服务器。&lt;/p&gt;&lt;p&gt;为什么要把一个流程拆分成两个程式呢？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Importer 与 TiKV 密不可分、Lightning 与 TiDB 密不可分，Toolset 的两者皆引用后者为库，而这样 Lightning 与 Importer 之间就出现语言冲突：TiKV 是使用 Rust 而 TiDB 是使用 Go 的。把它们拆分为独立的程式更方便开发，而双方都需要的 KV 对可以透过 gRPC 传递。&lt;/li&gt;&lt;li&gt;分开 Importer 和 Lightning 也使横向扩展的方式更为灵活，例如可以运行多个 Lightning，传送给同一个 Importer。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;以下我们会详细分析每个组件的操作原理。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Lightning&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9844bda340c571347842d5bd1c945fbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;343&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9844bda340c571347842d5bd1c945fbf&quot; data-watermark-src=&quot;v2-a8e7de501a49d2f57fb1e6ce50ecd95c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Lightning 现时只支持经 mydumper 导出的 SQL 备份。mydumper 将每个表的内容分别储存到不同的文件，与 mysqldump 不同。这样不用解析整个数据库就能平行处理每个表。&lt;/p&gt;&lt;p&gt;首先，Lightning 会扫描 SQL 备份，区分出结构文件（包含 CREATE TABLE 语句）和数据文件（包含 INSERT 语句）。结构文件的内容会直接发送到 TiDB，用以建立数据库构型。&lt;/p&gt;&lt;p&gt;然后 Lightning 就会并发处理每一张表的数据。这里我们只集中看一张表的流程。每个数据文件的内容都是规律的 INSERT 语句，像是：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;INSERT INTO `tbl` VALUES (1, 2, 3), (4, 5, 6), (7, 8, 9);  
INSERT INTO `tbl` VALUES (10, 11, 12), (13, 14, 15), (16, 17, 18);
INSERT INTO `tbl` VALUES (19, 20, 21), (22, 23, 24), (25, 26, 27);&lt;/code&gt;&lt;p&gt;Lightning 会作初步分析，找出每行在文件的位置并分配一个行号，使得没有主键的表可以唯一的区分每一行。此外亦同时将文件分割为大小差不多的区块（默认 256 MiB）。这些区块也会并发处理，让数据量大的表也能快速导入。以下的例子把文件以 20 字节为限分割成 5 块：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f671b93f00f67d5a0b3451d8694d7638_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1068&quot; data-rawheight=&quot;180&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f671b93f00f67d5a0b3451d8694d7638&quot; data-watermark-src=&quot;v2-d8e0c081f70d453289ba24500fd83d70&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Lightning 会直接使用 TiDB 实例来把 SQL 转换为 KV 对，称为「KV 编码器」。与外部的 TiDB 集群不同，KV 编码器是寄存在 Lightning 进程内的，而且使用内存存储，所以每执行完一个 INSERT 之后，Lightning 可以直接读取内存获取转换后的 KV 对（这些 KV 对包含数据及索引）。&lt;/p&gt;&lt;p&gt;得到 KV 对之后便可以发送到 Importer。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Importer&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-777753c0442df7718aa158c3333fb52e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;397&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-777753c0442df7718aa158c3333fb52e&quot; data-watermark-src=&quot;v2-8b1fb35fcb9de6b81fec9a8d35b28afe&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;因异步操作的缘故，Importer 得到的原始 KV 对注定是无序的。所以，Importer 要做的第一件事就是要排序。这需要给每个表划定准备排序的储存空间，我们称之为 engine file。&lt;/p&gt;&lt;p&gt;对大数据排序是个解决了很多遍的问题，我们在此使用现有的答案：直接使用 RocksDB。一个 engine file 就相等于本地的 RocksDB，并设置为优化大量写入操作。而「排序」就相等于将 KV 对全写入到 engine file 里，RocksDB 就会帮我们合并、排序，并得到 SST 格式的文件。&lt;/p&gt;&lt;p&gt;这个 SST 文件包含整个表的数据和索引，比起 TiKV 的储存单位 Regions 实在太大了。所以接下来就是要切分成合适的大小（默认为 96 MiB）。Importer 会根据要导入的数据范围预先把 Region 分裂好，然后让 PD 把这些分裂出来的 Region 分散调度到不同的 TiKV 实例上。&lt;/p&gt;&lt;p&gt;最后，Importer 将 SST 上传到对应 Region 的每个副本上。然后通过 Leader 发起 Ingest 命令，把这个 SST 文件导入到 Raft group 里，完成一个 Region 的导入过程。&lt;/p&gt;&lt;p&gt;我们传输大量数据时，需要自动检查数据完整，避免忽略掉错误。Lightning 会在整个表的 Region 全部导入后，对比传送到 Importer 之前这个表的 Checksum，以及在 TiKV 集群里面时的 Checksum。如果两者一样，我们就有信心说这个表的数据没有问题。&lt;/p&gt;&lt;p&gt;一个表的 Checksum 是透过计算 KV 对的哈希值（Hash）产生的。因为 KV 对分布在不同的 TiKV 实例上，这个 Checksum 函数应该具备结合性；另外，Lightning 传送 KV 对之前它们是无序的，所以 Checksum 也不应该考虑顺序，即服从交换律。也就是说 Checksum 不是简单的把整个 SST 文件计算 SHA-256 这样就了事。&lt;/p&gt;&lt;p&gt;我们的解决办法是这样的：先计算每个 KV 对的 CRC64，然后用 XOR 结合在一起，得出一个 64 位元的校验数字。为减低 Checksum 值冲突的概率，我们目时会计算 KV 对的数量和大小。若速度允许，将来会加入更先进的 Checksum 方式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结和下一步计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从这篇文章大家可以看到，Lightning 因为跳过了一些复杂、耗时的步骤使得整个导入进程更快，适合大数据量的初次导入，接下来我们还会做进一步的改进。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;提升导入速度&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现时 Lightning 会原封不动把整条 SQL 命令抛给 KV 编码器。所以即使我们省去执行分布式 SQL 的开销，但仍需要进行解析、规划及优化语句这些不必要或未被专门化的步骤。Lightning 可以调用更底层的 TiDB API，缩短 SQL 转 KV 的行程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行导入&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b3b31a81e0d75bc2783f64ad28f72904_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;279&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b3b31a81e0d75bc2783f64ad28f72904&quot; data-watermark-src=&quot;v2-da8c69e0cd6f70648ccf994cd7b3adbd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;另一方面，尽管我们可以不断的优化程序代码，单机的性能总是有限的。要突破这个界限就需要横向扩展：增加机器来同时导入。如前面所述，只要每套 TiDB-Lightning Toolset 操作不同的表，它们就能平行导进同一个集群。可是，现在的版本只支持读取本机文件系统上的 SQL dump，设置成多机版就显得比较麻烦了（要安装一个共享的网络盘，并且手动分配哪台机读取哪张表）。我们计划让 Lightning 能从网路获取 SQL dump（例如通过 S3 API），并提供一个工具自动分割数据库，降低设置成本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;在线导入&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB-Lightning 在导入时会把集群切换到一个专供 Lightning 写入的模式。目前来说 Lightning 主要用于在进入生产环境之前导入全量数据，所以在此期间暂停对外提供服务还可以接受。但我们希望支持更多的应用场景，例如回复备份、储存 OLAP 的大规模计算结果等等，这些都需要维持集群在线上。所以接下来的一大方向是考虑怎样降低 Lightning 对集群的影响。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;延伸阅读：&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;TiDB EcoSystem Tools 原理解读（一）：TiDB-Binlog 架构演进与实现原理&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-18-52699499</guid>
<pubDate>Tue, 18 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>天真贝叶斯学习机 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-18-52647715.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52647715&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3dafb6e9e934bab634b27352ad757cd_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦～&lt;br&gt;本文作者是来自 DSG 团队的杨文同学，他们的项目《PD 热点调度贝叶斯模型》在本届 Hackathon 中获得了三等奖+最佳创意奖。&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3dafb6e9e934bab634b27352ad757cd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2124&quot; data-rawheight=&quot;928&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e3dafb6e9e934bab634b27352ad757cd&quot; data-watermark-src=&quot;v2-6e73710c0e3a84e11baf80348b95e8ba&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;“在 TiDB Hackathon 2018 学习到不少东西&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;希望明年再来”&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;简述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;“pd ctl 天真学习机”&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;具体做法：用 naive bayes 模型来根据系统指标和人的 pd ctl 调用，来得到一个模型去根据系统指标去自动提供 pd ctl 调用的命令。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 贝叶斯算法举例&lt;/b&gt;&lt;/p&gt;&lt;p&gt;贝叶斯模型可以用来干这种事：&lt;/p&gt;&lt;p&gt;比如一个妈妈根据天气预报来跟儿子在出们的时候叮嘱：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;天气预报[ 晴, 温度: 28, 风力: 中 ], 妈妈会说 [好好玩]
天气预报[ 雨, 温度: 15, 风力: 低 ], 妈妈会说 [带上伞]
天气预报[ 阴, 温度: 02, 风力: 大 ], 妈妈会说 [多穿点]...&lt;/code&gt;&lt;p&gt;把这些输入输入到贝叶斯模型里以后, 模型可以根据天气预报来输出：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;天气预报[ 晴, 温度: 00, 风力中], 模型会说 [ 多穿点:0.7, 好好玩0.2, 带上伞0.1]
天气预报[ 雨, 温度: 10, 风力大], 模型会说 [ 带上伞:0.8, 多穿点0.1, 好好玩0.1]&lt;/code&gt;&lt;p&gt;这样通过一个妈妈的叮嘱就可以训练出一个也会根据天气预报给出叮嘱的模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 初步想法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们可以把一个模型单独的部署在一个 pod 里, 暴露一个 service ，然后集群上每次有人去调用 pd_ctl 的时候就在后台用 rest call 到模型服务上记录一下操作（叮嘱）和当前的系统指标(好比天气预报). 这样慢慢用一段时间以后，积累的操作多了以后，就可以打开某个自动响应，或者打开自动建议应该执行的命令的功能。&lt;/p&gt;&lt;p&gt;这样模型可以在某一组系统指标出现之前类似学习过的状态之后，给出相应的建议，当这些建议都很正确的时候直接让 pd 直接采纳，完全智能的自动化运作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 实际 Hackathon 方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在跟导师交流探讨后发现，目前 PD 已经比较自动化了，很少需要人为介入进行操作，需要的时候也是比较复杂的场景，或者自动化运作比较慢的场景。&lt;/p&gt;&lt;p&gt;我们团队在跟多名导师的沟通交流下，将初步想法进行了一些调整：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从热点调度策略入手，用热点调度策略的数值去用 naive bayes 模型去训练他们，然后再根据这些数值再去模型中去获取建议值。&lt;/li&gt;&lt;li&gt;统计建议值和热点调度策略进行比较；（从开始的测试结果来看，大概有 70% 匹配，但是我们实测发现，使用我们模型的建议值去真正的调度，热点 region 还是非常均衡的）&lt;/li&gt;&lt;li&gt;三组对照试验：不进行调度，只打印调度数据；正常使用原来的热点调度策略；使用原来的热点调度策略的数值，但是使用模型训练的建议值进行实际调度；&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Hackathon 回顾&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先，介绍一下我们团队（DSG），分别来自：丹麦、北京（山西）、广州。&lt;/p&gt;&lt;p&gt;D 先生是在比赛前一天早上到达北京的，我是比赛前一天晚上从广州出发，于比赛当日早上 6:38 才抵达北京的。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;说实话，时差和疲惫对于参赛还是有一点影响的。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;废话不多说，我就来回顾一下我的整个参赛过程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;比赛前一日 20:05 从广州南站出发，次日 6:38 抵达北京西站。&lt;/li&gt;&lt;li&gt;7:58 抵达地铁西小口&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-15e3857dfc8600b934bf1aa6edc63659_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1917&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-15e3857dfc8600b934bf1aa6edc63659&quot; data-watermark-src=&quot;v2-591490d18ed33c3cb18194f73ac275d6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;8:06 经过转转&lt;/li&gt;&lt;li&gt;8:12 抵达比赛所在地：东升科技园 C-1 楼&lt;/li&gt;&lt;li&gt;8:16 签到，逛 PingCAP&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2f2e7223147d1bd1e5b8a7a867f219bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1154&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2f2e7223147d1bd1e5b8a7a867f219bb&quot; data-watermark-src=&quot;v2-094e38deefe87c8e62e49ad79ac4d784&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;8:40 跟 D 先生汇合，了解贝叶斯模型&lt;/li&gt;&lt;li&gt;9:20 DSG 团队成员全部集结完毕&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-341eeab6fc17ded51af3a4fe8fe9b848_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;733&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-341eeab6fc17ded51af3a4fe8fe9b848&quot; data-watermark-src=&quot;v2-17c83d958e278ed7c96013dba27dae0a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;10:00 比赛正式开始&lt;/li&gt;&lt;li&gt;10:00 Hacking Time: Trello 构建整个比赛分工、准备工作、需求分析&lt;/li&gt;&lt;li&gt;搭建 TiDB 集群（2套）【熟悉 TiDB 集群，实操 PD-CTL】&lt;/li&gt;&lt;li&gt;12:17 午餐&lt;/li&gt;&lt;li&gt;13:00 Hacking Time: 熟悉 PD Command，贝叶斯模型，导师指导，本地 TiDB 环境构建（坑），分析 PD 热点调度，剖析调度流程，模拟热点数据&lt;/li&gt;&lt;li&gt;18:20 外出用餐（芦月轩羊蝎子(西三旗店)）【沾 D 先生的光，蹭吃蹭喝】&lt;/li&gt;&lt;li&gt;20:40 回到东升科技园&lt;/li&gt;&lt;li&gt;20:50 ~ 次日 1:10 Hacking Time: 模拟热点数据，实测调度上报和获取模型返回结果，本地测通调度参数上报和得到模型返回值&lt;/li&gt;&lt;li&gt;次日 1:10 ~ 5:50 会议室休息（在此期间，我的队友 D 先生，调好了模型，并将此模型通过 Docker 构建部署到 PD 机器上）&lt;/li&gt;&lt;li&gt;次日 5:50 Hacking Time: 部署修改过的 PD 服务到线上服务器，并打通 rust-nb-server，实时上报和实时获取模型返回结果&lt;/li&gt;&lt;li&gt;次日 7:30 早餐&lt;/li&gt;&lt;li&gt;次日 8:00 正式调试&lt;/li&gt;&lt;li&gt;次日 9:00 抽签确定 Demo 时间&lt;/li&gt;&lt;li&gt;次日 9:00 ~ 12:00 Hacking Time: 调优&lt;/li&gt;&lt;li&gt;次日 12:00 ~ 12:30 午餐时间&lt;/li&gt;&lt;li&gt;次日 13:00 ~ 14:00 Hacking Time:  PPT，调优&lt;/li&gt;&lt;li&gt;次日 14:30 ~ 18:30 Demo Time（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3227038cf7a5c0fceea2cf8dbe7b0ec8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;810&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3227038cf7a5c0fceea2cf8dbe7b0ec8&quot; data-watermark-src=&quot;v2-43c58a1dff6f5bf247b2315a873136bf&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f8d50461f274aab577f0629570103c53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;503&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f8d50461f274aab577f0629570103c53&quot; data-watermark-src=&quot;v2-4295925384afa95673a6d0a9eb1a78c7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17c0306743ed229a59ad4fca13237e76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-17c0306743ed229a59ad4fca13237e76&quot; data-watermark-src=&quot;v2-59b2751df6e57d6ad7639ecda0d31b0d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;ul&gt;&lt;li&gt;次日 18:30 ~ 19:00 颁奖（B 站直播）&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5be44353701d44af3fcfba2562379147_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5be44353701d44af3fcfba2562379147&quot; data-watermark-src=&quot;v2-27fe422c71dd698de08e22f7b0e037af&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3819854910b872e8c8131fbd54bef68d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3819854910b872e8c8131fbd54bef68d&quot; data-watermark-src=&quot;v2-ed6851744c7ee2430773c8e51b143067&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Hackathon 实操&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 搭建 TiDB 集群&lt;/b&gt;&lt;/p&gt;&lt;p&gt;完全参考&lt;u&gt;文档&lt;/u&gt;。&lt;/p&gt;&lt;p&gt;测试 TiDB 集群，可能遇到的坑（MySQL 8 client On MacOSX）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mysql client connect : Unknown charset 255 (MySQL 8 Client 不支持字符集，需要指定默认字符集为 UTF8) &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;mysql -hx.x.x.x --default-character-set utf8&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 天真贝叶斯的服务接口&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/model/service1&lt;/code&gt; PUT 上报数据：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;updates&quot;: [
    [
      &quot;transfer leader from store 7 to store 2&quot;,
      [
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;hotRegionsCount1&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;minRegionsCount1&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;hotRegionsCount2&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;minRegionsCount2&quot;,
          &quot;value&quot;: &quot;true&quot;
        },
        {
          &quot;feature_type&quot;: &quot;Category&quot;,
          &quot;name&quot;: &quot;srcRegion&quot;,
          &quot;value&quot;: &quot;7&quot;
        }
      ]
    ],
  ]}&lt;/code&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;/model/service1&lt;/code&gt; POST 获取模型结果：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;输入参数：上报的参数&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;predictions&quot;: [
    {
      &quot;transfer leader from store 1 to store 2&quot;: 0.27432775221072137,
      &quot;transfer leader from store 1 to store 7&quot;: 0.6209064350448428,
      &quot;transfer leader from store 2 to store 1&quot;: 0.024587894827775753,
      &quot;transfer leader from store 2 to store 7&quot;: 0.01862719305134528,
      &quot;transfer leader from store 7 to store 1&quot;: 0.02591609468013258,
      &quot;transfer leader from store 7 to store 2&quot;: 0.03563463018518229
    }
  ]} &lt;/code&gt;&lt;p&gt;&lt;b&gt;3. PD 集群部署&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先将 pd-server 替换到集群所在 &lt;code class=&quot;inline&quot;&gt;ansible/resources/bin&lt;/code&gt; 目录下，那如何让集群上的 PD 更新生效呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;更新：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;$ ansible-playbook rolling_update.yml --tags=pd&lt;/code&gt;&lt;p&gt;在实操过程中， 如果你在更新到一半的时候就关门了，可能会导致整个 PD 挂掉（非集群环境），可能是因为逻辑不严谨所导致的问题&lt;/p&gt;&lt;p&gt;直接停止了 ansible，导致 PD 集群机器节点有停止的情况，这个时候你可以通过以下命令启动它。&lt;/p&gt;&lt;p&gt;&lt;b&gt;启动：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;$ ansible-playbook start.yml --tags=pd&lt;/code&gt;&lt;p&gt;&lt;b&gt;4. PD 调度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1 取消热点数据调度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家都以为可以通过配置来解决：(调度开关方法: 用 config set xxx 0 来关闭调度)&lt;/p&gt;&lt;p&gt;配置如下：（虽然找的地方错误了，但是错打错着，我们来到了 Demo Time：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;config set leader-schedule-limit 0
config set region-schedule-limit 0
scheduler add hot-region-scheduler
config show
config set leader-schedule-limit 4
config set region-schedule-limit 8&lt;/code&gt;&lt;p&gt;实测发现，根本不生效，必须要改源代码。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;func (h *balanceHotRegionsScheduler) dispatch(typ BalanceType, cluster schedule.Cluster) []*schedule.Operator {
    h.Lock()
    defer h.Unlock()
    switch typ {
    case hotReadRegionBalance:
        h.stats.readStatAsLeader = h.calcScore(cluster.RegionReadStats(), cluster, core.LeaderKind)
        // return h.balanceHotReadRegions(cluster) // 将这一行注释
    case hotWriteRegionBalance:
        h.stats.writeStatAsLeader = h.calcScore(cluster.RegionWriteStats(), cluster, core.LeaderKind)
        h.stats.writeStatAsPeer = h.calcScore(cluster.RegionWriteStats(), cluster, core.RegionKind)
        // return h.balanceHotWriteRegions(cluster) // 将这一行注释
    }
    return nil
}&lt;/code&gt;&lt;p&gt;但是，我们要的不是不调度，而只是不给调度结果：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;func (h *balanceHotRegionsScheduler) balanceHotReadRegions(cluster schedule.Cluster) []*schedule.Operator {
    // balance by leader
    srcRegion, newLeader := h.balanceByLeader(cluster, h.stats.readStatAsLeader)
    if srcRegion != nil {
        schedulerCounter.WithLabelValues(h.GetName(), &quot;move_leader&quot;).Inc()
        // step := schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()} // 修改为不返回值或者返回 _
        _ = schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()}
        // return []*schedule.Operator{schedule.NewOperator(&quot;transferHotReadLeader&quot;, srcRegion.GetID(), srcRegion.GetRegionEpoch(), schedule.OpHotRegion|schedule.OpLeader, step)} // 注释这一行，并 return nil
        return nil
    }

    // balance by peer
    srcRegion, srcPeer, destPeer := h.balanceByPeer(cluster, h.stats.readStatAsLeader)
    if srcRegion != nil {
        schedulerCounter.WithLabelValues(h.GetName(), &quot;move_peer&quot;).Inc()
        return []*schedule.Operator{schedule.CreateMovePeerOperator(&quot;moveHotReadRegion&quot;, cluster, srcRegion, schedule.OpHotRegion, srcPeer.GetStoreId(), destPeer.GetStoreId(), destPeer.GetId())}
    }
    schedulerCounter.WithLabelValues(h.GetName(), &quot;skip&quot;).Inc()
    return nil
}

......

func (h *balanceHotRegionsScheduler) balanceHotWriteRegions(cluster schedule.Cluster) []*schedule.Operator {
    for i := 0; i &amp;lt; balanceHotRetryLimit; i++ {
        switch h.r.Int() % 2 {
        case 0:
            // balance by peer
            srcRegion, srcPeer, destPeer := h.balanceByPeer(cluster, h.stats.writeStatAsPeer)
            if srcRegion != nil {
                schedulerCounter.WithLabelValues(h.GetName(), &quot;move_peer&quot;).Inc()
                fmt.Println(srcRegion, srcPeer, destPeer)
                // return []*schedule.Operator{schedule.CreateMovePeerOperator(&quot;moveHotWriteRegion&quot;, cluster, srcRegion, schedule.OpHotRegion, srcPeer.GetStoreId(), destPeer.GetStoreId(), destPeer.GetId())} // 注释这一行，并 return nil
                return nil
            }
        case 1:
            // balance by leader
            srcRegion, newLeader := h.balanceByLeader(cluster, h.stats.writeStatAsLeader)
            if srcRegion != nil {
                schedulerCounter.WithLabelValues(h.GetName(), &quot;move_leader&quot;).Inc()
                // step := schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()} // 修改为不返回值或者返回 _
                _ = schedule.TransferLeader{FromStore: srcRegion.GetLeader().GetStoreId(), ToStore: newLeader.GetStoreId()}

                // return []*schedule.Operator{schedule.NewOperator(&quot;transferHotWriteLeader&quot;, srcRegion.GetID(), srcRegion.GetRegionEpoch(), schedule.OpHotRegion|schedule.OpLeader, step)} // 注释这一行，并 return nil
                return nil
            }
        }
    }

    schedulerCounter.WithLabelValues(h.GetName(), &quot;skip&quot;).Inc()
    return nil
}&lt;/code&gt;&lt;p&gt;当修改了 PD 再重新编译得到 pd-server，将其放到 &lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;tidb-ansible/resources/bin/pd-server&lt;/code&gt; 并替换原来的文件，然后执行 &lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;ansible-playbook rolling_update.yml --tags=pd&lt;/code&gt;，即可重启 pd-server 服务。&lt;/p&gt;&lt;p&gt;在调优的过程中发现，当前 &lt;code class=&quot;inline&quot;&gt;hot-region-scheduler&lt;/code&gt; 的调度时对于目标机器的选择并不是最优的，代码如下：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/pd/blob/master/server/schedulers/hot_region.go#L374&quot;&gt;https://github.com/pingcap/pd/blob/master/server/schedulers/hot_region.go#L374&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;简述：循环遍历 candidateStoreIDs 的时候，如果满足条件有多台，那么最后一个总会覆盖前面已经存储到 destStoreID 里面的数据，最终我们拿到的 destStoreID 有可能不是最优的。（&lt;/b&gt;pd issue: &lt;a href=&quot;https://github.com/pingcap/pd/issues/1359&quot;&gt;https://github.com/pingcap/pd/issues/1359&lt;/a&gt;）&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// selectDestStore selects a target store to hold the region of the source region.
// We choose a target store based on the hot region number and flow bytes of this store.
func (h *balanceHotRegionsScheduler) selectDestStore(candidateStoreIDs []uint64, regionFlowBytes uint64, srcStoreID uint64, storesStat core.StoreHotRegionsStat) (destStoreID uint64) {
    sr := storesStat[srcStoreID]
    srcFlowBytes := sr.TotalFlowBytes
    srcHotRegionsCount := sr.RegionsStat.Len()

    var (
        minFlowBytes    uint64 = math.MaxUint64
        minRegionsCount        = int(math.MaxInt32)
    )
    for _, storeID := range candidateStoreIDs {
        if s, ok := storesStat[storeID]; ok {
            if srcHotRegionsCount-s.RegionsStat.Len() &amp;gt; 1 &amp;amp;&amp;amp; minRegionsCount &amp;gt; s.RegionsStat.Len() {
                destStoreID = storeID
                minFlowBytes = s.TotalFlowBytes
                minRegionsCount = s.RegionsStat.Len()
                continue // 这里
            }
            if minRegionsCount == s.RegionsStat.Len() &amp;amp;&amp;amp; minFlowBytes &amp;gt; s.TotalFlowBytes &amp;amp;&amp;amp;
                uint64(float64(srcFlowBytes)*hotRegionScheduleFactor) &amp;gt; s.TotalFlowBytes+2*regionFlowBytes {
                minFlowBytes = s.TotalFlowBytes
                destStoreID = storeID
            }
        } else {
            destStoreID = storeID
            return
        }
    }
    return
}&lt;/code&gt;&lt;p&gt;&lt;b&gt;4.2 PD 重要监控指标详解之 HotRegion：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Hot write Region&#39;s leader distribution：每个 TiKV 实例上是写入热点的 leader 的数量&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s peer distribution：每个 TiKV 实例上是写入热点的 peer 的数量&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s leader written bytes：每个 TiKV 实例上热点的 leader 的写入大小&lt;/li&gt;&lt;li&gt;Hot write Region&#39;s peer written bytes：每个 TiKV 实例上热点的 peer 的写入大小&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s leader distribution：每个 TiKV 实例上是读取热点的 leader 的数量&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s peer distribution：每个 TiKV 实例上是读取热点的 peer 的数量&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s leader read bytes：每个 TiKV 实例上热点的 leader 的读取大小&lt;/li&gt;&lt;li&gt;Hot read Region&#39;s peer read bytes：每个 TiKV 实例上热点的 peer 的读取大小&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本次我们只 hack 验证了 Write Region Leader 这部分，所以我们重点关注一下监控和问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Hot write Region&#39;s leader distribution&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;i&gt;监控数据有一定的延时（粗略估计1-2分钟）&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;5. 模拟热点数据&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从本地往服务器 load 数据：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;修改 &lt;code class=&quot;inline&quot;&gt;tidb-bench&lt;/code&gt; 的 &lt;code class=&quot;inline&quot;&gt;Makefile#load&lt;/code&gt; 模块对应的主机地址，然后执行 &lt;code class=&quot;inline&quot;&gt;make tbl&lt;/code&gt;, &lt;code class=&quot;inline&quot;&gt;make load&lt;/code&gt; 即可往服务器 load 数据了。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;注意，这里你也需要进行一些配置修改：&lt;/i&gt;&lt;code class=&quot;inline&quot;&gt;--default-character-set utf8&lt;/code&gt;&lt;br&gt;&lt;i&gt;犯的错：受限于本地-服务器间网络带宽，导入数据很慢。&lt;/i&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;线上服务器上：&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;$ ./go-ycsb run mysql -p mysql.host=10.9.x.x -p mysql.port=4000 -p mysql.db=test1 -P workloads/workloada&lt;/code&gt;&lt;p&gt;注：&lt;code class=&quot;inline&quot;&gt;go-ycsb&lt;/code&gt; 支持 insert，也支持 update，你可以根据你的需要进行相对应的调整&lt;code class=&quot;inline&quot;&gt;workloada#recordcount&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;workloada#operationcount&lt;/code&gt; 参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.本地构建 rust-nb-server&lt;/b&gt;&lt;/p&gt;&lt;p&gt;rust 一天速成……&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;Demo Time 的时候听好几个团队都说失败了。我以前也尝试过，但是被编译的速度以及耗能给击败了。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;环境都可以把你 de 自信心击溃。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;rustup install nightly
cargo run
...&lt;/code&gt;&lt;p&gt;Mac 本地打包 Linux 失败：缺少 std 库，通过 Docker 临时解决。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7. 导师指导&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从比赛一开始，导师团就非常积极和主动，直接去每个项目组，给予直接指导和建议，我们遇到问题去找导师时，他们也非常的配合。&lt;/p&gt;&lt;p&gt;导师不仅帮我们解决问题（特别是热点数据构建，包括对于代码级别的指导），还跟我们一起探讨课题方向和实际可操作性，以及可以达到的目标。&lt;/p&gt;&lt;p&gt;非常感谢！！！&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;我们的准备和主动性真的不足，值得反思--也希望大家以后不要怕麻烦，有问题就大胆的去问。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;Hackathon Demo&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;整个 Demo show 进行的非常顺利，为每一个团队点赞！&lt;/p&gt;&lt;p&gt;很多团队的作品都让人尖叫，可想而知他们的作品是多么的酷炫和牛逼，印象中只有一个团队在 Demo 环境出现了演示时程序崩溃的问题（用Java Netty 基于 TiKV 做的 memcache（实现了大部分的协议））。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 颁奖&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;遗憾！！！&lt;/p&gt;&lt;p&gt;我们 DSG 团队荣获三等奖+最佳创意两项大奖，但是很遗憾我未能跟团队一起分享这一刻。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;因为我要赶着去火车站，所以在周日下午6点的时候，我跟队友和一些朋友道别后，我就去火车站了，后面几组的 Demo Show 也很非常遗憾未能参加。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;得奖感言：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;谢谢 DSG 团队，谢谢导师，谢谢评委老师，谢谢 PingCAP 给大家筹备了这么好的一次黑客马拉松比赛活动。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Hackathon 2018 总结&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;本次比赛的各个方面都做的完美，除了&lt;b&gt;网络&lt;/b&gt;。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;1. 环境（一定要提前准备）----这次被坑了不少时间和精力；&lt;/p&gt;&lt;p&gt;2. 配置文档中有一些注意事项，一定要认真阅读：&lt;i&gt;ext4&lt;/i&gt; 必须要每台机器都更新；&lt;/p&gt;&lt;p&gt;3. [10.9.97.254]: Ansible FAILED! =&amp;gt; playbook: bootstrap.yml; TASK: check_system_optional : Preflight check - Check TiDB server&#39;s RAM; message: {&quot;changed&quot;: false, &quot;msg&quot;: &quot;This machine does not have sufficient RAM to run TiDB, at least 16000 MB.&quot;}  - 内存不足的问题&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可以在执行的时候增加参数来避免&lt;/li&gt;&lt;li&gt;ansible-playbook bootstrap.yml --extra-vars &quot;dev_mode=True&quot;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4. 如果磁盘挂载有问题，可以重新清除数据后再重新启动；&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;ansible-playbook unsafe_cleanup_data.yml&lt;/code&gt;  （&lt;a href=&quot;https://github.com/pingcap/docs/blob/master/op-guide/ansible-operation.md&quot;&gt;https://github.com/pingcap/docs/blob/master/op-guide/ansible-operation.md&lt;/a&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;参考资料&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/pd&quot;&gt;https://github.com/pingcap/pd&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb-bench/tree/master/tpch&quot;&gt;tidb-bench tpch&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/go-ycsb&quot;&gt;https://github.com/pingcap/go-ycsb&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/op-guide/ansible-deployment.md&quot;&gt;Ansible 部署&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/op-guide/dashboard-pd-info.md&quot;&gt;PD 重要监控指标详解&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://pingcap.github.io/docs/op-guide/ansible-deployment-rolling-update/&quot;&gt;使用 TiDB-Ansible 升级 TiDB&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://tool.oschina.net/codeformat/json&quot;&gt;在线代码格式化&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/liufuyang/rust-nb-server&quot;&gt;rust-nb-server&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-18-52647715</guid>
<pubDate>Tue, 18 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB DevCon 2019 报名开启 ：年度最高规格的 TiDB 技术大会</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-17-52603507.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52603507&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6a42d7ca1d0fcdcda28d72e7485505e1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;&lt;i&gt;年度最高规格的 TiDB 技术大会&lt;/i&gt;&lt;/b&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;海内外动态及成果的综合呈现&lt;/i&gt;&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;最新核心技术解读&lt;/i&gt;&lt;br&gt;&lt;i&gt;多个成果首次亮相&lt;/i&gt;&lt;br&gt;&lt;i&gt;2019 RoadMap 展望&lt;/i&gt;&lt;br&gt;&lt;i&gt;14 位海内外基础架构领域技术大咖&lt;/i&gt;&lt;br&gt;&lt;i&gt;8 个跨行业多场景的用户实战经验&lt;/i&gt;&lt;br&gt;&lt;i&gt;1 小时 Demo Show&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;只为向你描述可以预见的&lt;/i&gt;&lt;br&gt;&lt;i&gt;数据库的未来&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;1970 年关系模型的提出，在数据库领域如同打开了一扇创世之门，近半个世纪以来相关产品迅速迭代，新旧共生，我们试图洞悉其中规律，寻求一个问题的答案：&lt;b&gt;什么是未来的数据库？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;然而谈到“未来”难免总是宏观而模糊，我们尝试&lt;b&gt;用自己的方式&lt;/b&gt;来表达对未来的定义：年度规格最高的 TiDB 技术大会&lt;b&gt;#TiDB DevCon 2019&lt;/b&gt; 正式开启。&lt;/p&gt;&lt;p&gt;我们将从技术理念开始，尽所能地向前&lt;b&gt;眺望&lt;/b&gt;，分享我们所看到的，和即将付诸行动的一切探索。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间：&lt;/b&gt;2019 年 1 月 19 日&lt;/li&gt;&lt;li&gt;&lt;b&gt;地点：&lt;/b&gt;北京市 朝阳区 樱花园东街甲 2 号 北京服装学院内 中关村时尚产业创新园 二层&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;一、讲师介绍&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2a26fa50a77cf471c140d147c90ae197_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2132&quot; data-rawheight=&quot;1222&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2a26fa50a77cf471c140d147c90ae197&quot; data-watermark-src=&quot;v2-a46f637fcb69f4c004b5823f8c894e04&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-398e8319675f83c5a139ad62a1a04736_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2192&quot; data-rawheight=&quot;1228&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-398e8319675f83c5a139ad62a1a04736&quot; data-watermark-src=&quot;v2-f189a9c9b3c7ed2bc84b78203c5d6eb4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;二、会议议程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;8:40 - 9:30&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;     入场签到&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;9:30 - 9:50  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Opening Keynote &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     刘奇，PingCAP 联合创始人兼 CEO    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;9:50 - 10:10    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Global Scale of TiDB (English)   &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     Morgan Tocker, PingCAP Senior Product and Community Manager&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;10:10 - 11:10   &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区实践专场 1    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     - 美团数据库架构演变及展望&lt;/p&gt;&lt;p&gt;        美团点评 - 赵应钢    &lt;/p&gt;&lt;p&gt;     - 智能物联网的高写入场景解决之道&lt;/p&gt;&lt;p&gt;        小米 - 潘友飞    &lt;/p&gt;&lt;p&gt;     - 新东方新一代报名系统数据库选型&lt;/p&gt;&lt;p&gt;        新东方 - 傅少峰    &lt;/p&gt;&lt;p&gt;     - TiDB 在银行核心金融领域的研究与实践&lt;/p&gt;&lt;p&gt;       北京银行 - 于振华    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;11:10 - 11:50    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Way to TiDB 3.0&lt;/b&gt;&lt;/p&gt;&lt;p&gt;     申砾，PingCAP Engineering VP  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;11:50 - 13:20  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;     午餐    &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;13:20 - 14:00&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区荣誉时刻  &lt;/b&gt; &lt;/p&gt;&lt;p&gt;     崔秋,  PingCAP 联合创始人  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;14:00 - 15:00    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;社区实践专场 2    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     - TiDB 在微众银行的实践与应用&lt;/p&gt;&lt;p&gt;        微众银行 - 胡盼盼    &lt;/p&gt;&lt;p&gt;    - All in！转转 TiDB 大规模实践之旅&lt;/p&gt;&lt;p&gt;        转转 - 孙玄    &lt;/p&gt;&lt;p&gt;    - TiDB 在小红书实时销售数据分析的最佳实践  &lt;/p&gt;&lt;p&gt;       小红书 - 张俊骏  &lt;/p&gt;&lt;p&gt;    - How to Add a New Feature to TiDB - From a Contributor’s “View”&lt;/p&gt;&lt;p&gt;       某金融机构 - 潘迪&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:00 - 15:20    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;      茶歇  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:20 - 15:40  &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;TiKV - A Strong Foundation (English)    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;      Ana Hobden，PingCAP Senior Database Engineer on TiKV&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;15:40 - 16:40    &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Demo Session    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     黄东旭，PingCAP 联合创始人兼 CTO&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;16:40 - 17:00&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;The Future of Database    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;     黄东旭，PingCAP 联合创始人兼 CTO&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、报名注册开启&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB DevCon 2019 即日起开放报名注册，分为社区贡献者注册和标准注册两种类型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;报名链接👉：&lt;a href=&quot;https://pingcap.com/community/devcon2019/&quot;&gt;DevCon 2019&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 社区贡献者注册（¥0）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们已经将 TiDB 社区贡献者专属&lt;b&gt;【验证码】&lt;/b&gt;发送到大家在 GitHub 上的预留个人邮箱，请注意查收。如果没有收到，可以联系 TiDB Robot（微信号：tidbai）。&lt;/li&gt;&lt;li&gt;大家填写报名表单时务必填写 GitHub ID 和验证码，否则无法审核通过。&lt;/li&gt;&lt;li&gt;即日起至 1 月 17 日 18:00 期间新增社区贡献者亦可注册。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. 标准注册（¥1970）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;标准注册无需审核。&lt;/li&gt;&lt;li&gt;另外，我们为教职人员和在校学生提供学术优惠（票价 ¥499），仅限优惠码注册，申请材料： &lt;/li&gt;&lt;ul&gt;&lt;li&gt;教职人员：校园网站个人信息页截图（或教师资格证） + 本人身份证扫描件 &lt;/li&gt;&lt;li&gt;在校学生：本人有效学生证 + 本人身份证扫描件 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;请将申请材料发送到 &lt;a href=&quot;mailto:%20langyuemeng@pingcap.com&quot;&gt;langyuemeng@pingcap.com&lt;/a&gt;，审核结果将通过邮件通知。优惠码申请截止时间 1 月 17 日 18:00。&lt;/p&gt;&lt;a href=&quot;https://pingcap.com/community/devcon2019/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-60ab5bd867c2434d70c957a02a2169e1&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; data-image-size=&quot;ipico&quot;&gt;DevCon 2019&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-17-52603507</guid>
<pubDate>Mon, 17 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiQuery：All Diagnosis in SQL | TiDB Hackathon 项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-17-52554018.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52554018&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e54c5c646a5e75cff2f65c5e0545b6ab_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自 TiNiuB 队的黄梦龙同学，他们的项目 TiQuery 在本届 TiDB Hackathon  2018 中获得了三等奖。 TiQuery 可以搜集诊断集群问题所需要的信息，包括集群拓扑，Region 分布，配置，各种系统信息，整理成结构化的数据，并在 TiDB 中支持直接使用 SQL 语言进行查询，开发和运维人员可以在 SQL 环境方便高效地进行问题诊断。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;“距离 Hackathon 结束已经一个多星期了，感觉心情还是没有从激情中平复过来。不过由于我读书少，这时候好像只能感慨一句，黑客马拉松真是太好玩了……”&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;组队和选题&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;组队的过程有些崎岖，过程不细表，总之最后我们凑成了 4 人团队：&lt;/p&gt;&lt;p&gt;我（menglong）和阿毛哥是 PingCAP 内部员工，我在 TiKV 团队，目前主要是负责 PD 部分的。阿毛哥是 TiDB 组的开发，同时也是 Go 语言圈的大佬。&lt;/p&gt;&lt;p&gt;晓峰（ID 米麒麟）是大数据圈的网红，可能很多人都在各种社区或各种微信群偶遇过，另外他的公司其实也是上线了 TiDB 集群的客户之一。&lt;/p&gt;&lt;p&gt;胡争来自小米，是 HBase 的 committer，在分布式系统方面有丰富的经验，Hackathon 的过程中还顺便给 TiKV 集群的全局备份方案提了几个很好的建议，也是不得不服……他还有另外一个身份，是我们 TiKV 组员大妹子的老公，大妹子回老家休产假了于是只得派家属来代为过个瘾。&lt;/p&gt;&lt;p&gt;我们大约从一周之前开始讨论选题的事情，我们所有人都是第一次参加 Hackathon，也没什么选题的经验，经过微信语音长时间的头脑风暴，前后大约提了有五六个方案，最终敲定了 TiQuery 这个方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案的主要灵感是来自 facebook 的开源项目 osquery，它能把系统的各种信息（CPU，内存，文件，挂载，设备，网络连接，crontab，ulimit，iptable，等等等等）整理成结构化数据并支持以 SQL 的方式进行查询。当然它是一个单机的，我们需要做个 proxy 把集群中所有节点的数据收集在一起，放在 TiDB 里供用户查询。再考虑 TiDB 产品生态的实际情况，我们还可以搜集 region 分布，配置，日志，metrics 等诊断所需要信息统一到 SQL 接口里，这样就升级成了一套完整的诊断工具。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后选 TiQuery 这个方案主要考虑了这几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;不需要写前端。我们几个虽然说平时或多或少写过点前端，但是毕竟手生，短时间可能很难搞出酷炫的效果，而且还有翻车的风险。事后证明这个扬长避短的思路无疑是正确的，最后拿到名次的 6 个团队只有 2 个是重点在可视化这块的，而且呈现出来的效果以我们的水平应该难以企及。&lt;/li&gt;&lt;li&gt;不容易翻车。因为有 facebook/osquery 这个强力项目做后盾，基本上不存在翻车的可能。做完 osquery 后，计划的其他部分可以到时候根据情况决定要不要做，可以说是既稳又浪。&lt;/li&gt;&lt;li&gt;&lt;b&gt;实用性强。&lt;/b&gt;我们在日常帮助用户排查问题的时候，经常需要在 SQL / 日志 / Grafana / pd-ctl / tidb-ctl / ssh 各种工具来回切换搜集各种信息。甚至有时候无法直接访问用户的环境，需要一步一步向用户说明如何去排查，在交流上花费了大量不必要的时间和精力。所以 TiQuery 这个项目解决的是切实存在的痛点，听闻已经有客户准备在生产集群中把 TiQuery 上线，也是很好的佐证。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;比赛过程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Day 1&lt;/b&gt;&lt;/p&gt;&lt;p&gt;比赛第一天。早上 10 点踩着点来到公司，在前台简单签了到领了周边，转身进入办公区域，一瞬间就被扑面而来的热烈的气氛给感染到了。平常安静空旷的工作空间此时已是济济一堂，各路大神有的围在一起探讨方案，有的已经打开电脑开始攻克技术难题，还有不少相互网上熟识已久的网友们在热情地打着招呼。&lt;/p&gt;&lt;p&gt;我也很快找到 TiNiuB 队的根据地，短暂寒暄后就进入了工作状态。根据之前商量的分工，我主要负责把 PD 相关的数据转成 table 的形式。因为对 Go 语言和 PD 的接口都很熟悉，很快就把 TiQuery 的大体框架给撸出来了，PD 相关的数据源也依次给整理出来了。&lt;/p&gt;&lt;p&gt;阿毛哥那边计划是魔改一版 TiDB，来达成特定表的数据从远程服务加载这个需求。本来我们想的是需要 hack 一下 physical plan，或者实现一个新的 storage 什么的，想想还有些复杂。到他具体做的时候猛然发现 InformationSchema 这个神奇的存在，简单介绍下，InformationSchema 包含了 TiDB 中一系列特殊的表，它们的数据是直接从内存中捞来的，不需要经过 physical plan，也不需要走 storage。所以我们仿照 InformationSchema 的方式处理一下就行了，只不过数据是从 TiQuery 获取而不是直接在内存里。&lt;/p&gt;&lt;p&gt;两边写完之后我们很快进行了简单的联调，直接就通过测试了。看到 “SELECT * FROM pd_store”跑出结果后大家不由地一阵欢呼。不得不说，当不需要考虑异常错误处理，不需要写测试，不需要 review 的时候写代码的效率是真高……&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6808c88d870791b49185215a3cba3efb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6808c88d870791b49185215a3cba3efb&quot; data-watermark-src=&quot;v2-0133f41bf3f82ff2a72446c402dbe5c2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;中午我们在园区的“那家小馆”吃午餐。这里有个小插曲，胡争早了为了方便进园区把大妹子的员工卡给带来了，我们一合计估摸她休假之前应该卡里还留了不少钱，便决定饱餐一顿后强行用她的卡买单。等到了结账的时候才发现卡里只给留了 8 分钱……最后只好我掏了卡，并暗地里下决心好歹得拿个奖回来，不然偷鸡不成还蚀把米，与此同时不禁为胡争未来漫长的婚姻生活捏了把汗。&lt;/p&gt;&lt;p&gt;下午我们加入了 osquery-agent 服务负责从不同节点搜集上报系统信息，并用脚本把 osquery 的所有 schema 转成了 MySQL 兼容的形式导入进 TiDB。跑通后简单的试玩了下，基本上能按预期的方式运行，但是实用性方面有一些不足。随后我们主要从这几个方面进行优化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增加集群拓扑结构的信息。osquery-agent 变身成 tiquery-agent，除了 wrap osquery 之外还上报节点所运行的所有服务信息。&lt;/li&gt;&lt;li&gt;引入 psutil 库，tiquery-agent 支持查询针对单个服务更精确的 CPU，内存等信息。&lt;/li&gt;&lt;li&gt;调研 prometheus 转成 table 的可行性，这个我们发现短时间不太好做，就放弃了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他同步在做的事情包括用 ansible 部署了一套测试集群，开始做演示用的 slides，梳理 TiQuery 能提供的功能整理一个有说服力的 user story。期间我还客串了下导师的角色，支持了几个涉及到 PD 的项目。&lt;/p&gt;&lt;p&gt;到了晚上，测试集群上已经能完全顺畅地跑起来了，slides 基本上完成，演示要用的查询也都准备好了。霸哥（导师团成员&lt;b&gt;韩飞&lt;/b&gt;，人称 SQL 小王子）帮我们手写了一个复杂的 4 表 JOIN，一气呵成，大家纷纷表示向大佬低头。&lt;/p&gt;&lt;p&gt;23 点左右，我又去整个赛场溜达了一圈，发现比较有竞争力的几个项目要么还在埋头苦干，要么就是 block 在技术难点上痛不欲生。当时我们就感觉胜券在握了，简单商量了一下就各自回家休息。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Day 2&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第二天早上过来先是又转了一圈探查敌情，当时脑海里就冒出来“龟兔赛跑”的故事。&lt;br&gt;&lt;/p&gt;&lt;p&gt;几个可视化的项目，昨天晚上走之前还几乎是白板一片，一夜之间就酷炫到没朋友了，尤其是凤凰队，真给人一种山鸡变凤凰的感觉。还有 TiBoys，昨天我琢磨着项目太宏大，铁定没法搞出来的，早上去一看，readme 里的 todo list 已经基本上全给勾上了，很难想像这一夜他们经历了什么……&lt;/p&gt;&lt;p&gt;我们当天其实就没做什么事情了，就整理了下项目文档什么的，还找了个马里奥的图片 ps 了下。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-068739583a743493fa8c42ee150f4bb7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;600&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-068739583a743493fa8c42ee150f4bb7&quot; data-watermark-src=&quot;v2-bc600f7bf47f245cd4610e8a97926983&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下午的 Demo 我们排在最后出场，由胡争出场演示。整个过程出奇地顺畅，评委的疑问也顺畅的解答了，其实我们的项目本身比较简单，要做的事情一两句话就能说清楚。略有遗憾的是当时胡争可能是过于激动，关于 TiQuery 具体有哪些实用场景没有细说。&lt;/p&gt;&lt;p&gt;最后我们在众多优秀的作品中杀出重围，侥幸拿到了三等奖，第一次参赛的大家都很兴奋，晚上自然是又出去好好腐败了一把，并相约以后再找机会参加类似的活动……&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7030f9a748d5ae252c05215e771c1edc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;594&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7030f9a748d5ae252c05215e771c1edc&quot; data-watermark-src=&quot;v2-e67afd03449cb4452535ebaab0959e9e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;感想和总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我本人喜欢跑步，也参加过多次马拉松赛，其实马拉松赛不仅在于竞技，也不仅在于坚持跑完那 42km，更重要的是它是一大群有共同爱好的人聚在一起的一次狂欢。从这个角度看，“黑客马拉松”真的是非常贴切的一个名字。在这里我想感谢 PingCAP 和志愿者们的精心组织，提供了这么好的一个机会让大家有机会互相欣赏，切磋，交流，学习。&lt;/p&gt;&lt;p&gt;通过这次比赛我也积攒了一些经验，或者说下次参加 Hackathon 会去考虑的一些点吧，在这里分享出来供大家探讨：&lt;/p&gt;&lt;p&gt;&lt;b&gt;THINK BIG&lt;/b&gt;。我们在选方案的时候特别怕想复杂了到时候做不出来然后翻车，实际上在 Hackathon 比赛时爆发出的潜能会远超自己的想象，结果就是我们迅速就完工了后面其实无所事事……可以简单针对编程速度算下账：不用考虑异常处理，效率 x2，不用写单元测试，效率 x3，不用 code review，效率 x2，不用考虑优雅设计前后兼容，效率 x2，全天 24 小时工作，效率 x3（根据贵司具体情况可调整为 x2），再加上是几个人一起做的，粗略算下来 24 小时内足以做完平时需要几个月才能完成的事情，因此我们设计方案时可以尽管往大了想。&lt;/p&gt;&lt;p&gt;&lt;b&gt;SHOW OFF ALL&lt;/b&gt;。比赛之前我们就已经意识到 Demo 展示环节非常关键，但是可惜这块还是做得不够好，有些花力气做了的功能最终没有最后在 Demo 时展现出来，今后参加类似的比赛时会更加注意这一点。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiQuery 项目及其未来&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后再花一些篇幅来推广下 TiQuery 这个项目吧。&lt;/p&gt;&lt;p&gt;首先明确一点，它不是要替换掉已有的查日志，看 metrics，调用 PD API 等现有的诊断问题手段，而是提供一种新的途径和可能，即直接使用 SQL 语言，并且这种途径在很多场景下会更方便甚至是变不可能为可能。&lt;/p&gt;&lt;p&gt;比如我们平常定位一个慢 SQL，可能需要先在 SQL 环境中确认有问题的语句，然后去日志中找出响应时间长的 Region，随后使用 pd-ctl 去查询 Region 的信息，然后再根据 leader 所在的 TiKV ID 查询到对应 TiKV 所在的节点，然后再 ssh 登录到对应的节点查询关键线程的 CPU 占用情况……&lt;/p&gt;&lt;p&gt;如果部署了 TiQuery，以上操作都可以在 SQL 环境中搞定，不用各种工具来回切换，而且通过 SQL 的关联查询功能，以上整个流程甚至只需要一条语句。&lt;/p&gt;&lt;p&gt;再比如，客户的环境可能对访问某些资源有限制，比如没有权限 ssh 登录对应的服务器，防火墙的原因无法查看 Grafana，这时 TiQuery 就能帮且我们拿到原本拿不到的信息。还有的时候，我们无法直接连接客户的集群，只能远程指导客户去诊断问题，这种情况下不用去费力教会客户用各种工具了，直接扔过去一条 SQL，岂不美哉。&lt;/p&gt;&lt;p&gt;另外，SQL 作为一门标准化的查询语言，在易用性方面有着天然的优势，不仅方便不了解 TiDB 的 DBA 快速上手，其他外部系统也能方便地对接（比如外部监控报警系统）。&lt;/p&gt;&lt;p&gt;当然了，TiQuery 项目如果真要在严肃的生产环境上线，还有许多工作要做：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;完善异常处理，重构，文档的完善&lt;/li&gt;&lt;li&gt;更多 schema 支持，包括日志文件，prometheus metrics 等&lt;/li&gt;&lt;li&gt;tidb-ansible 集成，包括部署 tiquery-agent 服务，配置生成，安装 osquery 等&lt;/li&gt;&lt;li&gt;TiDB 支持外部数据源加载数据（这个特性在 Hackathon 其他项目也有各自的实现，期待能合入 TiDB 主干）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;最后，项目的地址在&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://github.com/TiNiuB/tiquery&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-b79fd3efb6bcd737367065f0bda81a7a&quot; data-image-width=&quot;400&quot; data-image-height=&quot;400&quot; data-image-size=&quot;ipico&quot;&gt;TiNiuB/tiquery&lt;/a&gt;&lt;p&gt;&lt;b&gt;期待大家来共同完善！&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-17-52554018</guid>
<pubDate>Mon, 17 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Lab 诞生记 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-14-52414551.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52414551&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-391401a44b5cfc4f07ef0d321a10fce0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文由&lt;b&gt;红凤凰粉凤凰粉红凤凰队&lt;/b&gt;的成员主笔，他们的项目 &lt;b&gt;TiDB Lab&lt;/b&gt; 在本届 TiDB Hackathon 2018 中获得了二等奖。TiDB Lab 为 TiDB 培训体系增加了一个可以动态观测 TiDB / TiKV / PD 细节的动画教学 Lab，让用户可以一边进行真实操作一边观察组件之间的变化，例如 SQL 的解析，Region 的变更等等，从而生动地理解 TiDB 的工作原理。&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a8ca052d52e744806a93b786c597e257_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;246&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a8ca052d52e744806a93b786c597e257&quot; data-watermark-src=&quot;v2-1698678ea9ba1448ad082b2e043d13e5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;项目简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 简介&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Lab，全称 TiDB Laboratory&lt;/b&gt;，是一个集 TiDB 集群状态的在线实时可视化与交互式教学的平台。用户可以一边对 TiDB 集群各个组件 TiKV、TiDB、PD 进行各种操作，包括上下线、启动关闭、迁移数据、插入查询数据等，一边在 TiDB Lab 上以动画形式观察操作对集群的影响，例如数据是怎么流动的，Region 副本在什么情况下发生了变更等等。通过 TiDB Lab 这种对操作进行可视化反馈的交互模式，用户可以快速且生动地理解 TiDB 内部原理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.功能&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;实时动态展示 TiDB、TiKV 节点的新增、启动与关闭。&lt;/li&gt;&lt;li&gt;实时动态展示 TiDB 收到 SQL 后，物理算子将具体请求发送给某些 TiKV Region Leader 并获取数据的过程。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本状态的变化，例如新增、删除、分裂。&lt;/li&gt;&lt;li&gt;实时动态展示各个 TiKV 实例上 Region 副本内的数据量情况。&lt;/li&gt;&lt;li&gt;浏览集群事件历史（事件指上述四条功能所展示的各项内容）并查看事件的详细情况，包括事件的具体数据内容、SQL Plan 等等。&lt;/li&gt;&lt;li&gt;按 TiDB、TiKV 或 Region 过滤事件历史。&lt;/li&gt;&lt;li&gt;对事件历史进行时间穿梭：回到任意事件发生时刻重新观察当时的集群状态，或按事件单步重放观察集群状态的变化。&lt;/li&gt;&lt;li&gt;在线获取常用运维操作的操作指南。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3.愿景&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们其实为 TiDB Lab 规划了更大的愿景，但由于 Hackathon 时间关系，还来不及实现。我们希望能实现 TiDB Lab + TiDB 生态组件的沙盒，从而在 TiDB Lab 在线平台上直接提供命令执行与 SQL 执行功能。这样用户无需离开平台，无需自行准备机器下载部署，就可以直接在平台上根据提供的操作指南进行各类操作，并能观察操作带来的具体影响，形成操作与反馈的闭环，真正地实现零门槛浏览器在线教学。我们期望平台能提供用户以下的操作流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户获得一个 TiDB Lab 账户并登录（考虑到沙盒是占用实际资源的，需要通过账户许可来限制避免资源快速耗尽）。&lt;/li&gt;&lt;li&gt;用户在 TiDB Lab 上获得若干虚拟机器的访问权限，每个机器处于同一内网并具有独立 IP。这些虚拟机器的实际实现是资源受限的虚拟机或沙盒，因为作为教学实验不需要占用很多资源。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;例如：平台为用户自动分配了 5 个 IP 独立的沙盒的访问权限，地址为 192.168.0.1 ~ 192.168.0.5。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用户在平台上进行第零章「架构原理」的学习。平台提供了一个默认的拓扑部署，用户可以在平台提供的在线 SQL Shell 中进行数据的插入、删除、更新等操作。用户通过平台观察到 SQL 是如何对应到 TiKV 存储节点上的，以及数据是怎么切分到不同 Region 的等等。&lt;/li&gt;&lt;li&gt;用户在平台上进行第一章「TiDB 部署」的学习，了解到可以通过 ansible 进行部署。教学样例是一个典型的 TiDB + TiKV 三副本部署。对于这个教学样例，平台告知用户 inventory.ini 具体内容应当写成什么样子。用户可以在平台提供的在线 Terminal 上修改 inventory 文件，并执行部署与集群启动命令。部署和启动均能在平台上实时反馈可视化。&lt;/li&gt;&lt;li&gt;用户继续进行后续章节学习，例如「TiDB 单一服务启动与关闭」。用户在可视化界面上点击某个刚才已经部署出来的节点，可以了解启动或关闭单个 TiDB 的命令。用户可以在平台提供的在线 Terminal 上执行这些命令，尝试启动或关闭单一 TiDB。&lt;/li&gt;&lt;li&gt;用户继续学习基础运维操作，例如「TiKV 扩容」。平台告知 inventory.ini 应当如何进行修改，用户可以根据指南在在线 Terminal 上进行实践，并通过可视化界面观察扩容的过程，例如其他节点上的副本被逐渐搬迁到新节点上。&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b00fb1e54cf86282b73e86667a939732_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b00fb1e54cf86282b73e86667a939732&quot; data-watermark-src=&quot;v2-66a6b03f86e2e8fbc23c88126c249016&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;前期准备&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;团队&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们团队有三个人，是一个 PingCAP 同学与 TiDB 社区小伙伴钱同学的混合组队，其中 PingCAP 成员分别来自 TiKV 组与 OLAP 组。我们本着搞事情的想法，团队取名叫「&lt;b&gt;红凤凰粉凤凰粉红凤凰」&lt;/b&gt;，想围观主持人念团队名称（然而机智的主持人小姐姐让我们自报团队名称）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 原计划：干掉 gRPC&lt;/b&gt;&lt;/p&gt;&lt;p&gt;鉴于从报名开始直到 Hackathon 正式开始前几小时，我们都在为原计划做准备，因此值得详细说一下…&lt;/p&gt;&lt;p&gt;我们一开始规划的 Hackathon 项目是换掉 TiKV、TiDB 之间的 RPC 框架 gRPC，原因有几个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一是发现 TiKV、TiDB 中 gRPC 经常占用了大量 CPU，尤其是在请求较多但很简单的 benchmark 场景中经常比 Coprocessor 这块儿还高，这在客户机器 CPU 资源比较少的情况下是性能瓶颈；&lt;/li&gt;&lt;li&gt;二是发现 gRPC 性能一般，在各类常用 RPC 框架的性能测试中 gRPC 经常是垫底水平；&lt;/li&gt;&lt;li&gt;三是 gRPC 主要设计用于用户产品与 Google 服务进行通讯，因而考虑到了包括负载均衡友好、流量控制等方面，但对 TiKV 与 TiDB 这类内部通讯来说这些都是用不上的功能，为此牺牲的性能是无谓的开销；&lt;/li&gt;&lt;li&gt;另外近期 TiKV 内部有一个实验是将多个 RPC 请求 batch 到一起再发送（当然处理时候再拆开一个一个执行原来的 handler），性能可以瞬间提高一倍以上，这也从侧面说明了 gRPC 框架自身开销很大，因为用户侧请求总量是一致的，处理模式也是一致的，唯一的区别就是 RPC 框架批量发送或一条一条发送。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们早在几周前就开始写简单 Echo Server 进行可行性验证和性能测试，是以下三个方面的正交：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;协议：CapnProto RPC、brpc over gRPC、裸写 echo。&lt;/li&gt;&lt;li&gt;服务端：Rust、C++，对应用于 TiKV。&lt;/li&gt;&lt;li&gt;客户端：Golang，对应用于 TiDB。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;其中 TiKV 侧调研了 Rust 和 C++ 的服务端实现，原因是 Rust 可以通过 binding 方式调用 C++ 服务端。而 TiDB 侧客户端实现不包含 C++ 的原因是 Golang 进行 C / C++ FFI 性能很差，因此可以直接放弃 C/C++ 包装一层 binding 用于 TiDB 的想法。最后测试下来，有以下结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CapnProto：序列化性能很高，但其 RPC 性能没有很突出。最重要的是，CapnProto 的 Golang Client 实现有 bug，并不能稳定地与 Rust 或 C++ 的 Server 进行 RPC 交互。作者回复说这是一个已知缺陷，涉及重构。这对于 Hackathon 来说是一个致命的问题，我们并没有充足的时间解决这个问题，直接导致我们放弃这个方案。&lt;/li&gt;&lt;li&gt;brpc over gRPC：可以实现使用 brpc C++ 客户端 &amp;amp; gRPC Golang 服务端配合（注：brpc 没有 Golang 的实现）。但这个方案本质只是替换服务端的实现，并没有替换协议，并不彻底，我们不是特别喜欢。另外在这个换汤不换药的方案下，测试下来性能的提升有限，且随着 payload 越大会越小。我们最终觉得作为一个 Hackathon 项目如果仅有有限的性能提升（虽然可以在展示的时候掩盖缺陷只展示优点），那么意义不是很大，最终无法用于产品，因而放弃。&lt;/li&gt;&lt;li&gt;裸写：我们三个成员都不是这方面的老司机，裸写大概是写不完的。。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 新计划：做一个用于培训的可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Hackathon 开始的前一个晚上，我们决定推翻重来，于是 brain storm 了几个想法，最后觉得做一个用于教学的可视化比较可行，并且具有比较大的实际意义。另外，这个新项目「集群可视化」相比原项目「换 RPC」来说更适合 Hackathon，主要在于：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;具有图形化界面，容易拿奖可以直观地展现成果。&lt;/li&gt;&lt;li&gt;并不是一个「非零即一」的任务。新项目有很多子功能，可以逐一进行实现，很稳妥，且不像老项目那样只有换完才知道效果。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们继续在这个「可视化」的想法上进行了进一步的思考，想到如果可以做成在线教学的模式，则可以进一步扩展其项目意义，形成一个完整的在线教学体系，因此最终决定了项目的 scope。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;具体实现&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;在 TiKV、TiDB 与 PD 中各个关键路径上将发生的具体「事件」记录下来，并在前端进行一一可视化。&lt;/i&gt;&lt;/blockquote&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;事件&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们将 TiDB Lab 进行可视化所需要的信号称为「事件」，并规划了以下「事件」：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiDB&lt;/li&gt;&lt;li&gt;Ansible 事件：Ansible 部署 TiKV&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 启动&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到一条 SQL&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 KvGet 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 PreWrite / Commit 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 收到一条 Coprocessor 请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 创建&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Peer 删除&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 分裂&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region Snapshot 复制&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV Region 数据量发生显著变化&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，由于时间关系、技术难度和可视化需要，实际实现的是以下事件：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 事件：TiDB 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 关闭&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 KvGet 读请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 PreWrite / Commit 写入请求&lt;/li&gt;&lt;li&gt;TiDB 事件：TiDB 收到 SQL 并发起 Coprocessor 读请求&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 启动，若首次启动认为是新部署&lt;/li&gt;&lt;li&gt;TiKV 事件：TiKV 关闭&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 创建（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region Peer 删除（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 分裂（通过 Region 心跳实现）&lt;/li&gt;&lt;li&gt;PD 事件：TiKV Region 数据量发生显著变化（通过 Region 心跳实现）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 可视化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;可视化部分由前端（lab-frontend）和事件收集服务（lab-gateway）组成。&lt;/p&gt;&lt;p&gt;事件收集服务是一个简单的 HTTP Server，各个组件通过 HTTP Post 方式告知事件，事件收集服务将其通过 WebSocket 协议实时发送给前端。事件收集服务非常简单，使用的是 Node.js 开发，基于 ExpressJs 启动 HTTP Server 并基于 SocketIO 实现与浏览器的实时通讯。ExpressJs 收到事件 JSON 后将其通过 SocketIO 进行广播，总代码仅仅十几行。&lt;/p&gt;&lt;p&gt;可视化前端采用 Vue 实现，动画使用 animejs 和 CSS3。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过模板实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一部分事件通过「由事件更新集群状态数据 – 由集群状态通过 Vue 渲染模板」进行可视化。&lt;/p&gt;&lt;p&gt;这类可视化是最简单的，以 TiDB 启动与否为例，TiDB 的启动与否在界面上呈现为一个标签显示为「Started」或「Stopped」，那么就是一个传统的 Vue MVVM 流程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;数据变量 instances.x.online 代表 TiDB x 是否已启动。&lt;/li&gt;&lt;li&gt;收到 TiDB Started 事件后，更新 instances.x.online = true 。&lt;/li&gt;&lt;li&gt;收到 TiDB Stopped 事件后，更新 instances.x.online = false 。&lt;/li&gt;&lt;li&gt;前端模板上，根据 instances.x.online 渲染成 Started 或 Stopped 对应的界面。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这类可视化的动画采用的是 CSS3 动画。由于 Region Peer 位置是由 left, top CSS 属性给出的，因此为其加上 transition，即可实现 Region Peer 在屏幕上显示的位置改变的动画。位置改变会主要发生在分裂时，分裂时 Region 列表中按顺序会新增一个，那么后面各个 Region 都要向后移动（或换到下一行等）。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;244&quot; data-rawheight=&quot;232&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-31c499ed7e5e73fb418ee24608ffbc78_b.jpg&quot;&gt;&lt;p&gt;最后，使用 Vue Group Transition 功能，即可为 Region Peer 的新增与删除也加入动画效果。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;通过动画实现的可视化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另一部分事件并不反应为一个持久化 DOM 的变化，例如 TiDB 收到 SQL 并发请求到某个具体 TiKV 上 Region peer 的事件，在前端展示为一个 TiDB 节点到 TiKV Region Peer 的过渡动画。动画开始前和动画结束后，DOM 没有什么变化，动画是一个临时的可视元素。这类动画通过 animejs 实现。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;748&quot; data-rawheight=&quot;496&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-0275c3460d6d664862bb9a80c6ca3589_b.jpg&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_r.gif&quot; data-caption=&quot;这个浮夸的开场动画效果也是 animjs 做的&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;200&quot; data-rawheight=&quot;200&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-3a8408d8033ca7d0a57a10a282802493_b.jpg&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间穿梭&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;时间穿梭是一个在目前前端框架中提供的很时髦的功能，我们准备借鉴一波。主要包括：1. 回退到任意一个历史事件发生的时刻展示集群的状态；2. 从当前事件开始往后进行单步可视化重现。&lt;/p&gt;&lt;p&gt;时间穿梭的本质是需要实现两个基础操作：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于单一事件实现正向执行，即事件发生后，更新对应集群数据信息（如果采用「通过模板实现的可视化」），或创建临时动画 DOM（如果采用「通过动画实现的可视化」）。另外允许跳过「通过动画实现的可视化」这一步。&lt;/li&gt;&lt;li&gt;对于单一事件实现反向执行，即撤销这个事件造成的影响。对于「通过模板实现的可视化」，我们需要根据事件内容反向撤销它对集群数据信息的修改。对于「通过动画实现的可视化」，我们什么都不用做。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;时间穿梭的功能可以通过组合这两个基础操作实现。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;回退到任意历史事件发生时刻：若想要前往的事件早于当前呈现的事件，则对于这期间的事件逐一进行反向执行。若想要前往的事件晚于当前呈现的事件，则进行无动画的正向执行。&lt;/li&gt;&lt;li&gt;从当前事件开始单步可视化：执行一次有动画的正向执行。&lt;/li&gt;&lt;li&gt;实时展示新事件：执行一次有动画的正向执行。&lt;/li&gt;&lt;/ol&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;950&quot; data-rawheight=&quot;572&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-f94e3d2c834d8591c8c7e81ddb53b773_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;3. TiDB 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的历史事件收集略为 Hack。由于需要过滤任何非用户发起的查询（类似 GC 或者 meta 查询会由背景协程频繁发起打扰使用体验），因此在用户链接入口处添加了 context 标记一路携带到执行层，再修改相应的协程同步数据结构添加需要转发的标记信息。比较麻烦的是类似 Point Get 这样接口允许携带信息非常少的调用，只好将标记位编码进 Key 本身了。Plan 的可视化其实并没有花多少功夫，因为找到 TiDB 本身已经做了类似的功能，我们无非只是将这块代码直接偷来了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. PD 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;原本不少事件希望在 TiKV 端完成侦听，不过显然 KV 一小时编译一次的效率无法满足 Hackathon 中多次试错的需要。因此我们改为在 PD 中侦测心跳和汇报事件。其实并没有什么神秘，在原本 PD 自己检查 Region 变更的代码拆分成 Region 和 Peer 变更：每次 PD 接到 Region 心跳会在 PD Cache 中进行 Version 和 confVer 变更的检测，主要涉及 Peer 的增加和减少等。而 Region Split 会单独由 RegionSplitReport 进行汇报，这里也会做一次 Hook。另外就是每次心跳会检查是否有未上报给 Lab 的 Region 信息，如果有就转换成 Peer 信息进行补发。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. TiKV 事件收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上，原本我们计划了很多 TiKV 事件，但由于开发机器配置不佳，每次修改都要等待一小时进行一次编译，考虑到 Hackathon 上时间紧迫，因此最终大大缩减了 TiKV 上收集的事件数量，改为只收集启动和停止。基本架构是事件发生的时候，事件异步发送给一个 Channel，Channel 的另一端有一个异步的 worker 负责不断处理各个事件并通过 Hyper HTTP Client 发送出去。这个流程其实与 TiKV 中汇报 PD 事件有些类似，只是事件内容和汇报目标不一样。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感悟&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;以下文字 by 马晓宇 @ OLAP Team, PingCAP&lt;/i&gt;&lt;br&gt;&lt;i&gt;“由于比较能调侃干的活相对少一些，所以大王要我来巡山写感悟。”&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;就像队长说的，这个项目原本是希望做一个 bRPC 替换 gRPC 的试验，只是由于种种原因临到 Hackathon 前夜我们才确定这是个大概率翻车的点子。于是乎我们只好在酒店里抓耳挠腮，一边讨论可能的补救措施。&lt;/p&gt;&lt;p&gt;参加过、围观过几次 Hackathon，见过现场 Demo 效果最震撼的一次其实并不是一个技术上最优秀的作品，但是它的确赢得了大奖。那是一个脑洞奇大的点子：用 Kinect 体感加上 DirectX 的全 3D 展示做的网络流量实时展示；程序根据实时数据（举办方提供了实时网络测量统计信息）聚合显示不同粗细的炫酷弧光特效，而演示者则用手势操控地球模型的旋转。&lt;/p&gt;&lt;p&gt;于是在完全不了解大家是否能搞定前端的情况下，我们还是很轻率地决定了要做个重前端的项目。至于展示什么？既然时间不够，那么秀一个需要生产上线的可视化工具，翻车的概率就大的多，不如直接定位为 For Educational Purpose Only。而且大概是过度解读了炫酷对于成功的重要性，因此队长也轻率地决定了这个项目的基调是「极尽浮夸」。就在这样友好且不靠谱的讨论氛围下，这个项目的策划出炉了。&lt;/p&gt;&lt;p&gt;之后就是艰苦卓绝连绵无休的代码过程了。&lt;/p&gt;&lt;p&gt;龙毛二哥，晓峰和胡家属的 &lt;u&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&amp;amp;mid=2247487451&amp;amp;idx=2&amp;amp;sn=5f1ee6e838c3a86556fcd556662112c5&amp;amp;chksm=eb1628b1dc61a1a7e8f4cb82e2bfaab40cbfb27e986f9705f9166d629ff31a812f7ae45b1d73&amp;amp;scene=21#wechat_redirect&quot;&gt;TiNiuB Team&lt;/a&gt;&lt;/u&gt; 就在我们对面开工。讲道理这其实是我个人最喜欢的项目之一。第一天放学的时候，看着他们的进度其实我心里虚得不行：看看别人家的项目，我们的绝对主力还在折腾 TiDB Logo 动画，是我敢怒不敢言的状态。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f2ade45b86ee55be8b85a1198f684000_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;240&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f2ade45b86ee55be8b85a1198f684000&quot; data-watermark-src=&quot;v2-e650574cbe2a265f474817c1ac2c88e2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;原来我想偷一下懒回家睡个觉啥的，就临时改变主意继续配合队长努力干活；钱同学也抱着「TiKV 一天只能 Build 24 次必须珍惜」的态度写着人生的第一个 Rust 项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因此，这里必须鸣谢龙哥他们对我们的鞭策。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;事实证明，误打误撞但又深谋远虑的浮夸战略是有效的。Demo 的时候我很认真地盯着评委：在本项目最浮夸的 Logo 展示环节，大家的眼睛是发着光的，一如目睹了摩西分开红海。我个人认为这个动画 Logo 生生拉高了项目 50% 的评分。&lt;/p&gt;&lt;p&gt;只是具体要说感悟的话（似乎好多年没写感悟了呢），首先这次我们三个组员虽然有两个是 PingCAP 员工，不过由于技能的缺口大于人力，因此负责的任务都不是自己所属的模块：队长是 TiKV 组的但是在写前端，我是 OLAP 组的但是在改 PD 和 DB，钱同学也在做自己从来没写过的 Rust（参赛前一周简单入门了一下），因此其实还蛮有挑战的（笑）。然后偶尔写写自己不熟悉的语言，搞搞自己不熟悉的模块，会有一种别样的新鲜刺激感，这大概就是所谓的路边野花更香吧（嗯）？除此之外，Hackathon 更像一个大型社交活动，满足了猫一样孤僻的程序员群体被隐藏的社交欲，有利于码农的身心发展，因此可以多搞搞 :) 。&lt;/p&gt;&lt;p&gt;回到我们项目本身的话，其实 TiDB 的源码阅读或者其他介绍类文章其实并不能非常直观地帮助一头雾水的初学者理解这个系统。我们做这个项目的最大目的是能降低学习门槛，让所有人能以非常直观，互动的方式近距离理解她。所以希望这个项目能给大家带来方便吧。&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-14-52414551</guid>
<pubDate>Fri, 14 Dec 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
