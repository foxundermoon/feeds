<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sat, 08 Sep 2018 02:06:14 +0800</lastBuildDate>
<item>
<title>使用 TiKV 构建分布式类 Redis 服务</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-07-43959766.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43959766&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-828ba4512736d7f2cb3e6664c5fbf3b3_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：唐刘&lt;/p&gt;&lt;h2&gt;&lt;b&gt;什么是 Redis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt; 是一个开源的，高性能的，支持多种数据结构的内存数据库，已经被广泛用于数据库，缓存，消息队列等领域。它有着丰富的数据结构支持，譬如 String，Hash，Set 和 Sorted Set，用户通过它们能构建自己的高性能应用。&lt;/p&gt;&lt;p&gt;Redis 非常快，没准是世界上最快的数据库了，它虽然使用内存，但也提供了一些持久化机制以及异步复制机制来保证数据的安全。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Redis 的不足&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Redis 非常酷，但它也有一些问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;内存很贵，而且并不是无限容量的，所以我们不可能将大量的数据存放到一台机器。&lt;/li&gt;&lt;li&gt;异步复制并不能保证 Redis 的数据安全。&lt;/li&gt;&lt;li&gt;Redis 提供了 transaction mode，但其实并不满足 ACID 特性。&lt;/li&gt;&lt;li&gt;Redis 提供了集群支持，但也不能支持跨多个节点的分布式事务。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以有时候，我们需要一个更强大的数据库，虽然在延迟上面可能赶不上 Redis，但也有足够多的特性，譬如：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;丰富的数据结构&lt;/li&gt;&lt;li&gt;高吞吐，能接受的延迟&lt;/li&gt;&lt;li&gt;强数据一致&lt;/li&gt;&lt;li&gt;水平扩展&lt;/li&gt;&lt;li&gt;分布式事务&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;为什么选择 TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;大约 4 年前，我开始解决上面提到的 Redis 遇到的一些问题。为了让数据持久化，最直观的做法就是将数据保存到硬盘上面，而不是在内存里面。所以我开发了 &lt;a href=&quot;https://github.com/siddontang/ledisdb&quot;&gt;LedisDB&lt;/a&gt;，一个使用 Redis 协议，提供丰富数据结构，但将数据放在 RocksDB 的数据库。LedisDB 并不是完全兼容 Redis，所以后来，我和其他同事继续创建了 &lt;a href=&quot;https://github.com/reborndb/reborn&quot;&gt;RebornDB&lt;/a&gt;，一个完全兼容 Redis 的数据库。&lt;br&gt;无论是 LedisDB 还是 RebornDB，因为他们都是将数据放在硬盘，所以能存储更大量的数据。但它们仍然不能提供 ACID 的支持，另外，虽然我们可以通过 &lt;a href=&quot;https://github.com/CodisLabs/codis&quot;&gt;codis&lt;/a&gt; 去提供集群的支持，我们也不能很好的支持全局的分布式事务。&lt;/p&gt;&lt;p&gt;所以我们需要另一种方式，幸运的是，我们有&lt;a href=&quot;https://github.com/pingcap/tikv&quot;&gt;TiKV&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;TiKV 是一个高性能，支持分布式事务的 key-value 数据库。虽然它仅仅提供了简单的 key-value API，但基于 key-value，我们可以构造自己的逻辑去创建更强大的应用。譬如，我们就构建了 &lt;a href=&quot;https://github.com/pingcap/tidb&quot;&gt;TiDB&lt;/a&gt; ，一个基于 TiKV 的，兼容 MySQL 的分布式关系型数据库。TiDB 通过将 database 的 schema 映射到 key-value 来支持了相关 SQL 特性。所以对于 Redis，我们也可以采用同样的办法 - 构建一个支持 Redis 协议的服务，将 Redis 的数据结构映射到 key-value 上面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何开始&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-22f36db3ad8dd94b0d520e54185f28d7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;600&quot; data-rawheight=&quot;460&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-22f36db3ad8dd94b0d520e54185f28d7&quot; data-watermark-src=&quot;v2-f1bf888ee752ea16863a39c751e33f11&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;整个架构非常简单，我们仅仅需要做的就是构建一个 Redis 的 Proxy，这个 Proxy 会解析 Redis 协议，然后将 Redis 的数据结构映射到 key-value 上面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Redis Protocol&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Redis 协议被叫做 &lt;a href=&quot;https://redis.io/topics/protocol&quot;&gt;RESP&lt;/a&gt;(Redis Serialization Protocol)，它是文本类型的，可读性比较好，并且易于解析。它使用 “rn” 作为每行的分隔符并且用不同的前缀来代表不同的类型。例如，对于简单的 String，第一个字节是 “+”，所以一个 “OK” 行就是 “+OKrn”。&lt;br&gt;大多数时候，客户端会使用最通用的 Request-Response 模型用于跟 Redis 进行交互。客户端会首先发送一个请求，然后等待 Redis返回结果。请求是一个 Array，Array 里面元素都是 bulk strings，而返回值则可能是任意的 RESP 类型。Redis 同样支持其他通讯方式：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Pipeline - 这种模式下面客户端会持续的给 Redis 发送多个请求，然后等待 Redis 返回一个结果。&lt;/li&gt;&lt;li&gt;Push - 客户端会在 Redis 上面订阅一个 channel，然后客户端就会从这个 channel 上面持续受到 Redis push 的数据。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下面是一个简单的客户端发送 &lt;code class=&quot;inline&quot;&gt;LLEN mylist&lt;/code&gt; 命令到 Redis 的例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;C: *2\r\n
C: $4\r\n
C: LLEN\r\n
C: $6\r\n
C: mylist\r\n

S: :48293\r\n&lt;/code&gt;&lt;p&gt;客户端会发送一个带有两个 bulk string 的 array，第一个 bulk string 的长度是 4，而第二个则是 6。Redis 会返回一个 48293 整数。正如你所见，RESP 非常简单，自然而然的，写一个 RESP 的解析器也是非常容易的。&lt;/p&gt;&lt;p&gt;作者创建了一个 Go 的库 &lt;a href=&quot;http://github.com/siddontang/goredis&quot;&gt;goredis&lt;/a&gt;，基于这个库，我们能非常容易的从连接上面解析出 RESP，一个简单的例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;// Create a buffer IO from the connection.
br := bufio.NewReaderSize(conn, 4096)
// Create a RESP reader.
r := goredis.NewRespReader(br)
// Parse the Request
req := r.ParseRequest()&lt;/code&gt;&lt;p&gt;函数 &lt;code class=&quot;inline&quot;&gt;ParseRequest&lt;/code&gt; 返回一个解析好的 request，它是一个 &lt;code class=&quot;inline&quot;&gt;[][]byte&lt;/code&gt; 类型，第一个字段是函数名字，譬如 “LLEN”，然后后面的字段则是这个命令的参数。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV 事务 API&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们开始之前，作者将会给一个简单实用 TiKV 事务 API 的例子，我们调用 &lt;code class=&quot;inline&quot;&gt;Begin&lt;/code&gt; 开始一个事务：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;txn, err := db.Begin()&lt;/code&gt;&lt;p&gt;函数 &lt;code class=&quot;inline&quot;&gt;Begin&lt;/code&gt; 创建一个事务，如果出错了，我们需要判断 err，不过后面作者都会忽略 err 的处理。&lt;/p&gt;&lt;p&gt;当我们开始了一个事务之后，我们就可以干很多操作了：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;value, err := txn.Get([]byte(“key”))
// Do something with value and then update the newValue to the key.
txn.Put([]byte(“key”), newValue)&lt;/code&gt;&lt;p&gt;上面我们得到了一个 key 的值，并且将其更新为新的值。TiKV 使用乐观事务模型，它会将所有的改动都先缓存到本地，然后在一起提交给 Server。&lt;/p&gt;&lt;code lang=&quot;go&quot;&gt;// Commit the transaction
txn.Commit(context.TODO())&lt;/code&gt;&lt;p&gt;跟其他事务处理一样，我们也可以回滚这个事务：&lt;/p&gt;&lt;code lang=&quot;go&quot;&gt;txn.Rollback()&lt;/code&gt;&lt;p&gt;如果两个事务操作了相同的 key，它们就会冲突。一个事务会提交成功，而另一个事务会出错并且回滚。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;映射 Data structure 到 TiKV&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在我们知道了如何解析 Redis 协议，如何在一个事务里面做操作，下一步就是支持 Redis 的数据结构了。Redis 主要有 4 中数据结构：String，Hash，Set 和 Sorted Set，但是对于 TiKV 来说，它只支持 key-value，所以我们需要将这些数据结构映射到 key-value。&lt;/p&gt;&lt;p&gt;首先，我们需要区分不同的数据结构，一个非常容易的方式就是在 key 的后面加上 Type flag。例如，我们可以将 ’s’ 添加到 String，所以一个 String key “abc” 在 TiKV 里面其实就是 “abcs”。&lt;/p&gt;&lt;p&gt;对于其他类型，我们可能需要考虑更多，譬如对于 Hash 类型，我们需要支持如下操作：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;HSET key field1 value1
HSET key field2 value2
HLEN key&lt;/code&gt;&lt;p&gt;一个 Hash 会有很多 fields，我有时候想知道整个 Hash 的个数，所以对于 TiKV，我们不光需要将 Hash 的 key 和 field 合在一起变成 TiKV 的一个 key，也同时需要用另一个 key 来保存整个 Hash 的长度，所以整个 Hash 的布局类似：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;key + ‘h’ -&amp;gt; length
key + ‘f’ + field1 -&amp;gt; value
key + ‘f’ + field2 -&amp;gt; value &lt;/code&gt;&lt;p&gt;如果我们不保存 length，那么如果我们想知道 Hash 的 length，每次都需要去扫整个 Hash 得到所有的 fields，这个其实并不高效。但如果我们用另一个 key 来保存 length，任何时候，当我们加入一个新的 field，我们都需要去更新这个 length 的值，这也是一个开销。对于我来说，我倾向于使用另一个 key 来保存 length，因为 &lt;code class=&quot;inline&quot;&gt;HLEN&lt;/code&gt; 是一个高频的操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;例子&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;作者构建了一个非常简单的例子 &lt;a href=&quot;https://github.com/siddontang/redis-tikv-example&quot;&gt;example&lt;/a&gt; ，里面只支持 String 和 Hash 的一些操作，我们可以 clone 下来并编译：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;git clone https://github.com/siddontang/redis-tikv-example.git $GOPATH/src/github.com/siddontang/redis-tikv-example

cd $GOPATH/src/github.com/siddontang/redis-tikv-example
go build&lt;/code&gt;&lt;p&gt;在运行之前，我们需要启动 TiKV，可以参考&lt;a href=&quot;https://github.com/pingcap/tikv#deploying-to-production&quot;&gt;instruction&lt;/a&gt;，然后执行：&lt;/p&gt;&lt;code lang=&quot;bash&quot;&gt;./redis-tikv-example&lt;/code&gt;&lt;p&gt;这个例子会监听端口 6380，然后我们可以用任意的 Redis 客户端，譬如 &lt;code class=&quot;inline&quot;&gt;redis-cli&lt;/code&gt; 去连接：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;redis-cli -p 6380
127.0.0.1:6380&amp;gt; set k1 a
OK
127.0.0.1:6380&amp;gt; get k1
&quot;a&quot;
127.0.0.1:6380&amp;gt; hset k2 f1 a
(integer) 1
127.0.0.1:6380&amp;gt; hget k2 f1
&quot;a&quot;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;尾声&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在已经有一些公司基于 TiKV 来构建了他们自己的 Redis Server，并且也有一个开源的项目&lt;a href=&quot;https://github.com/yongman/tidis&quot;&gt;tidis&lt;/a&gt; 做了相同的事情。&lt;code class=&quot;inline&quot;&gt;tidis&lt;/code&gt; 已经比较完善，如果你想替换自己的 Redis，可以尝试一下。&lt;br&gt;正如同你所见，TiKV 其实算是一个基础的组件，我们可以在它的上面构建很多其他的应用。如果你对我们现在做的事情感兴趣，欢迎联系我：&lt;a href=&quot;mailto:tl@pingcap.com&quot;&gt;tl@pingcap.com&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文发表于唐刘老师博客：&lt;a href=&quot;https://www.jianshu.com/p/b4dee8372d8d&quot;&gt;使用 TiKV 构建分布式类 Redis 服务&lt;/a&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-07-43959766</guid>
<pubDate>Fri, 07 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读（十八）tikv-client（上）</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-09-06-43926052.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43926052&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-828ba4512736d7f2cb3e6664c5fbf3b3_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：周昱行&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在整个 SQL 执行过程中，需要经过 Parser，Optimizer，Executor，DistSQL 这几个主要的步骤，最终数据的读写是通过 tikv-client 与 TiKV 集群通讯来完成的。&lt;br&gt;为了完成数据读写的任务，tikv-client 需要解决以下几个具体问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如何定位到某一个 key 或 key range 所在的 TiKV 地址？&lt;/li&gt;&lt;li&gt;如何建立和维护和 tikv-server 之间的连接？&lt;/li&gt;&lt;li&gt;如何发送 RPC 请求？&lt;/li&gt;&lt;li&gt;如何处理各种错误？&lt;/li&gt;&lt;li&gt;如何实现分布式读取多个 TiKV 节点的数据？&lt;/li&gt;&lt;li&gt;如何实现 2PC 事务？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们接下来就对以上几个问题逐一解答，其中 5、6 会在下篇中介绍。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何定位 key 所在的 tikv-server&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们需要回顾一下之前 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-internal-1/&quot;&gt;《三篇文章了解 TiDB 技术内幕——说存储》&lt;/a&gt; 。这篇文章中介绍过的一个重要的概念：Region。&lt;/p&gt;&lt;p&gt;TiDB 的数据分布是以 Region 为单位的，一个 Region 包含了一个范围内的数据，通常是 96MB 的大小，Region 的 meta 信息包含了 StartKey 和 EndKey 这两个属性。当某个 key &amp;gt;= StartKey &amp;amp;&amp;amp; key &amp;lt; EndKey 的时候，我们就知道了这个 key 所在的 Region，然后我们就可以通过查找该 Region 所在的 TiKV 地址，去这个地址读取这个 key 的数据。&lt;/p&gt;&lt;p&gt;获取 key 所在的 Region, 是通过向 PD 发送请求完成的。PD client 实现了这样一个接口：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/pd/pd-client/client.go#L49&quot;&gt;GetRegion(ctx context.Context, key []byte) (*metapb.Region, *metapb.Peer, error)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;通过调用这个接口，我们就可以定位这个 key 所在的 Region 了。&lt;/p&gt;&lt;p&gt;如果需要获取一个范围内的多个 Region，我们会从这个范围的 StartKey 开始，多次调用 &lt;code class=&quot;inline&quot;&gt;GetRegion&lt;/code&gt; 这个接口，每次返回的 Region 的 EndKey 做为下次请求的 StartKey，直到返回的 Region 的 EndKey 大于请求范围的 EndKey。&lt;/p&gt;&lt;p&gt;以上执行过程有一个很明显的问题，就是我们每次读取数据的时候，都需要先去访问 PD，这样会给 PD 带来巨大压力，同时影响请求的性能。&lt;/p&gt;&lt;p&gt;为了解决这个问题，tikv-client 实现了一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L50&quot;&gt;RegionCache&lt;/a&gt; 的组件，缓存 Region 信息， 当需要定位 key 所在的 Region 的时候，如果 RegionCache 命中，就不需要访问 PD 了。RegionCache 的内部，有两种数据结构保存 Region 信息，一个是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L55&quot;&gt;map&lt;/a&gt;，另一个是 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L56&quot;&gt;b-tree&lt;/a&gt;，用 map 可以快速根据 region ID 查找到 Region，用 b-tree 可以根据一个 key 找到包含该 key 的 Region。&lt;/p&gt;&lt;p&gt;严格来说，PD 上保存的 Region 信息，也是一层 cache，真正最新的 Region 信息是存储在 tikv-server 上的，每个 tikv-server 会自己决定什么时候进行 Region 分裂，在 Region 变化的时候，把信息上报给 PD，PD 用上报上来的 Region 信息，满足 tidb-server 的查询需求。&lt;br&gt;当我们从 cache 获取了 Region 信息，并发送请求以后， tikv-server 会对 Region 信息进行校验，确保请求的 Region 信息是正确的。&lt;/p&gt;&lt;p&gt;如果因为 Region 分裂，Region 迁移导致了 Region 信息变化，请求的 Region 信息就会过期，这时 tikv-server 就会返回 Region 错误。遇到了 Region 错误，我们就需要&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L318&quot;&gt;清理 RegionCache&lt;/a&gt;，重新&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L329&quot;&gt;获取最新的 Region 信息&lt;/a&gt;，并重新发送请求。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何建立和维护和 tikv-server 之间的连接&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当 TiDB 定位到 key 所在的 tikv-server 以后，就需要建立和 TiKV 之间的连接，我们都知道， TCP 连接的建立和关闭有不小的开销，同时会增大延迟，使用连接池可以节省这部分开销，TiDB 和 tikv-server 之间也维护了一个连接池 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L83&quot;&gt;connArray&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;TiDB 和 TiKV 之间通过 gRPC 通信，而 gPRC 支持在单 TCP 连接上多路复用，所以多个并发的请求可以在单个连接上执行而不会相互阻塞。&lt;/p&gt;&lt;p&gt;理论上一个 tidb-server 和一个 tikv-server 之间只需要维护一个连接，但是在性能测试的时候发现，单个连接在并发-高的时候，会成为性能瓶颈，所以实际实现的时候，tidb-server 对每一个 tikv-server 地址维护了多个连接，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L159&quot;&gt;并以 round-robin 算法选择连接&lt;/a&gt;发送请求。连接的个数可以在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/config/config.toml.example#L215&quot;&gt;config&lt;/a&gt; 文件里配置，默认是 16。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何发送 RPC 请求&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;tikv-client 通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/kv.go#L127&quot;&gt;tikvStore&lt;/a&gt; 这个类型，实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L247&quot;&gt;kv.Storage&lt;/a&gt; 这个接口，我们可以把 tikvStore 理解成 tikv-client 的一个包装。外部调用 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 的接口，并不需要关心 RPC 的细节，RPC 请求都是 tikvStore 为了实现 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 接口而发起的。&lt;/p&gt;&lt;p&gt;实现不同的 &lt;code class=&quot;inline&quot;&gt;kv.Storage&lt;/code&gt; 接口需要发送不同的 RPC 请求。比如实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L233&quot;&gt;Snapshot.BatchGet&lt;/a&gt; 需要&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L61&quot;&gt;tikvpb.TikvClient.KvBatchGet&lt;/a&gt;方法；实现 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/kv/kv.go#L128&quot;&gt;Transaction.Commit&lt;/a&gt;，需要 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L57&quot;&gt;tikvpb.TikvClient.KvPrewrite&lt;/a&gt;, &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L58&quot;&gt;tikvpb.TikvClient.KvCommit&lt;/a&gt; 等多个方法。&lt;/p&gt;&lt;p&gt;在 tikvStore 的实现里，并没有直接调用 RPC 方法，而是通过一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L76&quot;&gt;Client&lt;/a&gt; 接口调用，做这一层的抽象的主要目的是为了让下层可以有不同的实现。比如用来测试的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/mockstore/mocktikv/rpc.go#L493&quot;&gt;mocktikv 就自己实现了 Client 接口&lt;/a&gt;，通过本地调用实现，并不需要调用真正的 RPC。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L180&quot;&gt;rpcClient&lt;/a&gt; 是真正实现 RPC 请求的 Client 实现，通过调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/tikvrpc/tikvrpc.go#L419&quot;&gt;tikvrpc.CallRPC&lt;/a&gt;，发送 RPC 请求。&lt;code class=&quot;inline&quot;&gt;tikvrpc.CallRPC&lt;/code&gt; 再往下层走，就是调用具体&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/tikvpb/tikvpb.pb.go#L152&quot;&gt;每个 RPC  生成的代码&lt;/a&gt;了，到了生成的代码这一层，就已经是 gRPC 框架这一层的内容了，我们就不继续深入解析了，感兴趣的同学可以研究一下 gRPC 的实现。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何处理各种错误&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们前面提到 RPC 请求都是通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/client.go#L76&quot;&gt;Client&lt;/a&gt; 接口发送的，但实际上这个接口并没有直接被各个 tikvStore 的各个方法调用，而是通过一个 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L46&quot;&gt;RegionRequestSender&lt;/a&gt; 的对象调用的。&lt;br&gt;&lt;code class=&quot;inline&quot;&gt;RegionRequestSender&lt;/code&gt; 主要的工作除了发送 RPC 请求，还要负责处理各种可以重试的错误，比如网络错误和部分 Region 错误。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RPC 请求遇到的错误主要分为两大类：Region 错误和网络错误。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/tikvrpc/tikvrpc.go#L359&quot;&gt;Region  错误&lt;/a&gt; 是由 tikv-server 收到请求后，在 response 里返回的，常见的有以下几种:&lt;/p&gt;&lt;p&gt;1.&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L207&quot;&gt;NotLeader&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这种错误的原因通常是 Region 的调度，PD 为了负载均衡，可能会把一个热点 Region 的 leader 调度到空闲的 tikv-server 上，而请求只能由 leader 来处理。遇到这种错误就需要 tikv-client 重试，把请求发给新的 leader。&lt;/p&gt;&lt;p&gt;2. &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L210&quot;&gt;StaleEpoch&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这种错误主要是因为 Region 的分裂，当 Region 内的数据量增多以后，会分裂成多个新的 Region。新的 Region 包含的 range 是不同的，如果直接执行，返回的结果有可能是错误的，所以 TiKV 就会拒绝这个请求。tikv-client 需要从 PD 获取最新的 Region 信息并重试。&lt;/p&gt;&lt;p&gt;3. &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/vendor/github.com/pingcap/kvproto/pkg/errorpb/errorpb.pb.go#L211&quot;&gt;ServerIsBusy&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个错误通常是因为 tikv-server 积压了过多的请求处理不完，tikv-server 如果不拒绝这个请求，队列会越来越长，可能等到客户端超时了，请求还没有来的及处理。所以做为一种保护机制，tikv-server 提前返回错误，让客户端等待一段时间后再重试。&lt;/p&gt;&lt;p&gt;另一类错误是网络错误，错误是由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L129&quot;&gt;SendRequest 的返回值&lt;/a&gt; 返回的 error 的，遇到这种错误通常意味着这个 tikv-server 没有正常返回请求，可能是网络隔离或 tikv-server down 了。tikv-client 遇到这种错误，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_request.go#L140&quot;&gt;OnSendFail&lt;/a&gt; 方法，处理这个错误，会在 RegionCache 里把这个请求失败的 tikv-server 上的&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/region_cache.go#L453&quot;&gt;所有 region 都 drop 掉&lt;/a&gt;，避免其他请求遇到同样的错误。&lt;br&gt;当遇到可以重试的错误的时候，我们需要等待一段时间后重试，我们需要保证每次重试等待时间不能太短也不能太长，太短会造成多次无谓的请求，增加系统压力和开销，太长会增加请求的延迟。我们用指数退避的算法来计算每一次重试前的等待时间，这部分的逻辑是在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L176&quot;&gt;Backoffer&lt;/a&gt; 里实现的。&lt;/p&gt;&lt;p&gt;在上层执行一个 SQL 语句的时候，在 tikv-client 这一层会触发多个顺序的或并发的请求，发向多个 tikv-server，为了保证上层 SQL 语句的超时时间，我们需要考虑的不仅仅是单个 RPC 请求，还需要考虑一个 query 整体的超时时间。&lt;/p&gt;&lt;p&gt;为了解决这个问题，&lt;code class=&quot;inline&quot;&gt;Backoffer&lt;/code&gt; 实现了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L267&quot;&gt;fork&lt;/a&gt; 功能， 在发送每一个子请求的时候，需要 fork 出一个 &lt;code class=&quot;inline&quot;&gt;child Backoffer&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;child Backoffer&lt;/code&gt; 负责单个 RPC 请求的重试，它记录了 &lt;code class=&quot;inline&quot;&gt;parent Backoffer&lt;/code&gt; 已经等待的时间，保证总的等待时间，不会超过 query 超时时间。&lt;/p&gt;&lt;p&gt;对于不同错误，需要等待的时间是不一样的，每个 &lt;code class=&quot;inline&quot;&gt;Backoffer&lt;/code&gt; 在创建时，会&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/backoff.go#L96&quot;&gt;根据不同类型，创建不同的 backoff 函数&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上就是 tikv-client 上篇的内容，我们在下篇会详细介绍实现分布式计算相关的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/coprocessor.go#L354&quot;&gt;copIterator&lt;/a&gt;&lt;/b&gt; &lt;b&gt;和实现分布式事务的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/v2.1.0-rc.1/store/tikv/2pc.go#L66&quot;&gt;twoPCCommiter&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-09-06-43926052</guid>
<pubDate>Thu, 06 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 加入 CNCF 沙箱托管项目</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-29-43278790.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43278790&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0b95a167ac111d328b7091dd4d9ce487_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;TiKV 加入 CNCF 沙箱托管项目&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://cncf.io/&quot;&gt;云原生计算基金会 (CNCF)&lt;/a&gt;今天宣布接纳 &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt; 开源分布式事务键值数据库作为 CNCF 沙箱的早期发展云原生项目。&lt;/p&gt;&lt;p&gt;TiKV 采用 &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; 构建，由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Raft_(computer_science)&quot;&gt;Raft&lt;/a&gt;（通过 etcd）驱动，并受到 Google Spanner 设计的激励，提供简化的调度和自动平衡，而不依赖于任何分布式文件系统。该项目是一个开源、统一分布式存储层，支持功能强大的数据一致性、分布式事务、水平可扩展性和云原生架构。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27ad63df5d6339a8264a57aec1efaee2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;975&quot; data-rawheight=&quot;513&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27ad63df5d6339a8264a57aec1efaee2&quot; data-watermark-src=&quot;v2-d8fa777cb71f3f36f0d4d0334607aa85&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.pingcap.com/en/&quot;&gt;PingCAP&lt;/a&gt; 的首席工程师 和 TiKV 项目负责人 Siddon Tang 表示：“随着我们产生和收集的数据量继续以惊人的速度增长，各组织需要一种方法确保云原生环境的水平可扩展性和高度可用性。”“通过加入 CNCF，我们期待着建立项目治理，并在这一开发商中立之家培育愈发壮大的贡献者基地，让我们能够构建更多组件，例如，支持更多语言和新的有用功能。”&lt;/p&gt;&lt;p&gt;TiKV 最初于 2016 年在 PingCAP 开发，现在得到三星、摩拜单车、今日头条、饿了么、腾讯云和 UCloud 的支持。用户包括北京银行、饿了么、Hulu、联想、摩拜单车和&lt;a href=&quot;https://github.com/tikv/tikv/blob/master/docs/adopters.md&quot;&gt;诸多其他企业&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;该项目的 TOC 赞助商是 Bryan Cantrill 和 Ben Hindman。&lt;/p&gt;&lt;p&gt;CNCF 沙箱是早期阶段项目的孵化器，如需进一步了解 CNCF 项目成熟度，请访问&lt;a href=&quot;https://github.com/cncf/toc/blob/master/process/graduation_criteria.adoc&quot;&gt;毕业标准&lt;/a&gt;纲要。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;h2&gt;&lt;b&gt;CNCF to Host TiKV in the Sandbox&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, the &lt;a href=&quot;http://cncf.io/&quot;&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; accepted &lt;a href=&quot;https://github.com/tikv/tikv&quot;&gt;TiKV&lt;/a&gt;, an open source distributed transactional key-value database, into the CNCF Sandbox for early stage and evolving cloud native projects.&lt;/p&gt;&lt;p&gt;Built in &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt;, powered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Raft_(computer_science)&quot;&gt;Raft&lt;/a&gt; (via etcd) and inspired by the design of Google Spanner, TiKV offers simplified scheduling and auto-balancing without dependency on any distributed file system. The project serves as an open source, unifying distributed storage layer that supports strong data consistency, distributed transactions, horizontal scalability, and cloud native architecture.&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27ad63df5d6339a8264a57aec1efaee2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;975&quot; data-rawheight=&quot;513&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-27ad63df5d6339a8264a57aec1efaee2&quot; data-watermark-src=&quot;v2-d8fa777cb71f3f36f0d4d0334607aa85&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;“As the amount of data we are producing and collecting continues to grow at an astounding pace, organizations need a way to ensure horizontal scalability and high availability for cloud native applications,” said Siddon Tang, Chief Engineer at &lt;a href=&quot;https://www.pingcap.com/en/&quot;&gt;PingCAP&lt;/a&gt; and TiKV project lead. “By joining CNCF, we look forward to establishing project governance and growing a broader contributor base in this vendor neutral home – allowing us to build additional components like support for more languages and new useful features.”&lt;/p&gt;&lt;p&gt;TiKV was originally developed at PingCAP in 2016, and today includes contributions from Samsung, Mobike, Toutiao.com, Ele.me, Tencent Cloud and UCloud. Users include Bank of Beijing, Ele.me, Hulu, Lenovo, Mobike and &lt;a href=&quot;https://github.com/tikv/tikv/blob/master/docs/adopters.md&quot;&gt;many others&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The TOC sponsors of the project are Bryan Cantrill and Ben Hindman.&lt;/p&gt;&lt;p&gt;The CNCF Sandbox is a home for early stage projects, for further clarification around project maturity levels in CNCF, please visit our outlined &lt;a href=&quot;https://github.com/cncf/toc/blob/master/process/graduation_criteria.adoc&quot;&gt;Graduation Criteria&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.cncf.io/blog/2018/08/28/cncf-to-host-tikv-in-the-sandbox/&quot;&gt;CNCF to Host TiKV in the Sandbox - Cloud Native Computing Foundation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-29-43278790</guid>
<pubDate>Wed, 29 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十七）DDL 源码解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-27-43088324.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/43088324&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eef68c3b98ddf29ee7a0311e6259636c_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者：陈霜&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;DDL 是数据库非常核心的组件，其正确性和稳定性是整个 SQL 引擎的基石，在分布式数据库中，如何在保证数据一致性的前提下实现无锁的 DDL 操作是一件有挑战的事情。&lt;/p&gt;&lt;p&gt;本文首先会介绍 TiDB DDL 组件的总体设计，以及如何在分布式场景下支持无锁 shema 变更，并描述这套算法的大致流程，然后详细介绍一些常见的 DDL 语句的源码实现，包括 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;drop column&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;drop table&lt;/code&gt; 这四种。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DDL in TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的 DDL 通过实现 Google F1 的在线异步 schema 变更算法，来完成在分布式场景下的无锁，在线 schema 变更。为了简化设计，TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 &lt;b&gt;owner&lt;/b&gt; 节点的 &lt;b&gt;worker &lt;/b&gt;串行执行的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;worker：每个节点都有一个 worker 用来处理 DDL 操作。&lt;/li&gt;&lt;li&gt;owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里只是简单概述了 TiDB 的 DDL 设计，下两篇文章详细介绍了 TiDB DDL 的设计实现以及优化，推荐阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md&quot;&gt;TiDB 的异步 schema 变更实现  &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://zimulala.github.io/2017/12/24/optimize/&quot;&gt;TiDB 的异步 schema 变更优化&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下图描述了一个 DDL 请求在 TiDB 中的简单处理流程：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b50b7dac9057fcc661c11624a87f5eb7_r.jpg&quot; data-caption=&quot;图 1：TiDB 中 DDL SQL 的处理流程&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1088&quot; data-rawheight=&quot;730&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b50b7dac9057fcc661c11624a87f5eb7&quot; data-watermark-src=&quot;v2-ec295f4a6baf2e4d815cefcca89fcacb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;TiDB 的 DDL 组件相关代码存放在源码目录的 &lt;code class=&quot;inline&quot;&gt;ddl&lt;/code&gt; 目录下。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-830cd4ddaaaf3919c05a60248d81cc42_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;635&quot; data-rawheight=&quot;276&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-830cd4ddaaaf3919c05a60248d81cc42&quot; data-watermark-src=&quot;v2-de4c148aff1c3ed3f886bcce72f5ebc5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;ddl owner&lt;/code&gt; 相关的代码单独放在 &lt;code class=&quot;inline&quot;&gt;owner&lt;/code&gt; 目录下，实现了 owner 选举等功能。&lt;/p&gt;&lt;p&gt;另外，&lt;code class=&quot;inline&quot;&gt;ddl job queue&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;history ddl job queue&lt;/code&gt; 这两个队列都是持久化到 TiKV 中的。&lt;code class=&quot;inline&quot;&gt;structure&lt;/code&gt; 目录下有 list，&lt;code class=&quot;inline&quot;&gt;hash&lt;/code&gt; 等数据结构在 TiKV 上的实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文接下来按照 TiDB 源码的&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;origin/source-code&lt;/a&gt;&lt;/b&gt; &lt;b&gt;分支讲解，最新的 master 分支和 source-code 分支代码会稍有一些差异。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Create table&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 需要把 table 的元信息（&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/model/model.go#L95&quot;&gt;TableInfo&lt;/a&gt;）从 SQL 中解析出来，做一些检查，然后把 table 的元信息持久化保存到 TiKV 中。具体流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;语法解析：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/session.go#L790&quot;&gt;ParseSQL&lt;/a&gt; 解析成抽象语法树 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ast/ddl.go#L393&quot;&gt;CreateTableStmt&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;编译生成 Plan：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/session.go#L805&quot;&gt;Compile&lt;/a&gt; 生成 DDL plan , 并 check 权限等。&lt;/li&gt;&lt;li&gt;生成执行器：&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L227&quot;&gt;buildExecutor&lt;/a&gt; 生成 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L33&quot;&gt; DDLExec&lt;/a&gt; 执行器。TiDB 的执行器是火山模型。&lt;/li&gt;&lt;li&gt;执行器调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L300&quot;&gt;e.Next&lt;/a&gt; 开始执行，即 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42&quot;&gt;DDLExec.Next&lt;/a&gt; 方法，判断 DDL 类型后执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L68&quot;&gt;executeCreateTable&lt;/a&gt; , 其实质是调用 &lt;code class=&quot;inline&quot;&gt;ddl_api.go&lt;/code&gt; 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739&quot;&gt;CreateTable&lt;/a&gt; 函数。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739&quot;&gt;CreateTable&lt;/a&gt; 方法是主要流程如下：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;会先 check 一些限制，比如 table name 是否已经存在，table 名是否太长，是否有重复定义的列等等限制。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L775&quot;&gt;buildTableInfo&lt;/a&gt; 获取 global table ID，生成 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; , 即 table 的元信息，然后封装成一个 DDL job，这个 job 包含了 &lt;code class=&quot;inline&quot;&gt;table ID&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt;，并将这个 job 的 type 标记为 &lt;code class=&quot;inline&quot;&gt;ActionCreateTable&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L793&quot;&gt;d.doDDLJob(ctx, job)&lt;/a&gt; 函数中的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L423&quot;&gt;d.addDDLJob(ctx, job)&lt;/a&gt; 会先给 job 获取一个 global job ID 然后放到 job queue 中去。&lt;/li&gt;&lt;li&gt;DDL 组件启动后，在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L318&quot;&gt;start&lt;/a&gt; 函数中会启动一个 &lt;code class=&quot;inline&quot;&gt;ddl_worker&lt;/code&gt; 协程运行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L37&quot;&gt;onDDLWorker&lt;/a&gt; 函数（最新 Master 分支函数名已重命名为 start），每隔一段时间调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L193&quot;&gt;handleDDLJobQueu&lt;/a&gt; 函数去尝试处理 DDL job 队列里的 job，&lt;code class=&quot;inline&quot;&gt;ddl_worker&lt;/code&gt; 会先 check 自己是不是 owner，如果不是 owner，就什么也不做，然后返回；如果是 owner，就调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L212&quot;&gt;getFirstDDLJob&lt;/a&gt; 函数获取 DDL 队列中的第一个 job，然后调 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L236&quot;&gt;runDDLJob&lt;/a&gt; 函数执行 job。&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L275&quot;&gt;runDDLJob&lt;/a&gt; 函数里面会根据 job 的类型，然后调用对应的执行函数，对于 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类型的 job，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L31&quot;&gt;onCreateTable&lt;/a&gt; 函数，然后做一些 check 后，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L56&quot;&gt;t.CreateTable&lt;/a&gt; 函数，将 &lt;code class=&quot;inline&quot;&gt;db_ID&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;table_ID&lt;/code&gt; 映射为 &lt;code class=&quot;inline&quot;&gt;key&lt;/code&gt;，&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 作为 value 存到 TiKV 里面去，并更新 job 的状态。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt; 函数将 job 从 DDL job 队列中移除，然后加入 history ddl job 队列中去。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L451&quot;&gt;doDDLJob&lt;/a&gt; 函数中检测到 history DDL job 队列中有对应的 job 后，返回。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Add index&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 主要做 2 件事：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;修改 table 的元信息，把 &lt;code class=&quot;inline&quot;&gt;indexInfo&lt;/code&gt; 加入到 table 的元信息中去。&lt;/li&gt;&lt;li&gt;把 table 中已有了的数据行，把 &lt;code class=&quot;inline&quot;&gt;index columns&lt;/code&gt; 的值全部回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中去。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;具体执行流程的前部分的 SQL 解析、Compile 等流程，和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 一样，可以直接从 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42&quot;&gt;DDLExec.Next&lt;/a&gt; 开始看，然后调用 &lt;code class=&quot;inline&quot;&gt;alter&lt;/code&gt; 语句的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L78&quot;&gt;e.executeAlterTable(x)&lt;/a&gt; 函数，其实质调 ddl 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L862&quot;&gt;AlterTable&lt;/a&gt; 函数，然后调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1536&quot;&gt;CreateIndex&lt;/a&gt; 函数，开始执行 add index 的主要工作，具体流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Check 一些限制，比如 table 是否存在，索引是否已经存在，索引名是否太长等。&lt;/li&gt;&lt;li&gt;封装成一个 job，包含了索引名，索引列等，并将 job 的 type 标记为 &lt;code class=&quot;inline&quot;&gt;ActionAddIndex&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;给 job 获取一个 global job ID 然后放到 DDL job 队列中去。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job，根据 job 的类型调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L177&quot;&gt;onCreateIndex&lt;/a&gt; 函数。&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;buildIndexInfo&lt;/code&gt; 生成 &lt;code class=&quot;inline&quot;&gt;indexInfo&lt;/code&gt;，然后更新 &lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 中的 &lt;code class=&quot;inline&quot;&gt;Indices&lt;/code&gt;，持久化到 TiKV 中去。&lt;/li&gt;&lt;li&gt;这里引入了 online schema change 的几个步骤，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L237&quot;&gt;需要留意 indexInfo 的状态变化&lt;/a&gt;：&lt;code class=&quot;inline&quot;&gt;none -&amp;gt; delete only -&amp;gt; write only -&amp;gt; reorganization -&amp;gt;  public&lt;/code&gt;。在 &lt;code class=&quot;inline&quot;&gt;reorganization -&amp;gt; public&lt;/code&gt; 时，首先调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L147&quot;&gt;getReorgInfo&lt;/a&gt; 获取 &lt;code class=&quot;inline&quot;&gt;reorgInfo&lt;/code&gt;，主要包含需要 &lt;code class=&quot;inline&quot;&gt;reorganization&lt;/code&gt; 的 range，即从表的第一行一直到最后一行数据都需要回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中。然后调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L72&quot;&gt;runReorgJob&lt;/a&gt; , &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L554&quot;&gt;addTableIndex&lt;/a&gt;函数开始填充数据到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;中去。&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L112&quot;&gt;runReorgJob&lt;/a&gt; 函数会定期保存回填数据的进度到 TiKV。&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L566&quot;&gt;addTableIndex&lt;/a&gt; 的流程如下：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;启动多个 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 用于并发回填数据到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;把 &lt;code class=&quot;inline&quot;&gt;reorgInfo&lt;/code&gt; 中需要 &lt;code class=&quot;inline&quot;&gt;reorganization&lt;/code&gt; 分裂成多个 range。扫描的默认范围是 &lt;code class=&quot;inline&quot;&gt;[startHandle , endHandle]&lt;/code&gt;，然后默认以 128 为间隔分裂成多个 range，之后并行扫描对应数据行。在 master 分支中，range 范围信息是从 PD 中获取。&lt;/li&gt;&lt;li&gt;把 range 包装成多个 task，发给 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 并行回填 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;等待所有 &lt;code class=&quot;inline&quot;&gt;worker&lt;/code&gt; 完成后，更新 &lt;code class=&quot;inline&quot;&gt;reorg&lt;/code&gt; 进度，然后持续第 3 步直到所有的 task 都做完。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;5. 后续执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt;，检测 history ddl job 流程和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类似。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Drop Column&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;drop Column&lt;/code&gt; 只要修改 table 的元信息，把 table 元信息中对应的要删除的 column 删除。&lt;code class=&quot;inline&quot;&gt;drop Column&lt;/code&gt; 不会删除原有 table 数据行中的对应的 Column 数据，在 decode 一行数据时，会根据 table 的元信息来 decode。&lt;/p&gt;&lt;p&gt;具体执行流程的前部分都类似，直接跳到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1093&quot;&gt;DropColumn&lt;/a&gt; 函数开始，具体执行流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Check table 是否存在，要 drop 的 column 是否存在等。&lt;/li&gt;&lt;li&gt;封装成一个 job, 将 job 类型标记为 &lt;code class=&quot;inline&quot;&gt;ActionDropColumn&lt;/code&gt;，然后放到 DDL job 队列中去&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job，根据 job 的类型调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/column.go#L174&quot;&gt;onDropColumn&lt;/a&gt; 函数：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;这里 &lt;code class=&quot;inline&quot;&gt;column info&lt;/code&gt; 的状态变化和 &lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 时的变化几乎相反：&lt;code class=&quot;inline&quot;&gt;public -&amp;gt; write only -&amp;gt; delete only -&amp;gt; reorganization -&amp;gt; absent&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L362&quot;&gt;updateVersionAndTableInfo&lt;/a&gt; 更新 table 元信息中的 Columns。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4. 后续执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt;，检测 history ddl job 流程和 &lt;code class=&quot;inline&quot;&gt;create table&lt;/code&gt; 类似。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Drop table&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;drop table&lt;/code&gt; 需要删除 table 的元信息和 table 中的数据。&lt;/p&gt;&lt;p&gt;具体执行流程的前部分都类似，&lt;code class=&quot;inline&quot;&gt;owner ddl worker&lt;/code&gt; 从 DDL job 队列中取出 job 后执行 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L76&quot;&gt;onDropTable&lt;/a&gt; 函数：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 的状态变化是：&lt;code class=&quot;inline&quot;&gt;public -&amp;gt; write only -&amp;gt; delete only -&amp;gt; none&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;tableInfo&lt;/code&gt; 的状态变为 &lt;code class=&quot;inline&quot;&gt;none&lt;/code&gt; 之后，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/meta/meta.go#L306&quot;&gt; DropTable&lt;/a&gt; 将 table 的元信息从 TiKV 上删除。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;至于删除 table 中的数据，后面在调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152&quot;&gt;finishDDLJob&lt;/a&gt; 函数将 job 从 job queue 中移除，加入 history ddl job queue 前，会调用 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L160&quot;&gt;delRangeManager.addDelRangeJob(job)&lt;/a&gt;，将要删除的 table 数据范围插入到表 &lt;code class=&quot;inline&quot;&gt;gc_delete_range&lt;/code&gt; 中，然后由 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/store/tikv/gcworker/gc_worker.go&quot;&gt;GC worker&lt;/a&gt; 根据 &lt;code class=&quot;inline&quot;&gt;gc_delete_range&lt;/code&gt; 中的信息在 GC 过程中做真正的删除数据操作。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;New Parallel DDL&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;目前 TiDB 最新的 Master 分支的 DDL 引入了并行 DDL，用来加速多个 DDL 语句的执行速度。因为串行执行 DDL 时，&lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 操作需要把 table 中已有的数据回填到 &lt;code class=&quot;inline&quot;&gt;index record&lt;/code&gt; 中，如果 table 中的数据较多，回填数据的耗时较长，就会阻塞后面 DDL 的操作。目前并行 DDL 的设计是将 &lt;code class=&quot;inline&quot;&gt;add index job&lt;/code&gt; 放到新增的 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中去，其它类型的 DDL job 还是放在原来的 job queue。相应的，也增加一个 &lt;code class=&quot;inline&quot;&gt;add index worker&lt;/code&gt; 来处理 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中的 job。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c2a566a46f5c077f474a4e17ccce2b6b_r.jpg&quot; data-caption=&quot;图 2：并行 DDL 处理流程&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1121&quot; data-rawheight=&quot;676&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c2a566a46f5c077f474a4e17ccce2b6b&quot; data-watermark-src=&quot;v2-6b88067473e2e6254b3b8cf922b0de11&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;并行 DDL 同时也引入了 job 依赖的问题。job 依赖是指同一 table 的 DDL job，job ID 小的需要先执行。因为对于同一个 table 的 DDL 操作必须是顺序执行的。比如说，&lt;code class=&quot;inline&quot;&gt;add column a&lt;/code&gt;，然后 &lt;code class=&quot;inline&quot;&gt;add index on column a&lt;/code&gt;, 如果 &lt;code class=&quot;inline&quot;&gt;add index&lt;/code&gt; 先执行，而 &lt;code class=&quot;inline&quot;&gt;add column&lt;/code&gt; 的 DDL 假设还在排队未执行，这时 &lt;code class=&quot;inline&quot;&gt;add index on column a&lt;/code&gt; 就会报错说找不到 &lt;code class=&quot;inline&quot;&gt;column a&lt;/code&gt;。所以当 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中的 job2 执行前，需要检测 job queue 是否有同一 table 的 job1 还未执行，通过对比 job 的 job ID 大小来判断。执行 job queue 中的 job 时也需要检查 &lt;code class=&quot;inline&quot;&gt;add index job queue&lt;/code&gt; 中是否有依赖的 job 还未执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;End&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 目前一共支持 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/model/ddl.go#L32&quot;&gt;十多种 DDL&lt;/a&gt;，具体以及和 MySQL 兼容性对比可以看 &lt;a href=&quot;https://github.com/pingcap/docs-cn/blob/master/sql/ddl.md&quot;&gt;这里&lt;/a&gt;。剩余其它类型的 DDL 源码实现读者可以自行阅读，流程和上述几种 DDL 类似。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-27-43088324</guid>
<pubDate>Mon, 27 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 2.1 RC1 Release Notes</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-24-42900414.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42900414&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-547ba61d15cf1a27d999b7155098de96_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 8 月 24 日，TiDB 发布 2.1 RC1 版。相比 2.1 Beta 版本，该版本对系统稳定性、优化器、统计信息以及执行引擎做了很多改进。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;SQL 优化器&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;修复某些情况下关联子查询去关联后结果不正确的问题 &lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;Explain&lt;/code&gt; 输出结果 &lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 驱动表选择策略&lt;/li&gt;&lt;li&gt;去掉非 &lt;code class=&quot;inline&quot;&gt;PREPARE&lt;/code&gt; 语句的 Plan Cache&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;INSERT&lt;/code&gt; 语句无法正常解析执行的问题&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复某些情况下使用唯一索引不能查询到 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值的问题 &lt;/li&gt;&lt;li&gt;修复 UTF-8 编码情况下前缀索引的范围计算不正确的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;Project&lt;/code&gt; 算子消除导致的结果不正确的问题 &lt;/li&gt;&lt;li&gt;修复主键为整数类型时无法使用 &lt;code class=&quot;inline&quot;&gt;USE INDEX(PRIMARY)&lt;/code&gt; 的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下使用关联列无法计算索引范围的问题&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. SQL 执行引擎&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复某些情况下夏令时时间计算结果不正确的问题&lt;/li&gt;&lt;li&gt;重构聚合函数框架，提升 &lt;code class=&quot;inline&quot;&gt;Stream&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;Hash&lt;/code&gt; 聚合算子的执行效率&lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;Hash&lt;/code&gt; 聚合算子不能正常退出的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;BIT_AND&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;BIT_OR&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;BIT_XOR&lt;/code&gt; 没有正确处理非整型数据的问题&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;REPLACE INTO&lt;/code&gt; 语句的执行速度，性能提升近 10 倍&lt;/li&gt;&lt;li&gt;优化时间类型的内存占用，时间类型数据的内存使用降低为原来的一半&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;UNION&lt;/code&gt; 语句整合有符号和无符号型整数结果时与 MySQL 不兼容的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;LPAD&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;RPAD&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;TO_BASE64&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;FROM_BASE64&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;REPEAT&lt;/code&gt; 因为申请过多内存导致 TiDB panic 的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;MergeJoin&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;IndexJoin&lt;/code&gt; 在处理 &lt;code class=&quot;inline&quot;&gt;NULL&lt;/code&gt; 值时结果不正确的问题&lt;/li&gt;&lt;li&gt;修复某些情况下 Outer Join 结果不正确的问题&lt;/li&gt;&lt;li&gt;增强 &lt;code class=&quot;inline&quot;&gt;Data Truncated&lt;/code&gt; 的报错信息，便于定位出错的数据和表中对应的字段&lt;/li&gt;&lt;li&gt;修复某些情况下 Decimal 计算结果不正确的问题&lt;/li&gt;&lt;li&gt;优化点查的查询性能&lt;/li&gt;&lt;li&gt;禁用 &lt;code class=&quot;inline&quot;&gt;Read Commited&lt;/code&gt; 隔离级别，避免潜在的问题 &lt;/li&gt;&lt;li&gt;修复某些情况下 &lt;code class=&quot;inline&quot;&gt;LTRIM&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;RTRIM&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;TRIM&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;MaxOneRow&lt;/code&gt; 算子无法保证返回结果不超过 1 行的问题&lt;/li&gt;&lt;li&gt;拆分 range 个数过多的 Coprocessor 请求&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 统计信息&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;优化统计信息动态收集机制&lt;/li&gt;&lt;li&gt;解决数据频繁更新场景下 &lt;code class=&quot;inline&quot;&gt;Auto Analyze&lt;/code&gt; 不工作的问题&lt;/li&gt;&lt;li&gt;减少统计信息动态更新过程中的写入冲突 &lt;/li&gt;&lt;li&gt;优化统计信息不准确情况下的代价估算&lt;/li&gt;&lt;li&gt;优化 &lt;code class=&quot;inline&quot;&gt;AccessPath&lt;/code&gt; 的代价估算策略 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. Server&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复加载权限信息时的 bug&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;Kill&lt;/code&gt; 命令对权限的检查过严问题&lt;/li&gt;&lt;li&gt;解决 Binary 协议中某些数值类型移除的问题&lt;/li&gt;&lt;li&gt;精简日志输出 &lt;/li&gt;&lt;li&gt;处理 &lt;code class=&quot;inline&quot;&gt;mismatchClusterID&lt;/code&gt; 问题&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;advertise-address&lt;/code&gt; 配置项&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;GrpcKeepAlive&lt;/code&gt; 选项&lt;/li&gt;&lt;li&gt;增加连接或者 &lt;code class=&quot;inline&quot;&gt;Token&lt;/code&gt; 时间监控&lt;/li&gt;&lt;li&gt;优化数据解码性能 &lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INFORMMATION_SCHEMA&lt;/code&gt; 中增加 &lt;code class=&quot;inline&quot;&gt;PROCESSLIST&lt;/code&gt; 表&lt;/li&gt;&lt;li&gt;解决权限验证时多条规则可以命中情况下的顺序问题&lt;/li&gt;&lt;li&gt;将部分编码相关的系统变量默认值改为 UTF-8 &lt;/li&gt;&lt;li&gt;慢查询日志显示更详细的信息&lt;/li&gt;&lt;li&gt;支持在 PD 注册 tidb-server 的相关信息并通过 HTTP API 获取 &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;5. 兼容性&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;Session&lt;/code&gt; 变量 &lt;code class=&quot;inline&quot;&gt;warning_count&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;error_count&lt;/code&gt; &lt;/li&gt;&lt;li&gt;读取系统变量时增加 Scope 检查&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;MAX_EXECUTION_TIME&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持更多的 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 语法 &lt;/li&gt;&lt;li&gt;Set 系统变量值过程中增加合法性校验&lt;/li&gt;&lt;li&gt;增加 &lt;code class=&quot;inline&quot;&gt;Prepare&lt;/code&gt; 语句中 &lt;code class=&quot;inline&quot;&gt;PlaceHolder&lt;/code&gt; 数量的校验&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;set character_set_results = null&lt;/code&gt; &lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;flush status&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;ENUM&lt;/code&gt;  类型在 &lt;code class=&quot;inline&quot;&gt;information_schema&lt;/code&gt; 里的 column size&lt;/li&gt;&lt;li&gt;支持建表语句里的 &lt;code class=&quot;inline&quot;&gt;NATIONAL CHARACTER&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;CHARACTER SET&lt;/code&gt; 语法 &lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;SET&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;ENUM&lt;/code&gt; 类型的 column info&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;CREATE USER&lt;/code&gt; 语句的 &lt;code class=&quot;inline&quot;&gt;IDENTIFIED WITH&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;TIMESTAMP&lt;/code&gt; 类型计算过程中丢失精度的问题 &lt;/li&gt;&lt;li&gt;支持更多 &lt;code class=&quot;inline&quot;&gt;SYSTEM&lt;/code&gt; 变量的合法性验证&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;CHAR_LENGTH&lt;/code&gt; 函数在计算 binary string 时结果不正确的问题&lt;/li&gt;&lt;li&gt;修复在包含 &lt;code class=&quot;inline&quot;&gt;GROUP BY&lt;/code&gt; 的语句里 &lt;code class=&quot;inline&quot;&gt;CONCAT&lt;/code&gt; 结果不正确的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;DECIMAL&lt;/code&gt; 类型 CAST 到 &lt;code class=&quot;inline&quot;&gt;STRING&lt;/code&gt; 类型时，类型长度不准确的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;6. DML&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;解决 &lt;code class=&quot;inline&quot;&gt;Load Data&lt;/code&gt; 语句的稳定性 &lt;/li&gt;&lt;li&gt;解决一些 &lt;code class=&quot;inline&quot;&gt;Batch&lt;/code&gt; 操作情况下的内存使用问题 &lt;/li&gt;&lt;li&gt;提升 &lt;code class=&quot;inline&quot;&gt;Replace Into&lt;/code&gt; 语句的性能&lt;/li&gt;&lt;li&gt;修复写入 &lt;code class=&quot;inline&quot;&gt;CURRENT_TIMESTAMP&lt;/code&gt; 时，精度不一致的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;7. DDL&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;改进 DDL 判断 &lt;code class=&quot;inline&quot;&gt;Schema&lt;/code&gt; 是否已经同步的方法, 避免某些情况下的误判&lt;/li&gt;&lt;li&gt;修复在 &lt;code class=&quot;inline&quot;&gt;ADD INDEX&lt;/code&gt; 过程中的 &lt;code class=&quot;inline&quot;&gt;SHOW CREATE TABLE&lt;/code&gt; 结果&lt;/li&gt;&lt;li&gt;非严格 &lt;code class=&quot;inline&quot;&gt;sql-mode&lt;/code&gt; 模式下, &lt;code class=&quot;inline&quot;&gt;text&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;blob&lt;/code&gt;/&lt;code class=&quot;inline&quot;&gt;json&lt;/code&gt; 的默认值可以为空 &lt;/li&gt;&lt;li&gt;修复某些特定场景下 &lt;code class=&quot;inline&quot;&gt;ADD INDEX&lt;/code&gt; 的问题&lt;/li&gt;&lt;li&gt;大幅度提升添加 &lt;code class=&quot;inline&quot;&gt;UNIQUE-KEY&lt;/code&gt; 索引操作的速度&lt;/li&gt;&lt;li&gt;修复 Prefix-index 在 UTF-8 字符集的场景下的截断问题&lt;/li&gt;&lt;li&gt;增加环境变量 &lt;code class=&quot;inline&quot;&gt;tidb_ddl_reorg_priority&lt;/code&gt; 来控制 &lt;code class=&quot;inline&quot;&gt;add-index&lt;/code&gt; 操作的优先级&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;information_schema.tables&lt;/code&gt; 中 &lt;code class=&quot;inline&quot;&gt;AUTO-INCREMENT&lt;/code&gt; 的显示问题 &lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;admin show ddl jobs &amp;lt;number&amp;gt;&lt;/code&gt; 命令, 支持输出 number 个 DDL jobs&lt;/li&gt;&lt;li&gt;支持并行 DDL 任务执行&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;8. &lt;a href=&quot;https://github.com/pingcap/tidb/projects/6&quot;&gt;Table Partition&lt;/a&gt;（实验性）&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;支持一级分区&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;Range Partition&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;引入版本控制机制，支持集群滚动兼容升级&lt;/li&gt;&lt;li&gt;开启 &lt;code class=&quot;inline&quot;&gt;Region merge&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;GetPrevRegion&lt;/code&gt; 接口&lt;/li&gt;&lt;li&gt;支持批量 &lt;code class=&quot;inline&quot;&gt;split Region&lt;/code&gt; &lt;/li&gt;&lt;li&gt;支持存储 GC safepoint&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 功能改进&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;优化系统时间回退影响 TSO 分配的问题&lt;/li&gt;&lt;li&gt;优化处理 Region heartbeat 的性能&lt;/li&gt;&lt;li&gt;优化 Region tree 性能&lt;/li&gt;&lt;li&gt;优化计算热点统计的性能问题&lt;/li&gt;&lt;li&gt;优化 API 接口错误码返回&lt;/li&gt;&lt;li&gt;新增一些控制调度策略的开关&lt;/li&gt;&lt;li&gt;禁止在 label 中使用特殊字符&lt;/li&gt;&lt;li&gt;完善调度模拟器&lt;/li&gt;&lt;li&gt;pd-ctl 支持使用统计信息进行 Region split&lt;/li&gt;&lt;li&gt;pd-ctl 支持调用 &lt;code class=&quot;inline&quot;&gt;jq&lt;/code&gt; 来格式化 JSON 输出&lt;/li&gt;&lt;li&gt;新增 etcd Raft 状态机相关 metrics&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. Bug 修复&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复 leader 切换后 namespace 未重新加载的问题&lt;/li&gt;&lt;li&gt;修复 namespace 调度超出 schedule limit 配置的问题&lt;/li&gt;&lt;li&gt;修复热点调度超出 schedule limit 的问题&lt;/li&gt;&lt;li&gt;修复 PD client 关闭时输出一些错误日志的问题&lt;/li&gt;&lt;li&gt;修复 Region 心跳延迟统计有误的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;支持 &lt;code class=&quot;inline&quot;&gt;batch split&lt;/code&gt;，防止热点 Region 写入产生超大 Region&lt;/li&gt;&lt;li&gt;支持设置根据数据行数 split Region，提升 index scan 效率&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 性能优化&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;code class=&quot;inline&quot;&gt;LocalReader&lt;/code&gt; 将 Read 操作从 raftstore 线程分离，减少 Read 延迟&lt;/li&gt;&lt;li&gt;重构 MVCC 框架，优化 memory 使用，提升 scan read 性能&lt;/li&gt;&lt;li&gt;支持基于统计估算进行 Region split，减少 I/O 开销&lt;/li&gt;&lt;li&gt;优化连续写入 Rollback 记录后影响读性能的问题&lt;/li&gt;&lt;li&gt;减少下推聚合计算的内存开销&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 功能改进&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;增加大量内建函数下推支持，更完善的 charset 支持&lt;/li&gt;&lt;li&gt;优化 GC 流程，提升 GC 速度并降低 GC 对系统的影响&lt;/li&gt;&lt;li&gt;开启 &lt;code class=&quot;inline&quot;&gt;prevote&lt;/code&gt;，加快网络异常时的恢复服务速度&lt;/li&gt;&lt;li&gt;增加 RocksDB 日志文件相关的配置项&lt;/li&gt;&lt;li&gt;调整 &lt;code class=&quot;inline&quot;&gt;scheduler latch&lt;/code&gt; 默认配置&lt;/li&gt;&lt;li&gt;使用 tikv-ctl 手动 compact 时可设定是否 compact RocksDB 最底层数据&lt;/li&gt;&lt;li&gt;增加启动时的环境变量检查&lt;/li&gt;&lt;li&gt;支持基于已有数据动态设置 &lt;code class=&quot;inline&quot;&gt;dynamic_level_bytes&lt;/code&gt; 参数&lt;/li&gt;&lt;li&gt;支持自定义日志格式&lt;/li&gt;&lt;li&gt;tikv-ctl 整合 tikv-fail 工具&lt;/li&gt;&lt;li&gt;增加 threads IO metrics&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. Bug 修复&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;修复 decimal 相关问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;gRPC max_send_message_len&lt;/code&gt; 设置有误的问题&lt;/li&gt;&lt;li&gt;修复 &lt;code class=&quot;inline&quot;&gt;region_size&lt;/code&gt; 配置不当时产生的问题&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;如今，在社区和 PingCAP 技术团队的共同努力下，TiDB 2.1 RC1 版已发布，在此感谢社区小伙伴们长久以来的参与和贡献。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;作为明星级开源的分布式关系型数据库，TiDB 灵感来自于 Google Spanner/F1，具备『分布式强一致性事务、在线弹性水平扩展、故障自恢复的高可用、跨数据中心多活』等核心特性。TiDB 于 2015 年 5 月在 GitHub 创建，同年 12 月发布 Alpha 版本，而后于 2016 年 6 月发布 Beta 版，12 月发布 RC1 版， 2017 年 3 月发布 RC2 版，6 月发布 RC3 版，8 月发布 RC4 版，10 月发版 TiDB 1.0，2018 年 3 月发版 2.0 RC1，4 月发版 2.0 GA。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-24-42900414</guid>
<pubDate>Fri, 24 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【详解】What’s New in TiDB 2.1 RC1</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-24-42900183.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42900183&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c5471d4c277c4314ece59344787b7a49_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 4 月底，我们发布了 TiDB 2.0 GA 版本，过去的几个月中，这个版本在上百家用户的生产环境中上线，覆盖了多个行业，包括大型互联网、银行、教育、电信、制造业等。与此同时，我们也开始了 2.1 版本的开发，经过 4 个月时间、1058 次代码提交，2.1 RC1 带着更全面的功能和大幅性能提升来到这里，与大家见面。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;新增特性&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Raft 新特性&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Raft 是整个 TiKV 存储引擎的基础，2.1 版本中我们引入了 PreVote、Learner、Region Merge、Region Batch Split 这样四个特性，提升这一基础组件的性能和稳定性。其中 Learner 也是由我们贡献给 Etcd 的新特性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;热点调度&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;热点是分布式系统最大的敌人之一，并且大家的业务场景复杂多变，让热点问题捉摸不定，也是最狡猾的敌人。2.1 版本中，我们一方面增强热点检测能力，尽可能详细地统计系统负载，更快的发现热点；另一方面优化热点调度策略，用尽可能小的代价，尽快地打散热点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行 DDL&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DDL 是 SQL 的基础。在 2.1 版本之前，所有的 DDL 操作的都是串行进行，比如在对一张大表进行 Add Index 操作时，所有的 Create Table、Create Database 语句都会被阻塞，我们在 2.1 版本中对此进行了优化。Add Index 操作和其他 DDL 操作的处理分离，不相关的表上面的操作不会相互阻塞。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Table Partition（实验性）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;分布式数据库可以很容易的存储海量数据，但是 Table Parition 也能找到用武之地。比如在存储日志并定期进行数据归档的场景下，可以通过 Drop Partition 来方便的清理历史数据。在高并发写入的场景下，将单表数据分成多个 Parition 也有助于将写入流量打散在集群上。我们期望这个特性能够在 2.2 或者 3.0 版本中稳定下来。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;统计信息动态更新&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 OLTP 场景中，数据的统计分布决定了查询计划的合理性，在数据变化频繁的场景下，维护统计信息的实时性和准确性非常重要。2.1 版本中我们重点优化了统计信息的实时更新，通过执行查询过程中的反馈信息，不断地纠正已有的统计信息。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;并行聚合算子&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 OLAP 场景中，聚合和 Join 是最重要的两个算子，其性能决定了语句的处理速度，Join 算子在 2.0 版本中已经是并行模式，2.1 版本中我们对聚合算子做了重点优化，一方面将单线程变成多线程模式，另一方面对聚合的框架做了重构，聚合算子的运行速度、内存使用效率都有极大地提升。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;性能优化&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的定位是一个 HTAP 数据库，OLTP 和 OLAP 都是目标场景。2.1 RC1 版本中，我们对点查、区间扫描、聚合运算这些通用场景进行了优化，也对 “replace into” 语句， Add Index 这些特定场景做了优化。这些场景都有很好的性能提升，有的甚至有数量级的提升。&lt;/p&gt;&lt;p&gt;我们将会在 2.1 GA 版本中发布相比 2.0 GA 的 Benchmark 结果，也希望大家在自己的业务场景中实测对比，然后告诉我们在实际业务上的表现。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;开源社区&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这些激动人心的特性背后，一方面是 PingCAP 开发团队的辛勤工作，另一方面是日益壮大的 TiDB 全球社区。我们欣喜地看到，从 TiDB 2.1 Beta 版发布到现在的短短两个月时间，新增 30 多位 Contributor，其中杜川成为了 TiDB Committer。在这里对社区贡献者表示由衷的感谢，也希望更多志同道合的人能加入进来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;做分布式关系型数据库这样一个通用基础软件如同在夜晚的茫茫大海中航行，充满无数的未知和挑战，社区就是照亮我们前进路线的满天星斗。&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;TiDB 2.1 Beta 版到 RC1 版期间新增 Contributor List&lt;/b&gt;&lt;br&gt;mz1999&lt;br&gt;liuzhengyang&lt;br&gt;cityonsky&lt;br&gt;mail2fish&lt;br&gt;ceohockey60&lt;br&gt;crazycs520&lt;br&gt;zbdba&lt;br&gt;laidahe&lt;br&gt;birdstorm&lt;br&gt;gregwebs&lt;br&gt;hhxcc&lt;br&gt;liukun4515&lt;br&gt;morgo&lt;br&gt;supernan1994&lt;br&gt;bb7133&lt;br&gt;maninalift&lt;br&gt;kbacha&lt;br&gt;ceohockey60&lt;br&gt;DorianZheng&lt;br&gt;GuillaumeGomez&lt;br&gt;TennyZhuang&lt;br&gt;lerencao&lt;br&gt;smallyard&lt;br&gt;sweetIan&lt;br&gt;arosspope&lt;br&gt;York Xiang&lt;br&gt;Mason Hua&lt;br&gt;ice1000&lt;br&gt;opensourcegeek&lt;br&gt;xiangyuf&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-24-42900183</guid>
<pubDate>Fri, 24 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Operator 开源技术细节详解</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-23-42752388.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42752388&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-48f8f9e31ea70d19e831ccc983565e3b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;i&gt;原标题：TiDB Operator，让 TiDB 成为真正的 Cloud-Native 数据库&lt;/i&gt;&lt;/p&gt;&lt;p&gt;TiDB Operator 是 TiDB 在 Kubernetes 平台上的自动化部署运维工具。目前，TiDB Operator 已正式开源（&lt;a href=&quot;https://github.com/pingcap/tidb-operator/&quot;&gt;pingcap/tidb-operator&lt;/a&gt;）。借助 TiDB Operator，TiDB 可以无缝运行在公有云厂商提供的 Kubernetes 平台上，让 TiDB 成为真正的 Cloud-Native 数据库。&lt;/p&gt;&lt;p&gt;要了解 TiDB Operator，首先需要对 TiDB 和 Kubernetes 有一定了解，相信长期以来一直关注 TiDB 的同学可能对 TiDB 已经比较熟悉了。本文将首先简单介绍一下 TiDB 和 Kubernetes，聊一聊为什么我们要做 TiDB Operator，然后讲讲如何快速体验 TiDB Operator，以及如何参与到 TiDB Operator 项目中来成为 Contributor。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 和 Kubernetes 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 作为一个开源的分布式数据库产品，具有多副本强一致性的同时能够根据业务需求非常方便的进行弹性伸缩，并且扩缩容期间对上层业务无感知。TiDB 包括三大核心组件：TiDB/TiKV/PD。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB Server：主要负责 SQL 的解析器和优化器，它相当于计算执行层，同时也负责客户端接入和交互。&lt;/li&gt;&lt;li&gt;TiKV Server：是一套分布式的 Key-Value 存储引擎，它承担整个数据库的存储层，数据的水平扩展和多副本高可用特性都是在这一层实现。&lt;/li&gt;&lt;li&gt;PD Server：相当于分布式数据库的大脑，一方面负责收集和维护数据在各个 TiKV 节点的分布情况，另一方面 PD 承担调度器的角色，根据数据分布状况以及各个存储节点的负载来采取合适的调度策略，维持整个系统的平衡与稳定。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的这三个组件，每个角色都是一个多节点组成的集群，所以最终 TiDB 的架构看起来是这样的。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ad51136bfaadda2d121e2f0c54d171e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;312&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2ad51136bfaadda2d121e2f0c54d171e&quot; data-watermark-src=&quot;v2-631273bf6d410f73038521245f886545&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Kubernetes 最早是作为一个纯粹的容器编排系统而诞生的，用户部署好 Kubernetes 集群之后，直接使用其内置的各种功能部署应用服务。&lt;/p&gt;&lt;p&gt;由于这个 PaaS 平台使用起来非常便利，吸引了很多用户，不同用户也提出了各种不同的需求。有些特性需求 Kubernetes 直接在其核心代码里面实现了，但是有些特性并不适合合并到主干分支。&lt;/p&gt;&lt;p&gt;为满足这类需求，Kubernetes 开放出一些 API 供用户自己扩展，实现自己的需求。当前 Kubernetes 内部的 API 变得越来越开放，使其更像是一个跑在云上的操作系统。用户可以把它当作一套云的 SDK 或 Framework 来使用，而且可以很方便地开发组件来扩展满足自己的业务需求。对有状态服务的支持就是一个很有代表性的例子。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么我们要做 TiDB Operator&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;第一，使用传统的自动化工具带来了很高的部署和运维成本。TiDB 的分层架构对于分布式系统是比较常见的，各个组件都可以根据业务需求独立水平伸缩，并且 TiKV 和 TiDB 都可以独立使用。比如，在 TiKV 之上可以构建兼容 Redis 协议的 KV 数据库，而 TiDB 也可以对接 LevelDB 这样的 KV 存储引擎。&lt;/p&gt;&lt;p&gt;但是，这种多组件的分布式系统增加了手工部署和运维的成本。一些传统的自动化部署和运维工具如 Puppet/Chef/SaltStack/Ansible，由于缺乏全局状态管理，不能及时对各种异常情况做自动故障转移，并且很难发挥分布式系统的弹性伸缩能力。其中有些还需要写大量的 DSL 甚至与 Shell 脚本一起混合使用，可移植性较差，维护成本比较高。&lt;/p&gt;&lt;p&gt;第二，在云时代，容器成为应用分发部署的基本单位，而谷歌基于内部使用数十年的容器编排系统 Borg 经验推出的开源容器编排系统 Kubernetes 成为当前容器编排技术事实上的标准。如今各大云厂商都开始提供托管的 Kubernetes 集群，部署在 Kubernetes 平台的应用可以不用绑定在特定云平台，轻松实现在各种云平台之间的迁移，其容器化打包和发布方式也解决了对操作系统环境的依赖。&lt;/p&gt;&lt;p&gt;Kubernetes 项目最早期只支持无状态服务（Stateless Service）的管理。无状态服务通过 ReplicationController 定义多个副本，由 Kubernetes 调度器来决定在不同节点上启动多个 Pod，实现负载均衡和故障转移。对于无状态服务，多个副本对应的 Pod 是等价的，所以在节点出现故障时，在新节点上启动一个 Pod 与失效的 Pod 是等价的，不会涉及状态迁移问题，因而管理非常简单。&lt;/p&gt;&lt;p&gt;但是对于有状态服务（Stateful Service），由于需要将数据持久化到磁盘，使得不同 Pod 之间不能再认为成等价，也就不能再像无状态服务那样随意进行调度迁移。&lt;/p&gt;&lt;p&gt;Kubernetes v1.3 版本提出 PetSet 的概念，用来管理有状态服务并于 v1.5 将其更名为 StatefulSet。StatefulSet 明确定义一组 Pod 中每个的身份，启动和升级都按特定顺序来操作。另外使用持久化卷存储（PersistentVolume）来作为存储数据的载体，当节点失效 Pod 需要迁移时，对应的 PV 也会重新挂载，而 PV 的底层依托于分布式文件系统，所以 Pod 仍然能访问到之前的数据。同时 Pod 在发生迁移时，其网络身份例如 IP 地址是会发生变化的，很多分布式系统不能接受这种情况。所以 StatefulSet 在迁移 Pod 时可以通过绑定域名的方式来保证 Pod 在集群中网络身份不发生变化。&lt;/p&gt;&lt;p&gt;但是由于有状态服务的特殊性，当节点出现异常时，出于数据安全性考虑，Kubernetes 并不会像无状态服务那样自动做故障转移。尽管网络存储能挂载到不同的节点上供其上的 Pod 使用，但是如果出现节点故障时，简单粗暴地将网络 PV 挂载到其它节点上是比较危险的。&lt;/p&gt;&lt;p&gt;Kubernetes 判断节点故障是基于部署在每个节点上的 Kubelet 服务是否能正常上报节点状态，Kubelet 能否正常工作与用户应用并没有必然联系，在一些特殊情况下，Kubelet 服务进程可能无法正常启动，但是节点上的业务容器还在运行，将 PV 再挂载到其它节点可能会出现双写问题。&lt;/p&gt;&lt;p&gt;为了在 Kubernetes 上部署和管理 TiDB 这种有状态的服务，我们需要扩展 StatefulSet 的功能。TiDB Operator 正是基于 Kubernetes 内置的 StatefulSet 开发的 TiDB 集群管理和运维工具。&lt;/p&gt;&lt;p&gt;Kubernetes 直到 v1.7 才试验性引入本地 PV，在这之前只有网络 PV，TiKV 自身在存储数据时就是多副本的，网络 PV 的多副本会增加数据冗余，降低 TiDB 的性能。在这之前我们基于 Kubernetes 内置的 hostPath volume 实现了本地 PV 满足 TiKV 对磁盘 IO 的要求。官方本地 PV 方案直到最近的 Kubernetes v1.10 才相对稳定地支持调度功能，满足用户对本地 PV 的需求。为了降低用户的使用和管理成本并且拥抱 Kubernetes 开源社区，我们又重新基于官方的本地 PV 方案实现了对数据的管理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Operator 原理解析&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Operator 本质上是 Kubernetes 的控制器（Controller），其核心思想是用户给定一个 Spec 描述文件，Controller 根据 Spec 的变化，在 Kubernetes 集群中创建对应资源，并且不断调整资源使其状态满足用户预期的 Spec。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-78ccc29e59b2fb801becd9a21a21f2c4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;326&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-78ccc29e59b2fb801becd9a21a21f2c4&quot; data-watermark-src=&quot;v2-ef84de5460a9792d5e27f7800786473c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图是 TiDB Operator 工作流程原理图，其中 TidbCluster 是通过 CRD（Custom Resource Definition）扩展的内置资源类型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;用户通过 Helm 往 Kubernetes API Server 创建或更新 TidbCluster 对象。&lt;/li&gt;&lt;li&gt;TiDB Operator 通过 watch API Server 中的 TidbCluster 对象创建更新或删除，维护 PD/TiKV/TiDB StatefulSet, Service 和 Deployment 对象更新。&lt;/li&gt;&lt;li&gt;Kubernetes 根据 StatefulSet, Service 和 Deployment 对象创建更新或删除对应的容器和服务。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在第 2 步中，TiDB Operator 在更新 StatefulSet 等对象时会参考 PD API 给出的集群状态来做出 TiDB 集群的运维处理。通过 TiDB Operator 和 Kubernetes 的动态调度处理，创建出符合用户预期的 TiDB 集群。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;快速体验 TiDB Operator&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB Operator 需要运行在 Kubernetes v1.10 及以上版本。TiDB Operator 和 TiDB 集群的部署和管理是通过 Kubernetes 平台上的包管理工具 Helm 实现的。运行 TiDB Operator 前请确保 Helm 已经正确安装在 Kubernetes 集群里。&lt;/p&gt;&lt;p&gt;如果没有 Kubernetes 集群，可以通过 TiDB Operator 提供的脚本快速在本地启动一个多节点的 Kubernetes 集群：&lt;/p&gt;&lt;code lang=&quot;bash&quot;&gt;git clone https://github.com/pingcap/tidb-operator
cd tidb-operator
NUM_NODES=3    # the default node number is 2
KUBE_REPO_PREFIX=uhub.ucloud.cn/pingcap manifests/local-dind/dind-cluster-v1.10.sh up&lt;/code&gt;&lt;p&gt;等 Kubernetes 集群准备好，就可以通过 Helm 和 Kubectl 安装部署 TiDB Operator 和 TiDB 集群了。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;安装 TiDB Operator&lt;/li&gt;&lt;/ol&gt;&lt;code lang=&quot;text&quot;&gt;kubectl apply -f manifests/crd.yaml
helm install charts/tidb-operator --name=tidb-operator --namespace=tidb-admin&lt;/code&gt;&lt;p&gt;2. 部署 TiDB 集群&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;helm install charts/tidb-cluster --name=demo-tidb --namespace=tidb --set clusterName=demo&lt;/code&gt;&lt;p&gt;集群默认使用 local-storage 作为 PD 和 TiKV 的数据存储，如果想使用其它持久化存储，需要修改 charts/tidb-cluster/values.yaml 里面的 storageClassName。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参与 TiDB Operator&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB Operator 让 TiDB 成为真正意义上的 Cloud-Native 数据库，开源只是一个起点，需要 TiDB 社区和 Kubernetes 社区的共同参与。&lt;/p&gt;&lt;p&gt;大家在使用过程发现 bug 或缺失什么功能，都可以直接在 GitHub 上面提 issue 或 PR，一起参与讨论。要想成为 Contributor 具体可以参考 &lt;a href=&quot;https://github.com/pingcap/tidb-operator/blob/master/docs/CONTRIBUTING.md&quot;&gt;这个文档&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;作者：邓栓&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-23-42752388</guid>
<pubDate>Thu, 23 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（十六）INSERT 语句详解</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-17-42287696.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/42287696&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-dbb96df5fd6db3b140bfbe6de5aaf9c0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：于帅鹏&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在之前的一篇文章 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/34512827&quot;&gt;《TiDB 源码阅读系列文章（四）INSERT 语句概览》&lt;/a&gt; 中，我们已经介绍了 INSERT 语句的大体流程。为什么需要为 INSERT 单独再写一篇？因为在 TiDB 中，单纯插入一条数据是最简单的情况，也是最常用的情况；更为复杂的是在 INSERT 语句中设定各种行为，比如，对于 Unique Key 冲突的情况应如何处理：是报错？是忽略当前插入的数据？还是覆盖已有数据？所以，这篇会为大家继续深入介绍 INSERT 语句。&lt;/p&gt;&lt;p&gt;本文将首先介绍在 TiDB 中的 INSERT 语句的分类，以及各语句的语法和语义，然后分别介绍五种 INSERT 语句的源码实现。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;INSERT 语句的种类&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从广义上讲，TiDB 有以下六种 INSERT 语句：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;Basic INSERT&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE ON DUPLICATE KEY UPDATE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;REPLACE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这六种语句理论上都属于 INSERT 语句。&lt;/p&gt;&lt;p&gt;第一种，&lt;code class=&quot;inline&quot;&gt;Basic INSERT&lt;/code&gt;，即是最普通的 INSERT 语句，语法 &lt;code class=&quot;inline&quot;&gt;INSERT INTO VALUES ()&lt;/code&gt;，语义为插入一条语句，若发生唯一约束冲突（主键冲突、唯一索引冲突），则返回执行失败。&lt;/p&gt;&lt;p&gt;第二种，语法 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE INTO VALUES ()&lt;/code&gt;，是当 INSERT 的时候遇到唯一约束冲突后，忽略当前 INSERT 的行，并记一个 warning。当语句执行结束后，可以通过 &lt;code class=&quot;inline&quot;&gt;SHOW WARNINGS&lt;/code&gt; 看到哪些行没有被插入。&lt;/p&gt;&lt;p&gt;第三种，语法 &lt;code class=&quot;inline&quot;&gt;INSERT INTO VALUES () ON DUPLICATE KEY UPDATE&lt;/code&gt;，是当冲突后，更新冲突行后插入数据。如果更新后的行跟表中另一行冲突，则返回错误。&lt;/p&gt;&lt;p&gt;第四种，是在上一种情况，更新后的行又跟另一行冲突后，不插入该行并显示为一个 warning。&lt;/p&gt;&lt;p&gt;第五种，语法 &lt;code class=&quot;inline&quot;&gt;REPLACE INTO VALUES ()&lt;/code&gt;，是当冲突后，删除表上的冲突行，并继续尝试插入数据，如再次冲突，则继续删除标上冲突数据，直到表上没有与改行冲突的数据后，插入数据。&lt;/p&gt;&lt;p&gt;最后一种，语法 &lt;code class=&quot;inline&quot;&gt;LOAD DATA INFILE INTO&lt;/code&gt; 的语义与 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 相同，都是冲突即忽略，不同的是 &lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt; 的作用是将数据文件导入到表中，也就是其数据来源于 csv 数据文件。&lt;/p&gt;&lt;p&gt;由于 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE ON DUPLICATE KEY UPDATE&lt;/code&gt; 是在 &lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt; 上做了些特殊处理，将不再单独详细介绍，而是放在同一小节中介绍；&lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt; 由于其自身的特殊性，将留到其他篇章介绍。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Basic INSERT 语句&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;几种 INSERT 语句的最大不同在于执行层面，这里接着 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/34512827&quot;&gt;《TiDB 源码阅读系列文章（四）INSERT 语句概览》&lt;/a&gt; 来讲语句执行过程。不记得前面内容的同学可以返回去看原文章。&lt;/p&gt;&lt;p&gt;INSERT 的执行逻辑在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert.go&quot;&gt;executor/insert.go&lt;/a&gt; 中。其实前面讲的前四种 INSERT 的执行逻辑都在这个文件里。这里先讲最普通的 &lt;code class=&quot;inline&quot;&gt;Basic INSERT&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;InsertExec&lt;/code&gt; 是 INSERT 的执行器实现，其实现了 Executor 接口。最重要的是下面三个接口：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Open：进行一些初始化&lt;/li&gt;&lt;li&gt;Next：执行写入操作&lt;/li&gt;&lt;li&gt;Close：做一些清理工作&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中最重要也是最复杂的是 Next 方法，根据是否通过一个 SELECT 语句来获取数据（&lt;code class=&quot;inline&quot;&gt;INSERT SELECT FROM&lt;/code&gt;），将 Next 流程分为，&lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert_common.go#L180:24&quot;&gt;insertRows&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert_common.go#L277:24&quot;&gt;insertRowsFromSelect&lt;/a&gt; 两个流程。两个流程最终都会进入 &lt;code class=&quot;inline&quot;&gt;exec&lt;/code&gt; 函数，执行 INSERT。&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;exec&lt;/code&gt; 函数里处理了前四种 INSERT 语句，其中本节要讲的普通 INSERT 直接进入了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/5bdf34b9bba3fc4d3e50a773fa8e14d5fca166d5/executor/insert.go#L42:22&quot;&gt;insertOneRow&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在讲 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/5bdf34b9bba3fc4d3e50a773fa8e14d5fca166d5/executor/insert.go#L42:22&quot;&gt;insertOneRow&lt;/a&gt; 之前，我们先看一段 SQL。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE t (i INT UNIQUE);
INSERT INTO t VALUES (1);
BEGIN;
INSERT INTO t VALUES (1);
COMMIT;&lt;/code&gt;&lt;p&gt;把这段 SQL 分别一行行地粘在 MySQL 和 TiDB 中看下结果。&lt;/p&gt;&lt;p&gt;MySQL：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;mysql&amp;gt; CREATE TABLE t (i INT UNIQUE);
Query OK, 0 rows affected (0.15 sec)

mysql&amp;gt; INSERT INTO t VALUES (1);
Query OK, 1 row affected (0.01 sec)

mysql&amp;gt; BEGIN;
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; INSERT INTO t VALUES (1);
ERROR 1062 (23000): Duplicate entry &#39;1&#39; for key &#39;i&#39;
mysql&amp;gt; COMMIT;
Query OK, 0 rows affected (0.11 sec)&lt;/code&gt;&lt;p&gt;TiDB：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;mysql&amp;gt; CREATE TABLE t (i INT UNIQUE);
Query OK, 0 rows affected (1.04 sec)

mysql&amp;gt; INSERT INTO t VALUES (1);
Query OK, 1 row affected (0.12 sec)

mysql&amp;gt; BEGIN;
Query OK, 0 rows affected (0.01 sec)

mysql&amp;gt; INSERT INTO t VALUES (1);
Query OK, 1 row affected (0.00 sec)

mysql&amp;gt; COMMIT;
ERROR 1062 (23000): Duplicate entry &#39;1&#39; for key &#39;i&#39;&lt;/code&gt;&lt;p&gt;可以看出来，对于 INSERT 语句 TiDB 是在事务提交的时候才做冲突检测而 MySQL 是在语句执行的时候做的检测。这样处理的原因是，TiDB 在设计上，与 TiKV 是分层的结构，为了保证高效率的执行，在事务内只有读操作是必须从存储引擎获取数据，而所有的写操作都事先放在单 TiDB 实例内事务自有的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/kv/memdb_buffer.go#L31&quot;&gt;memDbBuffer&lt;/a&gt; 中，在事务提交时才一次性将事务写入 TiKV。在实现中是在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/5bdf34b9bba3fc4d3e50a773fa8e14d5fca166d5/executor/insert.go#L42:22&quot;&gt;insertOneRow&lt;/a&gt; 中设置了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/e28a81813cfd290296df32056d437ccd17f321fe/kv/kv.go#L23&quot;&gt;PresumeKeyNotExists&lt;/a&gt; 选项，所有的 INSERT 操作如果在本地检测没发现冲突，就先假设插入不会发生冲突，不需要去 TiKV 中检查冲突数据是否存在，只将这些数据标记为待检测状态。最后到提交过程中，统一将整个事务里待检测数据使用 &lt;code class=&quot;inline&quot;&gt;BatchGet&lt;/code&gt; 接口做一次批量检测。&lt;/p&gt;&lt;p&gt;当所有的数据都通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/5bdf34b9bba3fc4d3e50a773fa8e14d5fca166d5/executor/insert.go#L42:22&quot;&gt;insertOneRow&lt;/a&gt; 执行完插入后，INSERT 语句基本结束，剩余的工作为设置一下 lastInsertID 等返回信息，并最终将其结果返回给客户端。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;INSERT IGNORE 语句&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 的语义在前面已经介绍了。之前介绍了普通 INSERT 在提交的时候才检查，那 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 是否可以呢？答案是不行的。因为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 如果在提交时检测，那事务模块就需要知道哪些行需要忽略，哪些直接报错回滚，这无疑增加了模块间的耦合。&lt;/li&gt;&lt;li&gt;用户希望立刻获取 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 有哪些行没有写入进去。即，立刻通过 &lt;code class=&quot;inline&quot;&gt;SHOW WARNINGS&lt;/code&gt; 看到哪些行实际没有写入。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这就需要在执行 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 的时候，及时检查数据的冲突情况。一个显而易见的做法是，把需要插入的数据试着读出来，当发现冲突后，记一个 warning，再继续下一行。但是对于一个语句插入多行的情况，就需要反复从 TiKV 读取数据来进行检测，显然，这样的效率并不高。于是，TiDB 实现了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/3c0bfc19b252c129f918ab645c5e7d34d0c3d154/executor/batch_checker.go#L43:6&quot;&gt;batchChecker&lt;/a&gt;，代码在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/batch_checker.go&quot;&gt;executor/batch_checker.go&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/3c0bfc19b252c129f918ab645c5e7d34d0c3d154/executor/batch_checker.go#L43:6&quot;&gt;batchChecker&lt;/a&gt; 中，首先，拿待插入的数据，将其中可能冲突的唯一约束在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/3c0bfc19b252c129f918ab645c5e7d34d0c3d154/executor/batch_checker.go#L85:24&quot;&gt;getKeysNeedCheck&lt;/a&gt; 中构造成 Key（TiDB 是通过构造唯一的 Key 来实现唯一约束的，详见 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-internal-2/&quot;&gt;《三篇文章了解 TiDB 技术内幕——说计算》&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;然后，将构造出来的 Key 通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/c84a71d666b8732593e7a1f0ec3d9b730e50d7bf/kv/txn.go#L97:6&quot;&gt;BatchGetValues&lt;/a&gt; 一次性读上来，得到一个 Key-Value map，能被读到的都是冲突的数据。&lt;/p&gt;&lt;p&gt;最后，拿即将插入的数据的 Key 到 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/c84a71d666b8732593e7a1f0ec3d9b730e50d7bf/kv/txn.go#L97:6&quot;&gt;BatchGetValues&lt;/a&gt; 的结果中进行查询。如果查到了冲突的行，构造好 warning 信息，然后开始下一行，如果查不到冲突的行，就可以进行安全的 INSERT 了。这部分的实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert_common.go#L490:24&quot;&gt;batchCheckAndInsert&lt;/a&gt; 中。&lt;/p&gt;&lt;p&gt;同样，在所有数据执行完插入后，设置返回信息，并将执行结果返回客户端。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;INSERT ON DUPLICATE KEY UPDATE 语句&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt; 是几种 INSERT 语句中最为复杂的。其语义的本质是包含了一个 INSERT 和 一个 UPDATE。较之与其他 INSERT 复杂的地方就在于，UPDATE 语义是可以将一行更新成任何合法的样子。&lt;/p&gt;&lt;p&gt;在上一节中，介绍了 TiDB 中对于特殊的 INSERT 语句采用了 batch 的方式来实现其冲突检查。在处理 &lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt; 的时候我们采用了同样的方式，但由于语义的复杂性，实现步骤也复杂了不少。&lt;/p&gt;&lt;p&gt;首先，与 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt; 相同，首先将待插入数据构造出来的 Key，通过 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/c84a71d666b8732593e7a1f0ec3d9b730e50d7bf/kv/txn.go#L97:6&quot;&gt;BatchGetValues&lt;/a&gt; 一次性地读出来，得到一个 Key-Value map。再把所有读出来的 Key 对应的表上的记录也通过一次 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/c84a71d666b8732593e7a1f0ec3d9b730e50d7bf/kv/txn.go#L97:6&quot;&gt;BatchGetValues&lt;/a&gt; 读出来，这部分数据是为了将来做 UPDATE 准备的，具体实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/3c0bfc19b252c129f918ab645c5e7d34d0c3d154/executor/batch_checker.go#L225:24&quot;&gt;initDupOldRowValue&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;然后，在做冲突检查的时候，如果遇到冲突，则首先进行一次 UPDATE。我们在前面 Basic INSERT 小节中已经介绍了，TiDB 的 INSERT 是提交的时候才去 TiKV 真正执行。同样的，UPDATE 语句也是在事务提交的时候才真正去 TiKV 执行的。在这次 UPDATE 中，可能还是会遇到唯一约束冲突的问题，如果遇到了，此时即报错返回，如果该语句是 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE ON DUPLICATE KEY UPDATE&lt;/code&gt; 则会忽略这个错误，继续下一行。&lt;/p&gt;&lt;p&gt;在上一步的 UPDATE 中，还需要处理以下场景，如下面这个 SQL：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE t (i INT UNIQUE);
INSERT INTO t VALUES (1), (1) ON DUPLICATE KEY UPDATE i = i;&lt;/code&gt;&lt;p&gt;可以看到，这个 SQL 中，表中原来并没有数据，第二句的 INSERT 也就不可能读到可能冲突的数据，但是，这句 INSERT 本身要插入的两行数据之间冲突了。这里的正确执行应该是，第一个 1 正常插入，第二个 1 插入的时候发现有冲突，更新第一个 1。此时，就需要做如下处理。将上一步被 UPDATE 的数据对应的 Key-Value 从第一步的 Key-Value map 中删掉，将 UPDATE 出来的数据再根据其表信息构造出唯一约束的 Key 和 Value，把这个 Key-Value 对放回第一步读出来 Key-Value map 中，用于后续数据进行冲突检查。这个细节的实现在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/2fba9931c7ffbb6dd939d5b890508eaa21281b4f/executor/batch_checker.go#L232&quot;&gt;fillBackKeys&lt;/a&gt;。这种场景同样出现在，其他 INSERT 语句中，如 &lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;REPLACE&lt;/code&gt;、&lt;code class=&quot;inline&quot;&gt;LOAD DATA&lt;/code&gt;。之所以在这里介绍是因为，&lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt; 是最能完整展现 &lt;code class=&quot;inline&quot;&gt;batchChecker&lt;/code&gt; 的各方面的语句。&lt;/p&gt;&lt;p&gt;最后，同样在所有数据执行完插入/更新后，设置返回信息，并将执行结果返回客户端。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;REPLACE 语句&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;REPLACE 语句虽然它看起来像是独立的一类 DML，实际上观察语法的话，它与 &lt;code class=&quot;inline&quot;&gt;Basic INSERT&lt;/code&gt; 只是把 INSERT 换成了 REPLACE。与之前介绍的所有 INSERT 语句不同的是，REPLACE 语句是一个一对多的语句。简要说明一下就是，一般的 INSERT 语句如果需要 INSERT 某一行，那将会当遭遇了唯一约束冲突的时候，出现以下几种处理方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;放弃插入，报错返回：&lt;code class=&quot;inline&quot;&gt;Basic INSERT&lt;/code&gt;&lt;/li&gt;&lt;li&gt;放弃插入，不报错：&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;放弃插入，改成更新冲突的行，如果更新的值再次冲突&lt;/li&gt;&lt;li&gt;报错：&lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE KEY UPDATE&lt;/code&gt;&lt;/li&gt;&lt;li&gt;不报错：&lt;code class=&quot;inline&quot;&gt;INSERT IGNORE ON DUPLICATE KEY UPDATE&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;他们都是处理一行数据跟表中的某一行冲突时的不同处理。但是 REPLACE 语句不同，它将会删除遇到的所有冲突行，直到没有冲突后再插入数据。如果表中有 5 个唯一索引，那有可能有 5 条与等待插入的行冲突的行。那么 REPLACE 语句将会一次性删除这 5 行，再将自己插入。看以下 SQL：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE TABLE t (
i int unique, 
j int unique, 
k int unique, 
l int unique, 
m int unique);

INSERT INTO t VALUES 
(1, 1, 1, 1, 1), 
(2, 2, 2, 2, 2), 
(3, 3, 3, 3, 3), 
(4, 4, 4, 4, 4);

REPLACE INTO t VALUES (1, 2, 3, 4, 5);

SELECT * FROM t;
i j k l m
1 2 3 4 5&lt;/code&gt;&lt;p&gt;在执行完之后，实际影响了 5 行数据。&lt;/p&gt;&lt;p&gt;理解了 REPLACE 语句的特殊性以后，我们就可以更容易理解其具体实现。&lt;/p&gt;&lt;p&gt;与 INSERT 语句类似，REPLACE 语句的主要执行部分也在其 Next 方法中，与 INSERT 不同的是，其中的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert_common.go#L277:24&quot;&gt;insertRowsFromSelect&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/ab332eba2a04bc0a996aa72e36190c779768d0f1/executor/insert_common.go#L180:24&quot;&gt;insertRows&lt;/a&gt; 传递了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/f6dbad0f5c3cc42cafdfa00275abbd2197b8376b/executor/replace.go#L27&quot;&gt;ReplaceExec&lt;/a&gt; 自己的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/f6dbad0f5c3cc42cafdfa00275abbd2197b8376b/executor/replace.go#L160&quot;&gt;exec&lt;/a&gt; 方法。在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/f6dbad0f5c3cc42cafdfa00275abbd2197b8376b/executor/replace.go#L160&quot;&gt;exec&lt;/a&gt; 中调用了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/f6dbad0f5c3cc42cafdfa00275abbd2197b8376b/executor/replace.go#L95&quot;&gt;replaceRow&lt;/a&gt;，其中同样使用了 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/3c0bfc19b252c129f918ab645c5e7d34d0c3d154/executor/batch_checker.go#L43:6&quot;&gt;batchChecker&lt;/a&gt; 中的批量冲突检测，与 INSERT 有所不同的是，这里会删除一切检测出的冲突，最后将待插入行写入。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;INSERT 语句是所有 DML 语句中最复杂，功能最强大多变的一个。其既有像 &lt;code class=&quot;inline&quot;&gt;INSERT ON DUPLICATE UPDATE&lt;/code&gt; 这种能执行 INSERT 也能执行 UPDATE 的语句，也有像 REPLACE 这种一行数据能影响许多行数据的语句。INSERT 语句自身都可以连接一个 SELECT 语句作为待插入数据的输入，因此，其又受到了来自 planner 的影响（关于 planner 的部分详见相关的源码阅读文章： &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35511864&quot;&gt;（七）基于规则的优化&lt;/a&gt; 和 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/36420449&quot;&gt;（八）基于代价的优化&lt;/a&gt; ）。熟悉 TiDB 的 INSERT 各个语句实现，可以帮助各位读者在将来使用这些语句时，更好地根据其特色使用最为合理、高效语句。另外，如果有兴趣向 TiDB 贡献代码的读者，也可以通过本文更快的理解这部分的实现。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-17-42287696</guid>
<pubDate>Fri, 17 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读（十五） Sort Merge Join</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-08-41535500.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/41535500&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e445e0a331d683cab8f11fda36478022_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者： &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/5202d0b6a9c623e0674ff36891c8ab52&quot; data-hash=&quot;5202d0b6a9c623e0674ff36891c8ab52&quot; data-hovercard=&quot;p$b$5202d0b6a9c623e0674ff36891c8ab52&quot;&gt;@姚维&lt;/a&gt; &lt;/p&gt;&lt;h2&gt;&lt;b&gt;什么是 Sort Merge Join&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在开始阅读源码之前, 我们来看看什么是 Sort Merge Join (SMJ)，定义可以看 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sort-merge_join&quot;&gt;wikipedia&lt;/a&gt;。简单说来就是将 Join 的两个表，首先根据连接属性进行排序，然后进行一次扫描归并, 进而就可以得出最后的结果。这个算法最大的消耗在于对内外表数据进行排序，而当连接列为索引列时，我们可以利用索引的有序性避免排序带来的消耗, 所以通常在查询优化器中，连接列为索引列的情况下可以考虑选择使用 SMJ。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Sort Merge Join 实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;执行过程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 的实现代码在 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/source-code/executor/merge_join.go&quot;&gt;tidb/executor/merge_join.go&lt;/a&gt; 中 &lt;code class=&quot;inline&quot;&gt;MergeJoinExec.NextChunk&lt;/code&gt; 是这个算子的入口。下面以 &lt;code class=&quot;inline&quot;&gt;SELECT * FROM A JOIN B ON A.a = B.a&lt;/code&gt; 为例，对 SMJ 执行过程进行简述，假设此时外表为 A，内表为 B，join-keys 为 a，A，B 表的 a 列上都有索引：&lt;/p&gt;&lt;p&gt;1.顺序读取外表 A 直到 join-keys 中出现另外的值，把相同 keys 的行放入数组 a1，同样的规则读取内表 B，把相同 keys 的行放入数组 a2。如果外表数据或者内表数据读取结束，退出。&lt;/p&gt;&lt;p&gt;2. 从 a1 中读取当前第一行数据，设为 v1。从 a2 中读取当前第一行数据，设为 v2。&lt;/p&gt;&lt;p&gt;3. 根据 join-keys 比较 v1，v2，结果分为几种情况：&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;cmpResult &amp;gt; 0, 表示 v1 大于 v2，把当前 a2 的数据丢弃，从内表读取下一批数据，读取方法同 1。重复 2。&lt;/li&gt;&lt;li&gt;cmpResult &amp;lt; 0, 表示 v1 小于 v2，说明外表的 v1 没有内表的值与之相同，把外表数据输出给 resultGenerator（不同的连接类型会有不同的结果输出，例如外连接会把不匹配的外表数据输出）。&lt;/li&gt;&lt;li&gt;cmpResult == 0, 表示 v1 等于 v2。那么遍历 a1 里面的数据，跟 a2 的数据，输出给 resultGenerator 作一次连接。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;4. 回到步骤 1。&lt;/p&gt;&lt;p&gt;下面的图展示了 SMJ 的过程：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b72f20067fcc79e607e567fbc8711bad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;942&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b72f20067fcc79e607e567fbc8711bad&quot; data-watermark-src=&quot;v2-08f69ef7725298bf5d409e0fc691037f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;读取内表 / 外表数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们分别通过 &lt;code class=&quot;inline&quot;&gt;fetchNextInnerRows&lt;/code&gt; 或者 &lt;code class=&quot;inline&quot;&gt;fetchNextOuterRows&lt;/code&gt; 读取内表和外表的数据。这两个函数实现的功能类似，这里只详述函数 &lt;code class=&quot;inline&quot;&gt;fetchNextInnerRows&lt;/code&gt; 的实现。&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;MergeSortExec&lt;/code&gt; 算子读取数据，是通过迭代器 &lt;code class=&quot;inline&quot;&gt;readerIterator&lt;/code&gt; 完成，&lt;code class=&quot;inline&quot;&gt;readerIterator&lt;/code&gt; 可以顺序读取数据。&lt;code class=&quot;inline&quot;&gt;MergeSortExec&lt;/code&gt; 算子维护两个 readerIterator：&lt;code class=&quot;inline&quot;&gt;outerIter&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;innerIter&lt;/code&gt;，它们在 &lt;code class=&quot;inline&quot;&gt;buildMergeJoin&lt;/code&gt; 函数中被构造。&lt;/p&gt;&lt;p&gt;真正读取数据的操作是在 &lt;code class=&quot;inline&quot;&gt;readerIterator.nextSelectedRow&lt;/code&gt; 中完成, 这里会通过 &lt;code class=&quot;inline&quot;&gt;ri.reader.NextChunk&lt;/code&gt; 每次读取一个 Chunk 的数据，关于 Chunk 的相关内容，可以查看我们之前的文章 &lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-10/&quot;&gt;TiDB 源码阅读系列文章（十）Chunk 和执行框架简介&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;这里值得注意的是，我们通过 &lt;code class=&quot;inline&quot;&gt;expression.VectorizedFilter&lt;/code&gt; 对外表数据进行过滤，返回一个 curSelected 布尔数组，用于外表的每一行数据是否是满足 filter 过滤条件。以 &lt;code class=&quot;inline&quot;&gt;select * from t1 left outer join t2 on t1.a=100;&lt;/code&gt; 为例, 这里的 filter 是 &lt;code class=&quot;inline&quot;&gt;t1.a=100&lt;/code&gt;, 对于没有通过这个过滤条件的行，我们通过 &lt;code class=&quot;inline&quot;&gt;ri.joinResultGenerator.emitToChunk&lt;/code&gt; 函数发送给 resultGenerator, 这个 resultGenerator 是一个 interface，具体是否输出这行数据，会由 join 的类型决定，比如外连接则会输出，内连接则会忽略。具体关于 resultGenerator, 可以参考之前的文章：&lt;a href=&quot;https://pingcap.com/blog-cn/tidb-source-code-reading-9/&quot;&gt;TiDB 源码阅读系列文章（九）Hash Join&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;rowsWithSameKey&lt;/code&gt; 通过 &lt;code class=&quot;inline&quot;&gt;nextSelectedRow&lt;/code&gt; 不断读取下一行数据，并通过对每行数据的 join-keys 进行判断是不是属于同一个 join-keys，如果是，会把相同 join-keys 的行分别放入到 &lt;code class=&quot;inline&quot;&gt;innerChunkRows&lt;/code&gt; 和 &lt;code class=&quot;inline&quot;&gt;outerIter4Row&lt;/code&gt; 数组中。然后对其分别建立迭代器 innerIter4Row 和 outerIter4Row。在 SMJ 中的执行过程中，会利用这两个迭代器来获取数据进行真正的比较得出 join result。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Merge-Join&lt;/b&gt;&lt;/p&gt;&lt;p&gt;实现 Merge-Join 逻辑的代码在函数 &lt;code class=&quot;inline&quot;&gt;MergeJoinExec.joinToChunk&lt;/code&gt;, 对内外表迭代器的当前数据根据各自的 join-keys 作对比，有如下几个结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;cmpResult &amp;gt; 0，代表外表当前数据大于内表数据，那么通过 &lt;code class=&quot;inline&quot;&gt;fetchNextInnerRows&lt;/code&gt; 直接读取下一个内表数据，然后重新比较即可。&lt;/li&gt;&lt;li&gt;cmpResult &amp;lt; 0，代表外表当前数据小于内表数据，这个时候就分几种情况了，如果是外连接，那么需要输出外表数据 + NULL，如果是内连接，那么这个外表数据就被忽略，对于这个不同逻辑的处理，统一由 &lt;code class=&quot;inline&quot;&gt;e.resultGenerator&lt;/code&gt; 来控制，我们只需要把外表数据通过 &lt;code class=&quot;inline&quot;&gt;e.resultGenerator.emitToChunk&lt;/code&gt; 调用它即可。然后通过 &lt;code class=&quot;inline&quot;&gt;fetchNextOuterRows&lt;/code&gt; 读取下一个外表数据，重新比较。&lt;/li&gt;&lt;li&gt;cmpResult == 0，代表外表当前数据等于内表当前数据，这个时候就把外表数据跟内表当前数据做一次连接，通过 &lt;code class=&quot;inline&quot;&gt;e.resultGenerator.emitToChunk&lt;/code&gt; 生成结果。之后外表跟内表分别获取下一个数据，重新开始比较。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;重复上面的过程，直到外表或者内表数据被遍历完，退出 Merge-Join 的过程。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们上面的分析代码基于 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/source-code&quot;&gt;Source-code&lt;/a&gt; 分支，可能大家已经发现了一些问题，比如我们会一次性读取内外表的 Join group（相同的 key）。这里如果相同的 key 比较多，是有内存 OOM 的风险的。针对这个问题，我们在最新的 master 分支做了几个事情来优化：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;外表其实不需要把相同的 keys 一次性都读取上来， 它只需要按次迭代外表数据，再跟内表逐一对比作连接即可。这里至少可以减少外表发生 OOM 的问题，可以大大减少 OOM 的概率。&lt;/li&gt;&lt;li&gt;对于内表，我们对 OOM 也不是没有办法，我们用 &lt;code class=&quot;inline&quot;&gt;memory.Tracker&lt;/code&gt; 这个内存追踪器来记录当前内表已经使用的中间结果的内存大小，如果它超过我们设置的阈值，我们会采取输出日志或者终止 SQL 继续运行的方法来规避 OOM 的发生。关于 &lt;code class=&quot;inline&quot;&gt;memory.Tracker&lt;/code&gt; 我们不在此展开，可以留意我们后续的源码分析文章。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;后续我们还会在 Merge-Join 方面做一些优化， 比如我们可以做多路归并，中间结果存外存等等，敬请期待。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-08-41535500</guid>
<pubDate>Wed, 08 Aug 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>30 分钟成为 TiKV Contributor</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-08-02-41103417.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/41103417&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3e6973721ffe23e838d18d1d6cebc858_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;作者：吴雪莲&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;背景知识&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SQL 语句发送到 TiDB 后经过 parser 生成 AST（抽象语法树），再经过 Query Optimizer 生成执行计划，执行计划切分成很多子任务，这些子任务以表达式的方式最后下推到底层的各个 TiKV 来执行。&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d54ea1017d24caf90ab9d7f987f40fae_r.jpg&quot; data-caption=&quot;图 1&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1228&quot; data-rawheight=&quot;860&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d54ea1017d24caf90ab9d7f987f40fae&quot; data-watermark-src=&quot;v2-d826905412f823c66e227f7c2ec3f603&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;如图 1，当 TiDB 收到来自客户端的查询请求&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;select count(*) from t where a + b &amp;gt; 5&lt;/code&gt;&lt;/p&gt;&lt;p&gt;时，执行顺序如下：&lt;/p&gt;&lt;p&gt;1.TiDB 对 SQL 进行解析，组织成对应的表达式，下推给 TiKV&lt;/p&gt;&lt;p&gt;2. TiKV 收到请求后，循环以下过程&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;获取下一行完整数据，并按列解析&lt;/li&gt;&lt;li&gt;使用参数中的 where 表达式对数据进行过滤&lt;/li&gt;&lt;li&gt;若上一条件符合，进行聚合计算&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;3. TiKV 向 TiDB 返回聚合计算结果&lt;/p&gt;&lt;p&gt;4. TiDB 对所有涉及的结果进行二次聚合，返回给客户端&lt;/p&gt;&lt;p&gt;这里的 where 条件便是以表达式树的形式下推给 TiKV。在此之前 TiDB 只会向 TiKV 下推一小部分简单的表达式，比如取出某一个列的某个数据类型的值，简单数据类型的比较操作，算术运算等。为了充分利用分布式集群的资源，进一步提升 SQL 在整个集群的执行速度，我们需要将更多种类的表达式下推到 TiKV 来运行，其中的一大类就是 MySQL built-in 函数。&lt;br&gt;目前，由于 TiKV 的 built-in 函数尚未全部实现，对于无法下推的表达式，TiDB 只能自行解决。这无疑将成为提升 TiDB 速度的最大绊脚石。好消息是，TiKV 在实现 built-in 函数时，可以直接参考 TiDB 的对应函数逻辑（顺便可以帮 TiDB 找找 Bug），为我们减少了不少工作量。&lt;/p&gt;&lt;p&gt;Built-in 函数无疑是 TiDB 和 TiKV 成长道路上不可替代的一步，如此艰巨又庞大的任务，我们需要广大社区朋友们的支持与鼓励。亲爱的朋友们，想玩 Rust 吗？想给 TiKV 提 PR 吗？想帮助 TiDB 跑得更快吗？动动您的小手指，拿 PR 来砸我们吧。您的 PR 一旦被采用，将会有小惊喜哦。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;手把手教你实现 built-in 函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Step 1：准备下推函数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 TiKV 的 &lt;a href=&quot;https://github.com/pingcap/tikv/issues/3275&quot;&gt;https://github.com/pingcap/tikv/issues/3275&lt;/a&gt; issue 中，找到未实现的函数签名列表，选一个您想要实现的函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 2：获取 TiDB 中可参考的逻辑实现&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 TiDB 的 &lt;a href=&quot;https://github.com/pingcap/tidb/tree/master/expression&quot;&gt;expression&lt;/a&gt; 目录下查找相关 builtinXXXSig 对象，这里 XXX 为您要实现的函数签名，本例中以 &lt;a href=&quot;https://github.com/pingcap/tikv/pull/3277&quot;&gt;MultiplyIntUnsigned&lt;/a&gt; 为例，可以在 TiDB 中找到其对应的函数签名（&lt;code class=&quot;inline&quot;&gt;builtinArithmeticMultiplyIntUnsignedSig&lt;/code&gt;）及 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/master/expression/builtin_arithmetic.go#L532&quot;&gt;实现&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 3：确定函数定义&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1.built-in 函数所在的文件名要求与 TiDB 的名称对应，如 TiDB 中，&lt;a href=&quot;https://github.com/pingcap/tidb/tree/master/expression&quot;&gt;expression&lt;/a&gt; 目录下的下推文件统一以 builtin_XXX 命名，对应到 TiKV 这边，就是 &lt;code class=&quot;inline&quot;&gt;builtin_XXX.rs&lt;/code&gt;。若同名对应的文件不存在，则需要自行在同级目录下新建。对于本例，当前函数存放于 TiDB 的 &lt;a href=&quot;https://github.com/pingcap/tidb/blob/master/expression/builtin_arithmetic.go#L532&quot;&gt;builtin_arithmetic.go&lt;/a&gt; 文件里，对应到 TiKV 便是存放在 &lt;a href=&quot;https://github.com/pingcap/tikv/blob/master/src/coprocessor/dag/expr/builtin_arithmetic.rs&quot;&gt;builtin_arithmetic.rs&lt;/a&gt; 中。&lt;/p&gt;&lt;p&gt;2. 函数名称：函数签名转为 Rust 的函数名称规范，这里 &lt;code class=&quot;inline&quot;&gt;MultiplyIntUnsigned&lt;/code&gt; 将会被定义为 &lt;code class=&quot;inline&quot;&gt;multiply_int_unsigned&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;3. 函数返回值，可以参考 TiDB 中实现的 &lt;code class=&quot;inline&quot;&gt;Eval&lt;/code&gt; 函数，对应关系如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4fb256c456c3220224e6055ca6ab6bf6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1158&quot; data-rawheight=&quot;838&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4fb256c456c3220224e6055ca6ab6bf6&quot; data-watermark-src=&quot;v2-6781bac667ad6b94c4a8b7ec58af83eb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;可以看到 TiDB 的 &lt;code class=&quot;inline&quot;&gt;builtinArithmeticMultiplyIntUnsignedSig&lt;/code&gt;  对象实现了 evalInt 方法，故当前函数（&lt;code class=&quot;inline&quot;&gt;multiply_int_unsigned&lt;/code&gt;）的返回类型应该为 &lt;code class=&quot;inline&quot;&gt;Result&amp;lt;Option&amp;lt;i64&amp;gt;&amp;gt;&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;4. 函数的参数, 所有 builtin-in 的参数都与 Expression 的 &lt;code class=&quot;inline&quot;&gt;eval&lt;/code&gt; 函数一致，即：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;环境配置量 (ctx:&amp;amp;StatementContext)&lt;/li&gt;&lt;li&gt;该行数据每列具体值 (row:&amp;amp;[Datum])&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;综上，&lt;code class=&quot;inline&quot;&gt;multiply_int_unsigned&lt;/code&gt; 的下推函数定义为：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;pub fn multiply_int_unsigned(
       &amp;amp;self,
       ctx: &amp;amp;mut EvalContext,
       row: &amp;amp;[Datum],
   ) -&amp;gt; Result&amp;lt;Option&amp;lt;i64&amp;gt;&amp;gt;&lt;/code&gt;&lt;p&gt;&lt;b&gt;Step 4：实现函数逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这一块相对简单，直接对照 TiDB 的相关逻辑实现即可。这里，我们可以看到 TiDB 的 &lt;code class=&quot;inline&quot;&gt;builtinArithmeticMultiplyIntUnsignedSig&lt;/code&gt; 的具体实现如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;func (s *builtinArithmeticMultiplyIntUnsignedSig) evalInt(row types.Row) (val int64, isNull bool, err error) {
  a, isNull, err := s.args[0].EvalInt(s.ctx, row)
  if isNull || err != nil {
     return 0, isNull, errors.Trace(err)
  }
  unsignedA := uint64(a)
  b, isNull, err := s.args[1].EvalInt(s.ctx, row)
  if isNull || err != nil {
     return 0, isNull, errors.Trace(err)
  }
  unsignedB := uint64(b)
  result := unsignedA * unsignedB
  if unsignedA != 0 &amp;amp;&amp;amp; result/unsignedA != unsignedB {
     return 0, true, types.ErrOverflow.GenByArgs(&quot;BIGINT UNSIGNED&quot;, fmt.Sprintf(&quot;(%s * %s)&quot;, s.args[0].String(), s.args[1].String()))
  }
  return int64(result), false, nil
}&lt;/code&gt;&lt;p&gt;参考以上代码，翻译到 TiKV 即可，如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;pub fn multiply_int_unsigned(
       &amp;amp;self,
       ctx: &amp;amp;mut EvalContext,
       row: &amp;amp;[Datum],
   ) -&amp;gt; Result&amp;lt;Option&amp;lt;i64&amp;gt;&amp;gt; {
       let lhs = try_opt!(self.children[0].eval_int(ctx, row));
       let rhs = try_opt!(self.children[1].eval_int(ctx, row));
       let res = (lhs as u64).checked_mul(rhs as u64).map(|t| t as i64);
       // TODO: output expression in error when column&#39;s name pushed down.
       res.ok_or_else(|| Error::overflow(&quot;BIGINT UNSIGNED&quot;, &amp;amp;format!(&quot;({} * {})&quot;, lhs, rhs)))
           .map(Some)
   }&lt;/code&gt;&lt;p&gt;&lt;b&gt;Step 5：添加参数检查&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 在收到下推请求时，首先会对所有的表达式进行检查，表达式的参数个数检查就在这一步进行。&lt;/p&gt;&lt;p&gt;TiDB 中对每个 built-in 函数的参数个数有严格的限制，这一部分检查可参考 TiDB 同目录下 builtin.go 相关代码。&lt;/p&gt;&lt;p&gt;在 TiKV 同级目录的 scalar_function.rs 文件里，找到 ScalarFunc 的 check_args 函数，按照现有的模式，加入参数个数的检查即可。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 6：添加下推支持&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 在对一行数据执行具体的 expression 时，会调用 eval 函数，eval 函数又会根据具体的返回类型，执行具体的子函数。这一部分工作在 scalar_function.rs 中以宏（dispatch_call）的形式完成。&lt;/p&gt;&lt;p&gt;对于 MultiplyIntUnsigned, 我们最终返回的数据类型为 Int，所以可以在 dispatch_call 中找到 INT_CALLS，然后照着加入 MultiplyIntUnsigned =&amp;gt; multiply_int_unsigned , 表示当解析到函数签名 MultiplyIntUnsigned 时，调用上述已实现的函数 multiply_int_unsigned。&lt;/p&gt;&lt;p&gt;至此 MultiplyIntUnsigned 下推逻辑已完全实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 7：添加测试&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在函数 multiply_int_unsigned 所在文件 &lt;a href=&quot;https://github.com/pingcap/tikv/blob/master/src/coprocessor/dag/expr/builtin_arithmetic.rs&quot;&gt;builtin_arithmetic.rs&lt;/a&gt; 底部的 test 模块中加入对该函数签名的单元测试，要求覆盖到上述添加的所有代码，这一部分也可以参考 TiDB 中相关的测试代码。本例在 TiKV 中实现的测试代码如下：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;#[test]
   fn test_multiply_int_unsigned() {
       let cases = vec![
           (Datum::I64(1), Datum::I64(2), Datum::U64(2)),
           (
               Datum::I64(i64::MIN),
               Datum::I64(1),
               Datum::U64(i64::MIN as u64),
           ),
           (
               Datum::I64(i64::MAX),
               Datum::I64(1),
               Datum::U64(i64::MAX as u64),
           ),
           (Datum::U64(u64::MAX), Datum::I64(1), Datum::U64(u64::MAX)),
       ];

       let mut ctx = EvalContext::default();
       for (left, right, exp) in cases {
           let lhs = datum_expr(left);
           let rhs = datum_expr(right);

           let mut op = Expression::build(
               &amp;amp;mut ctx,
               scalar_func_expr(ScalarFuncSig::MultiplyIntUnsigned, &amp;amp;[lhs, rhs]),
           ).unwrap();
           op.mut_tp().set_flag(types::UNSIGNED_FLAG as u32);

           let got = op.eval(&amp;amp;mut ctx, &amp;amp;[]).unwrap();
           assert_eq!(got, exp);
       }

       // test overflow
       let cases = vec![
           (Datum::I64(-1), Datum::I64(2)),
           (Datum::I64(i64::MAX), Datum::I64(i64::MAX)),
           (Datum::I64(i64::MIN), Datum::I64(i64::MIN)),
       ];

       for (left, right) in cases {
           let lhs = datum_expr(left);
           let rhs = datum_expr(right);

           let mut op = Expression::build(
               &amp;amp;mut ctx,
               scalar_func_expr(ScalarFuncSig::MultiplyIntUnsigned, &amp;amp;[lhs, rhs]),
           ).unwrap();
           op.mut_tp().set_flag(types::UNSIGNED_FLAG as u32);

           let got = op.eval(&amp;amp;mut ctx, &amp;amp;[]).unwrap_err();
           assert!(check_overflow(got).is_ok());
       }
   }&lt;/code&gt;&lt;p&gt;&lt;b&gt;Step 8：运行测试&lt;/b&gt;&lt;/p&gt;&lt;p&gt;运行 make expression，确保所有的 test case 都能跑过。&lt;/p&gt;&lt;p&gt;完成以上几个步骤之后，就可以给 TiKV 项目提 PR 啦。想要了解提 PR 的基础知识，尝试移步 &lt;a href=&quot;https://pingcap.com/blog-how-to-contribute-zh&quot;&gt;此文&lt;/a&gt;，看看是否有帮助。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;欢迎大家踊跃贡献代码，加入 TiDB community ！&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://github.com/pingcap.com&quot;&gt;http://github.com/pingcap.com&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-08-02-41103417</guid>
<pubDate>Thu, 02 Aug 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
