<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Wed, 05 Dec 2018 13:54:47 +0800</lastBuildDate>
<item>
<title>【重磅】年度宽客（2000 - 2019），全球Q-quant精英都在研究什么？</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-04-51569176.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51569176&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-59ae063fe28731207c2d2fb2b1daf7b4_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-35d94004313d9a2417c95c92a1fcbf1a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1334&quot; data-rawheight=&quot;828&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-35d94004313d9a2417c95c92a1fcbf1a&quot; data-watermark-src=&quot;v2-1bc025e058727882813cc73886e21dda&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;作者：王圣元&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1、&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289640&amp;amp;idx=1&amp;amp;sn=34e94fcbe99052b8e7381ecc48a36dc0&amp;amp;chksm=802e3ebdb759b7ab897cd329a680715b6f8294e63550ddf0c57b9e1320b2b7d1408c6fdca0c7&amp;amp;token=1883102744&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;机器学习、深度学习、量化金融、Python最新书籍汇总&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289623&amp;amp;idx=1&amp;amp;sn=28a3600fd7a72d7be00b066ca0f98244&amp;amp;chksm=802e3e82b759b7943f43a4f6ef4a91e4153fa6b8210de9590235fa8ee66eb9811ce177054dbc&amp;amp;token=1883102744&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、海量Wind数据，与全网用户零距离邂逅！&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289615&amp;amp;idx=1&amp;amp;sn=1cdc89afb997d0c580bf0cef296d946c&amp;amp;chksm=802e3e9ab759b78ce9f0cd152a680d4a413d6c8dcb02a7a296f4091993a7e4137e7520394575&amp;amp;token=1883102744&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、超级棒的机器学习资料下载&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文献给刚刚出生的儿子&lt;/b&gt;&lt;/p&gt;&lt;p&gt;前几天看到了 Quant of the Year 2019 颁布的新闻，回想从 2015 年开始自学机器学习时就没关注这个了，因为这个奖项通常都是 Q-quant （即在风险中性测度下玩转的 quant） 所拿，而我开始对机器学习感兴趣，已经向 P-quant 靠拢了。关于 P-quant 和 Q-quant 的区别可参照我之前写的&lt;u&gt;&lt;b&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjY0MjE1MA==&amp;amp;mid=2247485368&amp;amp;idx=1&amp;amp;sn=5872cd579616dadd93a33b29f15298f1&amp;amp;chksm=e89084b1dfe70da733bf4370687fc3f91f6b36ec6b0562a43978e7a873f91dde2834bec47282&amp;amp;scene=21#wechat_redirect&quot;&gt;这个帖子&lt;/a&gt;&lt;/b&gt;&lt;/u&gt;。但今天这个奖项是颁给一个渣打银行 （SCB）的数据分析执行董事，他用 P-quant 的机器学习方法来解决银行实际的问题。心血来潮把从 2000 年到 2019 年的这 20 年的新闻读了，论文也从 Risk 网站上下载了。（Risk 网站下载这些论文需注册会员，因此有些论文上有水印不让传播，为了尊重知识产权，只在公众号后台分享那些没打水印的论文，望理解）。&lt;b&gt;论文在文末下载。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;下面就简要回顾下这 20年年度最佳宽客做了什么吧。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2019 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Alexei Kondratyev&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文 1&lt;/b&gt;：&lt;i&gt;Curve Dynamics with Artificial Neural Networks&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 2&lt;/b&gt;：&lt;i&gt;Evolutionary Algos for Optimising MVA&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Quants these days tend to maintain expertise in specific fields. With Alexei, his expertise in multiple, unrelated fields gives him a broader perspective and makes him a great researcher. -- &lt;/i&gt; Alexander Sokol&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/6159246/quant-of-the-year-alexei-kondratyev&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;亮点：用机器学习的法来解决卖方 (sell side) 的问题。&lt;/p&gt;&lt;p&gt;机器学习其实在金融上的应用主要都在买方 (buy side), 比如私募或者基金，而 Alexei 的主要贡献是在卖方如银行中找到了两个应用：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;用人工神经网络 + 正则化，和自动编码器来捕捉利率曲线和商品远期曲线里的动态关系。【论文 1】&lt;br&gt;&lt;/li&gt;&lt;li&gt;用遗传算法 (Genetic Algorithm, GA) 和粒子群优化 (Particle Swarm Optimization, PSO) 来为银行压缩交易而减少保证金 (margin)。两个都是进化算法，GA 主要在离散型变量空间 (比如货币 currency, 交易对手 counterparty) 找最优解，而 PSO 主要在连续性变量空间 (比如年限 tenor, 本金 notional) 找最优解。【论文 2】&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;论文还要好好读，至少现在我觉得第一篇的 input 的选择就有些不合理，可能犯了数据窥探 （data snooping）的错误。Alexei 目前还在研究量子计算 （Quantum Computing），和 NASA 合作把量子计算应用在一个含 60 个资产的组合优化上，节省了一半的计算时间。（是不是有点小题大做了？）&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2018 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Leif Andersen&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Michael Pykhtin&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Alexander Sokol&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Does Initial Margin Eliminate Counterparty Risk&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;They looked at the entire complexities of the margining process and modelled it mathematically. They looked at things from first principles and the result was amazing.&lt;/i&gt; -- Alexei Kondratyev&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/5371021/quants-of-the-year-leif-andersen-michael-pykhtin-and-alexander-sokol&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：深挖交易对手违约后的细节，将保证金风险期 (Margin Period of Risk, MPOR) 分解成四个时段来分析并量化了之前从来没有人去想要量化的结算风险 (settlement risk)。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2017 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Jean-Philippe Bouchaud&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Cleaning Correlation Matrices&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;It is really more of a physics approach, to let the data speak. A lot of models used in mathematical finance seem to be more driven by their convenience and the possibility to answer a question with a number, rather than taking the time and thinking about the problem.&lt;/i&gt; -- Bouchaud&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/risk-magazine/analysis/2479713/quant-of-the-year-jean-philippe-bouchaud&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用实证研究 (emprical reseach) 和数据，而不是用理论的公式来处理相关系数矩阵。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2016 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Alexander Antonov&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文 1&lt;/b&gt;：&lt;i&gt;The Free Boundary SABR Natural Extension to Negative Rates&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 2&lt;/b&gt;：&lt;i&gt;FVA for General Instruments&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 3&lt;/b&gt;：&lt;i&gt;Backward Induction for Future Values&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;In all his papers there is a clear practical problem, amazing mathematics and practical implementation. I think the combination of those three elements is really quant work at its best&lt;/i&gt;. -- Paul Glasserman&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/2442477/quant-of-the-year-alexandre-antonov&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：在负利率环境下的提出自由边界 (free boundary) SABR 模型，欧式期权仍有解析解或者近似解析解，校正快，计算敏感度也没有之前 shifted SABR 模型产生的跳跃的现象。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2015 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Christoph Burgard&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Mats Kjaer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Funding Strategies, Funding Costs&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;The way they have approached the problem is revolutionary. They have gone back to basics and modified the Black-Scholes PDE. And because it is intuitive, it is very revealing in that you can see the cashflows in a very transparent way&lt;/i&gt;. -- Andrew Green&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/derivatives/2387793/quants-year-christoph-burgard-and-mats-kjaer&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：为业界各说各话的融资估值调整 (Funding Valuation Adjustment, FCA) 提出一个健全的理论框架。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2014 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Michael Pykhtin&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文：&lt;/b&gt;&lt;i&gt;Exposure under Systemic Impact&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Systemic risk is at the forefront of everyone’s mind but is notoriously difficult to quantify. Pykhtin’s clear and pragmatic approach goes a long way towards setting a rigorous framework to measure and control it.&lt;/i&gt; -- Vladimir Piterbarg&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/2320285/quant-year-michael-pykhtin&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：专门针对于系统重要性交易对手 (Systemically Important Counterparty, SIC) 提出系统内错向风险 (Systemic Wrong-Way Risk, SWWR) 来量化它们违约造成的后果。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;I see this as an important part of my role – communicating these technical details, This dialogue between industry and regulator is an increasingly valuable function as rules and guidelines get more technical. It’s familiar to Pykhtin – from both sides.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2013 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Pierre Henry-Labordère&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文 1&lt;/b&gt;：&lt;i&gt;Being Particular about Calibration&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 2&lt;/b&gt;：&lt;i&gt;Cutting CVA&#39;s Complexity&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;It’s not complicated, actually. Using Malliavin is no harder than using the Itô lemma or the Girsanov theorem.&lt;/i&gt; -- Pierre Henry-Labordère&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/2232028/quant-of-the-year-pierre-henry-labordere-societe-generale&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用法国人逆天的数学来在金融界炫耀，当然大量的减少了两大难题的计算量，分别是“局部随机波动率 (Local Stochastic Volatility, LSV) 模型校正”和“组合层面的信用估值调整 (Credit Valuation Adjustment, CVA) 计算”。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2012 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Jesper Andreasen&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Brian Huge&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文 1&lt;/b&gt;：&lt;i&gt;Volatility Interpolation&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 2&lt;/b&gt;：&lt;i&gt;Random Grids&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;There are no fundamental laws handed down from God on clay tablets. I think there is still a tendency to see the world through models, forgetting they are only as good as their implementation.&lt;/i&gt; -- Jesper andreasen&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/2133160/quants-year-jesper-andreasen-and-brian-huge-danske-bank&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：1. 找到一种无套利的波动率插值方法；2. 提出一个模型校正、偏微分方程有限差分和蒙特卡洛模拟的一致离散化的想法。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2011 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Vladimir Piterbarg&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Funding Beyond Discounting Collateral Agreements and Derivatives Pricing&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;What Piterbarg is doing is rewriting Black-Scholes post-financial crisis. After the crisis, you can’t ignore the cost of funding in any asset class or you lose money.&lt;/i&gt; -- Alex Langnau&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/1934297/quant-year-vladimir-piterbarg-barclays-capital&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：对衍生品定价时引入融资成本 (cost of funding)，而且这些调整可以完美的添加到整套 Black-Scholes 框架中。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2010 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Marco Avellaneda&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;A dynamic Model for Hard-to-Borrow Stocks&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Short selling is a common scapegoat during financial crises.  In 2008, the ban on short selling was also used as a form of protectionism for propping up the stock of financial firms.&lt;/i&gt; -- Marco Avellaneda&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/1567801/quant-of-the-year-marco-avellaneda&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：对于难以去借 (hard-to-borrow) 来做空的股票，买卖权平价关系 (put-call parity) 不在适用。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2009 年 &lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Lozenro Bergomi&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Smile Dynamics III&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;His idea of directly modelling the joint dynamics of the spot and variance swap volatility is theoretically sound and practically easy to implement. His quant of the year award is well deserved.&lt;/i&gt; -- Alexander Lipton&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/1496978/quant-year-lorenzo-bergomi&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：提出可以控制远期方差微笑 (smile of forward variance) 而且可以校正于波动率指数 (volatility index, VIX) 期货和期权的模型。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2008 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Dilip Madan&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Calibrating and Pricing with Embedded Local Volatility Models&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;The most important moment of my career was my meeting with professor Dilip Madan. He is one of the few academics that are aware that the future does not behave like the past.&lt;/i&gt; -- Peter Carr&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/1498261/quant-year-dilip-madan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：提出一个内嵌型的局部波动率模型 (embedded local volatility model) 来对波动率指数期权、股票和利率混合产品进行定价。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2007 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Paul Glasserman&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Michael Giles&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Smoking Adjoints Fast Monte Carlo Greeks&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;The adjoint method accelerates the calculation of Greeks via Monte Carlo simulation by, in essence, rearranging the order of calculations, as compared to the standard method.&lt;/i&gt; -- Paul Glasserman&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/1498251/quants-year-paul-glasserman-and-michael-giles&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用反向方法 (adjoint method) 来计算复杂衍生物的敏感度，比传统的有限差分 (finite)、路径微分和似然比在计算敏感度的精度不减速度却爆升。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2006 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Vladimir Piterbarg&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Time to Smile&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;The global skew is some sort of average of local skew.&lt;/i&gt; -- Vladimir Piterbarg&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/1497820/quant-year-vladimir-piterbarg&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：提出参数平均 (parameter averaging) 的想法，将和时间有关的参数比如偏斜 (skew)、波动率转换成和时间无关的有效 (effective) 参数，加快了复杂模型的校正速度确没有降低校正质量。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2005 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Philipp Schönbucher&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;A Measure of Survival&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Schönbucher is one of the most innovative researchers in credit and many of today’s practitioners have benefited from his insights. His work on CDS option pricing is typically focused and thorough, and will form the backbone of future work on the subject.&lt;/i&gt; -- Richard Martin&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/1497632/quant-year-philipp-schonbucher&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用可违约资产 (defaultable asset) 当计价物 (numeraire)，在各种波动率设定下推导出类似 Black-Scholes 公式来对信用违约互换期权 (CDS option) 进行定价。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2004 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Michael Gordy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Random Tranches&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Gordy’s work in portfolio credit risk is both distinguished and topical, with many of his papers being among the cornerstones of modern credit risk management practices. His work at the Federal Reserve has been highly influential with academics and practitioners alike.&lt;/i&gt; -- Leif Andersen&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;https://www.risk.net/awards/1498479/quant-year-michael-gordy-us-fed&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：在符合巴塞尔大框架下，提出了一个简单公式，能计算证券化分层所需的监管资本，解决了巴塞尔对此类产品提出过于复杂要求的监管痛点。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2003 年 &lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Peter Carr&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Black-Scholes Goes Hypergeometric&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Peter has contributed more fundamental ideas to the area of mathematical finance in the past couple of years than anyone I am aware of. Peter lives, eats and breathes mathematical finance.&lt;/i&gt; -- Keith Lewis&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/derivatives/1506232/quant-of-the-year-peter-carr&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用不同的波动率设定来推广 Black-Scholes 公式，并推导欧式期权和障碍期权的解析解。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2002 年 &lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Richard Martin&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：Taking to the Saddle&lt;/p&gt;&lt;p&gt;In credit risk modelling, he’s the most switched on person I know. -- Tom Wilde&lt;/p&gt;&lt;p&gt;新闻链接：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;https://www.risk.net/awards/1506446/2002-winner-quant-year-richard-martin&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：用鞍点法 (saddle-point method) 代替了蒙特卡洛模拟来对损失事件建模和计算组合损失分布。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2001 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Leif Andersen&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Jesper Andreasen&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文 1&lt;/b&gt;：&lt;i&gt;Jumping Smiles&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文 2&lt;/b&gt;：&lt;i&gt;Static Barriers&lt;/i&gt;&lt;/p&gt;&lt;p&gt;无新闻链接&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点：1. 提出跳跃扩散 (Jump-Diffusion) 模型改进局部波动率模型，因为后者生成的微笑曲线随着时间越来越平，不符合实证观察。2. 引进跳跃扩散模型来静态对冲障碍期权。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2000 年&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;Alexander Lipton&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;论文&lt;/b&gt;：&lt;i&gt;Similarities Via Self-Similarities &lt;/i&gt;&lt;/p&gt;&lt;p&gt;无新闻链接&lt;/p&gt;&lt;p&gt;到处都下载不到他的论文，甚至都很难搜索出来。听说是关于介绍复杂衍生物的定价方法论，但是细节不清楚因此不评价。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这 20 年的年度宽客和他们得奖论文主要分成&lt;b&gt;市场风险 （Market Risk）&lt;/b&gt;，&lt;b&gt;信用风险&lt;/b&gt;及估&lt;b&gt;值调整 （Credit Risk, XVA）&lt;/b&gt;，&lt;b&gt;方法论&lt;/b&gt;和&lt;b&gt;机器学习&lt;/b&gt;这四大类，如下图所示：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-96a0f0d2e04b553f7e3e14ac628091e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1336&quot; data-rawheight=&quot;738&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-96a0f0d2e04b553f7e3e14ac628091e5&quot; data-watermark-src=&quot;v2-904cda177909bb1f1405e975ec48be2e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;从上表来看，研究市场风险的趋势在下降；研究估值调整，融资成本 (funding cost) 和保证金 (initial margin) 越来越多；研究机器学习的从今年刚开始有第一篇，按着大趋势以后会越来越多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;亮点一评&lt;/b&gt;：2007 年的最佳论文 &lt;i&gt;Smoking Adjoints: Fast Monte Carlo Greeks&lt;/i&gt; 真实好东西，这个 Adjoint 方法其实和机器学习里面的反向传播非常类似，这种反向求导数的方法统称 Adjoint Automatic Differentation, AAD，在金融和机器学习中有太多应用，比如百慕大期权蒙特卡洛求敏感度，比如组合层面的 XVA，比如深度神经网络的反向传播，只要求少量输出对大量输入的导数，AAD 在效率和速度上会让你重新认识这个世界。&lt;/p&gt;&lt;p&gt;&lt;b&gt;感叹一声&lt;/b&gt;：传统的衍生品的定价方法不存在了，现在定个价单单看产品的风险因子完全不够，交易对手、融资成本和保证金都会影响衍生品的价格。&lt;/p&gt;&lt;p&gt;&lt;b&gt;世界越来越复杂，但能用简单优雅的模型来描述他的宽客才配的起 Quant of the Year 这个称号！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文下载：&lt;/b&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289664&amp;amp;idx=1&amp;amp;sn=ec7e1db34342cd1ef8e4d4d468c35dd8&amp;amp;chksm=802e3ed5b759b7c33bd98ddd8efbd1a8e4a396ce4320cc0128cea164b4ed8a25f2c446b7ef8e&amp;amp;token=1918232931&amp;amp;lang=zh_CN#rd&quot;&gt;【重磅】年度宽客 （2000 - 2019）&lt;/a&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-12-04-51569176</guid>
<pubDate>Tue, 04 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>收缩夏普率(Deflated Sharpe Ratio)</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-04-51506740.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51506740&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Marcos Lopez de Prado。AQR Capital Management, LLC; Cornell University - Operations Research &amp;amp; Industrial Engineering; RCC - Harvard University&lt;/p&gt;&lt;p&gt;&lt;b&gt;摘要：&lt;/b&gt;随着近年来大型金融数据集，机器学习和高性能计算的出现，分析师可以回溯测试数百万（如果不是数十亿）可选投资策略。回测优化器搜索参数组合，以最大化策略的模拟历史性能，从而导致回测过度拟合。性能膨胀问题超出了回测范围。更一般地说，研究人员和投资经理往往只报告积极的结果，这种现象称为选择偏差。不控制特定发现中涉及的试验数量会导致过度乐观的性能预期。收缩夏普比率（DSR）纠正了两个主要的业绩膨胀来源：多重测试下的选择偏差和非正态分布的回报。通过这样做，DSR帮助将合法的实证结果与统计欺骗分开。&lt;/p&gt;&lt;p&gt;&lt;b&gt;关键词：夏普率，非正态性，概率夏普率，回测过拟合，最小跟踪记录长度，最小回测长度&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;今天的定量团队经常扫描数PB的财务数据，寻找肉眼看不见的模式。在这一努力中，他们得到了许多技术和数学领域的推动。大数据，机器学习，云网络和并行处理意味着可以在给定数据集上执行数百万分析，搜索有利可图的投资策略。从正确的角度来看，今天大多数量化团队使用的数据量与Netflix存储的内存相当，以支持全国范围内的视频流业务。与几十年前的情况相比，这是一个根本性的变化，当时典型的金融分析师会在包含数千个数据点的电子表格上运行基本算术计算。在本文中，我们将讨论在不控制选择偏差的情况下利用科学技术和高性能计算的一些意想不到的后果。虽然这些问题并非针对金融问题，但金融研究中的例子尤为丰富。&lt;/p&gt;&lt;p&gt;回测就是一个很好的例子。回溯测试是对特定投资策略过去如何表现的历史模拟。虽然回测是一种强大而必要的研究工具，但它也可以很容易地进行操作。在本文中，我们将论证在学术期刊和投资产品中发布的几乎所有回溯中缺少的最重要的信息是尝试的试验次数。没有这些信息，就无法评估回溯测试的相关性。坦率地说，无论报告的表现有多优秀，研究人员无法控制他或她的发现所涉及的搜索范围的回溯测试都是毫无价值的。投资者和期刊审查员每当提交回溯测试时都应该要求提供这些信息，尽管这样做不会完全消除危险。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;1.1多次测试&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;投资策略通常根据业绩统计来判断。由于任何测量都与误差幅度相关，因此选择统计模型的过程就是不确定性下的决策实例。我们永远无法确定真实的表现是否超过某个阈值，即使估计的表现是。出现两种错误：类型I错误，概率 &lt;equation&gt;\alpha&lt;/equation&gt; （也称为“显着性水平”）和类型II错误，概率 &lt;equation&gt;\beta&lt;/equation&gt; 。当我们选择应该被丢弃的策略 &lt;equation&gt;\beta&lt;/equation&gt; （“误报”）时会发生类型I错误，而当我们丢弃应该被选择的策略（“假阴性”）时会发生类型II错误。决策者往往更关注“误报”而不是“假阴性”。原因是，他们宁愿排除一个真正的策略而不是冒着添加虚假策略的风险。对于规避风险的投资者而言，失去的机会不如实际损失那么令人担忧。因此，标准做法是设计统计测试，将I类错误概率设置为低阈值（例如 &lt;equation&gt;\alpha=5\%&lt;/equation&gt; ），同时最大化测试的效力，定义为 &lt;equation&gt;1-\beta&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;现在假设我们有兴趣分析同一数据集上的多个策略，目的是为将来的应用选择最好的，或至少是一个好的策略。 然后出现一个奇怪的问题：当我们测试越来越多的策略，每个策略处于相同的显着性水平时，选择至少一个不良策略的总体概率会增加。 这被称为多重测试问题，它是如此普遍和臭名昭着，美国统计学会明确警告它的道德准则（美国统计协会，1997年，指南＃8）：&lt;i&gt;在分析的同一阶段对同一数据集运行多个测试会增加获得至少一个无效结果的机会。 从多个并行测试中选择一个“重要”结果会导致错误结论的严重风险。 在这种情况下未能公开测试的全部范围及其结果将极具误导性。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;1.2选择偏差&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对相同数据进行多测试的研究人员往往只发布那些通过统计显着性测试的人，而其他人则隐藏其他数据。由于没有报告负面结果，投资者只会接触到有偏见的结果样本。这个问题被称为“选择偏差”，它是由多个测试与部分报告相结合引起的，参见Roulston和Hand [2013]。它以许多不同的形式出现：分析师没有报告所进行的实验的全部范围（“文件抽屉效应”），仅发布“正面”结果的期刊（“发布偏见”），仅跟踪对冲表现的指数没有爆炸的资金（“生存偏见”），只公布他们（迄今为止）盈利策略（“自我选择偏见”，“回填”）等历史的经理人等所有这些现象都有共同之处是关键信息是否隐藏在决策者之外，其影响远大于预期的I类错误概率。忽视试验的全部程度使得不可能的更有可能，参见Hand [2014]。&lt;/p&gt;&lt;p&gt;在高通量筛选（HTS）研究项目中，例如用于发现药物治疗，化学化合物设计或微阵列基因测试的研究项目中，遇到假阳性的危险是显而易见的。 Bennett等人。 [2010]被授予2012年诺贝尔奖，因为它表明即使是鲑鱼的死脑也可能在多次MRI检测中显示出显着的活动。更严重的是，制药领域最近发生了许多根据已发表的试验看起来很棒的产品，但实际上这些产品实际上都令人失望。在许多情况下，这里的问题是通常只发布成功测试的结果，从而在系统中引入基本偏差。这些经历导致AllTrials运动（见http://alltrials.net），这将要求公开所有试验的结果。&lt;/p&gt;&lt;p&gt;尽管有高温超导项目的经验，但很少发现考虑到隐藏多次测试导致的误报率增加的金融研究。这个问题的严重程度导致一些研究人员解释Ioannidis [2005]，并得出结论“大多数声称的金融经济学研究成果可能都是假的”，见Harvey等人。 [2013]。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;1.3回测过拟合&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;什么构成合法的实证研究结果？经过充分的试验，保证研究人员总能找到一种误导性的盈利策略，这是一种误报。随机样本包含模式，通过大量策略进行系统搜索最终将导致识别出随机数据如何下降的机会配置。当优化一组参数以最大化回测的性能时，投资策略可能适合这种吸虫。这种现象称为“回测过度拟合”，我们将读者推荐给Bailey等人。 [2014]进行详细讨论。尽管优化后验测试的历史性能看起来很有希望，但为其提供动力的随机模式在未来不太可能重演，因此使该策略变得毫无价值。在这种情况下，结构性休息与战略的失败无关。&lt;/p&gt;&lt;p&gt;让我们举一个例子来阐明这一点。扔了一个公平的硬币十次后我们偶然得到一个序列，如{+，+，+，+，+， - ， - ， - ， - ， - }，其中“+”表示头，“ - ”表示尾。研究人员可以确定投注此硬币结果的最佳策略是在前五次投掷中预期为“+”，在最后五次投掷时为“ - ”（投资界的典型“季节性”参数） 。当我们再扔十次硬币时，我们可能会获得一个序列，如{ - ， - ，+， - ，+，+， - ， - ，+， - }，我们赢了5次，输了5次。该研究人员的投注规则是过度适应，因为它的设计是为了从过去只出现的随机模式中获利。该规则对未来没有任何影响力，无论它在过去看起来有多好。&lt;/p&gt;&lt;p&gt;投资经理之间的竞争意味着金融系列中信噪比低，增加了“发现”机会配置的概率，而不是实际信号。这意味着难以避免回测过度拟合。显然，这是一个关键问题，因为大多数投资决策涉及选择多个候选人或替代人选。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;1.4一个在线工具，以探索回测过拟合&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;劳伦斯伯克利国家实验室的研究人员开发了一个在线应用程序来探索回测过度拟合的现象。 它首先生成模仿股票市场价格历史的伪随机时间序列。 然后，它会找到优化策略性能的参数组合（保持期，止损，入场日，边等）。 该工具通常可以找到具有任何所需夏普比率的“盈利”策略。 然而，当这种“有利可图”的策略应用于第二个类似长度的伪随机时间序列时，它通常会挣扎，产生很少的收益甚至损失。 要试用此工具，请访问http://datagrid.lbl.gov/backtest&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;1.5在记忆效应下的回测过拟合&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了理解回测过度拟合对样本外性能的影响，我们必须引入一个重要的区别：有和没有记忆的财务过程。硬币无论是公平的还是有偏见的，都没有记忆。由于硬币“记住”之前的投掷，50％的头部比率不会出现。模式出现，但随着额外的投掷序列产生，它们被“稀释”。现在假设我们为该硬币添加了一个内存芯片，以便它记住之前的抛出并分配其质量以补偿其结果。这种记忆积极地“撤消”最近的历史模式，使得50％的头部比率迅速恢复。正如一个春天记住它的均衡位置一样，已经获得高度紧张的金融变量将会剧烈地恢复平衡，从而消除先前的模式。&lt;/p&gt;&lt;p&gt;“稀释”和“破坏”历史模式之间的差异是巨大的。稀释与您的赌注并不矛盾，但撤消会产生系统性违背你的赌注的结果！&lt;/p&gt;&lt;p&gt;回测过度拟合倾向于识别可从样本中最极端随机模式中获利的交易规则。 在存在记忆效应的情况下，必须撤消那些极端模式，这意味着回测过度拟合将导致损失最大化。 见Bailey等。 （2014）获得本声明的正式数学证明。 不幸的是，大多数金融系列都表现出记忆效应，这种情况使得回测过度拟合成为一种特别繁重的做法，并且可以解释为什么如此多的系统性基金无法像宣传的那样执行。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;1.6回测过拟合和保持方法&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;从业者尝试使用几种方法验证他们的回测。阻碍方法可能是最着名的例子，参见Schorfheide和Wolpin [2012]的描述。研究人员将可用样本分成两个非重叠子集：样本内子集（IS）和样本外子集（OOS）。我们的想法是使用IS子集发现模型，然后验证其在OOS子集上的一般性。 k倍交叉验证方法重复样本分裂k次的过程，这有助于减少估计误差的方差。然后，测试OOS结果的统计显着性。例如，我们可以拒绝OOS性能与IS性能不一致的模型。&lt;/p&gt;&lt;p&gt;从我们之前的讨论中，读者应该理解为什么阻碍方法不能阻止回测过度拟合：Holdout评估模型的一般性，就好像进行了单次试验一样，再次忽略了随着更多试验发生的误报的增加。如果我们应用holdout方法足够多次（比如说95％置信水平的20次），则不再可能出现误报：它们是预期的。我们应用坚持的次数越多，无效策略通过测试的可能性就越大，然后将作为单一试验结果发布。虽然模型验证技术与防止数据建议的测试假设相关（称为“类型III错误”），但它们不能控制回测过度拟合。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;1.7多测试的一般方法&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在前面的部分中，我们介绍了多测试的问题。我们已经解释了多个测试如何导致假阳性的可能性增加，并且隐藏多个测试的负面结果会导致选择偏差。我们已经看到，由于金融系列中存在记忆效应，回测过度拟合是一种特别昂贵的选择偏差形式。最后，我们讨论了为什么流行的模型验证技术无法解决这些问题。那么什么才能构成适当的策略选择方法呢？&lt;/p&gt;&lt;p&gt;自二十世纪早期以来，统计学家已经意识到多种测试和选择偏差问题，并且已经开发出解决它们的方法（例如经典的Bonferroni多重测试方法，以及Heckman关于选择偏差的工作 - 这使他获得诺贝尔奖奖）。然而，最近随着大数据集的增加，特别是生物信息学带来的挑战，解决多个测试问题已成为一个热门的研究课题，并取得了许多新的进展。参见，例如，Dudoit和van der Laan [2008]和Dmitrienko等人。 [2010]。为了控制家庭错误率 - 多次无效假设的至少一次检验将被错误拒绝的可能性 - 已经开发了多种方法，但研究人员还研究了错误率的替代定义（例如，Holm [1979]）。特别是，互补的错误发现率（例如Benjamini和Hochberg [1995]）引起了极大的关注。这不是查看错误拒绝真零假设的概率，而是考虑被拒绝假设为空的概率（在大多数科学情况下，如果不是在Neyman和Pearson最初开发他们的制造情况下，这可能更为相关）假设检验的方法）。&lt;/p&gt;&lt;p&gt;Bailey等人。 [2013]引入了一种新的交叉验证技术来计算回测过度拟合的概率（PBO）。 PBO评估策略选择过程是否有助于过度拟合，因为所选择的策略往往会低于样本中试验的中位数。 PBO是非参数的，可以应用于任何性能统计，但是它需要大量信息。我们将本文的其余部分专门用于开发一种新的参数化方法，以便在错误发现率方法的启发下，针对多次测试的影响校正夏普比率。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.多试验下的期望夏普率&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;夏普比率（SR）是使用最广泛的表现统计量（Sharpe [1966,1975,1994]）。 它根据风险回报评估投资，而不是资本回报。 投资组合经理热衷于提高他们的SR，以便在对冲基金研究等数据库中排名更高，并获得更多的资本分配。 设定SR的常数截止阈值，高于该阈值，投资组合经理或策略被考虑用于资本分配，导致前面讨论的选择偏差相同：随着考虑的候选项越来越多，误报率持续增长。&lt;/p&gt;&lt;p&gt;更正式地，考虑一组N个独立的回溯或与特定策略类相关的跟踪记录。 该集合的每个元素称为试验，它与SR估计值 &lt;equation&gt;\widehat{SR}_n&lt;/equation&gt;相关联，其中n = 1,2，...，N。假设这些试验的 &lt;equation&gt;\left\{ \widehat{SR}_n \right\}&lt;/equation&gt; 遵循正态分布，均值为 &lt;equation&gt;E\left[ \left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; ，方差为 &lt;equation&gt;V\left[\left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; 。这不是一个不合理的假设，因为“策略类”的概念意味着试验受到一些共同特征模式的约束。 换句话说，我们假设对于给定的策略类，存在与试验 &lt;equation&gt;\left\{ \widehat{SR}_n \right\}&lt;/equation&gt; 相关的均值和方差。 例如，我们预计高频交易试验的 &lt;equation&gt;E\left[ \left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; 大于自由宏观策略试验的E [SR]。 &lt;/p&gt;&lt;p&gt;我们希望在N次独立试验后得出预期的夏普比率。 考虑从正态分布中抽取的一组独立且相同分布的随机变量， &lt;equation&gt;y_n\sim N(\mu,\sigma^{2}),n=1,2,...,N&lt;/equation&gt; 。Z标准化 &lt;equation&gt;y_n&lt;/equation&gt; ， &lt;equation&gt;x_n\equiv \frac{y_n-\mu}{\sigma}&lt;/equation&gt; ,此处 &lt;equation&gt;x_n\sim N(0,1)\equiv Z&lt;/equation&gt; 。因此，集合 &lt;equation&gt;\left\{ y_n \right\}&lt;/equation&gt; 与集合 &lt;equation&gt;\left\{ \mu +\sigma x_n \right\}&lt;/equation&gt;相同。对 &lt;equation&gt;\sigma&amp;gt;0&lt;/equation&gt; ,集合 &lt;equation&gt;\left\{ \mu +\sigma x_n \right\}&lt;/equation&gt; 的顺序是不变的，结果：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;max\left\{ y_n \right\}=max\left\{ \mu+\sigma x_n \right\}=\mu+\sigma max\left\{ x_n \right\}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其中相同的元素是两个集合中的最大值。 因为数学期望算子E[.]是线性的，所以:&lt;/p&gt;&lt;p&gt;&lt;equation&gt;E\left[ max\left\{ y_n \right\}\right]=\mu+\sigma E\left[  max\left\{ x_n \right\} \right]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;Bailey等人。 [2014a]证明给定一系列独立且相同分布的标准正态随机变量 &lt;equation&gt;x_n\sim Z，n=1,2,...,N&lt;/equation&gt;，在 &lt;equation&gt;N\gg1&lt;/equation&gt; 时，该系列的预期最大值 &lt;equation&gt;E\left[  max\left\{ x_n \right\} \right]&lt;/equation&gt; 可以近似为&lt;/p&gt;&lt;p&gt;&lt;equation&gt;E\left[ max_N \right]=\left[ \left( 1-\gamma \right) Z^{-1}\left( 1-\frac{1}{N} \right)+\gamma Z^{-1}\left( 1-\frac{1}{N}e^{-1} \right)\right]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其中 &lt;equation&gt;\gamma=\lim_{n \rightarrow \infty}{\left[ \sum_{k=1}^{n}{\frac{1}{k}}-ln(n) \right]}\approx0.5772&lt;/equation&gt; 是Euler-Mascheroni常数，Z是标准正态分布的累积函数。&lt;/p&gt;&lt;p&gt;可以证明，在这些假设下， &lt;equation&gt;N\gg1&lt;/equation&gt; 次独立试验后 &lt;equation&gt;\left\{ \widehat{SR}_n \right\}&lt;/equation&gt; 的预期最大值可近似为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;E\left[ max\left\{ \widehat{SR} \right\} \right]\approx E\left[ \left\{ \widehat{SR}_n \right\} \right]+\sqrt{V\left[ \left\{ \widehat{SR}_n \right\} \right]}\left[ \left( 1-\gamma \right) Z^{-1}\left( 1-\frac{1}{N} \right)+\gamma Z^{-1}\left( 1-\frac{1}{N}e^{-1} \right)\right]...(1)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;等式(1)告诉我们，随着独立试验（N）的数量增加， &lt;equation&gt;\left\{ \widehat{SR}_n \right\}&lt;/equation&gt; 最大值的期望也会增加。图1展示了 &lt;equation&gt;E\left[ \left\{ \widehat{SR}_n \right\} \right]=0&lt;/equation&gt; ， &lt;equation&gt;V\left[ \left\{ \widehat{SR}_n \right\} \right]=1&lt;/equation&gt; 和N&lt;equation&gt;\in&lt;/equation&gt;[10,1000]。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7ede3cb96fc23af4dbf22ada2edcdecb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1128&quot; data-rawheight=&quot;898&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7ede3cb96fc23af4dbf22ada2edcdecb&quot; data-watermark-src=&quot;v2-c1f228d1f39d14e9b77d886926441df9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;因此，当我们解析更多候选时，获得良好的回溯结果或满足更好的投资组合经理并不奇怪。 这是纯随机行为的结果，因为即使没有与此策略类相关的投资技能，我们也会观察到更好的候选。 在下一节中，我们将使用此事实来调整策略拒绝阈值，因为独立试验的数量会增加。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.收缩夏普率(THE DEFLATED SHARPE RATIO)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当投资者选择表现最佳的策略而不是大量的替代方案时，会将自己暴露给“赢家的诅咒”。正如我们在上一节中所示，可能会选择具有夸大的夏普比率的策略。样本中的性能可能令人失望，这种现象在收缩估算文献中称为“回归均值”，参见Efron [2011]。在下文中，我们将提供夏普比率的估计量，其消除由多次测试引入的选择偏差，同时还校正非正常回报的影响。&lt;/p&gt;&lt;p&gt;在Bailey和LópezdePrado [2012a]中开发的概率夏普比率（PSR）计算真实SR高于给定阈值的概率。该拒绝阈值由用户确定。 PSR考虑了样本长度和回报分布的前四个时刻。正如一些研究所证明的那样，其原因在于短样本和非正态收益分布抽样的通胀效应。我们将感兴趣的读者引用到Lo [2002]，Mertens [2002]，LópezdePrado和Peijan [2004]，Ingersoll等人。 [2007]进行讨论。&lt;/p&gt;&lt;p&gt;我们之前的分析显示出由选择偏差引起的第二个膨胀来源。两种膨胀因素在混淆时，即使真正的SR可能为零，也会导致极高的估计值 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 。在本文中，我们提出了一个Deflated Sharpe Ratio（DSR）统计量，它可以纠正两种膨胀来源，定义如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{DSR}=\widehat{PSR}\left( SR_0 \right)=Z\left[ \frac{\left( \widehat{SR}-\widehat{SR}_0 \right)\sqrt{T-1}}{\sqrt{1-\hat\gamma_3\widehat{SR}+\frac{\hat{\gamma_4}-1}{4}}\widehat{SR}^{2}} \right]...(2)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其中 &lt;equation&gt;SR_0=\sqrt{V\left[ \left\{ \widehat{SR}_n \right\} \right]}\left[ \left( 1-\gamma \right) Z^{-1}\left( 1-\frac{1}{N} \right)+\gamma Z^{-1}\left( 1-\frac{1}{N}e^{-1} \right)\right]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;还使用了有关所选策略的信息： &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 是估计的SR，T是样本长度，偏度、峰度。&lt;/p&gt;&lt;p&gt;DSR是PSR，其中调整拒绝阈值以反映试验的多样性。 DSR背后的基本原理如下：给定一组SR估计值 &lt;equation&gt;\left\{ \widehat{SR}_n \right\}&lt;/equation&gt; ，即使真实SR为零，其预期最大值也大于零。 在零假设下，实际夏普比率为零，由于期望为0，式1可推出最大值的期望可以由式(2)的 &lt;equation&gt;\widehat{SR}_0&lt;/equation&gt; 估计。 实际上，随着更多独立试验的尝试（增加N），或者试验涉及更大的方差 &lt;equation&gt;V\left[\left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; ， &lt;equation&gt;\widehat{SR}_0&lt;/equation&gt; 增加。&lt;/p&gt;&lt;p&gt;注意，标准 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 是作为两个估计值的函数计算的：收益的均值和标准差。 DSR通过考虑另外五个变量来收缩SR：收益的非正态性（偏度、峰度），收益序列的长度（T），所测试的SR的方差 &lt;equation&gt;V\left[\left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; 以及所涉及的独立试验的数量（N）来 选择投资策略。&lt;/p&gt;&lt;p&gt;在最近的一项优秀研究中，Harvey和Liu [2014]，计算新策略的夏普比率必须克服的阈值，以证明更高的性能。 HL的解决方案基于Benjamini和Hochberg的框架。 HL阈值的作用类似于我们 &lt;equation&gt;E\left[ max\left\{ \widehat{SR} \right\} \right]\&lt;/equation&gt;在式1中扮演的角色。我们通过极值理论得出的。 DSR使用此阈值来确定特定的夏普比率估计值参见式2。 换句话说，考虑到目前为止进行的一系列试验，DSR计算特定 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 的统计学意义。 在本文中，我们将DSR应用于&lt;equation&gt;E\left[ max\left\{ \widehat{SR} \right\} \right]\&lt;/equation&gt;阈值，但也可以根据HL的阈值计算DSR。 从这个角度来看，这两种方法是互补的，我们鼓励读者使用阈值 &lt;equation&gt;E\left[ max\left\{ \widehat{SR} \right\} \right]\&lt;/equation&gt;和HL来计算DSR。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4数值例子&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;假设一位策略师正在研究资金市场的季节性模式。 他认为，美国财政部的拍卖周期造成效率低下，可以通过在拍卖前几天出售非现金债券来利用，并在拍卖后几天购买新发行。 他通过结合不同的拍卖前和拍卖后期间，期限，持有期，止损等来回溯这个想法的替代配置。他发现许多组合产生的年化 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 为2，特定的产生 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 在5年的每日样本中为2.5。&lt;/p&gt;&lt;p&gt;被此结果激动，他呼吁投资者要求资金运行这一策略，并认为年度SR为2.5必须具有统计意义。 投资者熟悉“投资组合管理杂志”最近发表的一篇论文，要求该策略师披露：i）进行独立试验的次数（N）; ii）回测结果的方差 &lt;equation&gt;V\left[\left\{ \widehat{SR}_n \right\} \right]&lt;/equation&gt; ; iii）样本长度（T）; iv）回报的偏度和峰度;分析师回应了： &lt;equation&gt;N=100,V\left[\left\{ \widehat{SR}_n \right\} \right]=0.5,T=1250,\hat{\gamma}_3=-3,\hat{\gamma}_4=10&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;不久之后，投资者拒绝了分析师的提议。 为什么？ 因为投资者已经确定这不是一个95％置信水平的合法经验发现。特别地：代入式1式2计算得到 &lt;equation&gt;\widehat{SR}_0\approx0.1132&lt;/equation&gt; ,非年化, &lt;equation&gt;\widehat{DSR}=0.9004&amp;lt;0.95&lt;/equation&gt; &lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f874ae5f3d4ebf4b580232bde1a4d979_r.jpg&quot; data-caption=&quot;图2  - 随着独立试验次数的增加，预期的最大夏普比率（左侧y轴）和收缩夏普比率（右侧y轴）&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;722&quot; data-rawheight=&quot;1022&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f874ae5f3d4ebf4b580232bde1a4d979&quot; data-watermark-src=&quot;v2-db7d9d4d4a7ee6ebd93ce2da4982463b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图表2描绘了拒绝阈值 &lt;equation&gt;\widehat{SR}_0&lt;/equation&gt; 如何随N增加，从而减少 &lt;equation&gt;\widehat{DSR}&lt;/equation&gt; 。 投资者已经认识到，与该策略相关的真实SR只有90％的可能性大于零。 如果策略师在仅运行N = 46次独立试验后发现了他的发现，那么投资者可能已经分配了一些资金，比如0.9505，高于95％的置信水平。&lt;/p&gt;&lt;p&gt;非正态性也在放弃这项投资报价方面发挥了作用。 如果该策略在N = 88次独立试验后表现出正太回报（偏度为0，峰度为3）。 如果不是正常回报并未如此夸大表现，投资者本来愿意接受更多的试验。 这个例子表明，正如DSR所做的那样，投资者共同考虑两种夏普率通胀来源至关重要。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;5.我们该何时停止测试？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这项研究的一个重要实际意义是多次测试是一个不应被滥用的有用工具。应提前仔细规划多次测试练习，以避免进行不必要的大量试验。投资理论，而不是计算能力，应该激励哪些实验值得进行。这引出了一个问题，应该尝试的最佳试验次数是多少？&lt;/p&gt;&lt;p&gt;这个关键问题的优雅答案可以在最优停止理论中找到，更具体地说是所谓的“秘书问题”，或者最佳选择的1 / e律，参见Bruss [1984]。这个问题存在很多版本，但关键概念是我们希望对所进行的试验数量施加成本，因为每次额外的试验都会不可避免地增加误报的可能性.1&lt;/p&gt;&lt;p&gt;在我们的讨论中，它转换如下：从策略配置集&lt;/p&gt;&lt;p&gt;这在理论上是合理的，随机抽取其中1/e（约37％）并测量它们的性能。之后，继续绘制并逐一测量该组中其他配置的性能，直到找到一个胜过前一个的配置。这是最佳试验次数，并且“迄今为止最好”的策略是应该选择的那种。&lt;/p&gt;&lt;p&gt;此结果提供了一个有用的经验法则，其应用程序超出了应该进行重新测试的策略配置的数量。它可以应用于我们测试多种替代方案的情况，目的是尽快选择近乎最好的方案，以便最大限度地减少误报的可能性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;6 总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;机器学习，高性能计算和相关技术已经推动了许多科学领域的发展。例如，美国能源部的SciDAC（通过先进计算的科学发现）计划使用terascale计算机“研究传统理论和实验方法不可解决的问题，对实验室研究有害，或者耗费时间或昂贵传统方式。&lt;/p&gt;&lt;p&gt;其中许多技术已经可供金融分析师使用，他们使用这些技术来寻找有利可图的投资策略。学术期刊经常发布回溯报告，报告这些模拟策略的表现。这些创新的一个问题是，除非遵循严格的科学协议，否则存在选择和发布误报的重大风险。&lt;/p&gt;&lt;p&gt;我们认为，选择偏差在金融文献中无处不在，其中经常发布回溯，而没有报告选择特定策略所涉及的试验的全部范围。更糟糕的是，我们知道在存在记忆效应的情况下，回测过度拟合会导致样本外的负面表现。因此，选择偏差与回测过度拟合相结合，误导投资者将资金分配给将系统性地亏损的策略。通常的免责声明“过去的表现并不能保证未来的结果”过于宽松，实际上很可能会产生不利的结果。&lt;/p&gt;&lt;p&gt;在本文中，我们提出了一个测试，以确定在纠正两个主要的绩效通胀来源后，估计的SR是否具有统计显着性：选择偏差和非正常回报。 Deflated Sharpe Ratio（DSR）包含有关未选择试验的信息，例如所进行的独立实验的数量和SR的方差，以及考虑到回报分布的样本长度，偏度和峰度。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;附录A：估计独立试验的数量&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;关键点在于理解用于计算 &lt;equation&gt;E\left[ max\left\{ SR_n \right\} \right]&lt;/equation&gt; 的N是&lt;b&gt;&lt;i&gt;独立试验&lt;/i&gt;&lt;/b&gt;的数量，我们可能进行了M个试验，但其中只有N个是独立的， &lt;equation&gt;N\leq M&lt;/equation&gt;。显然，使用M替代N计算将会高估 &lt;equation&gt;E\left[ max\left\{ SR_n \right\} \right]&lt;/equation&gt; 。所以给定M个试验，我们需要导出隐含的独立试验的数目， &lt;equation&gt;\widehat{N}&lt;/equation&gt; 。&lt;/p&gt;&lt;p&gt;实现这一目标的一条途径是考虑到试验之间的平均相关性， &lt;equation&gt;\rho&lt;/equation&gt; 。&lt;/p&gt;&lt;p&gt;首先，考虑M*M阶相关矩阵C，C的每个元素都是实数值， &lt;equation&gt;C=\left\{ \rho_{i,j} \right\}&lt;/equation&gt; 。令 &lt;equation&gt;\widetilde{C}&lt;/equation&gt; 为修正的相关矩阵，其中所有非对角线相关性已被常数值 &lt;equation&gt;\rho&lt;/equation&gt; 替换，比如对所有非对角元素 &lt;equation&gt;\rho_{i,j}=\rho，\forall i\ne j&lt;/equation&gt; 。然后我们将加权平均相关定义为这样的值：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-569ad79d67f4c3ed38ec9fef0dbe0e52_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;982&quot; data-rawheight=&quot;180&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-569ad79d67f4c3ed38ec9fef0dbe0e52&quot; data-watermark-src=&quot;v2-a957bbdcd8e76224f7ad88256f1bf24d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;换句话说，我们感兴趣的是找到这样常数值 &lt;equation&gt;\rho&lt;/equation&gt;，如果我们使所有非对角线相关性等于 &lt;equation&gt;\rho&lt;/equation&gt;，则上述二次型保持不变。 对于x等于单位向量 &lt;equation&gt;x=1_M&lt;/equation&gt; 的情况，二次型减少到所有项 &lt;equation&gt;\left\{  \rho_{i,j} \right\}&lt;/equation&gt; 的总和，导致等加权平均相关：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\rho=\frac{\sum_{i=1}^{M}\sum_{j=1}^{M}{\rho_{i,j}}-M}{M(M-1)}=\frac{2\sum_{i=1}^{M}\sum_{j=1+1}^{M}{\rho_{i,j}}}{M(M-1)}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其次，正确的相关矩阵必须是正定的，因此保证其所有二次型都是严格正的，特别是 &lt;equation&gt;1_M^TC1_M=1_M^T\widetilde{C}1_M&amp;gt;0&lt;/equation&gt; 。然后 &lt;equation&gt;1_M^T\widetilde{C}1_M=M+M(M-1)\rho&lt;/equation&gt;。其含义是平均相关受 &lt;equation&gt;\rho\in \left( -\frac{1}{M-1},1\right]&lt;/equation&gt; 限制, &lt;equation&gt;(M&amp;gt;1)&lt;/equation&gt; 。 试验次数越多，平均相关性越可能为正，对足够大的M，有 &lt;equation&gt;-\frac{1}{M-1}\approx0&amp;lt;\rho\leq1&lt;/equation&gt; 。&lt;/p&gt;&lt;p&gt;第三 ，我们可以看出 &lt;equation&gt;\rho\rightarrow1\Leftrightarrow N\rightarrow1&lt;/equation&gt;，相似地 &lt;equation&gt;\rho\rightarrow0\Leftrightarrow N\rightarrow M&lt;/equation&gt; 。因此，给定一个平价相关系数 &lt;equation&gt;\hat{\rho}&lt;/equation&gt;，我们可以在这两个极端结果之间进行插值以获得:&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{N}=\hat{\rho}+\left(1-\hat{\rho} \right)M&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;图4展示了 &lt;equation&gt;\left\{ M,\hat{\rho},\widehat{N} \right\}&lt;/equation&gt; 之间的关系,通过结合Fisher变换（见Fisher [1915]）可以进一步丰富他的方法，从而控制 &lt;equation&gt;\hat{\rho}&lt;/equation&gt; 估计误差的方差。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-78d11d48d7156fe774f6106584f84bcf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1126&quot; data-rawheight=&quot;884&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-78d11d48d7156fe774f6106584f84bcf&quot; data-watermark-src=&quot;v2-c4293c1c1259a97160c04dad5554ab5a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这种和其他“平均相关”方法在实际应用中是直观和方便的。然而，应该结合“平均相关”公式强调两个有问题的方面：首先，相关性是线性依赖性的有限概念。其次，在实践中M几乎总是超过样本长度T。然后估计平均相关性本身可能过度拟合。一般来说，对于短样本 &lt;equation&gt;T&amp;lt;\frac{M(M-1)}{2}&lt;/equation&gt; ，相关性矩阵将是数字病态的，并不能保证 &lt;equation&gt;1_M^TC1_M&amp;gt;0&lt;/equation&gt;。估计平均相关性是没有意义的，因为有更多的相关性 &lt;equation&gt;\left\{ \rho_{i,j}|i&amp;lt;j,i=1,2,...,M \right\}&lt;/equation&gt; 而不是独立的观察对！处理这个数值问题的一种方法是减小相关矩阵的维数（参见Bailey和LópezdePrado [2012b]的一个这样的算法），并计算该减少的正定矩阵的平均相关性。&lt;/p&gt;&lt;p&gt;另一种更直接的途径是使用信息论来确定。熵涉及比相关更深刻的冗余概念。我们将感兴趣的读者引用到关于数据压缩，总相关和多信息的文献中。 Watabane [1960]和Studený和Vejnarová[1999]。这些和其他标准信息理论方法产生对一组M个随机变量中的非冗余源的数量N的准确估计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>淮浩</author>
<guid isPermaLink="false">2018-12-04-51506740</guid>
<pubDate>Tue, 04 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>夏普率有效前沿</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-04-51492836.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51492836&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Marcos Lopez de Prado。AQR Capital Management, LLC; Cornell University - Operations Research &amp;amp; Industrial Engineering; RCC - Harvard University&lt;/p&gt;&lt;p&gt;&lt;b&gt;摘要：&lt;/b&gt;我们评估在存在非正常收益时估计的夏普比率超过给定阈值的概率。我们表明，这种新的不确定性调整投资技能指标（称为概率夏普比率，或PSR）有许多重要的应用：首先，它允许我们建立所需的跟踪记录长度，以拒绝测量夏普比率低于的假设具有给定置信水平的特定阈值。其次，它模拟了跟踪记录长度和不期望的统计特征之间的折衷（例如，具有正超额峰度的负偏度）。第三，它解释了为什么具有这些不良特征的跟踪记录将受益于具有最高采样频率的报告性能，从而不违反IID假设。第四，它允许计算我们称之为夏普比率效率前沿（SEF），这使我们能够在非正态杠杆收益率下优化投资组合，同时结合从跟踪记录长度得出的不确定性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;关键词：夏普比率，有效前沿，IID，正态分布，偏度，超额峰度，跟踪记录。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Roy（1952）是第一个提出风险回报率以评估策略绩效的人。Sharpe（1966）将Roy的观点应用于Markowitz的均值-方差框架，这已经成为最着名的绩效评估指标之一。LópezdePrado和Peijan（2004）表明，隐含的假设（收益服从独立同分布、正态分布）可能隐藏大量的缩减风险，特别是在对冲基金策略的情况下。&lt;/p&gt;&lt;p&gt;知名学者（其中的夏普）试图说服投资界反对使用夏普率违反其基本假设。尽管存在许多不足，夏普比率已成为绩效评估的“黄金标准”。夏普比率受到对冲基金策略（特别是高频策略）固有的一些统计特征的极大影响，如非正态性和减少的粒度（由于回报汇总）。因此，这些策略的夏普比率往往会“膨胀”。Ingersoll，Spiegel，Goetzmann和Welch（2007）解释说，采样回报更频繁地降低了一些操纵策略对夏普比率的通胀效应。&lt;/p&gt;&lt;p&gt;我们接受向投资者重申夏普比率缺陷的徒劳无益。相反，本文的第一个目标是引入一种称为概率夏普比率（PSR）的新指标，它可以纠正这些膨胀效应。当回报分布的统计特征否则会使夏普比率膨胀时，这种不确定性调整的夏普比率需要更长的跟踪记录长度和/或采样频率。这就引出了我们的第二个目标，即如果我们学会要求适当的跟踪记录长度，夏普比率仍然可以证明技能。我们正式定义了在给定置信度下拒绝“超出给定阈值的技能”的零假设所需的最小跟踪记录长度（MinTRL）的概念。由于其特有的非正常回报，在备选投资的背景下，跟踪记录应该多长时间以证明技能的问题尤为重要。尽管如此，我们将从一般角度讨论“跟踪记录长度”这一主题，使我们的结果适用于任何类型的策略或投资。&lt;/p&gt;&lt;p&gt;本文的第三个目标是引入夏普比率效率前沿（SEF）的概念，该概念允许在非正态杠杆收益率下选择最优投资组合，同时考虑与跟踪记录长度相关的样本不确定性。本文提出的投资组合优化方法与其他高阶矩方法的不同之处在于，通过夏普比率估计器的标准偏差引入了偏度和峰度。这避免了必须对较高时刻在效用函数中具有的相对权重做出任意假设。我们认为从业者会发现这种方法很有用，因为夏普比率已经在一定程度上成为投资者使用的默认效用函数。SEF可以直观地向投资者解释为一组投资组合，可以在不同的置信度下最大化预期的夏普比率。最大夏普比率组合是SEF的成员，但它可能与最大化PSR的投资组合不同。虽然前者的投资组合忽略了围绕最大化夏普的最终信心带，但后者是最大化技能概率的投资组合，同时考虑到非正态性和跟踪记录长度对夏普比率信心带的影响。&lt;/p&gt;&lt;p&gt;我们没有明确解决连续条件化过程的情况。相反，我们依赖于Mertens（2002），他“最初”假设IID非正常回报。该框架与投资组合经理的技能和风格在观察期间不会发生变化的情况一致。幸运的是，Opdyke（2007）已经证明，Mertens方程具有限制分布，在更一般的静止和遍历返回假设下是有效的，而不仅仅是IID。因此，我们的结果在这样的条件下是有效的，超出了较窄的IID假设。&lt;/p&gt;&lt;p&gt;本文的其余部分安排如下：第2部分介绍了允许我们实现三个既定目标的理论框架。第3节介绍了概率夏普比率（PSR）的概念。第4节涉及应用这个概念来回答给定置信水平的可接受的跟踪记录长度的问题。第5节提供了数值例子，阐明了这些概念如何相互关联并可以在实践中使用。第6节将我们的方法应用于对冲基金研究数据。第7节通过引入夏普比率有效前沿（SEF）的概念进一步论证了这一论点。第8节概述了结论。两篇数学附录证明了论文正文中的陈述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.框架&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;2.1夏普率点估计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们认为夏普比率是投资技巧的不够好的衡量标准。 为了理解为什么，我们需要回顾它的理论基础，以及它假设正态回报的含义。特别是，将看到非正态性可能会增加夏普比率估计的方差，从而降低对其点估计的置信度。 如果没有得到解决，这意味着投资者可能会在不同的置信区间将夏普比率的估计值进行比较。&lt;/p&gt;&lt;p&gt;夏普率的目的是为了评估特定策略或投资者的技巧：&lt;/p&gt;&lt;p&gt;假设策略的超额收益 &lt;equation&gt;r_t&lt;/equation&gt; 是独立同分布的，服从正态分布 &lt;equation&gt;N(\mu,\sigma^{2})&lt;/equation&gt; ：&lt;/p&gt;&lt;p&gt;夏普率定义为&lt;equation&gt;SR=\frac{\mu}{\sigma}&lt;/equation&gt; ，&lt;b&gt;最大化夏普率等价于最小化超额收益小于0的概率。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于实际值 &lt;equation&gt;\mu,\sigma&lt;/equation&gt; 通常是未知的，因此无法确定夏普率的真实值。不可避免的后果是夏普率计算可能是实质估计误差的主题。我们将在下面讨论如何在不同的假设下确定它们。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2正态收益假设&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与任何估计量一样，夏普率有它的概率分布，在此节中我们将在收益服从独立同分布的正态分布假设的情况下推导。依据中心极限定理：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\sqrt{n}\left( \hat{\mu}-\mu \right)\overset{a}{\rightarrow}N(0,\sigma^{2})&lt;/equation&gt; , &lt;equation&gt;\sqrt{n}\left( \hat{\sigma^{2}}-\sigma^{2} \right)\overset{a}{\rightarrow}N(0,2\sigma^{4})&lt;/equation&gt; 其中 &lt;equation&gt;\overset{a}{\rightarrow}&lt;/equation&gt; 表示渐进收敛。&lt;/p&gt;&lt;p&gt;令 &lt;equation&gt;\theta=\begin{pmatrix} \mu \\\sigma^{2} \end{pmatrix}&lt;/equation&gt; 是正态分布参数的列向量， &lt;equation&gt;\widetilde{\theta}=\begin{pmatrix} \tilde{\mu} \\\tilde{\sigma^{2}} \end{pmatrix}&lt;/equation&gt;是其估计，如果收益独立同分布，由中心极限定理，其中 &lt;equation&gt;V_\theta = \begin{bmatrix} \sigma^{2} &amp;amp;0 \\   0&amp;amp;2\sigma^{4}  \end{bmatrix}&lt;/equation&gt; 是对 &lt;equation&gt;\theta&lt;/equation&gt; 的估计误差的方差。&lt;/p&gt;&lt;p&gt;数学基础：delta方法可参考&lt;/p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Delta_method&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;https://en.wikipedia.org/wiki/Delta_method&lt;/a&gt;&lt;p&gt;单变量delta方法：随机变量 &lt;equation&gt;X_n&lt;/equation&gt;满足 &lt;equation&gt;\sqrt{n}(X_n-\theta)\overset{D}{\rightarrow}N(0,\Sigma)&lt;/equation&gt;,其中n是观测数， &lt;equation&gt;\Sigma&lt;/equation&gt;是对称半正定协方差矩阵，则 &lt;equation&gt;\sqrt{n}\left( g(X_n)-g(\theta) \right) \overset{D}{\rightarrow}N\left( 0,\sigma^{2}\left( \frac{\partial g}{\partial \theta} \right)^{2} \right)&lt;/equation&gt; , &lt;equation&gt;\overset{D}{\rightarrow}&lt;/equation&gt; 表示分布收敛&lt;/p&gt;&lt;p&gt;多变量delta方法：根据定义，一致估计量B在概率上收敛于其真值β，并且通常可以应用中心极限定理来获得渐近正态性，&lt;equation&gt;\sqrt{n}\left( B-\beta \right) \overset{D}{\rightarrow}N(0,\Sigma)&lt;/equation&gt; ,其中n是观测数， &lt;equation&gt;\Sigma&lt;/equation&gt;是对称半正定协方差矩阵,则 &lt;equation&gt;\sqrt{n}\left( h(B)-h(\beta) \right) \overset{D}{\rightarrow}N\left( 0,\left[ \bigtriangledown h(\beta) \right]^T\cdot\Sigma\cdot\bigtriangledown h(\beta) \right)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;令 &lt;equation&gt;\widetilde{SR}=g(\tilde{\theta})&lt;/equation&gt; 表示夏普率的估计，其中 &lt;equation&gt;g(.)&lt;/equation&gt; 是夏普率的估计函数，运用delta方法，有：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\sqrt{n}\left( g(\hat{\theta})-g(\theta) \right)\overset{a}{\rightarrow}N(0,V_g),V_g=\left( \frac{\partial g}{\partial \theta}\right)^{T}V_\theta\frac{\partial g}{\partial \theta}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;equation&gt;\left( \frac{\partial g}{\partial \theta} \right)^T=\begin{pmatrix}  \frac{\partial g}{\partial \theta}&amp;amp; \frac{\partial g}{\partial \sigma^{2}} \end{pmatrix}=\begin{pmatrix}  -\frac{1}{\sigma^{2}}&amp;amp; -\frac{\mu^3}{2\sigma^{3}} \end{pmatrix}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;equation&gt;V_g = \left( \frac{\partial g}{\partial \theta} \right)^{2}\sigma^{2}+ \left( \frac{\partial g}{\partial \sigma^{2}} \right)^{2}2\sigma^{4}=1+\frac{1}{2}SR^{2}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;这意味着： &lt;equation&gt;(\widehat{SR}-SR)\overset{a}{\rightarrow}N(0,\frac{1+\frac{1}{2}SR^{2}}{n})&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;如果q是每年观测的次数，年化夏普率的点估计为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{SR}\overset{a}{\rightarrow}N\left( \sqrt{q}SR,qV_g \right)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;在收益服从独立同分布的正态分布的假设下，SR估计量服从正态分布。&lt;/p&gt;&lt;p&gt;结果很有趣，表明在其他条件不变的情况下，通常我们会更喜欢有较长历史记录的投资。这并不令人惊讶，在对冲基金行业中通常会要求提供超过3年或更多的月度收益的业绩记录。&lt;/p&gt;&lt;p&gt;此外&lt;equation&gt;(\widehat{SR}-SR)\overset{a}{\rightarrow}N(0,\frac{1+\frac{1}{2}SR^{2}}{n})&lt;/equation&gt; 精确的告诉我们更大的n如何影响夏普率估计的方差&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3夏普率和非正态性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2.3节举例介绍了具有非常不同风险概况点非正态分布都可以有相同的SR，略过。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.4包括非正态性：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上一节论述了具有非常不同风险概况的非正态分布都可以具有相同的SR。在本节中，我们将讨论一个关键事实，即虽然偏度和峰度不会影响SR的点估计，但它会极大地影响其置信区间，从而影响其统计显着性。当然，按照习惯，使用SR的点估计来对投资进行排序时，这一事实显然具有可怕的影响。&lt;/p&gt;&lt;p&gt;Mertens[2002]得出结论，关于收益的正态性假设可能会被忽略，并且估计的夏普比率将遵循带有参数的正态分布：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;(\widehat{SR}-SR)\overset{a}{\rightarrow}N\left( 0,\frac{1+\frac{1}{2}SR^{2}-\gamma_3SR+\frac{\gamma_4-3}{4}SR^{2}}{n} \right)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;好消息是，即使收益并不遵循正态分布，SR遵循正态分布。坏消息是，虽然大多数投资者更喜欢在均值-方差框架中工作，但他们需要考虑非正态性(当然，还要考虑样本长度)。图5说明了偏度和峰度的组合如何影响SR估计量的标准差。这具有严重的暗示，即非正态分布可能严重夸大SR估计值，以至于具有高SR可能不足以保证其统计显着性。&lt;/p&gt;&lt;p&gt;SR估计量的标准差对偏度和峰度敏感，对于SR = 1，我们看到对偏度特别敏感，正如从上式预期的那样。&lt;/p&gt;&lt;p&gt;Christie[2005]使用GMM方法得出一个限制分布，它只假设收益具有平稳性和遍历性，从而允许时变条件波动率，序列相关性甚至非独立同分布的收益。令人惊讶的是，Opdyke[2007]证明了Mertens[2002]和Christie[2005]实际上是相同的。对于Mertens博士而言，他的结果似乎在更一般的平稳和遍历假设下是有效的，而不仅仅是独立同分布。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.5置信区间&lt;/b&gt;&lt;/p&gt;&lt;p&gt;们已经提到偏度和峰度会影响SR估计量的置信区间，但是我们没有明确地推导出它的表达式。在一些代数运算之后，式8给出了 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 的估计标准差为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{\sigma}_{ \widehat{SR}}=\sqrt\frac{{1-\hat{\gamma_3 }\widehat{SR} +\frac{\hat{\gamma_4}-1}{4}\widehat{SR}^{2}}}{n-1}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;真实值受SR估计值的限制，具有显著性水平 &lt;equation&gt;\alpha&lt;/equation&gt; ，即：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;Pro b\left[ SR\in\left( \widehat{SR}-Z_{\frac{\alpha}{2}}\widehat{\sigma}_{\widehat{SR}}, \widehat{SR}+Z_{\frac{\alpha}{2}}\widehat{\sigma}_{\widehat{SR}}   \right) \right]=1-\alpha&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;一般而言，仅仅通过比较各自的SR的点估计量来判断策略的性能是错误的，而不考虑每次计算中涉及的估计误差。相反，我们可以用概率术语比较 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 的转换，我们将在下面定义。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.概率夏普率&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在我们已经得出了SR的置信区间的表达式，我们已经准备好瞄准引言中所述的第一个目标：提供SR的去膨胀估计。给定预定的基准夏普比率 &lt;equation&gt;SR^*&lt;/equation&gt;,观测得到的夏普率 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 可以用概率表示为：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{PSR}\left( SR^{*} \right)=Prob\left[ \widehat{SR}&amp;gt;SR^{*}\right]=1-\int_{-\infty}^{SR^{*}} Prob(\widehat{SR})d\widehat{SR}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;需要解答的问题是 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 大于假设 &lt;equation&gt;SR^{*}&lt;/equation&gt; 的概率是多少，我们在之前的章节中已经学到了什么，我们提出：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{PSR}\left( SR^{*} \right)=Z\left[ \frac{(\widehat{SR}-SR^{*})\sqrt{n-1}}{\sqrt{1-\hat{\gamma_3 }\widehat{SR} +\frac{\hat{\gamma_4}-1}{4}\widehat{SR}^{2}}} \right]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;此处Z[.]是标准正态分布的累积分布函数(CDF)。对于给定的 &lt;equation&gt;SR^{*}&lt;/equation&gt; , &lt;equation&gt;\widehat{PSR}&lt;/equation&gt; 随着 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; 的增加而增加(在原始采样频率中，即非年化)，随着或较长的跟踪记录(n)或正偏度的收益( &lt;equation&gt;\hat{\gamma_3}&lt;/equation&gt; )而增加，但是随着较胖的尾部( &lt;equation&gt;\hat{\gamma_4}&lt;/equation&gt; )而减小。由于对冲基金策略通常以负偏度和肥尾(Brooks and Kat[2002]，LópezdePrado and Rodrigo[2004])为特征，因此夏普比率倾向于“膨胀”。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\widehat{PSR}\left( SR^{*} \right)&lt;/equation&gt; 将这些特征考虑在内，并提供以概率技巧表示的经过纠正的非时态性衡量指标。“一位对冲基金经理在他的业绩记录中应用PSR后表示这方法应被命名为“夏普剃刀”。找到具有不规则交易频率的策略并不罕见，例如每周可能不交易的策略。这在计算年化夏普比率时存在问题，并且在不规则投注的背景下如何衡量skill没有达成共识。因为PSR以概率术语衡量skill，所以它对日历惯例是不变的。所有计算均以数据的原始频率完成，并且没有年化。这是在具有不规则频率的策略的背景下优选PSR与传统的年化SR的另一个支持点。&lt;/p&gt;&lt;p&gt;第2.3节指出，偏度和峰度的估计可能包含很大误差。如果研究人员认为这是他们估计的情况，我们建议输入下限代替式(8)中的上限，对于一定的置信度。但是，如果这些估计被认为是相当准确的，则不需要这种“最坏情况分析”。&lt;/p&gt;&lt;p&gt;一个例子将阐明PSR如何揭示SR应被解雇的信息。假设对冲基金根据过去两年的月度跟踪记录向你提供图6中显示的统计数据。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c8531ee06a30861d7f7365fdc29dbc61_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;474&quot; data-rawheight=&quot;236&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c8531ee06a30861d7f7365fdc29dbc61&quot; data-watermark-src=&quot;v2-18ea9d4bb2fa77854678748a0302f2d9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;乍一看，过去两年的年化夏普比率为1.59，似乎足以拒绝这结果完全是靠纯粹的运气实现的假设。问题是，“考虑跟踪记录的非正态性，长度和采样频率，这个年化的夏普比率膨胀了多少？&lt;/p&gt;&lt;p&gt;让我们首先将这个性能与无技能基准 &lt;equation&gt;SR^{*}=0&lt;/equation&gt; 进行比较，同时假设为正态分布( &lt;equation&gt;\hat{\gamma_3}=0,\hat{\gamma_4}=3&lt;/equation&gt; )，原始采样频率是月度的，代入式(11)的 &lt;equation&gt;SR^{*}=0.458&lt;/equation&gt;,这产生了一个令人放心的 &lt;equation&gt;\widehat{PSR}(0)=0.982&lt;/equation&gt; 。但是，当我们结合偏度和峰度信息时， &lt;equation&gt;\hat{\gamma_3}=-2.448,\hat{\gamma_4}=10.164&lt;/equation&gt; ,此时 &lt;equation&gt;\widehat{PSR}(0)=0.913&lt;/equation&gt; ,在95％的置信水平下，我们会首先接受这个记录，但不能拒绝这个夏普率在第二种情况下技能较低的假设。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-db808acacb9b1ceb3c0319e0cc83dc24_r.jpg&quot; data-caption=&quot;黑色虚线是正态分布的密度函数，它与图6中的Mean和StDev值相匹配。黑色线代表两个正态分布的混合，它们匹配表1中的所有四个矩(u1,u2,delat1,delta2,p)=(-0.1,0.06,0.12,0.03,0.15)。显然，正态分布假设是错误的，因为这会忽略关于对冲基金潜在亏损的关键信息。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;670&quot; data-rawheight=&quot;522&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-db808acacb9b1ceb3c0319e0cc83dc24&quot; data-watermark-src=&quot;v2-254750466b928025c80a8591fbb82a9a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;1.59的年化夏普比率隐藏的是从不利分布中得出的回报的相对较小的概率(15%)（混合分布模式的负数倍）。这通常是具有负偏度和正超额峰度的跟踪记录中的情况，与式(11)中的符号一致。这并不是说1.59夏普比率的记录是毫无价值的。事实上，如果有3年而不是2年的跟踪记录， &lt;equation&gt;\widehat{PSR}(0)=0.953&lt;/equation&gt; ,足以拒绝无技能的假设考虑到前四阶矩后的表现。换句话说，较长的跟踪记录可能能够补偿非正态分布收益引入的不确定性。下一节将量化非正态性与跟踪记录长度之间的“补偿效应”。&lt;/p&gt;&lt;p&gt;PSR考虑了不同偏度和峰度以及跟踪记录长度的SR点估计的统计准确性。从这个意义上讲，它包含了有关回报非正态性的信息。但是，我们提醒读者，PSR不会，也不会尝试将更高阶矩的效果纳入偏好。投资者仍然只关心均值和方差，正确地在存在偏斜和峰度的情况下，它的估计可能是不准确和“讨人喜欢”。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4.跟踪记录长度&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;理解夏普率估计受到重大误差的问题引出了一个问题：“为了获得其夏普比率高于给定阈值的统计信心，跟踪记录需要多长时间？”在数学术语中，因为这相当于问：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\left\{ n|\widehat{PSR}(\widehat{SR})&amp;gt;1-\alpha \right\}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;最小跟踪记录长度（MinTRL）：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;minTRL=n^{*}=1+\left[ 1-\hat{\gamma_{3}}\widehat{SR}+\frac{\hat{\gamma_4}-1}{4} \widehat{SR}^{2}\right]\left( \frac{Z_{\alpha}}{\widehat{SR}-SR^{*}} \right)^{2}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;我们再次观察到，更小的 &lt;equation&gt;\widehat{SR}&lt;/equation&gt; ，更大的负偏度、更大的肥尾，或置信水平越高，便需要更长的跟踪记录。第一个实际意义是如果跟踪记录比小，那么我们没有足够的置信度认为观测到的高于指定的阀值。第二个实际意义是投资组合经理因非正态收益而受到处罚，但可以随着时间的推移重新获得投资者的信心(通过延长其业绩记录的长度)。&lt;/p&gt;&lt;p&gt;值得注意的是，以观察数量表示，而不是年度或日历术语。此时需要注意的是：式11和式313建立在式8上的渐进分布，基于中心极限定理，通常认为中心极限定理(CLT)适用于超过30个观测值的样本(Hogg和Tanis[1996] )。因此，即使MinTRL可能需要少于2.5年的月度数据，或0.5769年的每周数据，或0.119年的每日数据等。在式13中输入的矩也必须在更长的序列上计算，也可以在式（13）中输入必须在更长的系列上计算，以便适用中心极限定理。这符合从业者在尽职调查过程中要求类似长度的标准做法。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;5.数值例子&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们在前面几节中学到的所有内容都可以通过一些实际例子来说明。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9139421306afe6a44eba2c8d914e2585_r.jpg&quot; data-caption=&quot;图8显示了基于日IID正态回报的95％置信水平下测量(行)和基准(列)的各种组合所需的最小跟踪记录长度(MinTRL)。例如，第五栏告知我们，观测年化夏普率2需要2.73年的记录，在95％的置信水平下被认为实际夏普率大于1。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;266&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9139421306afe6a44eba2c8d914e2585&quot; data-watermark-src=&quot;v2-3ee3806c00dbb451627c08952a29358a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于年化夏普率值为2的周频策略，MinTRL是多少？&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5b4eb3f3077bd1f9f87349370add9bcc_r.jpg&quot; data-caption=&quot;图9显示，对于周频IID正太回报，则要求2.83年的跟踪记录长度。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;292&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5b4eb3f3077bd1f9f87349370add9bcc&quot; data-watermark-src=&quot;v2-dd25af36e89be821e62df48ba97f95cc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e520f7666dd27892bb7eaf522638e3d_r.jpg&quot; data-caption=&quot;图10表明，如果我们使用月频IID正态回报，则需要的跟踪记录长度增加到3.24年，与日频IID正态回报相比增加了18.7％。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;298&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0e520f7666dd27892bb7eaf522638e3d&quot; data-watermark-src=&quot;v2-3932df89c11c51603c6065fd0e857e48&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-74ee65e237937179152b7a99dc0202b5_r.jpg&quot; data-caption=&quot;频率为月回报，Brooks和Kat[2002]报告称HFR综合对冲基金回报指数展示了偏度为-0.72，峰度为5.78。在此情况下，图11告诉我们，业绩记录现在应该是4.99年 这比我们正态月度回报所需的时间长54％，比正态日回报所需的时间长82.8％。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;322&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-74ee65e237937179152b7a99dc0202b5&quot; data-watermark-src=&quot;v2-fcc459429f5e1f2d952ba55993a624e0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;6.对冲基金技巧风格&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们现在准备在真实数据上运行我们的模型。图12将我们的方法应用于2000年1月1日至2011年5月1日的HFR月度指数(134个月观测值，或11.167年)。MinTRL以年为单位表示，置信水平为95％。PSR(0)&amp;gt;0.95表示SR大于0，置信水平为0.95。类似地，PSR(0.5)&amp;gt; 0.95意味着SR大于0.5(年化)，置信水平为0.95。概率夏普比率考虑了跟踪记录中存在的多个统计特征，例如其长度，频率和与异常偏差(偏度、峰度)。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-982147288cfd93926ee8506f4f8dfbe9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;678&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-982147288cfd93926ee8506f4f8dfbe9&quot; data-watermark-src=&quot;v2-c87664294410af7f4557bfc182db5174&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;因为我们的样本包含11.167年的月度观察结果，在95％置信度下，PSR(0)&amp;gt; 0.95等价于MinTRL(0)&amp;lt;11.167，PSR(0.5)&amp;gt; 0.95等价于MinTRL(0.5)&amp;lt; 11.167。我们的计算表明，大多数对冲基金风格证明了一定程度的技能，即他们的SR高于零基准。然而，考虑到PSR(0.5)，我们观察到只有9种风格指数证实了投资技巧：Distressed Securities 、Equity Market Neutral、 Event Driven、Fixed Asset-Backed 、Macro、Market Defensive、Mortgage Arbitrage、Relative Value、Systematic Diversified&lt;/p&gt;&lt;p&gt;这并不是说只考虑实现上述9种风格的对冲基金。我们对指数进行了分析，而不是特定的跟踪记录。但是，可以说，在分析上述9种风格以外的风格时，应特别注意。我们本来希望通过结构突破测试来完成这个分析，但是数据的数量和质量不允许有意义的估计。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;7.夏普率有效前沿&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;PSR根据不确定性调整后的SR评估个人投资的表现。将此参数扩展到投资组合优化或资本分配上下文似乎很自然我们将建立投资组合风险回报的均值-方差边界，而不是投资组合回报的均值-方差边界。继Markowitz(1952)之后，投资组合w属于有效前沿，如果它提供最大的预期超额资本回报E(rw),受到这些投资组合超额收益不确定性水平的影响( &lt;equation&gt;\widehat{\sigma}_{rw}&lt;/equation&gt; )&lt;/p&gt;&lt;p&gt;同样，我们将夏普比率效率前沿(SEF)定义为能够提供最高预期风险回报率（以夏普比率表示）的投资组合集{SEF}，这取决于这些投资组合超额的不确定性水平风险回报率（夏普比率的标准差）。但是，为什么我们会计算一个有效的夏普比率前沿，同时接受回报（r）是非正态的？因为绝大多数投资者使用SR作为其效用函数的代理。即使他们不关心更高矩本身，他们也必须使用三阶矩和四阶矩去除他们对SR的估计(均值-方差度量)。其他一些原因使这项分析变得有趣：&lt;/p&gt;&lt;p&gt;1. SEF在风险回报率(或夏普率)空间而非资本回报率范围内处理效率。与资本回报不同，夏普率对杠杆率不变。&lt;/p&gt;&lt;p&gt;2.即使回报是非正态分布的，夏普比率的分布遵循正态分布，因此是一个有效的前沿– 分析风格仍然有意义，只要过程是独立同分布，由于中心极限定理，累积回报分布渐近收敛到正态分布。&lt;/p&gt;&lt;p&gt;3.像Ingersoll，Spiegel所讨论的基金表现操纵方法，Goetzmann和Welch（2007）通常试图通过收益负偏分布来增加夏普率。当SEF考虑更高阶矩时，它会调整这种操纵。&lt;/p&gt;&lt;p&gt;4.这是第二种不确定性分析。标准（Markowitz）投资组合选择框架根据收益的标准差来衡量不确定性。在SEF的情况下，对已经包含不确定性估计 &lt;equation&gt;\widehat{\sigma}_{rw}&lt;/equation&gt; 的函数 &lt;equation&gt;\widehat{SR}\left( rw \right)&lt;/equation&gt; 测量不确定性。与Black-Litterman（1992）一样，这种方法并未假设完全了解均值- 方差估计，并且处理模型输入变量的不确定性。这反过来增加了解决方案的稳健性，这与均值- 方差优化的不稳定性形成对比（参见Best和Grauger（1991））。&lt;/p&gt;&lt;p&gt;5.计算SEF将允许我们识别为任何给定阈值提供最高PSR的投资组合，从而在投资组合选择的背景下处理由于跟踪记录长度导致的非正态性和样本不确定性。最高的PSR投资组合就是：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;argmax_w=\frac{\widehat{SR}_{rw}}{\widehat{\sigma}_{\widehat{SR}(rw)}}=argmax_w\frac{\widehat{SR}(rw)\sqrt{n-1}}{\sqrt{1-\widehat{\gamma}_3\widehat{SR}(rw)+\frac{\widehat{\gamma}_4-1}{4}\widehat{SR}(rw)^{2}}}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;数值例子将阐明这一新的分析框架。存在43,458个完全投资的long投资组合，这些投资组合是上一节中确定的9个HFR指数的线性组合，其权重为： &lt;equation&gt;w_i=\frac{j}{10},(j=0,1,...,10,i=1,2,...,9),\sum_{i=1}^{9}{w_i}=1&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;由于非正态性和样本长度会影响我们对每个投资组合的风险调整回报的信心，因此选择最高的夏普比率投资组合并不是最理想的。图13说明了这一点，其中最高的SR组合（SEF的右端）以牺牲该估计的实质性不确定性为代价，因为 &lt;equation&gt;\left( \widehat{\sigma}_{\widehat{SR}(rw)},\widehat{SR}(rw) \right)=\left( 0.155,0.818 \right)&lt;/equation&gt; 提供最高PSR的投资组合确实是完全不同的，由环绕的交叉 &lt;equation&gt;\left( \widehat{\sigma}_{\widehat{SR}(rw)},\widehat{SR}(rw) \right)&lt;/equation&gt; 标记。回想一下，该图中的x轴并不代表与投资相关的风险，而是围绕我们对SR的估计的统计不确定性。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f6a8a3bb90a28c015925aa308fed845d_r.jpg&quot; data-caption=&quot;夏普比率有效前沿可以根据风险调整回报的最优均值-方差组合得出。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1012&quot; data-rawheight=&quot;888&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f6a8a3bb90a28c015925aa308fed845d&quot; data-watermark-src=&quot;v2-159f16d770c0468536aafcef5bb567cf&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图14说明了随着 &lt;equation&gt;\widehat{\sigma}_{\widehat{SR}(rw)}&lt;/equation&gt; 增加，SEF的组成如何演变。 &lt;equation&gt;\widehat{\sigma}_{\widehat{SR}(rw)}=0.103&lt;/equation&gt; 处的垂直线表示最高PSR投资组合的组成，而 &lt;equation&gt;\widehat{\sigma}_{\widehat{SR}(rw)}=0.155&lt;/equation&gt; 处的垂直线表示最高SR投资组合的组成。由于这种方法的稳健性，SEF不同区域的过渡是非常渐进的。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-80ce4eafe522a2d24ac08092e12055ea_r.jpg&quot; data-caption=&quot;我们可以计算为每个置信水平提供最大夏普比率的资本分配。 与Markowitz的有效前沿的区别在于，SEF是根据风险调整后的收益计算的，而不是资本收益率&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1374&quot; data-rawheight=&quot;1048&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-80ce4eafe522a2d24ac08092e12055ea&quot; data-watermark-src=&quot;v2-795607342aa78238722794d5ca217f71&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;图15显示了Max PSR解决方案的优选原因：虽然它提供的夏普比率低于Max SR投资组合（按月计算为0.708对0.818），但其更好的多样化分配可以提供更大的信心（0.103对0.155标准差）。Max PSR投资5种风格，最大持股量为30％，而Max SR产品组合的风格和最大持股量为50％。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-22d98ee6afbe92c4a6cec492ad2ca326_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;926&quot; data-rawheight=&quot;462&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-22d98ee6afbe92c4a6cec492ad2ca326&quot; data-watermark-src=&quot;v2-547d69e85295a278dcf48edeb4000aff&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Max PSR产品系列显示出比Max SR产品组合更好的统计特性，如图16所示：Max PSR非常接近Normal（几乎无偏度和接近3的峰度），而Max SR产品组合具有左侧肥尾。风险规避投资者不应接受从不利分配中获得17.4％的回报概率，以换取针对略高的夏普比率（图17-18）。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e6466da8586d1a3101d5982e6589ae50_r.jpg&quot; data-caption=&quot;最大PSR投资组合是风险调整的最优，而最大SR投资组合是风险调整不理想的。 原因是，尽管最大SR组合可能与高预期夏普比率（点估计）相关联，但围绕该期望的置信区间可能相当宽。 因此，最大PSR投资组合分布更接近正常，并且要求MinTRL低于最大SR投资组合。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;566&quot; data-rawheight=&quot;452&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e6466da8586d1a3101d5982e6589ae50&quot; data-watermark-src=&quot;v2-7a06f0e1c3029c78c4a95510734e2496&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-21de4d50f5aa70c1bf36414b59162417_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;892&quot; data-rawheight=&quot;274&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-21de4d50f5aa70c1bf36414b59162417&quot; data-watermark-src=&quot;v2-95bec1eee98bbed81d6554df70b2d738&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2e04561830685fb7d44c3354fef3b1ce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1110&quot; data-rawheight=&quot;872&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2e04561830685fb7d44c3354fef3b1ce&quot; data-watermark-src=&quot;v2-4bb144a6abfd7ce9d610618b8d1277f1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c3dd27e6ab9527252509339baa5bc46c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1114&quot; data-rawheight=&quot;870&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c3dd27e6ab9527252509339baa5bc46c&quot; data-watermark-src=&quot;v2-8850c1607fc54b16c5a82596471ce0d5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;8.结论&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;提出了一种称为PSR的夏普比率的概率转换，以解释IID非正态框架中的估计误差。在评估夏普比率评估技能的能力时，我们发现较长的跟踪记录可能能够弥补回报概率分布的某些统计缺陷。换句话说，尽管夏普比率有充分证明的缺陷，但只要用户学会要求适当的跟踪记录长度，它仍然可以提供投资技巧的证据。&lt;/p&gt;&lt;p&gt;即使在IID返回的假设下，展示技能所需的跟踪记录长度也很大程度上受到回报分布的不对称性和峰度的影响。一个典型的对冲基金的业绩记录显示负偏度和正向过度峰度，其效果是“夸大”其夏普比率。一种解决方案是通过更长的跟踪记录来弥补这种缺陷。如果不可能，可行的选择可能是提供具有最高采样频率的回报，以便不违反IID假设。原因是，对于负偏态和胖尾回波分布，所需的年数实际上可能随着采样频率的增加而降低。这使我们确认，通过以更高的数据粒度形式提供尽可能大的透明度，“分布不良”的回报分布最有可能获得。&lt;/p&gt;&lt;p&gt;我们提供的经验证据表明，尽管夏普的几种对冲基金风格的公布比例很高，但在很多情况下，它们可能不足以表明在分析期间，信心水平和轨迹的超过0.5的中等夏普比率的统计上显着的投资技巧。记录长度。&lt;/p&gt;&lt;p&gt;最后，我们讨论了这种分析在资本配置背景下的含义。由于非正态性，杠杆率和跟踪记录长度会影响我们对每个投资组合的风险调整回报的信心，因此选择最高的夏普比率投资组合并不是最理想的。我们开发了一种新的分析框架，称为夏普比率效率前沿（SEF），并发现最大化夏普比率的对冲基金指数组合可能与提供最高PSR的投资组合非常不同。与夏普比率最大化的集中结果相比，最大化PSR可以实现更好的多元化和更均衡的对冲基金分配。&lt;/p&gt;</description>
<author>淮浩</author>
<guid isPermaLink="false">2018-12-04-51492836</guid>
<pubDate>Tue, 04 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>自适应滤波器(1)</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-04-50391691.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50391691&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1.数字滤波器介绍：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;(1)基本概念：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-554a8f3443358679ce0c542c7bd7ab5c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;704&quot; data-rawheight=&quot;926&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;群延迟响应：反映了各频率分量通过滤波器后在时间上的延迟情况，&lt;/b&gt;如果滤波器通带内 &lt;equation&gt;\tau_{j\omega}=常数&lt;/equation&gt; ，那么滤波器对各频率分量的延迟相同，为线性相位滤波器&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-96ad3ee34348333a29cbe0bc3de20075_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;758&quot; data-rawheight=&quot;792&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;(4)零极点：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在Z变换里，零点的位置表示系统的“谷”，极点的位置表示系统的“峰”，我们把有峰的地方看做信号可以通过的地方，而有谷的地方看做信号被截止的地方。选择单位圆为频域的一个周期，那么可以得出，如果无零点时，极点在虚轴左半边为高通，极点在虚轴右边为低通；如果无无极点时，而零点在虚轴左边为低通，在虚轴右边为高通；如果同时有零点和极点，以零点指向单位圆向量的模除以极点指向单位圆的模，对于一阶系统，往往极点和零点靠的越近，其带宽越大。&lt;/p&gt;&lt;p&gt;（5）线性滤波器，非线性滤波器：&lt;/p&gt;&lt;p&gt;设F为滤波函数，是否满足F[ax(n)++by(n)]=aF[x(n)]+bF[y(n)]&lt;/p&gt;&lt;p&gt;在金融价格序列中，通常高频短周期的毛刺信号视为随机扰动噪声，即需要低通滤波器滤除噪声，观测长周期趋势&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.均线滤波器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;2.1简单均线&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d5c10de5cd4f93a80a34373799941a64_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1274&quot; data-rawheight=&quot;804&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;1&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d639be90b789f1346033927c7367b262_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;580&quot; data-rawheight=&quot;578&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;频率响应函数&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-85a882504f07c9ffb9a6bdedbc2d95f2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;560&quot; data-rawheight=&quot;1396&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;简单曲线SMA的频率响应特性&lt;/p&gt;&lt;p&gt;可以看出，简单均线能起到一定的低通滤波效果，滤除短期高频噪声，但是时间上延迟非常大，对所有频率分量SMA在时间上滞后了(N-1)/2&lt;/p&gt;&lt;p&gt;双均线趋势策略的原理解释：滤除了高频噪声后，当存在趋势时，由于短周期均线和长周期均线对低频趋势的滞后不同，有趋势时短均线变化值大于长均线变化值，离差扩大，导致金叉和死叉。在市场趋势不强时会不断发生洗盘交易导致亏损。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2线性加权移动平均线&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-858602308e1c933e635218b7b8fa4c40_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1218&quot; data-rawheight=&quot;530&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-22a22f611da4d7a637de186fd9d2bc76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;612&quot; data-rawheight=&quot;578&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a382a2fa5e81d1b243e840d985301c53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;560&quot; data-rawheight=&quot;1396&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;LWMA对近期数据增加权重减少了滞后性，以20天窗口为例，SMA滞后9.5天，对于高频短周期小于25信号，WMA几乎没有滞后，对周期超过50的信号平均滞后6.5天左右。WMA是非线性相位滤波器，对短周期噪声，几乎没有滞后，但是由于对短周期信号(&amp;lt;25)都在阻带内，影响不大。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-54b45f3ee5e3120fd44ce537ff270a0f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;818&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;2.3指数加权移动平均线&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-89f5b2ba3bdda3510442558a077cacf1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1046&quot; data-rawheight=&quot;802&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;指数加权移动平均（Exponential Moving Average, EMA）和线性加权移动平均类似，但不同之处是各数值的加权按指数递减，而非线性递减。此外，在指数衰减中，无论往前看多远的数据，该期数据的系数都不会衰减到0，而仅仅是向0 逼近。因此，指数移动平均实际上是一个无穷级数，即无论多久远的数据都会在计算当期的指数移动平均数值时有一定的作用，只不过离当前太远的数据权重非非常低，因此它们的作用往往可以忽略。指数移动平均由于对近期的数据赋予了更高的权重，因此它比加权移动平均对近期的变化更加敏感，但这种效果并不显著，指数移动平均也存在一定的滞后。无论是加权还是指数移动平均，它们都是通过对近期的数值赋予更高的权重来提高低频趋势对近期变化的敏感程度。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-73a6905a1a24a44ef6877362b9ab9b1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;658&quot; data-rawheight=&quot;260&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这是一个IIR滤波器，实际运算中不可能无限冲激响应，实际上是时刻0递推到当前时刻，存在截断效应。下面取N=20,截断长度M=5N=100。超过100天的项影响可忽略不计&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e384ac2fab871ab60ac67dd8a551c2be_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;612&quot; data-rawheight=&quot;578&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b50fce60ae50f228eaada1211f1b3e71_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;558&quot; data-rawheight=&quot;1396&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9101ce58128ae332ea6094c7343e214e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;722&quot; data-rawheight=&quot;818&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;2.4HMA均线(Hull移动平均)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;HMA计算过程：series表示原始时间序列&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;(1)&lt;/b&gt; 计算最近T=N/2期数据的加权移动平均，记为MA1，如果T=N/2不是整数则取整:MA1 = WMA(series,int(T))&lt;/p&gt;&lt;p&gt;&lt;b&gt;(2)&lt;/b&gt; 计算整个T期数据的加权移动平均，记为MA2:MA2 = WMA(series,2T)&lt;/p&gt;&lt;p&gt;&lt;b&gt;(3)&lt;/b&gt; 令：MA3 = MA1 + (MA1-MA2)&lt;/p&gt;&lt;p&gt;&lt;b&gt;(4)&lt;/b&gt; 以MA3这个时间序列为对象，计算sqrt(T)期的加权移动平均，所得的结果就是HMA，如果sqrt(T)不是整数则取整：HMA = WMA(MA3,int(sqrt(2T)))&lt;/p&gt;&lt;p&gt;&lt;b&gt;HMA的原理：&lt;/b&gt;假设最近的T（=10）个观测数据为0到9：&lt;/p&gt;&lt;p&gt;0 1  2  3 4  5  6 7  8  9&lt;/p&gt;&lt;p&gt;如果按照简单移动平均计算的话，在当前时点得到的均值为4.5，它和当前时点的实际值9相差甚远，滞后性很大，4.5根本无法反映近期数据的变化趋势。Hull的做法是将这10期的数据一分为二，考虑最近的T/2（=5）个数据，即5，6，7，8，9。它们的简单平均为7，反映最近这T/2期数据的（滞后）趋势。接下来的步骤最为关键：Hull用最近T/2期的均值7减去整个T期的均值4.5，并把它们的差2.5再加到最近T/2期的均值7上，得到最终的数据9.5。用整个T期观测值计算出来的均值4.5近似为T/2时点（而非T时点）附近的低频趋势；同理，用最近T/2期的观测值计算出来的均值7为0.75T时点附近的低频趋势。因此，它们的差值（在本例中是2.5）反映了低频趋势在T/2时点到0.75T时点之间的变化；最后假设该趋势的变化会延续，从而将它的值（2.5）与最近T/2期的均值（7）相加，便得到了T时刻的低频趋势9.5。简而言之，Hull通过将T时刻的数据分解为长短两个部分，给近期的数据更高的“权重”，捕捉近期低频趋势的变化，从而减小移动平均算法固有的滞后性。在实际应用中，为了更有效的捕捉趋势变化，在计算长短两个不同时期的均值时，Hull以加权移动平均代替了简单移动平均。这样的结果是对滞后性降低的非常明显，但却牺牲了部分平滑性。为此，在HMA的算法中，Hull采用了最后一步，即将上面步骤得到的均值做了最后一次加权移动平均。这是为了确保在实际观测数据没有剧烈变化时，低频趋势也足够平滑。考虑到这个最后的平滑过程不应该破坏低滞后性，Hull采用的平滑窗口是T的开方sqrt(T)&lt;/p&gt;&lt;p&gt;记H_WMA(z,N)表示窗口长度为N的WMA的系统函数,由于H_WMA(z,N)是线性滤波器&lt;/p&gt;&lt;p&gt;H_HMA(z,N)表示窗口长度为N的HMA的系统函数&lt;/p&gt;&lt;p&gt;那么：H_HMA(z,N) =H_WMA(z,M)( 2*H_WMA(z,T)- H_WMA(z,2T))&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f0260e0f1a03b2fb26254b9bedb125fb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;584&quot; data-rawheight=&quot;578&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0d9fc277475d41698bb0915169541c0e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;560&quot; data-rawheight=&quot;1396&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cd73c6212de1f659a35d59e6d9db10f6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;818&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;2.5考夫曼自适应均线KAMA(series,n,f=2,s=30)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;计算过程：KAMA&lt;/b&gt; &lt;b&gt;的本质是动态计算衰减系数的指数平均线&lt;/b&gt;&lt;/p&gt;&lt;p&gt;(1)先根据最近n天的价格序列计算当前时刻t的效率系数,n为滑动窗口大小：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ER_t=\frac{\left| P_t-P_{t-n} \right|}{\sum_{i=t-n+1}^{t}{\left| P_i-P_{i-1} \right|}}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(2)计算趋势速度：需要两个快慢参数，通常默认f=2,s=30：&lt;/p&gt;&lt;p&gt;在考夫曼均线KAMA 的计算中，用到了快、慢两个时间窗口，它们作为输入的常数，用于产生快、慢两个默认的衰减系数。令f（fastest）和s（slowest）分别代表快、慢两个时间窗口的长度，则两个衰减系数为： &lt;equation&gt;\alpha_f = \frac{1}{f+2},\alpha_s=\frac{1}{s+2}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;在t 时刻，KAMA 的衰减系数为： &lt;equation&gt;\alpha_t = \left[ ER_t\times \left(\alpha_f-\alpha_s \right)+\alpha_s \right]^{2}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(3)计算KAMA均线： &lt;equation&gt;KAMA_t=\alpha_t\times P_t+(1-\alpha_t)\times KAMA_{t-1}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.6FRAMA均线：分形自适应移动平均FRIMA(series,n,h=0.2),n=2T&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;计算过程：FRAMA&lt;/b&gt; &lt;b&gt;的本质是动态计算衰减系数的指数平均线&lt;/b&gt;&lt;/p&gt;&lt;p&gt;具体的，对于当前时点t和给定的窗口n=2T，该方法用到了三个时间窗口，即t到t-T+1（记为窗口W1，长度为T），t-T到t-2T+1（记为窗口W2，长度为T），以及t到t-2T+1（记为窗口W，长度为2T）。不难看出，W=W1+W2。FRAMA的计算过程如下：&lt;/p&gt;&lt;p&gt;(1) 用窗口W1内的最高价和最低价计算:N1=(W1最高价–W1最低价)/T&lt;/p&gt;&lt;p&gt;(2) 用窗口W2内的最高价和最低价计算:N2=(W2最高价–W2最低价)/T&lt;/p&gt;&lt;p&gt;(3) 用整个窗口W内的最高价和最低价计算:N3=(W最高价–W最低价)/(2T)&lt;/p&gt;&lt;p&gt;(4) 计算分形维数: &lt;equation&gt;D=log_2\left( \frac{N1+N2}{N3} \right)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(5) 计算指数移动平均的参数： &lt;equation&gt;\alpha_t = e^{-4.6(D-1)}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;D的取值在1～2之间， &lt;equation&gt;\alpha_t &lt;/equation&gt; 在0.01～1之间&lt;/p&gt;&lt;p&gt;(6)计算FRAMA均线： &lt;equation&gt;FRAMA_t=\alpha_t\times P_t+(1-\alpha_t)\times FRAMA_{t-1}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.7 Laguerre滤波器&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Laguerre过滤器会在滤波器系数中扭曲时间&lt;/p&gt;&lt;p&gt; - 只需几个滤波器项即可实现极端平滑&lt;/p&gt;&lt;p&gt;•NonLinear Laguerre过滤器测量当前价格与上次计算的滤波器输出之间的差异。&lt;/p&gt;&lt;p&gt; - 目标是将此“错误”驱动为零&lt;/p&gt;&lt;p&gt; - 在选定时间段内归一化到误差范围的“误差”是Laguerre滤波器的alpha&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2f717f7c7570fb89910b76a087efd23e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;990&quot; data-rawheight=&quot;502&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;2.8MAMA均线滤波器：MAMA(series,fastlimit,slowlimit)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本质：动态计算衰减系数alpha的EMA。&lt;/b&gt;MESA自适应移动平均线（MAMA）以全新且独特的方式适应价格变动。 这种自适应是基于希尔伯特变换鉴别器测量的相位速率变化。这种自适应方法的优点是它具有快速的攻击平均值和缓慢的衰减平均值，因此复合平均值迅速增加。 价格变化并保持平均值，直到下一个周期出现。MAMA的概念是将相位变化率与EMA alpha相关联，从而使EMA自适应。循环阶段在每个循环中从0度到360度。 相位是连续的，但通常在每个循环开始时快速绘制。 因此，相位变化率为每循环360度。 周期越短，相变率越快。 例如，36bar的循环的相位变化率为每bar10度，而10bar的循环的变化率为每bar36度。 当市场处于趋势模式时，周期时间往往更长。&lt;/p&gt;&lt;p&gt;循环阶段是从正交分量与InPhase分量的比率的反正切值计算的。通过取连续相位测量的差值来获得相位变化率值。反正切函数仅测量半个周期的相位，从-90度到+90度。由于相位测量每半个周期快速恢复，因此每半个周期的相位的巨大负的速率变化是由相位的速率变化的计算引起的。当市场处于趋势模式时，也可以发生测量的负相位变化。从理论上讲，任何负相位变化都是不可能的，因为随着时间的推移，相位必须提前。因此，将相位的所有速率变化限制为不小于1。&lt;/p&gt;&lt;p&gt;允许MAMA中的alpha在最大值和最小值之间的范围内，这些值被建立为输入。建议的最大值为FastLimit = 0.5，建议的最小值为SlowLimit = 0.05。变量alpha计算为FastLimit除以相位变化率。每当存在负相位变化率时，alpha的值被设置为FastLimit，因为相位变化率可以不小于1.如果相位变化率很大，则变量alpha在SlowLimit处有界。这使得MAMA无法对较短的市场周期做出反应。&lt;/p&gt;&lt;p&gt;反正切函数产生-90度到+90度之间的相位响应，相位回绕到-90度。这个相位包裹边界的相位存在巨大的负速率变化。通过将相位的负速率变化限制为+1，EMA中使用的alpha设置为FastLimit。由于希尔伯特变换的90度滞后，相位缠绕边界发生在理论正弦波的零度和180度处。&lt;/p&gt;&lt;p&gt;由于测量的相位快速恢复，变量α保证每半个周期设置为FastLimit。这个相对较大的alpha值使MAMA快速接近价格。在相位快照恢复之后，alpha返回到通常较小的值。 alpha值较小会导致MAMA几乎保持alpha在FastLimit时所达到的值。在相对较大和相对较小的α值之间切换会产生您在波形中观察到的棘轮动作。当市场处于趋势模式时，棘轮发生的频率较低，因为在这些情况下循环周期较长。&lt;/p&gt;&lt;p&gt;如果将MAMA应用于第一个MAMA线以产生以下自适应移动平均值（FAMA），则会产生一组有趣的指标。通过在FAMA中使用的alpha值是MAMA中alpha值的一半，FAMA与MAMA具有时间同步的步骤，但垂直移动不是那么大。因此，除非市场方向发生重大变化，否则MAMA和FAMA不会交叉。这表明自适应移动平均线交叉系统几乎没有洗盘交易。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-794fc0b5a52c723b1bc262eb180eba79_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1128&quot; data-rawheight=&quot;978&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;3.顺序统计滤波器Order Statistic(OS)：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;中值滤波器、中值差分滤波器、Ether滤波器等等。与保持样本的时间顺序的线性滤波器相比，OS滤波器基于滤波器窗口内的样本的排序来进行操作。数据按其汇总统计排序，例如它们的中位数或方差，而不是它们的时间位置。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.1中值滤波器MedianFilter(series,n)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;MedianFilterMovingAverage中位数滤波器：&lt;/p&gt;&lt;p&gt;•排序过滤器&lt;/p&gt;&lt;p&gt;•易于计算&lt;/p&gt;&lt;p&gt;•通常用于锐化视频图像&lt;/p&gt;&lt;p&gt;•通过忽略异常值有助于平滑脉冲噪声&lt;/p&gt;&lt;p&gt;中值滤波是一种非线性数字滤波器技术，经常用于去除图像或者其它信号中的噪声。这个设计思想就是检查输入信号中的采样并判断它是否代表了信号，使用奇数个采样组成的观察窗实现这项功能。观察窗口中的数值进行排序，位于观察窗中间的中值作为输出。然后，丢弃最早的值，取得新的采样，重复上面的计算过程。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;Median_t=median\left[ P(n),P(n-1),...,p(n-k-1) \right]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;在OS滤波器中，中值滤波器是最为人所知的。它用于视频电路中以锐化图像边缘并消除脉冲噪声。在中值滤波器中&lt;b&gt;，输出是观察窗口内所有数据值的中值&lt;/b&gt;。与平均滤波器相反，中值滤波器简单地丢弃除中值之外的所有数据。通过这种方式，可以消除冲动噪声峰值和极端价格数据，而不是包含在平均值中。中值可以落在数据窗口中的第一个样本，最后一个样本或两者之间的任何位置。因此，时间特征丢失。例如，如果输入到五抽头FIR滤波器的数据是顺序的[3 4 3 3 9]，则中值为4.该中值既不是平均值（4.4是平均值）也不是滤波器中心的值。在这种情况下，忽略9的大数据尖峰值。另一个例子，如果数据输入为[3 3 3 8 9]，则中值为8，而平均值为5.2。此示例显示中值过滤器可以快速跟踪数据的快速变化。&lt;/p&gt;&lt;p&gt;中值滤波器倾向于消除导致线性滤波器进行洗盘交易的短期变化。另一方面，&lt;b&gt;响应于急剧和持续的价格变动的中值滤波器的滞后是很大的-它必然是滤波器窗口宽度的大约一半&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.2中值差分滤波器MedianMA(series,n,Threshold)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本质：动态计算衰减系数alpha的EMA，&lt;/b&gt;根据当前中值和均值滤波器的差分调整EMA的alpha&lt;/p&gt;&lt;p&gt;•考虑10个1s的价格数据，其中位数和MA均为1，新价格数据点的值为10&lt;/p&gt;&lt;p&gt;- 中位数输出仍为1，SMA值为1.9&lt;/p&gt;&lt;p&gt;•&lt;b&gt;搜索输出差异低于所选阈值的滤波器长度：&lt;/b&gt;快速变化的市场将产生响应快的过滤器&lt;/p&gt;&lt;p&gt;EMA默认: &lt;equation&gt;\alpha=\frac{2}{n+1}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;设定中位数和均值的差异阀值：Threshold&lt;/p&gt;&lt;p&gt;记 &lt;equation&gt;diff=\left| Median(series,n)-EMA(series,n) \right|&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;搜索使得&lt;equation&gt;\frac{diff}{Median(series,n)}&amp;lt;Threshold&lt;/equation&gt;最大的n（最小的 &lt;equation&gt;\alpha&lt;/equation&gt; ）&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.3可变指数动态移动平均VIDMA(series,n,m=None,h=0.2)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;•可变指数动态移动平均：Variable Index Dynamic Moving Average&lt;/p&gt;&lt;p&gt;•Developed by Tushar Chande and Stanly Kroll in “The new Technical Trader”,John Wiley&amp;amp;Sons,1984&lt;/p&gt;&lt;p&gt;&lt;b&gt;本质：动态计算衰减系数alpha的EMA&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;•根据一段时间内价格标准差与长期价格标准差的比率，动态调整EMA的衰减系数alpha值&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算步骤：&lt;/p&gt;&lt;p&gt;(1) 在t时刻，计算短窗口和长窗口的价格标准差的比率(默认m=int(n/3))：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;r_t=\frac{std\left[ P_t,P_{t-1},...,P_{t-m-1} \right]}{std\left\{ \left[ P_t,P_{t-1},...,P_{t-n-1} \right] \right\} }&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(2)计算动态衰减系数： &lt;equation&gt;\alpha_t=h\times r_t&lt;/equation&gt; ,默认h=0.2&lt;/p&gt;&lt;p&gt;(3)计算VIDMA均线： &lt;equation&gt;VIDMA_t=\alpha_t\times P_t+(1-\alpha_t)\times VIDMA_{t-1}&lt;/equation&gt; &lt;/p&gt;&lt;h2&gt;&lt;b&gt;3.4Etherl滤波器&lt;/b&gt;&lt;/h2&gt;</description>
<author>淮浩</author>
<guid isPermaLink="false">2018-12-04-50391691</guid>
<pubDate>Tue, 04 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器学习资产配置</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-03-51426250.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51426250&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Marcos Lopez de Prado。AQR Capital Management, LLC; Cornell University - Operations Research &amp;amp; Industrial Engineering; RCC - Harvard University&lt;/p&gt;&lt;p&gt;&lt;i&gt;摘要：&lt;/i&gt;介绍了&lt;b&gt;分层风险平价（Hierarchical Risk Parity HRP）&lt;/b&gt;方法。HRP组合涉及二次优化器的三个主要问题，特别是Markowitz的关键线算法（Critical Line Algorithm CLA）：不稳定性，集中度和性能不佳。HRP应用现代数学（图论和机器学习技术），根据协方差矩阵中包含的信息构建多元化投资组合。然而，与二次优化器不同，HRP不需要协方差矩阵的可逆性。实际上，HRP可以在一个恶化的甚至是奇异的协方差矩阵上计算投资组合，这对于二次优化器来说是不可能的。蒙特卡罗实验表明，尽管最小方差是CLA的优化目标，但HRP提供的样本间差异小于CLA。与传统的风险平价方法相比，HRP在样本外产生的风险较低的投资组合。历史分析还表明，HRP的表现优于标准方法（Kolanovic等[2017]，Raffinot [2017]）。&lt;b&gt;HRP的实际应用是确定跨多个机器学习策略的权重分配。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;投资组合构建可能是最经常出现的金融问题。投资经理必须每天建立投资组合，包含他们对风险和回报的看法和预测。这是24岁的马科维茨在六十多年前试图回答的原始问题。他深刻的洞察力是认识到风险调整后的收益与不同的最优投资组合有关，因此提出了“有效前沿”的概念（Markowitz [1952]）。其中一个含义是，将所有资产分配给预期回报最高的投资很少是最优的。相反，我们应该考虑替代投资之间的相互关系，以建立多元化的投资组合。&lt;/p&gt;&lt;p&gt;在1954年获得博士学位之前，Markowitz离开了学术界，为兰德公司工作，在那里他开发了关键线算法(CLA)。CLA是一种二次优化程序，专门针对不等式约束的投资组合优化问题而设计。该算法值得注意的是，它保证了在已知次数的迭代之后找到精确解，并且它巧妙地绕过了Karush-Kuhn-Tucker条件。该算法的描述和开源实现可以在Bailey和Lo pez de Prado [2013]中找到。令人惊讶的是，大多数金融从业者似乎仍然没有意识到CLA，因为他们经常依赖通用的二次规划方法，这些方法无法保证正确的解决方案或停止时间。&lt;/p&gt;&lt;p&gt;尽管马科维茨理论的辉煌，但许多实际问题使得CLA解决方案有些不可靠。一个主要的警告是，预测回报的微小偏差将导致CLA产生非常不同的投资组合（Michaud [1998]）。鉴于很少能够以足够的准确度预测回报，许多作者选择完全放弃回归并专注于协方差矩阵。这导致了基于风险的资产配置方法，其中“风险平价”就是一个突出的例子（Jurczenko [2015]）。降低对回报的预测会改善，但不会阻止不稳定性问题。原因是二次规划方法需要正定协方差矩阵的逆（所有特征值必须为正）。当协方差矩阵在数值上受到病态调节时，即当它具有高条件数时（Bailey和Lo pez de Prado [2012]），这种求逆容易出现大的误差。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.马科维茨曲线&lt;/b&gt;&lt;/p&gt;&lt;p&gt;     协方差，相关矩阵的条件数是其最大和最小特征值之间的比率的绝对值。图1绘制了几个相关矩阵的分类特征值，其中条件数是每条线的第一个和最后一个值之间的比率。这个数字对于对角线相关矩阵是最低的，这是它自己的逆矩阵。随着我们添加相关（多线性）投资，条件数量也会增加。在某些时候，条件数很高，数值误差使得逆矩阵太不稳定：任何条目的微小变化都会导致非常不同的逆。这是Markowitz的诅咒：投资越相关，多样化的需求就越大，但我们越有可能获得不稳定的解决方案。多样化的好处往往被估计错误所抵消。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7bfb526ca0437bf98e76f0cfdc104df9_r.jpg&quot; data-caption=&quot;图1 Markowitz诅咒的可视化。对角线相关矩阵具有最低的条件数。 当我们添加相关投资时，最大特征值更大，最小特征值更低。条件数迅速上升，导致逆相关矩阵不稳定。在某些时候，多样化的好处远远超过估计误差。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;710&quot; data-rawheight=&quot;558&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7bfb526ca0437bf98e76f0cfdc104df9&quot; data-watermark-src=&quot;v2-6d612d3fc07aec228b68545a325d4d2f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;增加协方差矩阵的大小只会使事情变得更糟，因为每个协方差系数都用较少的自由度进行估计。通常，我们需要至少N(N+1)/2个独立且相同分布（IID）的观测，以便估计大小为N的非单数的协方差矩阵。例如，估计大小为50的可逆协方差矩阵需要至少5年的日IID数据。正如大多数投资者所知，相关结构在这么长的时间内不会因任何合理的置信水平而保持不变。这些挑战的严重性体现在这样一个事实上：甚至简单1/N投资组合已被证明能够超出均值-方差和基于风险的优化样本（De Miguel等人[2009]）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.从几何到分层关系&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近年来，这些不稳定性问题受到了极大关注。大多数替代方案试图通过引入额外约束（Clarke等人[2002]），引入贝叶斯先验（Black and Litterman [1992]）或改进协方差矩阵逆的数值稳定性来实现稳健性（Ledoit和Wolf [2003]）。&lt;/p&gt;&lt;p&gt;到目前为止所讨论的所有方法，尽管近年来发表，都是从（非常）经典的数学领域得出的：几何学，线性代数和微积分。相关矩阵是线性代数对象，其测量由返回序列形成的向量空间中的任意两个向量之间的角度的余弦（参见Calkin和Lo pez de Prado [2014a，2015b]）。二次优化器不稳定的一个原因是向量空间被建模为完全图，其中每个节点都是替代另一个节点的潜在候选者。在算法术语中，矩阵求逆意味着评估整个图形中的部分相关性。图16.2（a）显示了50×50的协方差矩阵所暗示的关系，即50个节点和1225个边缘。这种复杂的结构放大了小的估计误差，导致不正确的解决方案。直观地说，希望丢弃不必要的边缘。&lt;/p&gt;&lt;p&gt;让我们暂时考虑这种拓扑结构的实际意义。假设投资者希望建立多元化的证券组合，包括数百种股票，债券，对冲基金，房地产，私募等。一些投资看起来更接近彼此，其他投资似乎相互补充。例如，可以根据流动性，规模，行业和地区对股票进行分组，其中给定组中的股票竞争分配。在决定向像摩根大通这样的大型上市美国金融股进行分配时，我们会考虑增加或减少分配给另一家大型公开交易的美国银行，如高盛，而不是瑞士的一家小型社区银行或持有的房地产银行。加勒比。然而，对于相关矩阵，所有投资都是彼此的潜在替代品。换句话说，相关矩阵缺乏层次结构的概念。缺乏层次结构允许权重以非预期的方式自由变化，这是CLA不稳定的根本原因。图16.2（b）显示了称为树的层次结构。树结构引入了两个理想的特征：（1）它只有N-1个边连接N个节点，因此权重只在不同层次级别的对等体之间重新平衡; （2）权重从上到下分布，与有多少资产管理者建立其投资组合（例如，从资产类别到部门到个别证券）一致。由于这些原因，层次结构的设计更好，不仅可以提供稳定而且直观的结果。&lt;/p&gt;&lt;p&gt;在本章中，我们将研究一种新的投资组合构建方法，该方法使用现代数学解决CLA的陷阱：图论和机器学习。该分层风险奇偶校验方法使用协方差矩阵中包含的信息，而不需要其反演或正定性。HRP甚至可以基于奇异协方差矩阵来计算投资组合。该算法分三个阶段运行：树聚类，准对角化和递归二分法.&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7c4a47f4c34540863fdcef46d45f853a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;708&quot; data-rawheight=&quot;740&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7c4a47f4c34540863fdcef46d45f853a&quot; data-watermark-src=&quot;v2-85b53990e02d318d7e18c72274de3d9a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-30b549ec40239bc032e3b5afbd0a0047_r.jpg&quot; data-caption=&quot;图 2完全图和树图结构。相关矩阵可以表示为完全图，缺少层次结构的概念：每个投资都可以替代另一个投资；相反，树结构包含层次关系。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;702&quot; data-rawheight=&quot;532&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-30b549ec40239bc032e3b5afbd0a0047&quot; data-watermark-src=&quot;v2-d74a1b4bab82c1ad5842625a6863b247&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;3.1树聚类&lt;/b&gt;&lt;/p&gt;&lt;p&gt;考虑一个T*N阶的观测矩阵X，比如N个变量在T个周期上的收益序列。我们希望将这N个列向量组合成簇的分层结构，以便分配可以通过树图向下游流动。&lt;/p&gt;&lt;p&gt;(1)计算协方差矩阵和距离矩阵：&lt;/p&gt;&lt;p&gt;根据收益率序列计算相关系数矩阵： &lt;equation&gt;\rho = \left\{ \rho_{i,j} \right\}_{i,j=1,2,...,N}&lt;/equation&gt; 其中 &lt;equation&gt;\rho_{i,j}= \rho(X_i,X_j)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;定义距离度量：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;d:(X_i,X_j)\subset B\rightarrow d_{i,j}=d(X_i,X_j)=\sqrt{\frac{1}{2}(1-\rho_{i,j})}, d_{i,j}\in[0,1]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其中B是集合{1,2,...,N}中项的笛卡尔积。&lt;/p&gt;&lt;p&gt;可以计算得到N*N阶距离矩阵D， &lt;equation&gt;D = \left\{ d_{i,j} \right\}_{i,j=1,2,...,N}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;矩阵D是一个合适的度量空间，度量空间(Metric Space)，在数学中是指一个集合，并且该集合中的任意元素之间的距离是可定义的，也称距离空间。即满足：&lt;/p&gt;&lt;p&gt;(i)正定性： &lt;equation&gt;d(x,y)\geq0,d(x,y)=0\Leftrightarrow x=y&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(ii)对称性： &lt;equation&gt;d(x,y)=d(y,x)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(iii)三角不等式： &lt;equation&gt;d(x,z)\leq d(x,y) +d(y,z)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;证明略&lt;/p&gt;&lt;p&gt;例：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;{\rho_{i,j}}=\left (\begin{smallmatrix} 1 &amp;amp; 0.7 &amp;amp;0.2 \\  0.7 &amp;amp; 1 &amp;amp; -0.2\\  0.2 &amp;amp;-0.2  &amp;amp;1  \end{smallmatrix}\right )\rightarrow  {d_{i,j}}=\left (\begin{smallmatrix} 0 &amp;amp; 0.5659 &amp;amp;0.9747 \\  0.5659 &amp;amp; 0 &amp;amp; 1.1225\\  0.9747 &amp;amp;1.1225  &amp;amp;0  \end{smallmatrix}\right )&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(2)计算距离矩阵D中两两列向量的欧几里得距离&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\tilde{d}:(D_i,D_j)\subset B \rightarrow \mathbb{R}\in[0,\sqrt{N}]&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;equation&gt;\tilde{d}_{i,j}=\tilde{d}(D_i,D_j)=\sqrt{\sum_{n=1}^{N}{(d_{n,i}-d_{n,j})^{2}}}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;注意到 &lt;equation&gt;d_{i,j}&lt;/equation&gt; 和 &lt;equation&gt;\tilde{d}_{i,j}&lt;/equation&gt; 的不同：&lt;/b&gt; &lt;equation&gt;d_{i,j}&lt;/equation&gt; 定义在矩阵X的列向量上， &lt;equation&gt;\tilde{d}_{i,j}&lt;/equation&gt; 定义在矩阵D的列向量上(距离的距离)，因此， &lt;equation&gt;\bar{d}&lt;/equation&gt; 是在定义在整个度量空间D上的距离，每个 &lt;equation&gt;\bar{d}_{i,j}&lt;/equation&gt; 是整个相关矩阵(而不是特定的互相关对)的函数。&lt;/p&gt;&lt;p&gt;例：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;{d_{i,j}}=\left (\begin{smallmatrix} 0 &amp;amp; 0.3873 &amp;amp;0.6325 \\  0.3873 &amp;amp; 0 &amp;amp; 0.7746\\  0.6325 &amp;amp;0.7746  &amp;amp;0  \end{smallmatrix}\right )\rightarrow  {\tilde{d}_{i,j}}=\left (\begin{smallmatrix} 0 &amp;amp; 0.5659 &amp;amp;0.9747 \\  0.5659 &amp;amp; 0 &amp;amp; 1.1225\\  0.9747 &amp;amp;1.1225  &amp;amp;0  \end{smallmatrix}\right )&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(3)将一对 &lt;equation&gt;(i^*,j^*)&lt;/equation&gt; 聚类在一起，使得 &lt;equation&gt;(i^*,j^*)=argmin_{(i,j),i\ne j} \left\{ \tilde{d}_{i,j} \right\}&lt;/equation&gt; ，并将该聚类表示为 &lt;equation&gt;u(1)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;例：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;{\tilde{d}_{i,j}}=\left (\begin{smallmatrix} 0 &amp;amp; 0.5659 &amp;amp;0.9747 \\  0.5659 &amp;amp; 0 &amp;amp; 1.1225\\  0.9747 &amp;amp;1.1225  &amp;amp;0  \end{smallmatrix}\right )\rightarrow u(1)=(1,2)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(4)需要定义新形成的簇u(1)和单个项(非簇)之间的距离，以便可以更新 &lt;equation&gt;\left\{ \tilde{d}_{i,j} \right\}&lt;/equation&gt;。在层次聚类分析中，这被称为“链接标准”。例如，我们可以将 &lt;equation&gt;\tilde{d}&lt;/equation&gt;的项i和新簇u(1)之间的距离定义为： &lt;equation&gt;\dot{d}_{i,u(1)}=min\left[ \left\{ \tilde{d}_{i,j} \right\} \right]&lt;/equation&gt; (最近点算法)&lt;/p&gt;&lt;p&gt;例：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;u(1)=(1,2)\rightarrow \left\{ \dot{d}_{i,u(1)} \right\}=\begin{bmatrix} min[0,0.5969]\\  min[0.5969,0]\\  min[0.9747,1.1225]\end{bmatrix}=\begin{bmatrix} 0\\  0\\  0.9747\end{bmatrix}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(5)更新矩阵 &lt;equation&gt;\left\{ \tilde{d}_{i,j} \right\}&lt;/equation&gt;通过附加 &lt;equation&gt; \dot{d}_{i,u(1)} &lt;/equation&gt; ，之后删除被聚类的行和列 &lt;equation&gt;j\in u(1)&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;例：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\left\{ \tilde{d}_{i,j} \right\}_{i,j={1,2,3,4}}  = \begin{bmatrix} 0&amp;amp; 0.5659 &amp;amp;0.9747 &amp;amp;0 \\   0.5659&amp;amp; 0&amp;amp; 1.1225&amp;amp; 0\\   0.9747&amp;amp; 1.1225 &amp;amp; 0 &amp;amp; 0.9747\\   0&amp;amp;0  &amp;amp;0.9747  &amp;amp;0  \end{bmatrix}\rightarrow\left\{ \tilde{d}_{i,j} \right\}_{i,j={3,4}} = \begin{bmatrix}  0&amp;amp; 0.9747\\  0.9747 &amp;amp;0  \end{bmatrix}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;(6)递归应用，步骤3,4和5允许将N-1个这样的簇附加到矩阵D，此时最终的簇包含所有原始项，聚类算法终止：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;\left\{ \tilde{d}_{i,j} \right\}_{i,j={3,4}} = \begin{bmatrix}  0&amp;amp; 0.9747\\  0.9747 &amp;amp;0  \end{bmatrix}\rightarrow u(2)=(3,4)\rightarrow Stop&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;图3显示了此示例在每次迭代时形成的聚类，以及触发每个聚类的距离（第三步）。此过程可应用于各种距离度量，参见Rokach和Maimon [2005]的替代指标，关于Fiedler矢量的讨论和Stewart在Brualdi [2010]中的谱聚类方法，以及scipy库中的算法。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bd0f39328217a321d156db8fdaa6e2e4_r.jpg&quot; data-caption=&quot;图3簇形成的顺序。数值示例导出的树结构，此处绘制为树形图。 y轴测量两个合并叶片之间的距离。&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;698&quot; data-rawheight=&quot;560&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bd0f39328217a321d156db8fdaa6e2e4&quot; data-watermark-src=&quot;v2-cf72a00ddaf4e64cfb745d18f40bc98c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在这个阶段，允许我们将链接矩阵定义为(N-1)*4阶矩阵，具有结构 &lt;equation&gt;Y=\left\{ y_{m,1},y_{m,2},y_{m,3},y_{m,4} \right\}_{m=1,2,...,N-1}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;项 &lt;equation&gt;\left( y_{m,1},y_{m,2} \right)&lt;/equation&gt; 报告成分，项 &lt;equation&gt;y_{m,3}&lt;/equation&gt; 报告 &lt;equation&gt;y_{m,1}&lt;/equation&gt; 和 &lt;equation&gt;y_{m,2}&lt;/equation&gt; 的距离,项 &lt;equation&gt;y_{m,4}&lt;/equation&gt; 报告聚类m中包含的原始项数&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2准对角化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;此阶段重新组织协方差矩阵的行和列，以便最大值位于对角线上。协方差矩阵的这种准对角化（不需要改变基础）提供了一个有用的特性：类似的投资被放在一起，不同的投资相隔很远(例如，参见图5和图6)。该算法的工作原理如下：我们知道链接矩阵的每一行将两个分支合并为一个。我们用递归的方式替换 &lt;equation&gt;\left( y_{N-1,1},y_{N-1,2} \right)&lt;/equation&gt;中的聚类，直到没有聚类为止。这些替换保留了聚类的顺序。输出是原始（非聚集）项目的排序列表。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3递归二分法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;阶段2提供了准对角矩阵,逆方差分配对于对角协方差矩阵是最优的。阶段3将权重与子集的变量成反比分配，当协方差矩阵是对角线时，这种分配是最优的，证明如下：&lt;/p&gt;&lt;p&gt;考虑大小为N的标准二次优化问题：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;min_{w}\left\{ w^TVw \right\} &lt;/equation&gt; &lt;/p&gt;&lt;p&gt;&lt;equation&gt;s.t.:w^Ta=1&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;解为: &lt;equation&gt;w=\frac{V^{-1}a}{a^TV^{-1}a}&lt;/equation&gt; ，对特征向量 &lt;equation&gt;a=1_N&lt;/equation&gt; ,解是最小方差组合。&lt;/p&gt;&lt;p&gt;如果V是对角矩阵，那么 &lt;equation&gt;w_n=\frac{V^{-1}_{n,n}}{\sum_{i=1}^{N}{V^{-1}_{i,i}}}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;特别地，当N=2时： &lt;equation&gt;w_1=\frac{V^{-1}_{1,1}}{V^{-1}_{1,1}+V^{-1}_{2,2}}=1-\frac{V_{1,1}}{V_{1,1}+V_{2,2}}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;我们可以用两种不同的方式利用这些事实：自下而上，将连续子集的方差定义为逆方差分配的方差; 或者自上而下，将相邻子集之间的分配与其聚合方差成反比。以下算法形式化了这个想法：&lt;/p&gt;&lt;p&gt;(1)算法初始化为：&lt;/p&gt;&lt;p&gt;(a)设定项清单： &lt;equation&gt;L=\left\{ L_0 \right\},L_0=\left\{ n \right\}_{n=1,2,...,N}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(b)为所有项分配单位权重： &lt;equation&gt;w_n=1,\forall n=1,2,...,N&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(2)如果 &lt;equation&gt;\left| L_i \right|=1, \forall L_i\in L&lt;/equation&gt;，那么算法终止；&lt;/p&gt;&lt;p&gt;(3)对每个 &lt;equation&gt;\left| L_i \right|&amp;gt;1, \forall L_i\in L&lt;/equation&gt; :&lt;/p&gt;&lt;p&gt;(a)把 &lt;equation&gt;L_{i}&lt;/equation&gt;二分为两个子集 &lt;equation&gt;L^{(1)}_i\cup L^{(2)}_i =L_i&lt;/equation&gt;，其中 &lt;equation&gt;\left| L^{(1)}_i \right|=int\left( \frac{1}{2}\left| L_i \right| \right)&lt;/equation&gt; ，顺序被保留；&lt;/p&gt;&lt;p&gt;(b)定义 &lt;equation&gt;L^{(j)}_i,j=1,2&lt;/equation&gt;的方差为二次型 &lt;equation&gt;\tilde{V^{(j)}_i}\equiv (\tilde{w}^{(j)}_i)^TV^i_j\tilde{w}^{(j)}_i&lt;/equation&gt; ，其中 &lt;equation&gt;V^{(j)}_i&lt;/equation&gt; 是二分 &lt;equation&gt;L^{(j)}_i&lt;/equation&gt;的成分的协方差矩阵：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;w^{(j)}_i= diag\left[ V^{(j)}_i \right] ^{-1}\frac{1}{tr \left[ diag\left[ V^{(j)}_i \right] ^{-1} \right]}&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;其中：diag[A]表示取出矩阵A的对角元然后构建一个以对角元为对角的对角矩阵；&lt;/p&gt;&lt;p&gt;tr[A]表示矩阵A的迹&lt;/p&gt;&lt;p&gt;(c)计算分裂因子： &lt;equation&gt;\alpha_i = 1-\frac{V^1_i}{V^1_i + V^2_i},0 \leq\alpha_i\leq1&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;(d)对 &lt;equation&gt;\forall n\in L^1_i&lt;/equation&gt;,重新调整分配 &lt;equation&gt;w_n&lt;/equation&gt; 的因子为 &lt;equation&gt;\alpha_i&lt;/equation&gt; &lt;/p&gt;&lt;p&gt;对 &lt;equation&gt;\forall n\in L^2_i&lt;/equation&gt;，重新调整分配 &lt;equation&gt;w_n&lt;/equation&gt; 的因子为 &lt;equation&gt;1-\alpha_i&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;(4)回到步骤2；&lt;/p&gt;&lt;p&gt;步骤3b利用准对角化自下而上，因为它使用逆方差加权来定义划分的方差。步骤3c利用准对角化自顶向下，因为它将权重与群集的方差成反比分割。该算法保证 &lt;equation&gt;0\leq w_i\leq1,\forall n=1,2,...,N&lt;/equation&gt; 和 &lt;equation&gt;\sum_{i=1}^{N}{w_i}=1&lt;/equation&gt; ，因为在每次迭代中分割从较高层级接收的权重。通过根据用户的偏好替换步骤3c，3d和3e中的等式，可以在该阶段容易地引入约束。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.数值例子：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;数值例子1：上证50&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面考虑上证50：2013-01-01～2018-12-01采样5日的价格数据，数据来源ricequant，简单起见，成分股取2013-01-01的成分股，在此期间保持不变&lt;/p&gt;&lt;p&gt;&lt;b&gt;取最近120天数据为窗口(约6月)，根据最近120天的数据计算权重，在未来20天以此权重配比，每隔20日重配权重，对比1/N，IVP，HRP方法,下面计算年化收益和波动率时以每年245日计，年化收益为年化复合收益，无风险收益率以0%计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1.数据预处理：生成收益序列、协方差矩阵、相关系数矩阵、距离矩阵、链接矩阵&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1a2058c7d5a391db3ba7228a24ee5a41_r.jpg&quot; data-caption=&quot;上证50 2013-01-01 到 2015-01-27相关系数矩阵热力图，可以看见上证50之间的相关性都比较强，甚至没有负相关...&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;686&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1a2058c7d5a391db3ba7228a24ee5a41&quot; data-watermark-src=&quot;v2-da25c898d2705e96a6117cffe9311190&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;2.准对角化：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-aaeaf89070d2340bbd9b3b740173c745_r.jpg&quot; data-caption=&quot;准对角化后的相关系数矩阵，相关系数矩阵的这种准对角化提供了一个有用的特性：类似的投资被放在一起，不同的投资相隔很远&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;830&quot; data-rawheight=&quot;684&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-aaeaf89070d2340bbd9b3b740173c745&quot; data-watermark-src=&quot;v2-fe6d461f30df4390141a02e58865391a&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;3.HRP分配：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fe98cb3e8029e4bbdda860badd3bf951_r.jpg&quot; data-caption=&quot;窗口120日 超前预测20日 每隔20日调仓。1/N:年化收益:13.01%,年化标准差:24.97%,年化夏普:0.52IVP:年化收益:17.15%,年化标准差:23.26%,年化夏普:0.74HRP:年化收益:18.38%,年化标准差:23.26%,年化夏普:0.79&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;716&quot; data-rawheight=&quot;468&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fe98cb3e8029e4bbdda860badd3bf951&quot; data-watermark-src=&quot;v2-f0d2649cd7255e6267bae22af1f33915&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fc04554bfb05bd9600cd2ae4f4a11a9e_r.jpg&quot; data-caption=&quot;窗口120日 超前预测10日 每隔10日调仓。1/N:年化收益:12.58%,年化标准差:24.25%,年化夏普:0.52IVP:年化收益:15.91%,年化标准差:22.87%,年化夏普:0.70HRP:年化收益:15.99%,年化标准差:22.87%,年化夏普:0.70&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;716&quot; data-rawheight=&quot;482&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fc04554bfb05bd9600cd2ae4f4a11a9e&quot; data-watermark-src=&quot;v2-34e4aa6279928ea015f399083b447c31&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1c31162f7278ea10be976eb3e7d94264_r.jpg&quot; data-caption=&quot;窗口120日 超前预测30日 每隔30日调仓。1/N:年化收益:12.50%,年化标准差:27.97%,年化夏普:0.45IVP:年化收益:16.38%,年化标准差:25.47%,年化夏普:0.64HRP:年化收益:18.16%,年化标准差:25.47%,年化夏普:0.71&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;710&quot; data-rawheight=&quot;482&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1c31162f7278ea10be976eb3e7d94264&quot; data-watermark-src=&quot;v2-45d6a2d33e5acb08a923ae1cbae0550d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;此处仅测试了一下，没有进一步增加约束，还有很大改进空间，只利用了历史收益数据，比如可以将对收益的预测加入，权重低的都近乎为0，可以加入最小最大权重约束和基数约束&lt;/p&gt;</description>
<author>淮浩</author>
<guid isPermaLink="false">2018-12-03-51426250</guid>
<pubDate>Mon, 03 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器学习、深度学习、量化金融、Python等最新书籍汇总（打包带走）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-12-01-51318034.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51318034&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a1594e46a0e39d2446d364c9b50d8793_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;时&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;间&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;就&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;这&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;样&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;悄&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;无&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;声&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;息&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;的&lt;/b&gt;&lt;/i&gt; &lt;i&gt;&lt;b&gt;溜 了&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;似乎还没有开始&lt;/p&gt;&lt;p&gt;&lt;b&gt;2018年，就只剩下一个月了&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;从现在起&lt;/p&gt;&lt;p&gt;抓紧这最后一个月&lt;/p&gt;&lt;p&gt;好好读书&lt;/p&gt;&lt;p&gt;好好学习&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-152d1af38dec1620848299a7b9d615ee_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;300&quot; data-rawheight=&quot;300&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-152d1af38dec1620848299a7b9d615ee_b.jpg&quot;&gt;&lt;p&gt;一本本分享太费劲，为了刷阅读量可以这么干。&lt;/p&gt;&lt;p&gt;&lt;b&gt;但我们比较任性！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;把近期所有能找到的，一次性分享给大家。希望大家多看学，多学习。我们希望你有所收获，更重要的是&lt;b&gt;谢谢你们对公众号的不离不弃！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;书单如下，整整&lt;b&gt;400M+&lt;/b&gt;，&lt;b&gt;文末免费下载&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;b&gt;（年底还会有一次更强的更新）&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9c7c0fdf3ab1f81d04475c6ef5f05c62_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1552&quot; data-rawheight=&quot;4096&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9c7c0fdf3ab1f81d04475c6ef5f05c62&quot; data-watermark-src=&quot;v2-181442e6cf3a74ded2baed2509f67340&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;01、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;02、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;03、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;04、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=2020206794&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;05、使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289314&amp;amp;idx=1&amp;amp;sn=87c5a12b23a875966db7be50d11f09cd&amp;amp;chksm=802e3977b759b061675d1988168c1fec06c602e8583fbcc9b76f87008e0c10b702acc85467a0&amp;amp;token=1972390229&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;06、美丽的回测——教你定量计算过拟合概率&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289347&amp;amp;idx=1&amp;amp;sn=bf5d7899bc4a854d4ba9046fdc6fe0d6&amp;amp;chksm=802e3996b759b080287213840987bb0a0c02e4e1d4d7aae23f10a225a92ef6dd922d8006123d&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;07、利用动态深度学习预测金融时间序列基于Python&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289394&amp;amp;idx=1&amp;amp;sn=24a836136d730aa268605628e683d629&amp;amp;chksm=802e39a7b759b0b1dcf7aaa560699130a907716b71fc9c45ff0e5d236c5ae8ef80ebdb09dbb6&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;08、Facebook开源神器Prophet预测时间序列基于Python&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289437&amp;amp;idx=1&amp;amp;sn=f0dca7da8e69e7ba736992cb3d034ce7&amp;amp;chksm=802e39c8b759b0de5bce401c580623d0729ecca69d13926479d36e19aff8c9c9e8a20265afff&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;09、Facebook开源神器Prophet预测股市行情基于Python&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289358&amp;amp;idx=1&amp;amp;sn=db6e8ab85b08f6e67790ec0e401e586e&amp;amp;chksm=802e399bb759b08d6eec855f9901ea856d0da68c7425cba62791b8948da6ad761a3d88543dad&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;10、2018第三季度最受欢迎的券商金工研报前50（附下载）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289447&amp;amp;idx=1&amp;amp;sn=f2948715bf82569a6556d518e56c1f9e&amp;amp;chksm=802e39f2b759b0e4502d1aaac562b87789573b55c76b3c85897d8c9d88dbf9a0b7ee34d86a4e&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;11、实战交易策略的精髓（公众号深度呈现）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289478&amp;amp;idx=1&amp;amp;sn=f8e01a641be021993d8ef2d84e94a299&amp;amp;chksm=802e3e13b759b7055cf27a280c672371008a5564c97c658eee89ce8481396a28d254836ff9af&amp;amp;token=290397496&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;12、Markowitz有效边界和投资组合优化基于Python&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289495&amp;amp;idx=1&amp;amp;sn=c4eeaa2e9f9c10995be9ea0c56d29ba7&amp;amp;chksm=802e3e02b759b7148227675c23c403fb9a543b733e3d27fa237b53840e030bf387a473d83e3c&amp;amp;token=1260956004&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;13、使用LSTM模型预测股价基于Keras&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289507&amp;amp;idx=1&amp;amp;sn=f0ca71aa07531bbbdbd33213f0bab89f&amp;amp;chksm=802e3e36b759b720138b3b17a4dd0e198e054b9de29a038fdd50805f824effa55831111ad026&amp;amp;token=1936245282&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;14、量化金融导论1：资产收益的程式化介绍基于Python&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289533&amp;amp;idx=1&amp;amp;sn=4ef964834e84a9995111bb057b0fc5dd&amp;amp;chksm=802e3e28b759b73e0618eb1262c53aa0601fbf5805525a7c7ff40dc3db62c7704496611bdbf1&amp;amp;token=1950551577&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;15、预测股市崩盘基于统计机器学习与神经网络（Python+文档）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289609&amp;amp;idx=1&amp;amp;sn=c7f0b3e47025862d10bb53b6ab88bcda&amp;amp;chksm=802e3e9cb759b78abf6b8b049c59bf18ccfb2ead7580d1f557d36de2292f59dcbd94dcd41910&amp;amp;token=2085008037&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;16、实现最优投资组合有效前沿基于Python（附代码）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289615&amp;amp;idx=1&amp;amp;sn=1cdc89afb997d0c580bf0cef296d946c&amp;amp;chksm=802e3e9ab759b78ce9f0cd152a680d4a413d6c8dcb02a7a296f4091993a7e4137e7520394575&amp;amp;token=2085008037&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;17、精心为大家整理了一些超级棒的机器学习资料（附链接）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;点击下载：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/Gm8LMkNWhInnV5rq7ebNeA&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-4ae6c1c900adc8dfcc5d6652d1abac2f&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;544&quot; data-image-size=&quot;180x120&quot;&gt;机器学习、深度学习、量化金融、Python等最新书籍汇总（打包带走）&lt;/a&gt;&lt;p&gt;&lt;b&gt;对2019有什么期望呢？&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9d33bcd502322d6a6eccdf680756fefe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1051&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9d33bcd502322d6a6eccdf680756fefe&quot; data-watermark-src=&quot;v2-dc81867bcbad27306b2380dca95c753e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-12-01-51318034</guid>
<pubDate>Sat, 01 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>精心为大家整理了一些超级棒的机器学习资料（附链接）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-11-27-50999794.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50999794&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98e852b076f7238c1cbb2d0693dde079_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;整理：公众号编辑部&lt;/p&gt;&lt;p&gt;公众号精心整理，先发40个，后面还有一些。希望大家在学习道路上学有所成。&lt;/p&gt;&lt;p&gt;免费的资源这么多，干嘛还花钱买别人的呢？&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c0e7e174515b70c155f918ffd3c8af7b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;660&quot; data-rawheight=&quot;264&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c0e7e174515b70c155f918ffd3c8af7b&quot; data-watermark-src=&quot;v2-9ddb47401654d08181e0060ed7aafe1b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;人生，要学无止境！&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、Scikit-Learn机器学习进阶&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/glemaitre/pyparis-2018-sklearn/blob/master/notebook.ipynb&quot;&gt;https://github.com/glemaitre/pyparis-2018-sklearn/blob/master/notebook.ipynb&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、机器学习基础：(Python)手把手Logistic回归&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8&quot;&gt;https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、美国加州大学圣地亚哥分校Python入门课程资料（Jupyter Notebooks）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://cogs18.github.io/materials/00-Introduction/&quot;&gt;https://cogs18.github.io/materials/00-Introduction/&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4、21世纪统计思维（斯坦福本科生统计课程教材）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/poldrack/psych10-book&quot;&gt;https://github.com/poldrack/psych10-book&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5、《统计学习方法》所有算法实现代码&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://github.com/WenDesi/lihang_book_algorithm&quot;&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;6、（MIT课程）Python机器学习——从线性模型到深度学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://www.edx.org/course/machine-learning-with-python-from-linear-models-to-deep-learning&quot;&gt;https://www.edx.org/course/machine-learning-with-python-from-linear-models-to-deep-learning&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;7、（高等经济学院）Coursera高级机器学习课程资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://github.com/MaxPoon/coursera-Advanced-Machine-Learning-specialization&quot;&gt;https://github.com/MaxPoon/coursera-Advanced-Machine-Learning-specialization&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;10、深度学习之自然语言处理斯坦福大学CS224n课程集训营&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;网址：&lt;a href=&quot;https://github.com/learning511/cs224n-learning-camp&quot;&gt;https://github.com/learning511/cs224n-learning-camp&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;11、深度强化学习教程（高质量PyTorch实现集锦）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/qfettes/DeepRL-Tutorials&quot;&gt;https://github.com/qfettes/DeepRL-Tutorials&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;12、R语言教/学课程资源集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/rstudio-education/rstats-ed&quot;&gt;https://github.com/rstudio-education/rstats-ed&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;13、（彭博）视频课程：机器学习基础&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://bloomberg.github.io/foml/#home&quot;&gt;https://bloomberg.github.io/foml/#home&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;14、CMU课程：深度学习导论（Fall 2018）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://deeplearning.cs.cmu.edu/&quot;&gt;http://deeplearning.cs.cmu.edu/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;14、机器学习数学基础&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://www.doc.ic.ac.uk/~mpd37/teaching/2017/496/notes.pdf&quot;&gt;https://www.doc.ic.ac.uk/~mpd37/teaching/2017/496/notes.pdf&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;15、UC Berkeley深度强化学习课程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;http://rail.eecs.berkeley.edu/deeprlcourse/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;16、哥伦比亚新闻学院Lede项目算法课程材料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/jstray/lede-algorithms&quot;&gt;https://github.com/jstray/lede-algorithms&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;17、CMU神经网络自然语言处理课程（2018）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://phontron.com/class/nn4nlp2018/&quot;&gt;http://phontron.com/class/nn4nlp2018/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;18、人工智能导论课程讲义&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/glouppe/info8006-introduction-to-ai&quot;&gt;https://github.com/glouppe/info8006-introduction-to-ai&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;19、旧金山大学数据科学训练营课程教材&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/parrt/msds501&quot;&gt;https://github.com/parrt/msds501&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;20、帝国理工学院数学系深度学习课程的代码和作业资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/pukkapies/dl-imperial-maths&quot;&gt;https://github.com/pukkapies/dl-imperial-maths&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;21、伯克利课程：通用人工智能安全与控制&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://inst.eecs.berkeley.edu/~cs294-149/fa18/#lecture-schedule&quot;&gt;http://inst.eecs.berkeley.edu/~cs294-149/fa18/#lecture-schedule&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;22、斯坦福CS230深度学习课程资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://cs230-stanford.github.io/&quot;&gt;https://cs230-stanford.github.io/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;23、fast.ai深度学习课程第二季完整笔记&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://www.kdnuggets.com/2018/07/fast-ai-deep-learning-part-2-notes.html&quot;&gt;https://www.kdnuggets.com/2018/07/fast-ai-deep-learning-part-2-notes.html&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;24、斯坦福深度学习课程(2018)课程设计集锦&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://cs230.stanford.edu/proj-spring-2018.html&quot;&gt;http://cs230.stanford.edu/proj-spring-2018.html&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;25、UCF课程：高级计算机视觉(Keras)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://www.youtube.com/playlist?list=PLd3hlSJsX_ImoNaeX5vFrxogGXTSmS993&quot;&gt;https://www.youtube.com/playlist?list=PLd3hlSJsX_ImoNaeX5vFrxogGXTSmS993&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;26、面向金融的机器学习与强化学习-金融市场预测算法与工具&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://www.coursera.org/specializations/machine-learning-reinforcement-finance&quot;&gt;https://www.coursera.org/specializations/machine-learning-reinforcement-finance&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;27、Python/Pandas数据分析课程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/cuttlefishh/python-for-data-analysis&quot;&gt;https://github.com/cuttlefishh/python-for-data-analysis&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;28、CS224n深度学习自然语言处理课程设计报告集锦（2018）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://web.stanford.edu/class/cs224n/reports.html&quot;&gt;http://web.stanford.edu/class/cs224n/reports.html&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;29、深度学习数学基础&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://joanbruna.github.io/MathsDL-spring18/&quot;&gt;https://joanbruna.github.io/MathsDL-spring18/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;30、纽约大学课程资料：深度学习数学原理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/joanbruna/MathsDL-spring18&quot;&gt;https://github.com/joanbruna/MathsDL-spring18&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;31、加泰罗尼亚理工大学课程：AI深度学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://telecombcn-dl.github.io/2017-dlai/&quot;&gt;https://telecombcn-dl.github.io/2017-dlai/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;32、高级数据分析基础（CMU课程笔记）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&quot;&gt;http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;33、EPFL深度学习课程资料（2018）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://fleuret.org/ee559/&quot;&gt;https://fleuret.org/ee559/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;34、哥伦比亚大学课程：应用机器学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/amueller/COMS4995-s18&quot;&gt;https://github.com/amueller/COMS4995-s18&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;35、芝加哥大学深度学习课程资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://ttic.uchicago.edu/~shubhendu/Pages/CMSC35246.html&quot;&gt;http://ttic.uchicago.edu/~shubhendu/Pages/CMSC35246.html&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;36、斯坦福大学面向Tensorflow深度学习研究课程（2018）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://web.stanford.edu/class/cs20si/&quot;&gt;https://web.stanford.edu/class/cs20si/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;37、斯坦福课程：深度学习理论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://stats385.github.io/&quot;&gt;https://stats385.github.io/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;38、宾夕法尼亚大学课程：面向自然语言处理的高级机器学习技术&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/lectures.html&quot;&gt;http://www.cis.upenn.edu/~danroth/Teaching/CIS-700-006/lectures.html&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;39、马里兰大学CMSC723计算语言学课程资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;http://www.cs.umd.edu/class/fall2017/cmsc723/&quot;&gt;http://www.cs.umd.edu/class/fall2017/cmsc723/&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;40、乔治·华盛顿大学”数据挖掘”、“机器学习”课程资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;网址：&lt;a href=&quot;https://github.com/jphall663/GWU_data_mining&quot;&gt;https://github.com/jphall663/GWU_data_mining&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-11-27-50999794</guid>
<pubDate>Tue, 27 Nov 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>一招鲜，判断哪些输入特征对神经网络是重要的！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47475888.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47475888&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f07bd4bd990b9118b93d7a50e51c6bf5_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d0adadbc7e374710cc6f20b574638ffb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d0adadbc7e374710cc6f20b574638ffb&quot; data-watermark-src=&quot;v2-e96895b3f9d106aef40902f9660c108d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Muhammad Ryan&lt;/p&gt;&lt;p&gt;本期编译：Peter&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289289&amp;amp;idx=1&amp;amp;sn=7c6cf3ed7f4f2859a0e95bc87914814c&amp;amp;chksm=802e395cb759b04adcee6afc1d44ffa2f3ceac2137796dad4ee9c51cdb165ce3463258647a3c&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【ML系列】一招鲜，判断哪些输入特征对神经网络是重要的！&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=2020206794&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;7、使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289274&amp;amp;idx=1&amp;amp;sn=f40be8372658c2c79fdd47c03d62e037&amp;amp;chksm=802e392fb759b039435fc6700ef5d45142cdfe72234586bd8de9b8dfabcc3264f2ae826def80&amp;amp;token=1003651614&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;8、手把手教你用Numpy构建神经网络(附代码)&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;正文&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果你看到这个题目，可能会马上回答&lt;/p&gt;&lt;p&gt;主成分分析（PCA），因为冗余的输入是无用的。&lt;/p&gt;&lt;p&gt;没错，但这不是今天的重点。我们想知道的是输入特征对神经网络的预测计算有多重要。例如，通过学习时间、年龄、身高和缺席人数等几个预测因素来预测谁会通过考试。直觉上，决定学生是否通过考试的最重要的因素是学习时间。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e9e14454680947af05adac3d88a04cfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e9e14454680947af05adac3d88a04cfc&quot; data-watermark-src=&quot;v2-49065bcacadfa623c6b3bc8df99c6106&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在一个简单的线性回归中，我们可以通过看它的线性方程的权重来测量它。当然，假设预测器(X)已经标准化(X &#39;)所以数据的量纲是相同的。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-732d6954009b21d9c0296fb9c6b8d151_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;562&quot; data-rawheight=&quot;170&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-732d6954009b21d9c0296fb9c6b8d151&quot; data-watermark-src=&quot;v2-40e2e1ff16bd5b4837fa1736b96842bc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6df5e22be3ab1a5ec28d7c7e10de6d20_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;688&quot; data-rawheight=&quot;160&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6df5e22be3ab1a5ec28d7c7e10de6d20&quot; data-watermark-src=&quot;v2-b864c7239b970257bf1e661c598d807d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;你可以在这两个函数中选择一个来归一化你的预测器。为了理解为什么只有使用权重我们才能衡量一个预测器相对于其他预测器的重要性，这里有一个例子。假设我们有一个线性方程。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9937466fdccb75067e8d73753ae3f966_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;598&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们把所有的x用5代替：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d019039c29cf24b3edf3d8e010aaa6e9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;758&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这是它贡献的部分，直观上来说，如果这个部分很大，当输入出错时，输出就会出错。例如，当我把x3从5换成1，我们得到：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c598d476fa2bf0150cf886de0ab4bf7d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;78&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果把x2换成1，得到的是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-db8d134f22a232d9e30ecfd5bf12a04b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;758&quot; data-rawheight=&quot;80&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这里我们可以看到，由于权重的不同，x2值的变化比x3值的变化影响更大。这很明显，但我想强调的是，除了权重之外，我们可以从输出值与参考值的偏差来看我们的输入有多重要。&lt;/p&gt;&lt;p&gt;在神经网络中，输入的权重不是直接连接到输出层，而是连接到隐藏层。此外，与线性回归不同，神经网络是非线性的。为了看到输入的显著水平，我们寻找我们之前找到的第二个参数，如果我们随机改变输入值，它与神经网络输出值的偏差有多大。这里我们使用的参考值是原始错误值。为什么我称之为“original”。&lt;/p&gt;&lt;p&gt;让我们来看看真实的数据和真实的神经网络。预测学生在考试中的表现。&lt;/p&gt;&lt;p&gt;数据下载地址：&lt;i&gt;https://archive.ics.uci.edu/ml/datasets/student+performance&lt;/i&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-279572efbfce56c3c8b6a91f9c60e34c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;396&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-279572efbfce56c3c8b6a91f9c60e34c&quot; data-watermark-src=&quot;v2-622255806ae8777e1a22b8b8198fc078&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下面是逐步来实现到在神经网络中输入显著水平：&lt;/p&gt;&lt;p&gt;1、使用下面的代码&lt;b&gt;构建&lt;/b&gt;、&lt;b&gt;训练&lt;/b&gt;和&lt;b&gt;保存神经网络&lt;/b&gt;。在训练神经网络之后，我们不会直接使用它来预测，而是将训练过的模型保存到一个文件中。我们为什么要这么做？因为我们需要一个稳定的模型（记住，每次对模型进行训练，每次得到的权重和偏差都会不同）来计算每个输入的显著水平。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import csv
#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
def generatemodel(totvar):
# create and fit the LSTM network
model = Sequential()
model.add(Dense(3, batch_input_shape=(batch_size, totvar), activation=&#39;sigmoid&#39;))
model.add(Dense(hidden_neuron, activation=&#39;sigmoid&#39;))
model.add(Dense(1))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
return model
#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]
#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]
testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]
trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]
testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]
###############
#normalization#
###############
trainparamnorm = np.zeros(np.shape(trainparam))
trainlabelnorm = np.zeros(np.shape(trainlabel))
testparamnorm = np.zeros(np.shape(testparam))
testlabelnorm = np.zeros(np.shape(testlabel))
print &#39;shape label adalah&#39;, np.shape(testlabelnorm)
#for param
for i in xrange(len(trainparam[0])-2):
trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
for i in xrange(2):
trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)
#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
######################
#build and save model#
######################
mod = generatemodel(len(trainparamnorm[0]))
mod.fit(trainparamnorm, trainlabelnorm, epochs=iterasi, batch_size=batch_size, verbose=2, shuffle=True)
#save trained model
mod.save(studentmodel)&lt;/code&gt;&lt;p&gt;2、加载模型并计算其误差：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import csv
import random
#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]
#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]
testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]
trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]
testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]
###############
#normalization#
###############
trainparamnorm = np.zeros(np.shape(trainparam)).astype(&#39;float32&#39;)
trainlabelnorm = np.zeros(np.shape(trainlabel)).astype(&#39;float32&#39;)
testparamnorm = np.zeros(np.shape(testparam)).astype(&#39;float32&#39;)
testlabelnorm = np.zeros(np.shape(testlabel)).astype(&#39;float32&#39;)
#for param
for i in xrange(len(trainparam[0])-2):
trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
for i in xrange(2):
trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)
#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
#load trained model
mod = load_model(studentmodel)
G3pred = mod.predict(testparamnorm, batch_size=batch_size)
G3real = G3pred*20.0
err = mean_squared_error(testlabel, G3real)
print &#39;our error value is&#39;, err&lt;/code&gt;&lt;p&gt;在自己的电脑上，错误是3.44525143751。这是初始误差。&lt;/p&gt;&lt;p&gt;3、&lt;b&gt;为随机改变每个输入值&lt;/b&gt;。我们将随机生成0到1之间的数字，替换测试数据测中的归一化输入参数，并立即将修改后的输入数据应用到刚刚加载的神经网络中。为什么在0和1之间随机生成值呢？因为我们在上面一段使用了第二个归一化函数（使用最大值和最小值）来归一化我们的输入。对每个归一化输入进行迭代，随机改变其值，反复进行，得到大量的样本，从而得到误差的平均值和标准差。这样可以消除偶然因素（记住，我们随机产生值）。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from keras.models import load_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import random
import os
import csv

#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
randsample = 100

#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]

#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]

testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]

trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]

testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]


###############
#normalization#
###############

trainparamnorm = np.zeros(np.shape(trainparam)).astype(&#39;float32&#39;)
trainlabelnorm = np.zeros(np.shape(trainlabel)).astype(&#39;float32&#39;)

testparamnorm = np.zeros(np.shape(testparam)).astype(&#39;float32&#39;)
testlabelnorm = np.zeros(np.shape(testlabel)).astype(&#39;float32&#39;)

#for param
for i in xrange(len(trainparam[0])-2):
 trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
 testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))

for i in xrange(2):
 trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
 testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)

#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))


#load trained model
mod = load_model(studentmodel)

G3pred = mod.predict(testparamnorm, batch_size=batch_size)
G3real = G3pred*20.0

errreal = mean_squared_error(testlabel, G3real)
print &#39;our error value is&#39;, errreal

################################
#permutation importance session#
################################

permutsample = np.zeros((randsample, len(testparamnorm[0])))
for trying in xrange(randsample):
 randval = np.zeros((len(testlabelnorm)))
 for i in xrange(len(testlabelnorm)):
   randval[i] = random.uniform(0,1)

 for i in xrange(len(testparamnorm[0])):
   permutinput = np.zeros(np.shape(testparamnorm))
   permutinput[:] = testparamnorm
   permutinput[:,i] = randval
   G3pred = mod.predict(permutinput, batch_size=batch_size)
   G3real = G3pred*20.0
   err = mean_squared_error(testlabel, G3real)
   permutsample[trying, i] = err

print permutsample
#print testparamnorm

#print mean and standard deviation of error
errperformance = np.zeros((len(testparamnorm[0]), 2))
for i in xrange(len(testparamnorm[0])):
 errperformance[i,0] = np.mean(permutsample[:,i])
 errperformance[i,1] = np.std(permutsample[:,i])
errperformance[:,0] = errreal - errperformance[:,0]

print errperformance&lt;/code&gt;&lt;p&gt;代码输出示例：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9efba29f4c31f95bdc6157d69bc19352_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;674&quot; data-rawheight=&quot;274&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9efba29f4c31f95bdc6157d69bc19352&quot; data-watermark-src=&quot;v2-6d2adb237ef461da3799a0ea8fdefa66&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;4、解释结果。我们得到了一些有趣的结果。首先是第二行，从随机输入值结果中得到的误差变化较小。这表明，参数“出行时间”对学生期末考试的成绩根本没有影响。在最后一行（G2）中，我们得到了一个非常高的误差。这说明第二阶段考试的成绩与期末考试成绩高度相关。从这个结果中，我们得到了输入的显著水平，即G2、G1、考试不及格、空闲时间、缺勤、学习时间和上学时间。另一个有趣的结果是学习时间对期末考试的价值没有明显的影响。这个结果非常违反直觉。在现实生活研究中，必须进一步研究。&lt;/p&gt;&lt;p&gt;这就是一种简单的方法来测量神经网络输入的显著水平。该技术可应用于神经网络、支持向量机和随机森林等其他机器学习算法。&lt;/p&gt;&lt;p&gt;来源：https://medium.com/datadriveninvestor/a-simple-way-to-know-how-important-your-input-is-in-neural-network-86cbae0d3689&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗！&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47475888</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>手把手教你用Numpy构建神经网络！（附代码）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47475047.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47475047&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3996db44efbd7026f85736c5f4937ab_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d0adadbc7e374710cc6f20b574638ffb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d0adadbc7e374710cc6f20b574638ffb&quot; data-watermark-src=&quot;v2-e96895b3f9d106aef40902f9660c108d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Piotr Skalski&lt;/p&gt;&lt;p&gt;本期编译：1+1=3&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289274&amp;amp;idx=1&amp;amp;sn=f40be8372658c2c79fdd47c03d62e037&amp;amp;chksm=802e392fb759b039435fc6700ef5d45142cdfe72234586bd8de9b8dfabcc3264f2ae826def80#rd&quot;&gt;【ML系列】手把手教你用Numpy构建神经网络！（附代码）&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=2020206794&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;7、使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;比如使用Keras，TensorFlow或PyTorch这样的高级框架，我们可以快速构建非常复杂的模型。但是，需要花时间去了解其内部结构并理解基本原理。今天，将尝试利用现有知识，并仅使用Numpy去构建一个完全可操作的神经网络。最后，我们还将测试我们的模型，并将其性能与Keras构建的NN进行比较。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fbc3feb1f21134e08a59b3d75110e69f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;600&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fbc3feb1f21134e08a59b3d75110e69f&quot; data-watermark-src=&quot;v2-a2bdf0d6dba1a06ec247a7cbbf0b8d82&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;准备操作&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们开始编程之前，先准备一个基本的路线图。 我们的目标是创建一个程序，该程序能够创建具有指定体系结构（层的数量和大小以及适当的激活函数）的密集连接的神经网络。 下图给出了这种网络的例子。最重要的是，我们必须能够训练该网络并使用它进行预测。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2971d973dcafd7df30aea49b9cf88d5a_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;585&quot; data-rawheight=&quot;545&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-2971d973dcafd7df30aea49b9cf88d5a_b.jpg&quot;&gt;&lt;p&gt;上图展示了在训练NN期间必须执行的操作。 它还显示了在不同阶段单次迭代，我们需要更新和读取的参数数量。 构建正确的数据结构并巧妙地管理其状态是我们任务中最困难的部分之一。 &lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2fa2d45da50e231cc9d401d7d1b0a8b9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2fa2d45da50e231cc9d401d7d1b0a8b9&quot; data-watermark-src=&quot;v2-5e3c2c7f16f6f2befea95df90c3d1a67&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;创建神经网络层&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从每一层启动权重矩阵W和偏置向量b开始。上标[l]表示当前层的索引（从1开始计数），值n表示给定层中的单元数。 假设描述NN架构的信息将以类似于Snippet 1中所示的列表的形式传递给我们的程序。列表中的每个项目都是描述单个网络层的基本参数的字典：input_dim——作为输入层提供信号矢量的大小，output_dim - 作为输出层获得激活矢量的大小。activation - 在层内使用的激活函数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np

NN_ARCHITECTURE = [
   {&quot;input_dim&quot;: 2, &quot;output_dim&quot;: 25, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 25, &quot;output_dim&quot;: 50, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 50, &quot;output_dim&quot;: 50, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 50, &quot;output_dim&quot;: 25, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 25, &quot;output_dim&quot;: 1, &quot;activation&quot;: &quot;sigmoid&quot;},
]&lt;/code&gt;&lt;p&gt;最后，让我们关注在这一部分中必须完成的主要任务——层参数的初始化。那些已经看过上面代码并对Numpy有一定经验的人注意到，矩阵W和向量b已经被小的随机数填充了。这种做法并非偶然。权重值不能用相同的数字初始化，因为这会导致破坏对称问题。基本上，如果所有的权值都是一样的，不管输入X是多少，隐藏层中的所有单位也是一样的。在某种程度上，我们陷入了最初的状态，没有任何逃脱的希望，无论训练我们的模型多长时间，我们的网络有多深。线性代数是不会原谅你的。&lt;/p&gt;&lt;p&gt;在第一次迭代中，使用小值可以提高算法的效率。下图中的sigmoid函数，我们可以看到，对于较大的值，它几乎是平的，这对NN的学习速度有显著的影响。总之，使用小随机数进行参数初始化是一种简单的方法，但它保证了我们的算法有足够好的起点。准备好的参数值存储在python字典中，带有唯一标识其父层的键。字典在函数末尾返回，因此我们将在算法的下一个阶段访问它的内容。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def init_layers(nn_architecture, seed = 99):
   # random seed initiation
   np.random.seed(seed)
   # number of layers in our neural network
   number_of_layers = len(nn_architecture)
   # parameters storage initiation
   params_values = {}
   
   # iteration over network layers
   for idx, layer in enumerate(nn_architecture):
       # we number network layers from 1
       layer_idx = idx + 1
       
       # extracting the number of units in layers
       layer_input_size = layer[&quot;input_dim&quot;]
       layer_output_size = layer[&quot;output_dim&quot;]
       
       # initiating the values of the W matrix
       # and vector b for subsequent layers
       params_values[&#39;W&#39; + str(layer_idx)] = np.random.randn(
           layer_output_size, layer_input_size) * 0.1
       params_values[&#39;b&#39; + str(layer_idx)] = np.random.randn(
           layer_output_size, 1) * 0.1
       
   return params_values&lt;/code&gt;&lt;h2&gt;&lt;b&gt;激活函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们将要使用的所有函数中，有一些非常简单但功能强大的函数。激活函数可以写在一行代码中，但是它们提供了他们所需要的神经网络非线性和表现力。&lt;b&gt;“没有它们，我们的神经网络就会变成线性函数的组合，所以它本身就是一个线性函数”&lt;/b&gt;。它有很多激活功能，但在这个项目中，使用其中两种功能——sigmoid和ReLU。为了能够运行一个完整的循环并同时向前和向后传播，我们还需要准备它们的导数。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f60fc77b2f2f43ff83d6539e3ac42275_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;320&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-f60fc77b2f2f43ff83d6539e3ac42275_b.jpg&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def sigmoid(Z):
   return 1/(1+np.exp(-Z))

def relu(Z):
   return np.maximum(0,Z)

def sigmoid_backward(dA, Z):
   sig = sigmoid(Z)
   return dA * sig * (1 - sig)

def relu_backward(dA, Z):
   dZ = np.array(dA, copy = True)
   dZ[Z &amp;lt;= 0] = 0;
   return dZ;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;向前传播&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这部分代码可能是最直观、最容易理解的。给定上一层输入信号，计算仿射变换Z，然后应用选定的激活函数。通过使用Numpy，我们可以利用向量化执行矩阵操作。这样做消除了迭代，大大加快了计算速度。除了计算出的矩阵A，我们的函数还返回中间值Z。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0e917d27c37135252c2978e126c3dee5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;220&quot; data-rawheight=&quot;58&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0e917d27c37135252c2978e126c3dee5&quot; data-watermark-src=&quot;v2-33d02233463183e2351ab7940307fff8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=&quot;relu&quot;):
   Z_curr = np.dot(W_curr, A_prev) + b_curr
   
   if activation is &quot;relu&quot;:
       activation_func = relu
   elif activation is &quot;sigmoid&quot;:
       activation_func = sigmoid
   else:
       raise Exception(&#39;Non-supported activation function&#39;)
       
   return activation_func(Z_curr), Z_curr&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f34aad10434a9609a0118a8509991781_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f34aad10434a9609a0118a8509991781&quot; data-watermark-src=&quot;v2-26426c7a10e3798415cef206644c05c1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;随着single_layer_forward_propagation函数的完成，我们可以轻松地向前构建整个步骤。这是一个稍微复杂一点的函数，它的作用不仅是执行预测，还包含中间值的集合。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def full_forward_propagation(X, params_values, nn_architecture):
   memory = {}
   A_curr = X
   
   for idx, layer in enumerate(nn_architecture):
       layer_idx = idx + 1
       A_prev = A_curr
       
       activ_function_curr = layer[&quot;activation&quot;]
       W_curr = params_values[&quot;W&quot; + str(layer_idx)]
       b_curr = params_values[&quot;b&quot; + str(layer_idx)]
       A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)
       
       memory[&quot;A&quot; + str(idx)] = A_prev
       memory[&quot;Z&quot; + str(layer_idx)] = Z_curr
      
   return A_curr, memory&lt;/code&gt;&lt;h2&gt;&lt;b&gt;损失函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了监控项目进展并确保我们正在朝着期望的方向前进，我们也应该计算损失函数的值。“一般来说，损失函数是用来表示我们离‘理想的’解决方案还有多远”。它是根据我们计划解决的问题进行选择的，像Keras这样的框架有很多选择。因为我打算测试我们的NN在两个类之间的点的分类，我们决定使用二进制交叉熵，它是由下面的公式定义的。还有，我们还决定实现一个函数来计算我们的准确性。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19363a3c73098c10c2b444742e477beb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;656&quot; data-rawheight=&quot;156&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-19363a3c73098c10c2b444742e477beb&quot; data-watermark-src=&quot;v2-fb8686f3e8098e506c5b0955697b86c5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def get_cost_value(Y_hat, Y):
   m = Y_hat.shape[1]
   cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))
   return np.squeeze(cost)

def convert_prob_into_class(probs):
   probs_ = np.copy(probs)
   probs_[probs_ &amp;gt; 0.5] = 1
   probs_[probs_ &amp;lt;= 0.5] = 0
   return probs_

def get_accuracy_value(Y_hat, Y):
   Y_hat_ = convert_prob_into_class(Y_hat)
   return (Y_hat_ == Y).all(axis=0).mean()&lt;/code&gt;&lt;h2&gt;&lt;b&gt;单层反向传播步骤&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;遗憾的是，许多缺乏经验的深度学习爱好者认为反向传播是一种令人生畏且难以理解的算法。微积分和线性代数的结合常常使那些没有受过扎实的数学训练的人望而却步。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=&quot;relu&quot;):
   m = A_prev.shape[1]
   
   if activation is &quot;relu&quot;:
       backward_activation_func = relu_backward
   elif activation is &quot;sigmoid&quot;:
       backward_activation_func = sigmoid_backward
   else:
       raise Exception(&#39;Non-supported activation function&#39;)
   
   dZ_curr = backward_activation_func(dA_curr, Z_curr)
   dW_curr = np.dot(dZ_curr, A_prev.T) / m
   db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m
   dA_prev = np.dot(W_curr.T, dZ_curr)

   return dA_prev, dW_curr, db_curr&lt;/code&gt;&lt;p&gt;人们常常把反向宣传播与梯度下降混淆，但实际上这是两个独立的问题。第一种方法的目的是有效地计算梯度，而第二种方法是利用计算得到的梯度进行优化。在NN中，我们计算代价函数关于参数的梯度，但是反向传播可以用来计算任何函数的导数。该算法的本质是递归使用微分学中已知的链式法则——计算集合其他函数而得到的函数的导数，我们已经知道这些函数的导数。这个过程对于一个网络层可以用下面的公式来描述。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1cfe28d2da4e0bb672eabd440edc9580_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;562&quot; data-rawheight=&quot;398&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1cfe28d2da4e0bb672eabd440edc9580&quot; data-watermark-src=&quot;v2-f905eff7e0e46b508fa8185843c811fb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6ad0d6a59d04b349a86636044252a487_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;772&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6ad0d6a59d04b349a86636044252a487&quot; data-watermark-src=&quot;v2-140252dd0494cd1927b8dec7d4bf692c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;就像前向传播一样，将计算分为两个独立的函数。第一个侧重于一个单独的层，可以归结为用Numpy重写上面的公式。第二个表示完全反向传播，主要处理在三个字典中读取和更新值的关键值。我们从计算成本函数对正向传播的预测向量结果的导数开始。这很简单，因为它只包括重写下面的公式。然后遍历网络的各个层，从最后开始，根据上图所示的图计算关于所有参数的导数。最后，函数返回一个Python字典，其中包含我们要查找的梯度。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-efe79cb3b79d26219a9b6b041de1edcc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;298&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-efe79cb3b79d26219a9b6b041de1edcc&quot; data-watermark-src=&quot;v2-a7901c7204da9afccd6fe56ed2d580f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):
   grads_values = {}
   m = Y.shape[1]
   Y = Y.reshape(Y_hat.shape)
  
   dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));
   
   for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):
       layer_idx_curr = layer_idx_prev + 1
       activ_function_curr = layer[&quot;activation&quot;]
       
       dA_curr = dA_prev
       
       A_prev = memory[&quot;A&quot; + str(layer_idx_prev)]
       Z_curr = memory[&quot;Z&quot; + str(layer_idx_curr)]
       W_curr = params_values[&quot;W&quot; + str(layer_idx_curr)]
       b_curr = params_values[&quot;b&quot; + str(layer_idx_curr)]
       
       dA_prev, dW_curr, db_curr = single_layer_backward_propagation(
           dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)
       
       grads_values[&quot;dW&quot; + str(layer_idx_curr)] = dW_curr
       grads_values[&quot;db&quot; + str(layer_idx_curr)] = db_curr
   
   return grads_values&lt;/code&gt;&lt;h2&gt;&lt;b&gt;更新参数值&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;该方法的目标是使用梯度优化更新网络参数。通过这种方式，我们试图使我们的目标函数更接近最小值。为了完成这项任务，我们将使用两个字典作为函数参数：params_values（存储参数的当前值）和grads_values（存储相对于这些参数计算的成本函数导数）。现在你只需要对每一层应用下面的方程。这是一个非常简单的优化算法，我们决定使用它，因为它是更高级优化器的一个很好的起点。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cb4b0f0369b81201f0a2729c1bf58072_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;94&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-cb4b0f0369b81201f0a2729c1bf58072&quot; data-watermark-src=&quot;v2-95847b50ac2c7cc10a90aae425866a89&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def update(params_values, grads_values, nn_architecture, learning_rate):
   for idx, layer in enumerate(nn_architecture):
       layer_idx = idx + 1
       params_values[&quot;W&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;dW&quot; + str(layer_idx)]        
       params_values[&quot;b&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;db&quot; + str(layer_idx)]

   return params_values;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;综合讨论&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最难的部分已经在前面全部概述完毕。我们已经准备好了所有必要的功能，现在我们只需要把它们按正确的顺序放在一起。为了进行预测，我们只需要使用接收到的权重矩阵和一组测试数据运行完整的正向传播。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def train(X, Y, nn_architecture, epochs, learning_rate):
   params_values = init_layers(nn_architecture, 2)
   cost_history = []
   accuracy_history = []
   
   for i in range(epochs):
       Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)
       cost = get_cost_value(Y_hat, Y)
       cost_history.append(cost)
       accuracy = get_accuracy_value(Y_hat, Y)
       accuracy_history.append(accuracy)
       
       grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)
       params_values = update(params_values, grads_values, nn_architecture, learning_rate)
       
   return params_values, cost_history, accuracy_history&lt;/code&gt;&lt;h2&gt;&lt;b&gt;与Keras比较&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在是时候看看我们的模型能否解决一个简单的分类问题了。我们生成了一个由两个类的组成的数据集，如下图所示。让我们试着训练我们的模型去分类这个数据集。为了便于比较，还使用Keras编写了一个框架进行比较。两种模型具有相同的体系结构和学习速率。最终，Numpy模型和Keras模型在测试集上的准确率都达到了95%，但是我们的模型需要几十倍的时间才能达到这样的准确率。在我看来，这种状态主要是由于缺乏适当的优化。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;N_SAMPLES = 1000
TEST_SIZE = 0.1

X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)

def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):
   if (dark):
       plt.style.use(&#39;dark_background&#39;)
   else:
       sns.set_style(&quot;whitegrid&quot;)
   plt.figure(figsize=(16,12))
   axes = plt.gca()
   axes.set(xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;)
   plt.title(plot_name, fontsize=30)
   plt.subplots_adjust(left=0.20)
   plt.subplots_adjust(right=0.80)
   if(XX is not None and YY is not None and preds is not None):
       plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)
       plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=&quot;Greys&quot;, vmin=0, vmax=.6)
   plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors=&#39;black&#39;)
   if(file_name):
       plt.savefig(file_name)
       plt.close()
make_plot(X, y, &quot;Dataset&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bbcb694cad85b9a6dba5bd05273bfe7c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;651&quot; data-rawheight=&quot;633&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bbcb694cad85b9a6dba5bd05273bfe7c&quot; data-watermark-src=&quot;v2-e4c8c6426b0d3613280789dbf607efcc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;NN模型测试&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;# 训练
params_values = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01)
# 预测
Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)

acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))
print(&quot;Test set accuracy: {:.2f}&quot;.format(acc_test))
Test set accuracy: 0.98 &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8c73432bca699946ef2e702e6af86de4_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;224&quot; data-rawheight=&quot;231&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-8c73432bca699946ef2e702e6af86de4_b.jpg&quot;&gt;&lt;h2&gt;&lt;b&gt;Keras模型测试&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;model = Sequential()
model.add(Dense(25, input_dim=2,activation=&#39;relu&#39;))
model.add(Dense(50, activation=&#39;relu&#39;))
model.add(Dense(50, activation=&#39;relu&#39;))
model.add(Dense(25, activation=&#39;relu&#39;))
model.add(Dense(1, activation=&#39;sigmoid&#39;))

model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&quot;sgd&quot;, metrics=[&#39;accuracy&#39;])

# 训练
history = model.fit(X_train, y_train, epochs=200, verbose=0)

Y_test_hat = model.predict_classes(X_test)
acc_test = accuracy_score(y_test, Y_test_hat)
print(&quot;Test set accuracy: {:.2f} - Goliath&quot;.format(acc_test))
Test set accuracy: 0.98 &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-496d82d8a82b4190d0f68954dd5613b8_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;309&quot; data-rawheight=&quot;311&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-496d82d8a82b4190d0f68954dd5613b8_b.jpg&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗！&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47475047</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>使用LSTM预测股票市场基于Tensorflow</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47473497.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47473497&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8cc95b7ae2a14c2c7311e1798ace8010_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f389a4f62cb51f3ae4c0db4468b7ecfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f389a4f62cb51f3ae4c0db4468b7ecfc&quot; data-watermark-src=&quot;v2-91940980656c229c6cb060d187a770fd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Thushan Ganegedara&lt;/p&gt;&lt;p&gt;本期编辑：1+1=3&lt;/p&gt;&lt;p&gt;原来链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【年度系列】使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本文开始前，作者并没有提倡LSTM是一种高度可靠的模型，它可以很好地利用股票数据中的内在模式，或者可以在没有任何人参与的情况下使用。写这篇文章，纯粹是出于对机器学习的热爱。在我看来，该模型已经观察到了数据中的某些模式，因此它可以在大多数时候正确预测股票的走势。但是，这个模型是否可以用于实际，有待用更多回测和实践去验证。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么需要时间序列模型?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你想要正确地模拟股票价格，因此作为股票买家，你可以合理地决定什么时候买股票，什么时候卖股票。这就是时间序列建模的切入点。你需要良好的机器学习模型，可以查看数据序列的历史记录，并正确地预测序列的未来元素是什么。&lt;/p&gt;&lt;p&gt;提示：股市价格高度不可预测且不稳定。这意味着在数据中没有一致的模式可以让你近乎完美地模拟股票价格。就像普林斯顿大学经济学家Burton Malkiel在他1973年的书中写到的：“随机漫步华尔街”，如果市场确实是有效的，那么当股票价格反应的所有因素一旦被公开时，那我们闭着眼睛都可以做的和专业投资者一样好。&lt;/p&gt;&lt;p&gt;但是，我们不要一直相信这只是一个随机过程，觉得机器学习是没有希望的。你不需要预测未来的股票确切的价格，而是股票价格的变动。做到这点就很不错了！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据准备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;使用以下数据源：&lt;/p&gt;&lt;p&gt;地址：https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3ba5be7bb5480c118b1bb0b50e1575f6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;567&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3ba5be7bb5480c118b1bb0b50e1575f6&quot; data-watermark-src=&quot;v2-3de8059f0031a3e442345642c2e844c5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;当然你也可基于Wind数据库去研究。因为Wind数据相对于其他平台和数据商而言，总体上在国内算是比较全面和准确的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从Kaggle获得数据&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在Kaggle上找到的数据是csv文件，所以，你不需要再进行任何的预处理，因此你可以直接将数据加载到DataFrame中。同时你还应该确保数据是按日期排序的，因为数据的顺序在时间序列建模中至关重要。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;df = df.sort_values(&#39;Date&#39;)
df.head()&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c85bc3652efa2f6c80364df245e32d26_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;692&quot; data-rawheight=&quot;362&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c85bc3652efa2f6c80364df245e32d26&quot; data-watermark-src=&quot;v2-5b38149d68e6483f322e0de7759079a2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;数据可视化&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df[&#39;Low&#39;]+df[&#39;High&#39;])/2.0)
plt.xticks(range(0,df.shape[0],500),df[&#39;Date&#39;].loc[::500],rotation=45)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4ed665620beacacc51f8acf2477cd650_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1064&quot; data-rawheight=&quot;584&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4ed665620beacacc51f8acf2477cd650&quot; data-watermark-src=&quot;v2-789653dddc505399eae48474899df116&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图已经说明了很多东西。我选择这家公司而不是其他公司的具体原因是，随着时间的推移，这张图中展现了不同的股价行为。这将使学习更加稳健，并且可以更改以便测试各种情况下预测的好坏程度。&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据拆分训练集和测试集&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;计算一天中最高和最低价的平均值来计算的中间价格。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;high_prices = df.loc[:,&#39;High&#39;].as_matrix()
low_prices = df.loc[:,&#39;Low&#39;].as_matrix()
mid_prices = (high_prices+low_prices)/2.0&lt;/code&gt;&lt;p&gt;现在你可以分离训练数据和测试数据。训练数据是时间序列的前11000个数据，其余的是测试数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_data = mid_prices[:11000]
test_data = mid_prices[11000:]&lt;/code&gt;&lt;p&gt;现在需要定义一个标准对数据进行归一化。MinMaxScalar方法将所有数据归到0和1之间。你还可以将训练和测试数据重新组为[data_size, num_features]。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;scaler = MinMaxScaler()
train_data = train_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)&lt;/code&gt;&lt;p&gt;根据之前得数据，可以看出不同时间段有不同的取值范围，你可以将整个序列拆分为窗口来进行归一化。如果不这样做，早期的数据接近于0，并且不会给学习过程增加太多价值。这里你选择的窗口大小是2500。&lt;/p&gt;&lt;p&gt;当选择窗口大小时，确保它不是太小，因为当执行窗口规范化时，它会在每个窗口的末尾引入一个中断，因为每个窗口都是独立规范化的。&lt;/p&gt;&lt;p&gt;在本例中，4个数据点将受此影响。但假设你有11000个数据点，4个点不会引起任何问题。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
   scaler.fit(train_data[di:di+smoothing_window_size,:])
   train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data 
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])&lt;/code&gt;&lt;p&gt;将数据重新塑造为[data_size]的Shape：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_data = train_data.reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)&lt;/code&gt;&lt;p&gt;现在可以使用指数移动平均平滑数据。可以帮助你避免股票价格数据的杂乱，并产生更平滑的曲线。&lt;/p&gt;&lt;p&gt;我们只使用训练数据来训练MinMaxScaler，&lt;b&gt;通过将MinMaxScaler与测试数据进行匹配来规范化测试数据是错误的。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：你应该只平滑训练数据。&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;EMA = 0.0
gamma = 0.1
for ti in range(11000):
 EMA = gamma*train_data[ti] + (1-gamma)*EMA
 train_data[ti] = EMA

all_mid_data = np.concatenate([train_data,test_data],axis=0)&lt;/code&gt;&lt;p&gt;下面是平均结果。它非常接近股票的实际行为。接下来您将看到一个更精确的一步预测方法：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d6807ad52a0984a928ee20d42035b0e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;538&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d6807ad52a0984a928ee20d42035b0e5&quot; data-watermark-src=&quot;v2-a915d2dff38a71304da508f595170f2b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上面的图（和MSE）说明了什么呢？对于非常短的predictiosn（一天之后）来说，这个模型似乎不算太坏。考虑到股票价格在一夜之间不会从0变化到100，这种行为是明智的。接下来我们来看一种更有趣的平均技术，称为指数移动平均。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;指数移动平均线&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你可能在互联网上看到过一些文章使用非常复杂的模型来预测股票市场的行为。但是要小心！我所看到的这些都只是视觉错觉，不是因为学习了有用的东西。下面你将看到如何使用简单的平均方法复制这种行为。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;window_size = 100
N = train_data.size

run_avg_predictions = []
run_avg_x = []

mse_errors = []

running_mean = 0.0
run_avg_predictions.append(running_mean)

decay = 0.5

for pred_idx in range(1,N):
   
   running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]
   run_avg_predictions.append(running_mean)
   mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)
   run_avg_x.append(date)

print(&#39;MSE error for EMA averaging: %.5f&#39;%(0.5*np.mean(mse_errors)))

MSE error for EMA averaging: 0.00003&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-64d809481eb42a34b13fddf7c90d1d37_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;538&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-64d809481eb42a34b13fddf7c90d1d37&quot; data-watermark-src=&quot;v2-66a21d911e4ca21d6d2c2d47769a9968&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果指数移动平均线很好，为什么需要更好的模型呢？&lt;/p&gt;&lt;p&gt;可以看到，它符合遵循真实分布的完美直线（通过非常低的MSE证明了这一点）。实际上，仅凭第二天的股票市值，你就做不了什么。就我个人而言，我想要的不是第二天股市的确切价格，而是未来30天股市的价格会上涨还是下跌&lt;/p&gt;&lt;p&gt;让我们试着在窗口中进行预测（假设你预测接下来两天的窗口，而不是第二天）。然后你就会意识到EMA会有多么的失败。让我们通过一个例子来理解这一点。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d3ca79365a08fc602ab3b34c0a6bdfd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;848&quot; data-rawheight=&quot;370&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5d3ca79365a08fc602ab3b34c0a6bdfd&quot; data-watermark-src=&quot;v2-3b2cfd64eb4c384341489cedfb2d39ee&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;不管你预测未来的步骤是多少，你都会得到相同的答案。&lt;/p&gt;&lt;p&gt;输出有用信息的一种解决方案是查看基于动量算法。他们的预测是基于过去的近期值是上升还是下降（而不是精确的数值）。例如，如果过去几天的价格一直在下降，第二天的价格可能会更低。这听起来很合理。然而，我们将使用更复杂的模型：LSTM。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;评价结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们将使用均值平方误差来计算我们的模型有多好。均值平方误差(MSE)的计算方法是先计算真实值与预测值之间的平方误差，然后对所有的预测进行平均。但是：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;平均预测是一种很好的预测方法（这对股票市场的预测不是很有用），但对未来的预测并不是很有用。&lt;/b&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;LSTM简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;长短时记忆模型是非常强大的时间序列模型。它们可以预测未来任意数量的步骤。LSTM模块(或单元)有5个基本组件，可以对长期和短期数据进行建模。&lt;/p&gt;&lt;p&gt;LSTM单元格如下所示：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b468b958243ed3c233a36fe3cf07c519_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;372&quot; data-rawheight=&quot;374&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b468b958243ed3c233a36fe3cf07c519&quot; data-watermark-src=&quot;v2-1af93db67e4b64692505034900e4e131&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;计算方程如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-24c5135b5db25ff4eed06f710156aa96_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;460&quot; data-rawheight=&quot;256&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-24c5135b5db25ff4eed06f710156aa96&quot; data-watermark-src=&quot;v2-af18212fe95ba20c7aa7394a7132c424&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Tensorflow为实现时间序列模型提供了一个很好的子API。后面我们会使用到它。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;LSTM数据生成器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先要实现一个数据生成器来训练LSTM。这个数据生成器将有一个名为&lt;b&gt;unroll_batch(…)&lt;/b&gt;的方法，该方法将输出一组按顺序批量获取num_unrollings的输入数据，其中批数据的大小为[batch_size, 1]。然后每批输入数据都有相应的输出数据。&lt;br&gt;&lt;/p&gt;&lt;p&gt;例如，如果num_unrollings=3和batch_size=4则看起来像一组展开的批次。&lt;/p&gt;&lt;p&gt;输入数据： [x0,x10,x20,x30],[x1,x11,x21,x31],[x2,x12,x22,x32]&lt;/p&gt;&lt;p&gt;输出数据： [x1,x11,x21,x31],[x2,x12,x22,x32],[x3,x13,x23,x33]&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据生成器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面将演示如何可视化创建一批数据。基本思想是将数据序列划分为N / b段，使每个段的大小为b，然后定义游标每段为1。然后对单个数据进行抽样，我们得到一个输入（当前段游标索引）和一个真实预测（在[当前段游标+1，当前段游标+5]之间随机抽样）。请注意，我们并不总是得到输入旁边的值，就像它的预测一样。这是一个减少过拟合的步骤。在每次抽样结束时，我们将光标增加1。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-06baea34f8a276d4a60233a04c934c90_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;771&quot; data-rawheight=&quot;471&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-06baea34f8a276d4a60233a04c934c90&quot; data-watermark-src=&quot;v2-f5e9d7802d185e1e6ab51fd634cf538b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;定义超参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本节中，将定义几个超参数。D是输入的维数。很简单，你以之前的股票价格作为输入并预测下一个应该为1。&lt;/p&gt;&lt;p&gt;然后是num_unrollings，它表示单个优化步骤需要考虑多少连续时间步骤。越大越好。&lt;/p&gt;&lt;p&gt;然后是batch_size。批量处理大小是在单个时间步骤中考虑的数据样本的数量。越大越好，因为在给定的时间内数据的可见性越好。&lt;/p&gt;&lt;p&gt;接下来定义num_nodes，它表示每个单元格中隐藏的神经元数量。在这个示例中，你可以看到有三层LSTM。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;D = 1 
num_unrollings = 50 
batch_size = 500 
num_nodes = [200,200,150] 
n_layers = len(num_nodes) 
dropout = 0.2 

tf.reset_default_graph()&lt;/code&gt;&lt;h2&gt;&lt;b&gt;定义输入和输出&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;接下来为训练输入和标签定义占位符。这非常简单，因为你有一个输入占位符列表，其中每个占位符包含一批数据。 该列表包含num_unrollings占位符，它将用于单个优化步骤。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_inputs, train_outputs = [],[]
for ui in range(num_unrollings):
   train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name=&#39;train_inputs_%d&#39;%ui))
   train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = &#39;train_outputs_%d&#39;%ui))&lt;/code&gt;&lt;h2&gt;&lt;b&gt;定义LSTM和回归层的参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;用三个LSTM层和一个线性回归层，用w和b表示，该层提取最后一个长短期内存单元的输出并输出对下一个时间步骤的预测。你可以使用TensorFlow中的MultiRNNCell来封装创建的三个LSTMCell对象。此外，还可以使用dropout实现LSTM单元格，因为它们可以提高性能并减少过拟合。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;lstm_cells = [
   tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],
                           state_is_tuple=True,
                           initializer= tf.contrib.layers.xavier_initializer()
                          )
for li in range(n_layers)]

drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(
   lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout
) for lstm in lstm_cells]
drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)
multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)

w = tf.get_variable(&#39;w&#39;,shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())
b = tf.get_variable(&#39;b&#39;,initializer=tf.random_uniform([1],-0.1,0.1))&lt;/code&gt;&lt;h2&gt;&lt;b&gt;计算LSTM输出并将其输入回归层，得到最终预测结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先创建TensorFlow变量（c和h），它将保持单元状态和长短期记忆单元的隐藏状态。 然后将train_input列表转换为[num_unrollings, batch_size, D]，使用tf.nn.dynamic_rnn计算所需输出。然后使用tf.nn.dynamic_rnn计算LSTM输出。并将输出分解为一列num_unrolling的张量。预测和真实股价之间的损失。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;c, h = [],[]
initial_state = []
for li in range(n_layers):
 c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))
 h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))
 initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))

all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)

all_lstm_outputs, state = tf.nn.dynamic_rnn(
   drop_multi_cell, all_inputs, initial_state=tuple(initial_state),
   time_major = True, dtype=tf.float32)

all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])

all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)

split_outputs = tf.split(all_outputs,num_unrollings,axis=0)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;损失计算和优化器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在，要计算损失。然而，在计算损失时，你应该注意到有一个独特的特征。对于每一批预测和真实输出，计算均方误差。然后把所有这些均方损失加起来（不是平均值）。最后，定义要用来优化神经网络的优化器在这种情况下，您可以使用Adam，这是一个非常新且性能良好的优化器。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print(&#39;Defining training Loss&#39;)
loss = 0.0
with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+
                            [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):
 for ui in range(num_unrollings):
   loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)

print(&#39;Learning rate decay operations&#39;)
global_step = tf.Variable(0, trainable=False)
inc_gstep = tf.assign(global_step,global_step + 1)
tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)
tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)

learning_rate = tf.maximum(
   tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),
   tf_min_learning_rate)

# Optimizer.
print(&#39;TF Optimization operations&#39;)
optimizer = tf.train.AdamOptimizer(learning_rate)
gradients, v = zip(*optimizer.compute_gradients(loss))
gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
optimizer = optimizer.apply_gradients(
   zip(gradients, v))

print(&#39;\tAll done&#39;)&lt;/code&gt;&lt;p&gt;这里定义了与预测相关的TensorFlow操作。首先，为输入（sample_inputs）定义一个占位符，然后与训练阶段类似，定义预测的状态变量（sample_c和sample_h）。最后用tf.nn.dynamic_rnn计算预测。后通过回归层（w和b）发送输出。 还应该定义reset_sample_state操作，该操作将重置单元状态和隐藏状态。 每次进行一系列预测时，都应该在开始时执行此操作。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print(&#39;Defining prediction related TF functions&#39;)

sample_inputs = tf.placeholder(tf.float32, shape=[1,D])

sample_c, sample_h, initial_sample_state = [],[],[]
for li in range(n_layers):
 sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))
 sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))
 initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))

reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],
                              *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])

sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),
                                  initial_state=tuple(initial_sample_state),
                                  time_major = True,
                                  dtype=tf.float32)

with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+
                             [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  
 sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)

print(&#39;\tAll done&#39;)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;运行LSTM&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这里，你将训练和预测几个时期的股票价格走势，看看这些预测是否会随着时间的推移而变得更好或更糟。按照以下步骤操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在时间序列上定义一组测试起点（test_points_seq）来计算LSTM&lt;/li&gt;&lt;li&gt;对于每一个epoch&lt;/li&gt;&lt;ul&gt;&lt;li&gt;用于训练数据的完整序列长度&lt;/li&gt;&lt;ul&gt;&lt;li&gt;展开一组num_unrollings批次&lt;/li&gt;&lt;li&gt;使用展开的批次LSTM进行训练&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;计算平均训练损失&lt;/li&gt;&lt;li&gt;对于测试集中的每个起点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;通过迭代在测试点之前找到的以前的num_unrollings数据点来更新LSTM状态&lt;/li&gt;&lt;li&gt;使用先前的预测作为当前输入，连续预测n_predict_once步骤&lt;/li&gt;&lt;li&gt;计算预测到的n_predict_once点与当时股票价格之间的MSE损失&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;部分代码&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;epochs = 30
valid_summary = 1 
n_predict_once = 50
train_seq_length = train_data.size
train_mse_ot = [] 
test_mse_ot = [] 
predictions_over_time = []
session = tf.InteractiveSession()
tf.global_variables_initializer().run()
loss_nondecrease_count = 0
loss_nondecrease_threshold = 2 

print(&#39;Initialized&#39;)
average_loss = 0
data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings) 
x_axis_seq = []
test_points_seq = np.arange(11000,12000,50).tolist() 

for ep in range(epochs):       
   
   # ========================= Training =====================================
   for step in range(train_seq_length//batch_size):
       
       u_data, u_labels = data_gen.unroll_batches()

       feed_dict = {}
       for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            
           feed_dict[train_inputs[ui]] = dat.reshape(-1,1)
           feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)
       
       feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})

       _, l = session.run([optimizer, loss], feed_dict=feed_dict)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;可视化预测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;可以看到MSE损失是如何随着训练量的减少而减少的。这是一个好迹象，表明模型正在学习一些有用的东西。你可以看到LSTM比标准平均值做得更好。标准平均（虽然不完美）合理地跟随真实的股票价格运动。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;best_prediction_epoch = 28 
plt.figure(figsize = (18,18))
plt.subplot(2,1,1)
plt.plot(range(df.shape[0]),all_mid_data,color=&#39;b&#39;)

predictions with high alpha
start_alpha = 0.25
alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))
for p_i,p in enumerate(predictions_over_time[::3]):
   for xval,yval in zip(x_axis_seq,p):
       plt.plot(xval,yval,color=&#39;r&#39;,alpha=alpha[p_i])

plt.title(&#39;Evolution of Test Predictions Over Time&#39;,fontsize=18)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.xlim(11000,12500)

plt.subplot(2,1,2)

plt.plot(range(df.shape[0]),all_mid_data,color=&#39;b&#39;)
for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):
   plt.plot(xval,yval,color=&#39;r&#39;)
   
plt.title(&#39;Best Test Predictions Over Time&#39;,fontsize=18)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.xlim(11000,12500)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-622bf3317cdbb3fd1f5dab2040f38541_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1067&quot; data-rawheight=&quot;1051&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-622bf3317cdbb3fd1f5dab2040f38541&quot; data-watermark-src=&quot;v2-bbb5d3febe3d4c1005d52102564453c4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;尽管LSTM并不完美，但它似乎在大多数情况下都能正确预测股价走势。请注意，你的预测大致在0和1之间（也就是说，不是真实的股票价格）。这是可以的，因为你预测的是股价的走势，而不是股价本身。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结论&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;股票价格/移动预测是一项极其困难的任务。就我个人而言，我认为任何股票预测模型都不应该被视为理所当然，并且盲目地依赖它们。然而，模型在大多数情况下可能能够正确预测股票价格的变动，但并不总是如此。&lt;/p&gt;&lt;p&gt;&lt;b&gt;不要被那些预测曲线完全与真实股价重叠的文章所迷惑。这可以用一个简单的平均技术来复制，但实际上它是无用的。更明智的做法是预测股价走势。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;模型的超参数对你得到的结果非常敏感。因此，一个非常好的事情是在超参数上运行一些&lt;b&gt;超参数优化技术&lt;/b&gt;（例如，网格搜索/随机搜索）。这里列出了一些最关键的超参数：&lt;b&gt;优化器的学习率&lt;/b&gt;、&lt;b&gt;层数&lt;/b&gt;、&lt;b&gt;每层的隐藏单元数&lt;/b&gt;，优化器Adam表现最佳，模型的类型（GRU / LSTM / LSTM with peepholes）。&lt;/p&gt;&lt;p&gt;由于本文由于数据量小，我们用测试损耗来衰减学习速率。这间接地将测试集的信息泄露到训练过程中。处理这个问题更好的方法是有一个单独的验证集（除了测试集）与验证集性能相关的衰减学习率。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47473497</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
