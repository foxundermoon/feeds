<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>公众号主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Wed, 14 Mar 2018 11:50:43 +0800</lastBuildDate>
<item>
<title>【必看】机器学习应用量化投资必须要踩的那些坑！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-12-34475061.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34475061&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1f33ef2a74bc4f6928f716dc70b99b1e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;前四期传送门：&lt;/p&gt;&lt;p&gt;【系列54】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287512&amp;amp;idx=1&amp;amp;sn=14ee62549dab3c64468f78b3dbfd39a5&amp;amp;chksm=802e364db759bf5bb5abffc6a50f72d0e31722c178e01ce11a3d48fb28386055e741c9ecce8d&amp;amp;scene=21#wechat_redirect&quot;&gt;因子的有效性分析基于7种机器学习算法&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列53】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287481&amp;amp;idx=1&amp;amp;sn=dcb1dda1e2362d8297ae1a97845cf02e&amp;amp;chksm=802e362cb759bf3a3aaea75af824451a3dba7345ecc73e27facc4b917792835fdd2878403c8c&amp;amp;scene=21#wechat_redirect&quot;&gt;基于XGBoost的量化金融实战&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e38fa3ed8bcdd825f7746f42bd2e4775_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;570&quot;&gt;&lt;p&gt;今天，继续我们的机器学习应用量化投资系列。本期我们介绍一篇杨勇团队撰写的研究报告。希望大家在写策略注意这些问题。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;从高频到低频&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器学习在高频量化策略上应用更加容易。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从线性到非线性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器学习下的非线性比线性更能榨取数据的价值，但也更容易过度拟合，因此需要合理使用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从单次分析到推进分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推进分析更加符合实盘状态下盘后更新模型的实际情况。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从分类到回归&lt;/b&gt;&lt;/p&gt;&lt;p&gt;回归经常能优于简单的分成两类。&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测值相关&lt;/b&gt;&lt;/p&gt;&lt;p&gt;好的预测值不一定带来好的交易信号。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;标准神经网络回归大盘择时策略&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1.1. 设想和目标&lt;/b&gt;&lt;/p&gt;&lt;p&gt;运用机器学习对过去的模式进行识别，并预测未来。也即，用当前实时数据与过去所有数据进行模式匹配，若过去模式显示会大概率上涨下跌，则相应做多做空，否则不做操作。原本该模型是为日内策略设计的，也就是收盘平仓，但由于目前平今仓手续费昂贵，所以改为第二天开盘平。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2. 理论、方法及数据源&lt;/b&gt;&lt;/p&gt;&lt;p&gt;和所有量化策略相似，研究假设过去发生的事情未来会重复发生（也即挑战市场弱有效的假设）。另一重要假设是指数现货和指数期货之间相关性很高，接近1。这个假设是合理，因为在流动性充足的市场，如果现货和期货之间的任何偏差都可以造成套利机会。故可以用现货做期货。方法为传统的深度学习方法。数据源来自天软、万得，主要是中证500指数，沪深300指数，以及对应的期货主力合约。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3. 交易成本与策略执行&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在此策略的历史数据回测中，成交成本假设为日内单边千分之一，隔日单边万分之3。也即在成交中假设1.5个指数点的冲击成本。这样的假设充分包含了目前股指期货低流动性的现实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该算法共有7个模型，分别对应10:00,10:30,11:00,13:00,13:30,14:00,14:30的决策时间点。每个模型的本质是相似的，唯一的不同只在于越向后的模型，所能拥有的供机器决策数据越多。例如在10:30做决策会比10:00做决策多出半小时的数据。每个模型本身都是监督式学习。用价量指标来预测收益。若基于机器学习的预测值触及多头开仓阈值，则做多；若基于机器学习的预测值触及空头开仓阈值，则做空。反之维持原来仓位。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.5. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：3.55&lt;br&gt;最大回撤：17.05%&lt;br&gt;胜率：62.69&lt;br&gt;盈亏比：1.31&lt;br&gt;年化：80.36%&lt;/blockquote&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-969ba198529defa23be5ac24dd392b76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;890&quot; data-rawheight=&quot;337&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-59e13eea274e23ab5a664f85cb34d77d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;887&quot; data-rawheight=&quot;351&quot;&gt;&lt;p&gt;&lt;b&gt;1.6. 策略因子归因&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用前述的策略因子归因方法，可以看出一些非常重要的特点。&lt;/p&gt;&lt;p&gt;可以从图中看到，开盘的前一个小时行情数据产生的因子和收盘最后一个小时行情数据产生的因子是非常重要的，如14:30到15:00的收益。这与人的主观经验是一致的，开盘前一小时交投最活跃，基本能反应当天的市场情绪和主导全天的走势。而收盘最后一小时由于经常是对第二天情绪的猜测，所以从它的走势经常能推断出第二天市场的方向。&lt;/p&gt;&lt;p&gt;另外，盘中两个小时交投最不活跃，随机性也越大。单独用盘中两个小时作为因子去预测未来收益相对来说效果会差一些。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8885a17924d2d9cec6ddec3a38dc52ed_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;428&quot; data-rawheight=&quot;413&quot;&gt;&lt;p&gt;&lt;b&gt;1.7. 风险点及未来的改进方向&lt;/b&gt;&lt;/p&gt;&lt;p&gt;风险点主要有：&lt;/p&gt;&lt;p&gt;（1）期货和现货突然性的暂时偏离（在当前负基差的情况下和低成交量下，尤其可能发生）&lt;/p&gt;&lt;p&gt;（2）市场结构发生了深刻的变化（投资者类型，投资者风险偏好等等），导致过去的数据不再能预测未来。例如去年股灾期间国家队的大规模救市。&lt;/p&gt;&lt;p&gt;（3）市场流动性不足，导致成交需要付出巨大成本或者无法成交。以中证500为例，本模型单笔收益大约在千分之三左右，如果买卖价差长期超过5个指数点，将对策略的盈利能力造成毁灭性的打击。目前股指期货受限以来买卖价差大约为1-2个指数点。&lt;/p&gt;&lt;p&gt;（4）没有合适的报撤单逻辑。如果出现单边市场，简单的用限价单的报撤会导致以最不利的价格成交。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从低频到高频&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了比较高频分钟线与日线策略的区别，我们也设计了一个日线策略。这个策略是基于传统技术指标做特征，例如昨日收盘价相对于过去几日的均线的位置，以及高开低开情况等等。去预测未来的一日的收益。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该算法每日决策一次，每个模型本身都是监督式学习。用价量指标来预测收益。若基于机器学习的预测值触及多头开仓阈值，则做多；若基于机器学习的预测值触及空头开仓阈值，则做空。反之维持原来仓位。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：0.68&lt;br&gt;最大回撤：36.92%&lt;br&gt;胜率：53.21&lt;br&gt;盈亏比：0.99&lt;br&gt;年化： 19.02%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb9ce977f34138a563fde4edbb01e911_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;885&quot; data-rawheight=&quot;326&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d018a9cba0517416247afae7184babb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;889&quot; data-rawheight=&quot;342&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2c5ecabff105fc8b09af9ef450ac22c3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;805&quot; data-rawheight=&quot;163&quot;&gt;&lt;p&gt;&lt;b&gt;1.3. 高频背后的一些逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1. 数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通常来说，对机器学习模型，数据量越大越好。在假设能反映出目前市场的前提下，尽可能多的增加训练集的长度，对机器学习的模型算法收敛和模型稳定性是大有裨益的。假设是日线，以每年250个交易日为例，那么2010年到2017年就是大约1700个数据点。但如果是分钟线，同样是每年250个交易日，每个交易日240根分钟线，那么一共就有1700*240=408,000的数据点。显然后者就比前者多了好几个数量级。&lt;/p&gt;&lt;p&gt;但是并不是数据量的增加可以无限的，数据量的增加会收到其他客观条件所约束，如运算速度和交易成本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.1. 运算速度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;举例期货的例子来说，交易所每500毫秒推送一个tick，所以理论上，2010年到2017年就可以有49,000,000个数据点。如果假设交易策略是简单的每500毫秒预测一次，那么数据点的增加在实盘中就并没有什么用处。因为在CPU下，神经网络的计算用时不太可能在500毫秒之内。所以在一个决策时间点内没有算完，就已经进入了下一个决策时间点，实盘当中根本交易不到。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.2. 交易成本&lt;/b&gt;&lt;/p&gt;&lt;p&gt;同样上一个期货的例子。如果是非做市类策略，那么算上冲击成本后的交易成本通常双边至少要达到千分之一。在500毫秒乃至更长一些的时间尺度，由于时间时间尺度偏短，波动很难非常大，所以这是一个非常难覆盖交易成本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.3. 日内消息面&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国内股市实行每日交易时间是四份小时。四个小时内出现基本面新消息的概率较小。而隔日的话，各种消息面容易打破股价自身的运行规律。使得预测的准确性大幅降低。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.4.&lt;/b&gt; 行为金融&lt;/p&gt;&lt;p&gt;人的行为在短期内是比较固定的。比如日内短线的追涨杀跌等等，这些都是由人性所决定的。但是随着时间的拉长，特别是两个交易日之间，人会冷静下来，情绪会淡化。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从线性到非线性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了比较线性模型与非线性模型的区别，我们也设计了一个线性模型。这个策略是基于传统线性核函数的支持向量机回归，使用标准的神经网络回归策略一样的因子和预测目标。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，神经网络被替换成了线性核函数的支持向量机回归。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：0.95&lt;br&gt;最大回撤：29.71%&lt;br&gt;胜率：49.64&lt;br&gt;盈亏比：1.23&lt;br&gt;年化：17.67%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6f5f4fb6371357a81405e6feabb7fe4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;887&quot; data-rawheight=&quot;338&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-38f522e07fa61e47311117d06f41a886_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;894&quot; data-rawheight=&quot;367&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-83118faa54190d798b117e216be46a8f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;892&quot; data-rawheight=&quot;337&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6e3d0f4d07ea71a48f83947786d68033_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;163&quot;&gt;&lt;p&gt;&lt;b&gt;3.3. 非线性背后的一些逻辑和讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.1. 金融市场大概率是非线性的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;金融市场大概率是非线性的，举例而言，业内研究发现，不是高开幅度越大，当日的后续走势就越向上。如果当日高开0~0.5%左右，那么当日大概率是上扬的，但是如果高开的过大，当日就容易高开低走。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.2.  Bias-VarianceTrade off&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e463aeb59570d5950456182d6adeb3fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;723&quot; data-rawheight=&quot;1188&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1acc02f20b8c80ed566fa29a70784ddf_r.jpg&quot; data-caption=&quot;Dropout 算法&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;500&quot; data-rawheight=&quot;230&quot;&gt;&lt;p&gt;如上图所示，被舍弃的神经元用X表示出来了。所有指向它和从它发出的有向箭头都被斩断。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.3. 人的理解方式经常是线性的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;正向线性思维的特点是，思维从某一个点开始，沿着正向向前以线性拓展，经过一个或是几个点，最终达到思维的正确结果。举例而言，经常有人会认为，如果过去100年平均每年的生产力提升是一千亿，那么未来一年统计上的期望生产力提升也是一千亿。这里就犯了一个常见错误。人类生产力的提升经常是指数级别上升的，所以未来一年统计上的期望生产力提升应该不止一千亿。&lt;/p&gt;&lt;p&gt;正是因为人的思维方式是线性的，在理解非线性的时候会直观上比较困难。为了增加读者对非线性的直观理解，我们将在下一篇中重点阐述。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从单次分析到推进分析&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;4.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，我们这次要比较第一篇报告中写了单次分析和推进分析。&lt;/p&gt;&lt;p&gt;下面是单次分析的常见方法：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-10eaa572edc2c9bb80992ac07ad96e93_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;74&quot;&gt;&lt;p&gt;上图给了一个单次分析的实例。实际上单次分析就是把整个样本分为互不重叠的两个部分。白色的是样本内，灰色的是样本外。首先用样本内的数据训练机器学习模型，然后用这个建立好的机器学习模型直接放入样本外数据进行检验，如果在样本外的数据依然说明该模型效果很好，那么在一定程度上说明该模型可以处理实际的问题而推进分析的样本内外常常变化。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-47f7b1b20a6dda17406cb528e7ab7c56_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;730&quot; data-rawheight=&quot;194&quot;&gt;&lt;p&gt;上图是一种推进分析的方法。推进分析有个最为明显的特点，就是样本外的交易长度仅为一个交易周期。同样的，首先用样本内的数据训练机器学习模型，然后用这个建立好的机器学习模型直接放入样本外数据进行检验。在T1时刻，用0~T1的数据训练模型，然后在T1~T2的数据去检验模型；在T2时刻，用0~T2的数据训练模型，然后在T2~T3的数据去检验模型；在T3时刻，用0~T3的数据训练模型，然后在T3~T4的数据去检验模型，以此类推。最后将所有灰色框内的检验结果汇总，就是推进分析下总的样本外结果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：2.66&lt;br&gt;最大回撤：17.24%&lt;br&gt;胜率：57.56&lt;br&gt;盈亏比：1.22&lt;br&gt;年化：56.38%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-93f1c4b4391948be4591dd72568ab5d4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;885&quot; data-rawheight=&quot;282&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a03e23c9026dca225287389073ab33a8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;298&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b029cadec3c51aa0d869a68378d78f88_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;343&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bfd31babbaccc71c254dd3bd81034547_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;794&quot; data-rawheight=&quot;162&quot;&gt;&lt;p&gt;&lt;b&gt;4.3. 单次分析和推进分析的逻辑讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在逻辑上，推进分析更接近实盘。因为在实盘中，经常地，模型会每日在盘后更新。所以如果是这样，在回测时候，也应该假定，在T日末，模型会被重新训练，也即是站在T日的模型和站在T+1的模型也应当是不一样的。这样做的显著好处是，不论是回测还是实盘，每个模型都能用到站在当前时点上最新的数据。坏处也很显著，在回测时候，对历史上的每一天都要建立一个模型，这样的计算量是巨大的。&lt;/p&gt;&lt;p&gt;另外推进分析也有不同的方法。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9627ee0c4ec83e3ce7b73450a6051e84_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;810&quot; data-rawheight=&quot;208&quot;&gt;&lt;p&gt;上图是另一种推进分析的方法（Rolling）。与之前推进分析的方法不同，在T2时刻，用0~T2的数据训练模型，然后在T2~T3的数据去检验模型；在T3时刻，我们并不像之前一样，用0~T3的数据训练模型，而是用T1~T3的数据训练模型，然后同样的在T3~T4的数据去检验模型，以此类推。最后将所有灰色框内的检验结果汇总，就是推进分析下总的样本外结果。&lt;/p&gt;&lt;p&gt;使用全样本做推进分析和使用过去n期样本做推进分析之间没有优劣之分。选择时候大体上要遵循两个基本原则，一个是数据要具有对当前市场状态的代表性，另一个数据量要尽可能多。使用过去n期样本通常能对当前市场状态的代表性，使用全样本做推进分析。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从分类到回归&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;5.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，预测目标不再是某段时间的收益，而是一个二分类。也即，大于0的时候是上涨分类，小于0的时候是下跌分类。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：1.66&lt;br&gt;最大回撤：25.30%&lt;br&gt;胜率：49.72%&lt;br&gt;盈亏比：1.39&lt;br&gt;年化：30.91%&lt;/blockquote&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3362bc9ff3f7b250fcbf5675523f1925_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;294&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0d537f23041608c16a00817b2b13dfb1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;319&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-91b71a3b927eb85ca174dba987737674_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;895&quot; data-rawheight=&quot;341&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-edb04cc8fc9e8ed1d0c010294eddc976_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;162&quot;&gt;&lt;p&gt;&lt;b&gt;5.3. 分类与回归的逻辑讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分类的逻辑是，市场的状态是离散可分的。如果按照上例，涨0.1%和涨10%都会归结到上涨一栏。但是事实上，涨0.1%和涨10%是截然不同的，前者很可能是随机扰动，而后者一定是市场情绪的体现。但是随机扰动与市场情绪的分界点是很难确定的，涨0.1%是随机扰动，但是涨0.5%是不是随机扰动呢？所以分类有个天生的问题，以什么标准来划分类，如何划分类？我们也曾经尝试过划分成5类，7类，但是由于划分的类过多，效果也不及二分类。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;预测值相关&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;6.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样。在标准神经网络回归，我们只有当大于一个阈值的时候，做多。小于一个阈值的时候做空。但是在该策略中，再算出预测值后，直接预测值大于0就做多，小于0就做空。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：2.17&lt;br&gt;最大回撤：26.05%&lt;br&gt;胜率：46.68%&lt;br&gt;盈亏比：1.75&lt;br&gt;年化：43.92%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4a1631f8ec1b942036d0c37561aca1e4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;318&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1d079e829a45e76f1099cc8e856aed04_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;890&quot; data-rawheight=&quot;338&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2e27e4c6c26a27ecaa9f320b06d5db53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;354&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-808a4d46bf83936d11d563571d80d7a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;164&quot;&gt;&lt;p&gt;&lt;b&gt;6.3. 预测值相关的逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;预测值可以从上述的回测看到，预测值如果按照简单的大于0，小于0交易，效果并不是特别出色。目前业内比较公认的结论是，预测值的强度代表方向的概率。举例而言，如果一个预测值是0.1%，一个是1%，那么后者实际上涨的概率大于前者。因此，选择一个合适的阈值变的至关重要。可能的确定阈值的方法可以是，历史上预测值的平均数加上一个历史上预测值的标准差作为看多阈值；历史上预测值的平均数减去一个历史上预测值的标准差作为看空阈值。&lt;/p&gt;&lt;p&gt;一个绝对值较大的预测阈值容易漏掉一些真正的上涨机会（统计上的Type II error），而一个绝对值比较小的预测阈值容易错误的开多仓（统计上的Type I error）。同时，绝对值预测阈值越小，越容易达到阈值，越容易触发交易，交易频率就越高，交易成本就越高。反之交易就越不频繁，交易成本就越低。&lt;/p&gt;&lt;p&gt;杨勇团队&lt;/p&gt;&lt;p&gt;分析师：周袤&lt;/p&gt;&lt;p&gt;联系方式：18601798125&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-12-34475061</guid>
<pubDate>Mon, 12 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>基于XGBoost的量化金融实战【系列53】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-24-33948430.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33948430&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-72db6022f891e159901bddd2f66a3ad7_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;前两期传送门：&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;今天，我们介绍一篇&lt;b&gt;王老板&lt;/b&gt;写的文章，关于极度梯度提升(XGBoost)应用量化金融方向的，而且知道几乎每个参加 Kaggle 比赛的人都会用它。用今天我们来预测借贷俱乐部 (Lending Club) 的贷款的良恶性。&lt;/p&gt;&lt;p&gt;获取数据集，请在文末获取。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;XGBoost 基础版&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from xgboost import XGBClassifier&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;首先调用本章需要通用的包，在之后每小节中额外需要的包会特别指出来。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;numpy: 提供数组结构和辅助函数&lt;/li&gt;&lt;li&gt;pandas: 提供数据表结构来处理数据&lt;/li&gt;&lt;li&gt;matplotlib: 用来画图 &lt;/li&gt;&lt;li&gt;sklearn: 用来机器学习&lt;/li&gt;&lt;li&gt;xgboost: 极度梯度提升&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外&lt;/p&gt;&lt;ul&gt;&lt;li&gt;train_test_split 划分训练和测试集&lt;/li&gt;&lt;li&gt;accuracy_score 计算精度和得分&lt;/li&gt;&lt;li&gt;XGBclassifier 是 XGBoost 的分类器&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;整篇代码都用以下设置&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;seed = 7
test_size = 0.33&lt;/code&gt;&lt;p&gt;用一个固定的 seed 为了重现结果，训练集和测试集的样例个数比例为 2 比 1。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.1 模型初探&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;学陌生东西上手最快的方式就是用例子。先不管 XGBclassifier 每个参数是什么，先用它的默认值跑跑看看结果如何。&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;比马印第安人糖尿病 (Pima Indians Diabetes)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据根据医疗记录预测比马印第安人 5 年内糖尿病的发病情况。它是一个二元分类问题，一共有 768 个样例。该数据集包含了 8 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;怀孕次数&lt;/li&gt;&lt;li&gt;2 小时的血浆葡萄糖浓度。&lt;/li&gt;&lt;li&gt;舒张压&lt;/li&gt;&lt;li&gt;三头肌皮肤褶层厚度&lt;/li&gt;&lt;li&gt;2 小时血清胰岛素含量&lt;/li&gt;&lt;li&gt;体重指数&lt;/li&gt;&lt;li&gt;糖尿病家族史&lt;/li&gt;&lt;li&gt;年龄&lt;/li&gt;&lt;li&gt;类变量 (0 或 1）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [1]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1.1. 准备数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;读取 csv 文件，并打印出头尾 3 行。&lt;b&gt;点击下图&lt;/b&gt;发现该数据都是数值型，因此不用做任何转换。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# load data
data = pd.read_csv(&#39;pima-indians-diabetes.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e9532cb117669c05356f1a0be8af9754_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;210&quot;&gt;&lt;p&gt;&lt;b&gt;点击下图&lt;/b&gt;查看每列特征的统计值，比如个数和均值，没有发现什么异常。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;data.describe()&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3606a337b7ab9c86e5cfa0b714b6d380_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;269&quot;&gt;&lt;p&gt;定义函数 splitXy，划分 data 中的特征 X 和标记 y。通常&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最后一列是 y，用 dataset[:, -1]&lt;/li&gt;&lt;li&gt;前面所有列是 X, 用 dataset[:, 0:-1]&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;def splitXy( data ):
   dataset = data.values
   X = dataset[:,0:-1]
   y = dataset[:,-1]
   return X, y&lt;/code&gt;&lt;p&gt;划分出训练集 (X_train, y_train) 和测试集 (X_test, y_test)&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# split data into train and test sets
X_train, X_test, y_train, y_test =
train_test_split(X, y, test_size=test_size,
random_state=seed)&lt;/code&gt;&lt;p&gt;打印每个变量大小，确保集合划分正确。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print( &#39;The size of X is&#39;, X.shape )
print( &#39;The size of y is&#39;, y.shape )
print( &#39;The size of X_train is&#39;, X_train.shape )
print( &#39;The size of y_train is&#39;, y_train.shape )
print( &#39;The size of X_test is&#39;, X_test.shape )
print( &#39;The size of y_test is&#39;, y_test.shape )
The size of X is (768, 8)
The size of y is (768,)
The size of X_train is (514, 8)
The size of y_train is (514,)
The size of X_test is (254, 8)
The size of y_test is (254,)&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.2. 训练模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;定义函数 fit 训练 XGBClassifier(&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def fit( X, y ):
   model = XGBClassifier()
   model.fit(X, y)
   return model&lt;/code&gt;&lt;p&gt;在训练集上调用函数 fit 并打印模型，注意模型用 binary:logistic 作为目标，因为该问题是二元分类问题。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model = fit(X_train, y_train)
print(model)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
     gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
     min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
     objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
     scale_pos_weight=1, seed=0, silent=True, subsample=1)&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.3. 预测结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;定义函数 predict 预测新示例并计算精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def predict( model, X, y ):
   # make predictions for X
   y_pred = model.predict(X)
   predictions = [round(value) for value in y_pred]
   # evaluate predictions
   accuracy = accuracy_score(y, predictions)
   return predictions, accuracy&lt;/code&gt;&lt;p&gt;在测试集上调用函数 predict 并打印精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;_, accuracy = predict(model, X_test, y_test)
print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))
Accuracy: 77.95%&lt;/code&gt;&lt;p&gt;&lt;b&gt;对于一个简单的没有经过调参的模型，在测试集上的 77.95% 精度还算不错。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1.4 保存和加载模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有时候训练一次模型会花很长时间，保存训练好的模型以便下次直接用可以节省很多时间和资源。首先引进 pickle 包。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import pickle&lt;/code&gt;&lt;p&gt;用 dump 函数来保存模型，将 model 命名成 pima.dat。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# save model to file
pickle.dump(model, open(&quot;pima.dat&quot;, &quot;wb&quot;))&lt;/code&gt;&lt;p&gt;用 load 函数来加载数据 pima.dat 命名为模型 pima_model。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# load model from file
pima_model = pickle.load(open(&quot;pima.dat&quot;, &quot;rb&quot;))&lt;/code&gt;&lt;p&gt;最后检查模型，发现 pima_model 在测试集的精度为 77.95%，和上节的结果一样。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;_, accuracy = predict(pima_model, X_test, y_test)
print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))
Accuracy: 77.95%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.5 可视化树&lt;/b&gt;&lt;/p&gt;&lt;p&gt;安装并引入 graphviz 包里面的 Digraph 和 xgboost 包里面的 plot_tree 用于画出模型中的某一棵树。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from graphviz import Digraph
from xgboost import plot_tree&lt;/code&gt;&lt;p&gt;比如你想看第 5 棵树是如何分裂的，设 num_trees = 4，注意 python 第一个对应的是 index 0。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;plot_tree(model, num_trees = 4)
plt.show()
plt.savefig(&#39;Tree from Top to Bottom.png&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-cdebdeb6f8711e858687c4760c9ea29f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;525&quot;&gt;&lt;p&gt;对上图的一些解释：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;f0, f1, ... f7 代表 pima 数据里的 8 个特征简写，其中 f1 对应着 “2 小时的血浆葡萄糖浓度”，树的根部一开始由这个特征在 127.5 的特征值上分裂。&lt;/li&gt;&lt;li&gt;XGBoost 可以自动处理缺失值，从上图可看出 missing 和 yes 归成一起&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外你想看第 1 棵树是如何分裂的，并且喜欢树从左往右分裂，而不是从上往下 (默认方式) 分裂，只需设置 rankdir = &#39;LR&#39;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;plot_tree(model, num_trees=0, rankdir=&#39;LR&#39;)
plt.show()
plt.savefig(&#39;Tree from Left to Right.png&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ff41d578069fa5b9b6526077ba460313_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;694&quot;&gt;&lt;h2&gt;&lt;b&gt;1.2 数据预处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;先载入 LabelEncoder 和 OneHotEncoder&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder&lt;/code&gt;&lt;p&gt;LabelEncoder (LE) 将字符转换成整数&lt;/p&gt;&lt;p&gt; LE(猫, 狗, 鸡) = (0, 1, 2)&lt;/p&gt;&lt;p&gt;OneHotEncoder (OHE) 将字符转换成向量&lt;/p&gt;&lt;p&gt; OHE(猫, 狗, 鸡) = [1,0,0; 0 1 0; 0 0 1]&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2.1. 解码字符型输出&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;鸢尾花数据集 (Iris Flower)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Iris 以鸢尾花的特征作为数据来源，它是一个多元分类问题，一共有 150 个样例。该数据集包含了 4 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;花萼长度&lt;/li&gt;&lt;li&gt;花萼宽度&lt;/li&gt;&lt;li&gt;花瓣长度&lt;/li&gt;&lt;li&gt;花瓣宽度&lt;/li&gt;&lt;li&gt;类变量：Iris Setosa (山鸢尾, Iris Versicolour (杂色鸢尾), Iris Virginica (维吉尼亚鸢尾)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [2]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的类别是字符型变量，用 LabelEncoder。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;iris.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-022a372a2d303a22a2c8efae846669a5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;449&quot; data-rawheight=&quot;185&quot;&gt;&lt;p&gt;用 fit_transform 函数将 (Iris-setosa, Iris-versicolor, Iris-virginica) 转换成 (0, 1, 2)。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)
# split data into train and test sets
X_train, X_test, y_train, y_test 
= train_test_split(X, label_encoded_y, test_size=test_size,
random_state=seed)&lt;/code&gt;&lt;p&gt;定义函数 fit_predict 整合 fit 和 predict 函数，并当 verbose 设置为 True 时打印模型和精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def fit_predict(X_train, y_train, X_test, y_test, verbose=True):
   model = fit(X_train, y_train)
   _, accuracy = predict(model, X_test, y_test)
   if verbose == True:
       print(model)
       print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))&lt;/code&gt;&lt;p&gt;调用 fit_predict 函数得到 92% 的精度，而且注意该模型用 multi:softprob 作为目标，因为该问题是个多元分类问题，而且 XGBoost 内部自动将“类变量”作独热编码，要不然目标应该是 multi:softmax。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;multi:softprob&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 92.00%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.2.2 独热编码分类型特征&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;肯特岗乳癌 (Breast + Cancer)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据集可用于进行患者乳腺癌治疗结果预测。它是一个二元分类问题，一共有 286 个样例。该数据集包含了 9 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;age&lt;/li&gt;&lt;li&gt;menopause&lt;/li&gt;&lt;li&gt;tumor-size&lt;/li&gt;&lt;li&gt;inv-nodes&lt;/li&gt;&lt;li&gt;node-caps&lt;/li&gt;&lt;li&gt;deg-malig&lt;/li&gt;&lt;li&gt;breast&lt;/li&gt;&lt;li&gt;breast-quad&lt;/li&gt;&lt;li&gt;irradiat&lt;/li&gt;&lt;li&gt;类变量：复发，未复发&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [3]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的 9 个特征都是字符型变量，这里用 OneHotEncoder。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;datasets-uci-breast-cancer.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9b0f77dc4bc4cd726d20afce671edacf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;792&quot; data-rawheight=&quot;186&quot;&gt;&lt;p&gt;从下表 unique 那行可看出每个特征的类数，比如 age 有 6 类，breast 有 2 类等&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data.describe()&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8a48cc8cc6d07983e7d1d69d5734836e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;896&quot; data-rawheight=&quot;160&quot;&gt;&lt;p&gt;对每个特征 X 做独热编码，对类 y 做普通编码。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
X = X.astype(str)
# encode string input values as integers
columns = []
for i in range(0, X.shape[1]):
   feature = LabelEncoder().fit_transform(X[:,i]).reshape(X.shape[0], 1)
   onehot_encoder = OneHotEncoder(sparse=False)
   feature = onehot_encoder.fit_transform(feature)
   print(&quot;X[&quot;,i, &quot;] shape ::&quot;, feature.shape)
   columns.append(feature)
# collapse columns into array
encoded_x = np.column_stack(columns)
print(&quot;X shape ::&quot;, encoded_x.shape)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)
# split data into train and test sets
X_train, X_test, y_train, y_test = 
train_test_split(encoded_x, label_encoded_y,
test_size=test_size, random_state=seed)
X[0] shape :: (286, 6)
X[1] shape :: (286, 3)
X[2] shape :: (286, 11)
X[3] shape :: (286, 7)
X[4] shape :: (286, 3)
X[5] shape :: (286, 3)
X[6] shape :: (286, 2)
X[7] shape :: (286, 6)
X[8] shape :: (286, 2)
X shape :: (286, 43)&lt;/code&gt;&lt;p&gt;上面的结果有些奇怪，比如 X[4] 的大小是 (286, 3), 286 代表数据个数，3 代表类别个数，但是从前一张图得知这个特征是 node-caps，对应的 unique 数为 2。看了 csv 文件才知道，这一特征栏下面有缺失值，读进 pandas 的数据表中赋值为 NaN, 也当成了一类&lt;/p&gt;&lt;p&gt;最后调用 fit_predict 函数得到 71.58% 的精度，结果不算太好，那是因为该数据中有不少缺失值，下节就来说明如何处理它们，即便 XGBoost 模型也可以自行处理。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 71.58%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.2.3 缺失值处理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;病马疝气症 (Horse Colic)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据集可用于进行患者乳腺癌治疗结果预测。它是一个二元分类问题，一共有 368 个样例。该数据集包含了 27 个特征 (其中有 30% 的缺失值) 和 1 个类变量。由于数据太多就一一列出，用这个数据集是为了测试 XGBoost 处理缺失值的能力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [4]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的类型都是数值型，其中符号 ? 代表缺失值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&quot;horse-colic.csv&quot;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20738cac42da42901d3e36cde4e97444_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;711&quot; data-rawheight=&quot;226&quot;&gt;&lt;p&gt;首先将缺失值用 0 来填充，填充完记得将 X 转化成 float 格式。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# set missing values to 0
X[X == &#39;?&#39;] = 0
# convert to numeric
X = X.astype(&#39;float32&#39;)
# encode Y class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)&lt;/code&gt;&lt;p&gt;该模型达到了 83.84% 的精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 83.84%&lt;/code&gt;&lt;p&gt;接着我们做一下实验，将缺失值分别 1 和 NaN 来填充，得到的精度分别为 79.80% 和 85.86%。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;X[X == &#39;?&#39;] = 1
X[X == &#39;?&#39;] = np.nan&lt;/code&gt;&lt;p&gt;&lt;b&gt;发现将缺失值设为独一的 NaN 最好，得到的精度最高，因为其独一性 XGBoost 把缺失值也当成“一类”。设为 0 或 1 都不太好，因为数据本身可能也含有一些 0 或 1。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.3 交叉验证&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;交叉验证在选取超参数时非常重要，首先载入 KFold, StratifiedKFold 和 cross_val_score。其中&lt;/p&gt;&lt;ul&gt;&lt;li&gt;KFold 适用于二分类且类别平衡&lt;/li&gt;&lt;li&gt;StratifiedKFold 适用于多分类或类别不平衡&lt;/li&gt;&lt;li&gt;cross_val_score 计算一些指标&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score&lt;/code&gt;&lt;p&gt;这里用 5 折交叉验证，分别用 KFold 和 StratifiedKFold 来跑。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# CV model for binary class or balanced class
kfold = KFold(n_splits=5, random_state=7)
results = cross_val_score(model, X, y, cv=kfold)
print(&quot;Accuracy: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))
Accuracy: 76.44% (5.09%)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# CV model for multi-class or inbalanced class
kfold = StratifiedKFold(n_splits=5, random_state=7)
results = cross_val_score(model, X, y, cv=kfold)
print(&quot;Accuracy: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))
Accuracy: 76.57% (3.74%)&lt;/code&gt;&lt;p&gt;该数据有 768 个，其中 268 正类 500 反类，属于不平衡的二分类问题，从上面结果来看，的确&lt;/p&gt;&lt;ul&gt;&lt;li&gt;StratifiedKFold 的精度 76.57% 比 KFold 的精度 76.44% 要高&lt;/li&gt;&lt;li&gt;StratifiedKFold 的标准差 3.74% 比 KFold 的标准差 5.09% 要低&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;1.4 特征选择&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;特征选择是一个重要课题，由于 XGBoost 包含随机森林的性质，因此也可以用来排序特征重要性并选择特征。&lt;/p&gt;&lt;p&gt;首先载入 plot_importance 和 SelectFromModel&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from xgboost import plot_importance
from sklearn.feature_selection import SelectFromModel&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.4.1 特征重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;训练好的模型会给特征打分，用 feature_importances_ 属性查看其分数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# feature importance
print(pima_model.feature_importances_)
# feature
feature = data.columns.tolist()[0:-1]
print(feature)
[ 0.07094595  0.1858108   0.08952703  0.08445946  0.07263514  0.16047297
 0.12837838  0.20777027]
[&#39;Number of times pregnant &#39;, &#39;Plasma glucose concentration&#39;, &#39;Diastolic blood pressure&#39;, &#39;Triceps skin fold thickness&#39;, &#39;Serum insulin&#39;, &#39;BMI&#39;, &#39;Diabetes pedigree function&#39;, &#39;Age &lt;/code&gt;&lt;p&gt;一图胜万字，手动画图可看出年龄 (灰色柱体) 特征最重要，怀孕次数 (蓝色柱体) 特征最不重要。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# mannually plot
df = pd.DataFrame(data=model.feature_importances_, index=feature).T
df.plot.bar(figsize=(12,6));&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cbe992e9bb20248beac84a4aafac7fa2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;723&quot; data-rawheight=&quot;357&quot;&gt;&lt;p&gt;别傻了，这年头谁还手动画图，直接用 XGBoost 自带函数 plot_importance 就可以了。(虽然方便，但是丑，有没有颜色区分，而且特征都用没有实际意义的 f0, ..., f8 代表)&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Using the XGBoost built-in function to plot
plot_importance(pima_model)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2b888755e373ec27b8db9be01985eae1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;744&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;查查 Pima 数据的特征名字，发现 f7 对应着是年龄，f0 对应着是怀孕次数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4.2 特征选择&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算出特征重要性后就可以选择特征了。用 SelectFromModel 将特征重要性作为阈值 (threshold)，得到一个 X_train 和 X_test 在特征维度上的子集，然后重新训练并打印出精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;thresholds = np.sort(pima_model.feature_importances_)
for c in thresholds:
   selection = SelectFromModel(pima_model, threshold=c, prefit=True)
   select_X_train = selection.transform(X_train)
   select_X_test = selection.transform(X_test)
   # train model
   selection_model = fit(select_X_train, y_train)
   # eval model
   _, accuracy = predict(selection_model, select_X_test, y_test)
   print(&quot;Threshold = %.3f, n = %d, Accuracy: %.2f%%&quot; % (c, select_X_train.shape[1], accuracy*100.0))
Threshold = 0.071, n = 8, Accuracy: 77.95%
Threshold = 0.073, n = 7, Accuracy: 76.38%
Threshold = 0.084, n = 6, Accuracy: 77.56%
Threshold = 0.090, n = 5, Accuracy: 76.38%
Threshold = 0.128, n = 4, Accuracy: 76.38%
Threshold = 0.160, n = 3, Accuracy: 74.80%
Threshold = 0.186, n = 2, Accuracy: 71.65%
Threshold = 0.208, n = 1, Accuracy: 63.78%&lt;/code&gt;&lt;p&gt;从上面结果可知，一般特征个数越多，精度越高，但是 n = 4 对应着 76.38% 的精度不比 n = 8 对应着 77.95% 的精度低很多。权衡精度和模型复杂度，我们会选择 n = 4。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.5 提前终止&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;提前终止 (early stopping) 可以防止过拟合 (overfitting)。具体做法可分三步：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;选评估指标 - error 或 logloss 或都选&lt;/li&gt;&lt;li&gt;选评估集 - 训练集或测试集或都选&lt;/li&gt;&lt;li&gt;调用 fit 函数，设置 verbose = True (&lt;b&gt;重要&lt;/b&gt;) 并监控&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下例选了测试集和 error 来评估，打印出 error 值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, eval_metric=&quot;error&quot;, eval_set=eval_set, verbose=True)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;[0]  validation_0-error:0.259843
[1]  validation_0-error:0.26378
[2]  validation_0-error:0.26378
...
[17]  validation_0-error:0.228346
[18]  validation_0-error:0.224409
[19]  validation_0-error:0.232283
[20]  validation_0-error:0.232283
[21]  validation_0-error:0.23622
[22]  validation_0-error:0.23622&lt;/code&gt;&lt;p&gt;发现 error 一开始在减低，但从 [19] 回合开始就升高了，这说明模型有可能过拟合了。&lt;/p&gt;&lt;p&gt;下例选了 [训练集, 测试集] 和 [error, logloss] 来评估，并画图。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_train, y_train), (X_test, y_test)]
model.fit(X_train, y_train, eval_metric=[&quot;error&quot;, &quot;logloss&quot;], eval_set=eval_set,
verbose=False)&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5ec0025a201c8c23a10c748c7b93016f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;762&quot; data-rawheight=&quot;391&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-386c1cd97545e521ad96e4a79f98fd3d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;773&quot; data-rawheight=&quot;383&quot;&gt;&lt;p&gt;从上两图看出，训练集 (蓝线) 上的 logloss 和 error 一直稳定下降，但是测试集 (黄线) 上的 logloss 和 error 在 20-40 之间开始上升，我们需要在 20 和 40 之间某个点提前终止训练。&lt;/p&gt;&lt;p&gt;XGBoost 中设置 early_stopping_rounds 可以提前终止，当该值设为 10，意思是说如果 logloss &lt;b&gt;在某一回合连续 10 个回合上升，那么在这个回合停止&lt;/b&gt;。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=&quot;logloss&quot;,
eval_set=eval_set, verbose=True)
[0]  validation_0-logloss:0.660186
Will train until validation_0-logloss hasn&#39;t improved in 10 rounds.
[1]  validation_0-logloss:0.634854
[2]  validation_0-logloss:0.612239
[3]  validation_0-logloss:0.593118
...
[29]  validation_0-logloss:0.491407
[30]  validation_0-logloss:0.488828
[31]  validation_0-logloss:0.487867
[32]  validation_0-logloss:0.487297
[33]  validation_0-logloss:0.487562
[34]  validation_0-logloss:0.487788
[35]  validation_0-logloss:0.487962
[36]  validation_0-logloss:0.488218
[37]  validation_0-logloss:0.489582
[38]  validation_0-logloss:0.489334
[39]  validation_0-logloss:0.490969
[40]  validation_0-logloss:0.48978
[41]  validation_0-logloss:0.490704
[42]  validation_0-logloss:0.492369
Stopping. Best iteration:
[32]  validation_0-logloss:0.487297

Accuracy: 78.35%&lt;/code&gt;&lt;p&gt;上图的信息非常明确，logloss 从 [32] 回合一直上升到 [42] 回合，因此在此停止。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.6 多线程运行&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;from time import time&lt;/code&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;Otto Group Product&lt;/b&gt; &lt;/p&gt;&lt;p&gt;训练集里包含了 6 万多个样本，每个样本有一个 id，93 个特征值 feat_1 ~ feat_93,以及类别 target，一共 9 种类别：class_1 ~ class_9。测试集里有 14 万多测试样本。它是一个多元分类问题&lt;/p&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [5]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;读取并预处理数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;Otto train.csv&#39;)
X, y = splitXy(data)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)&lt;/code&gt;&lt;p&gt;XGBoost 是用 C++ 实现的而且用 OpenMP API 做并行处理。将模型里面 nthread 设为 -1 代表使用系统里所有的线程，这也是默认设置。&lt;/p&gt;&lt;p&gt;此外将 nthread 设为 1, 2, 3, 4 看看用时比较。由下面结果可看出线程越多耗时越少，但从 3 到 4 时间减少并不是很明显。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;results = []
num_threads = [1, 2, 3, 4]
for n in num_threads:
   start = time()
   model = XGBClassifier(nthread=n)
   model.fit(X_train, y_train)
   elapsed = time() - start
   print(n, elapsed)
   results.append(elapsed)
1 78.58481550216675
2 50.53288269042969
3 44.90712261199951
4 39.1385235786438&lt;/code&gt;&lt;p&gt;除了 XGBClassifier, k-Fold 验证也可以并行化，它是通过设置 n_jobs 来实现的。接下来做三个实验：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只并行化 k-Fold 验证 (n_jobs = -1, nthread = 1)&lt;/li&gt;&lt;li&gt;只并行化 XGBClassifier (n_jobs = 1, nthread = -1)&lt;/li&gt;&lt;li&gt;两个都并行化 (n_jobs = -1, nthread = -1)&lt;/li&gt;&lt;/ol&gt;&lt;code lang=&quot;python&quot;&gt;# Single Thread XGBoost, Parallel Thread CV
model = XGBClassifier(nthread=1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=-1)

# Parallel Thread XGBoost, Single Thread CV
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=1)

# Parallel Thread XGBoost and CV
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=-1)
Single XGB Parallel CV: 187.554039
Parallel XGB Single CV: 145.135988
Parallel XGB and CV:    149.078977&lt;/code&gt;&lt;p&gt;结果显示只并行化 k-Fold 验证耗时最长，因为真正慢的是 XGBClassifier。此外奇怪的是两个都并行耗时并不是最短 (按道理应该是)。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.7 调整超参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Otto 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于 XGBoost 模型，很多学者和实践者通过无数尝试，给出了一些超参数的合理范围，这些建议在调参时非常有用，起码可以给个初始值尝试。关于那些参数建议来源，可参考 [6], [7], [8], [9], [10], [11], [12], [13], [14]。里面有一些学术大牛比如 Friedman 和 Hastie，有业界大牛 Owen Zhang，有 R, scikit-learn 和 XGBoost 官方参数建议。&lt;/p&gt;&lt;p&gt;接下来我们从三组最重要的超参数对来看是调参，这里需要用的到 GridSearchCV。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.model_selection import GridSearchCV&lt;/code&gt;&lt;p&gt;GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。它的好处是，只需增加几行代码，就能遍历多种组合。比如模型 M 有三个参数 P1, P2, P3, 其中&lt;/p&gt;&lt;p&gt; P1 可选值 1, 2&lt;/p&gt;&lt;p&gt; P2 可选值 4, 5, 6&lt;/p&gt;&lt;p&gt; P3 可选值 1, 3, 5, 7&lt;/p&gt;&lt;p&gt;建立一个参数字典 Para = {P1: [1 2], P2: [4 5 6], P3: [1 3 5 7]}，直接调用&lt;/p&gt;&lt;p&gt; GS = GridSearchCV(M, Para)&lt;/p&gt;&lt;p&gt;遍历所有 Para 组合，跑模型 24 遍，最后输出为 GS。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.7.1 树的个数和深度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;XGBoost 整个过程就是一个按顺序加树的过程，因此树的个数和树的深度绝对算是一组重要的超参数。接下来做三组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调树的个数&lt;/li&gt;&lt;li&gt;只调树的深度&lt;/li&gt;&lt;li&gt;同时调树的个数和深度&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;调树的个数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数从 50 到 350，以 50 为间隔，在 5 折交叉验证中要运行模型 30 次，最后最佳树的个数是 200。观察 logloss 发现其实树的个数在 100 和 350 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = range(50, 400, 50)
param_grid = dict(n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001034 using {&#39;n_estimators&#39;: 200}
-0.010886 (0.000810) with: {&#39;n_estimators&#39;: 50}
-0.001098 (0.001313) with: {&#39;n_estimators&#39;: 100}
-0.001038 (0.001348) with: {&#39;n_estimators&#39;: 150}
-0.001034 (0.001344) with: {&#39;n_estimators&#39;: 200}
-0.001035 (0.001346) with: {&#39;n_estimators&#39;: 250}
-0.001036 (0.001349) with: {&#39;n_estimators&#39;: 300}
-0.001037 (0.001351) with: {&#39;n_estimators&#39;: 350}&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d8c29bfb56a30f19bf120b71d5d69bad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;780&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;&lt;b&gt;调树的深度&lt;/b&gt; &lt;/p&gt;&lt;p&gt;树的深度从 1 到 9，以 2 为间隔，在 5 折交叉验证中要运行模型 25 次，最后最佳树的深度是 5。观察 logloss 发现其实树的深度在 3 和 9 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;max_depth = range(1, 11, 2)
param_grid = dict(max_depth=max_depth)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold,
verbose=1)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001098 using {&#39;max_depth&#39;: 5}
-0.026149 (0.000743) with: {&#39;max_depth&#39;: 1}
-0.001098 (0.001313) with: {&#39;max_depth&#39;: 3}
-0.001098 (0.001295) with: {&#39;max_depth&#39;: 5}
-0.001100 (0.001295) with: {&#39;max_depth&#39;: 7}
-0.001100 (0.001295) with: {&#39;max_depth&#39;: 9}&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9946e313afa58382c7d1be3aa006352b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;779&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;&lt;b&gt;调树的个数和深度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数为 [50, 100, 150, 200]，树的深度为 [2, 4, 6, 8]，在 5 折交叉验证中要运行模型 80 次，最后最佳树的个数和深度是 200 和 4，这个和单独调试的 最佳树的个数 200 和最佳树的深度 5 很接近。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = [50, 100, 150, 200]
max_depth = [2, 4, 6, 8]
param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold,
verbose=1)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001010 using {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 200}&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-41a037a10ccd4f3f0b90adf571846f2e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;483&quot; data-rawheight=&quot;286&quot;&gt;&lt;p&gt;&lt;b&gt;1.7.2 学习率和树的个数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一般来说，学习率越小，需要增加树的个数就越大。接下来做两组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调学习率&lt;/li&gt;&lt;li&gt;同时调树的个数和学习率&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;调学习率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;学习率为 [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]，在 5 折交叉验证中要运行模型 30 次，最后最佳学习率是 0.2。观察 logloss 发现其实学习率在 0.1 和 0.3 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
param_grid = dict(learning_rate=learning_rate)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001028 using {&#39;learning_rate&#39;: 0.2}
-2.155502 (0.000038) with: {&#39;learning_rate&#39;: 0.0001}
-1.841039 (0.000362) with: {&#39;learning_rate&#39;: 0.001}
-0.597303 (0.000606) with: {&#39;learning_rate&#39;: 0.01}
-0.001098 (0.001313) with: {&#39;learning_rate&#39;: 0.1}
-0.001028 (0.001336) with: {&#39;learning_rate&#39;: 0.2}
-0.001029 (0.001349) with: {&#39;learning_rate&#39;: 0.3}&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d3ef31a3b7316af22f0b808528d8de22_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;801&quot; data-rawheight=&quot;418&quot;&gt;&lt;p&gt;&lt;b&gt;调树的个数和学习率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数为 [100, 200, 300, 400, 500]，学习率为 [0.0001, 0.001, 0.01, 0.1]，在 5 折交叉验证中要运行模型 100 次，最后最佳树的个数和学习率是 200 和 0.1，这个和单独调试的 最佳树的个数 200 和最佳学习率 0.2 很接近。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = [100, 200, 300, 400, 500]
learning_rate = [0.0001, 0.001, 0.01, 0.1]
param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001034 using {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 200}&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2aa1d095364cfbd4b206ee06e1deccd1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;288&quot;&gt;&lt;p&gt;&lt;b&gt;1.7.3 采样比率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随机森林有列采样和行采样，XGBoost 也有。 接下来做两组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调行采样比率&lt;/li&gt;&lt;li&gt;只调列采样比率&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;行采样比率和列采样比率都从 0.1 到 1，以 0.1 为间隔，那么分别在 5 折交叉验证中要运行模型 50 次，最后发现最佳行采样比率是 0.3 和最佳列采样比率是 0.7。图表展示如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7842e64e5ece90e899741face90d70bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;300&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e51db7d6ce0f4beec3ee55523205ec89_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;477&quot; data-rawheight=&quot;311&quot;&gt;&lt;h2&gt;&lt;b&gt;2. XGBoost 进阶版&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本章介绍如何将 XGBoost 在 Lending Club 的预测贷款的应用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.1数据预处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;读取并概览数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;lending-club-data.csv&#39;, low_memory=False)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6ec4b58bf8a5bdea10c78dff30c2beee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;829&quot; data-rawheight=&quot;357&quot;&gt;&lt;p&gt;处理数据步骤有三步：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;平衡样本 (sample balancing)&lt;/li&gt;&lt;li&gt;特征子集 (feature subset)&lt;/li&gt;&lt;li&gt;独热编码 (one-hot encoding)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本帖，我们不会平衡样本，保留原来良性贷款和恶性贷款的比例。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;N1 = len(data[data[target] == 0])
N2 = len(data[data[target] == 1])

print( &quot;%% of safe loans  : %.2f%%&quot; %(N1/(N1+N2)*100.0) )
print( &quot;%% of risky loans : %.2f%%&quot; %(N2/(N1+N2)*100.0) )
% of safe loans  : 81.12%
% of risky loans : 18.88%&lt;/code&gt;&lt;h2&gt;&lt;b&gt;2.2 模型比对&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先我们用相同的参数，比对决策树 (DT)、随机森林 (RF)、梯度提升树 (GBT) 和极度梯度提升 (XGB)。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT = DecisionTreeClassifier(max_depth=5)
RF = RandomForestClassifier(n_estimators=10, max_depth=5)
GBT = GradientBoostingClassifier(n_estimators=10, max_depth=5)
XGB = XGBClassifier(n_estimators=10, max_depth=5)&lt;/code&gt;&lt;p&gt;训练这四个模型但是发现错误，原因是数据里有缺失值，XGB 可以自动处理，但是 DT, RF 和 GBT 不能。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT.fit( X_train, y_train )
RF.fit( X_train, y_train )
GBT.fit( X_train, y_train )
XGB.fit( X_train, y_train )
ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;).&lt;/code&gt;&lt;p&gt;将缺失值用 NaN 来填充。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;X_train1 = np.nan_to_num(X_train)
X_test1 = np.nan_to_num(X_test)&lt;/code&gt;&lt;p&gt;再训练模型并打印出它们的精度值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT  - Accuracy (Train): 0.8199
DT  - Accuracy (Test):  0.8172
RF  - Accuracy (Train): 0.8127
RF  - Accuracy (Test):  0.8128
GBT - Accuracy (Train): 0.8197
GBT - Accuracy (Test):  0.8173
XGB - Accuracy (Train): 0.8217
XGB - Accuracy (Test):  0.8179&lt;/code&gt;&lt;p&gt;从测试误差来看，确实&lt;/p&gt;&lt;p&gt;XGB &amp;gt; GBT &amp;gt; RF &amp;gt; DT &lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.3参数介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;XGBoost 的设置有三种参数：&lt;b&gt;一般参数&lt;/b&gt;，&lt;b&gt;提升参数&lt;/b&gt;和&lt;b&gt;学习参数&lt;/b&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一般参数&lt;/b&gt; 取决于提升器，通常是树或线性模型&lt;/li&gt;&lt;li&gt;&lt;b&gt;提升参数&lt;/b&gt; 取决于选择的提升器的相关参数&lt;/li&gt;&lt;li&gt;&lt;b&gt;学习参数&lt;/b&gt; 取决于指定学习任务和相应的学习目标&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;一般参数 (general parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;booster&lt;/i&gt;：选择提升器，默认是 tree&lt;/li&gt;&lt;li&gt;&lt;i&gt;silent&lt;/i&gt;：是否打印信息，默认是 0 不打印&lt;/li&gt;&lt;li&gt;&lt;i&gt;nthread&lt;/i&gt;：线程数，默认为最大可用线程数&lt;/li&gt;&lt;li&gt;&lt;i&gt;num_pbuffer&lt;/i&gt;：缓冲区大小，默认为训练实例的数量&lt;/li&gt;&lt;li&gt;&lt;i&gt;num_feature&lt;/i&gt;：特征纬度，默认为特征的最高纬&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;提升参数 (booster parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;eta&lt;/i&gt;：学习率，范围 [0, 1]，默认为 0.3。该参数越小，计算速度越慢；该参数越大，有可能无法收敛&lt;/li&gt;&lt;li&gt;&lt;i&gt;gamma&lt;/i&gt;：控制叶子个数的参数，范围 [0, +∞)，默认为 0。该参数越大，越不容易过拟合&lt;/li&gt;&lt;li&gt;&lt;i&gt;max_depth&lt;/i&gt;：每颗树的最大深度，范围 [0, +∞)，默认为 6。该参数越大，越容易过拟合 &lt;/li&gt;&lt;li&gt;&lt;i&gt;min_child_weight&lt;/i&gt;：每个叶子里面的最小权重和，范围 [0, +∞)，默认为 1。该参数越大，越不容易过拟合&lt;/li&gt;&lt;li&gt;&lt;i&gt;subsample&lt;/i&gt;：样本采样比率，范围 (0, 1]，默认为 1。如果取 0.5 代表随机用 50% 的样本集用来训练&lt;/li&gt;&lt;li&gt;&lt;i&gt;colsample_bytree&lt;/i&gt;：列采样比率，范围 (0, 1]，默认为 1。对每棵树的生成用的特征进行列采样，类似于随机森林的列采样&lt;/li&gt;&lt;li&gt;&lt;i&gt;lambda&lt;/i&gt;：L2 正则化参数，范围 [0, +∞)，默认为 1。该参数越大，越不容易过拟合。 &lt;/li&gt;&lt;li&gt;&lt;i&gt;alpha&lt;/i&gt;：L1 正则化参数，范围 [0, +∞)，默认为 0。该参数越大，越不容易过拟合。 &lt;/li&gt;&lt;li&gt;&lt;i&gt;scale_pos_weight&lt;/i&gt;：控制正反类的平衡参数，范围 [0, +∞)，默认为 1。该参数通常设为“反类的总和/正类的总和”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;学习参数 (learning parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;objective&lt;/i&gt;：损失函数，默认为 linear。其他常见类型有： &lt;/li&gt;&lt;ul&gt;&lt;li&gt;reg:logistic – 二分类&lt;/li&gt;&lt;li&gt;binary:logistic  – 二分类概率&lt;/li&gt;&lt;li&gt;multi:softmax – 多分类&lt;/li&gt;&lt;li&gt;multi:softprob – 多分类概率&lt;/li&gt;&lt;li&gt;rank:pairwise – 排序&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;i&gt;base_score&lt;/i&gt;：预测分数，默认为 0.5。最初每个样例的预测分数。&lt;/li&gt;&lt;li&gt;&lt;i&gt;eval_metric&lt;/i&gt;：评估指标。该指标用在验证集上，比如回归任务默认的是 rmse；分类任务默认为 error；排序任务默认为 map。其他常见类型有：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;rmse – root mean square error &lt;/li&gt;&lt;li&gt;mae – mean absolute error &lt;/li&gt;&lt;li&gt;logloss – negative log-likelihood &lt;/li&gt;&lt;li&gt;error – binary classification error rate&lt;/li&gt;&lt;li&gt;merror – multiclass classification error rate &lt;/li&gt;&lt;li&gt;mlogloss – multiclass logloss &lt;/li&gt;&lt;li&gt;auc – area under the curve&lt;/li&gt;&lt;li&gt;map – mean average precision&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;i&gt;seed&lt;/i&gt;：随机种子，默认为 0，用于产生可复现的结果 &lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;2.4 调参步骤&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;调参像是一门艺术，需要足够的经验和前人的建议。一个不错的调参思路有如下四步：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;开始用一个业界公认的学习率 0.1，然后根据交叉验证误差来确定“最优树的个数”&lt;/li&gt;&lt;li&gt;决定好学习率和树的个数之后，调“树相关的参数”，比如 &lt;i&gt;max_depth, min_child_weight, gamma, subsample, colsample_bytree&lt;/i&gt;&lt;/li&gt;&lt;li&gt;为了进一步避免过拟合，调“正则化参数”，比如 &lt;i&gt;lambda&lt;/i&gt;, &lt;i&gt;alpha&lt;/i&gt;&lt;/li&gt;&lt;li&gt;用之前调好的参数，尝试小一点的学习率，看是否能提高模型表现&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这里我编写了一个函数 modelfit，主要为了方便打印精度值和画图，由于程序比较长，大家可以去 notebook 上去看。&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤 1&lt;/b&gt;：固定学习率调最优树的个数&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb1 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=1, gamma=0, subsample=0.8,
                     colsample_bytree=0.8, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb1, X_train, X_test, y_train, y_test )
Best Iteration: 402

Model Report
Accuracy (Train): 0.8481
Accuracy (Test): 0.8247
AUC Score (Train): 0.850696
AUC Score (Test): 0.762747&lt;/code&gt;&lt;p&gt;固定学习率 0.1，以交叉误差作为评估指标，得到 &lt;b&gt;402&lt;/b&gt; 作为最优树的个数。此外，特征重要性的图如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a515f41bf869c45c856cd34dbcc70476_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;291&quot;&gt;&lt;p&gt;&lt;b&gt;步骤 2&lt;/b&gt;：调“树相关参数”&lt;/p&gt;&lt;p&gt;在这一步中，树的个数用步骤 1 确定好的 &lt;b&gt;402&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;5&lt;/b&gt; 和 &lt;b&gt;3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test1 = { &#39;max_depth&#39;:range(3,10,2), &#39;min_child_weight&#39;:range(1,6,2) }
Best: 0.764367 using {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;5&lt;/b&gt; 和 &lt;b&gt;3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test2 = { &#39;max_depth&#39;:[4,5,6], &#39;min_child_weight&#39;:[2,3,4] }
Best: 0.764367 using {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3}&lt;/code&gt;&lt;p&gt;再确定 gamma 最优值 &lt;b&gt;0.3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test3 = { &#39;gamma&#39;:[i/10.0 for i in range(0,5)] }
Best: 0.764989 using {&#39;gamma&#39;: 0.3}&lt;/code&gt;&lt;p&gt;将上面三个参数最优值 (下图红色) 带入模型重新再调树的个数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb2 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8,
                     colsample_bytree=0.8, objective= &#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb2, X_train, X_test, y_train, y_test )
Best Iteration: 482

Model Report
Accuracy (Train): 0.8505
Accuracy (Test): 0.8249
AUC Score (Train): 0.856505
AUC Score (Test): 0.763606&lt;/code&gt;&lt;p&gt;现在最优树的个数为 &lt;b&gt;482&lt;/b&gt;，而且发现 AUC 变好了，从 &lt;b&gt;0.762747&lt;/b&gt; 提升到 &lt;b&gt;0.763606&lt;/b&gt;，再接再厉继续调整其他“采样参数”。&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;0.8&lt;/b&gt; 和 &lt;b&gt;0.8&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test4 = { &#39;subsample&#39;: [i/10.0 for i in range(6,10)],
               &#39;colsample_bytree&#39;: [i/10.0 for i in range(6,10)] }
Best: 0.764960 using {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.8}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;0.8&lt;/b&gt; 和 &lt;b&gt;0.8&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test5 = { &#39;subsample&#39;: [i/100.0 for i in range(70,95,5)],
               &#39;colsample_bytree&#39;: [i/100.0 for i in range(70,95,5)] }
Best: 0.764960 using {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.8}&lt;/code&gt;&lt;p&gt;&lt;b&gt;步骤 3&lt;/b&gt;：调“正则化参数”&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;1&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test6 = { &#39;reg_alpha&#39;:[1e-5, 1e-2, 0.1, 1, 100] }
Best: 0.765376 using {&#39;reg_alpha&#39;: 1}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;5&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test7 = { &#39;reg_alpha&#39;:[0.1, 0.5, 1, 5, 10] }
Best: 0.766167 using {&#39;reg_alpha&#39;: 5}&lt;/code&gt;&lt;p&gt;发现 Best Score 的确增加了。最后把所有调好参数最优值 (下图红色) 带入模型再调树的个数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb3 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8, colsample_bytree=0.8, 
                     reg_alpha=5, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb3, X_train, X_test, y_train, y_test )
Best Iteration: 361

Model Report
Accuracy (Train): 0.8417
Accuracy (Test): 0.8245
AUC Score (Train): 0.830397
AUC Score (Test): 0.763912&lt;/code&gt;&lt;p&gt;现在最优树的个数为 &lt;b&gt;361&lt;/b&gt;，AUC 继续变好，从 &lt;b&gt;0.763606 &lt;/b&gt;提升到 &lt;b&gt;0.763912&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤 4&lt;/b&gt;：尝试小一点的学习率&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb4 = XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8, colsample_bytree=0.8, 
                     reg_alpha=5, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb4, X_train, X_test, y_train, y_test )
Best Iteration: 3702

Model Report
Accuracy (Train): 0.8424
Accuracy (Test): 0.8256
AUC Score (Train): 0.832793
AUC Score (Test): 0.767145&lt;/code&gt;&lt;p&gt;学习率从 0.1 降低到 0.01，其他参数不变，AUC 从 &lt;b&gt;0.763912 &lt;/b&gt;提升到 &lt;b&gt;0.767145&lt;/b&gt;。再回顾一路从来 AUC 的提升历程：&lt;/p&gt;&lt;p&gt;&lt;b&gt;0.767145 &amp;gt; 0.763912 &amp;gt; 0.763606 &amp;gt; 0.762747&lt;/b&gt; &lt;/p&gt;&lt;p&gt;可不要小瞧这一点点的提升哦，在 kaggle 比赛中可会压不少人的。当然本节也只是对调参给个思路，抛砖引玉。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4. 总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本贴总结的东西超越了 XGBoost 带来的东西，有着更广的使用范围，一些心得如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;学新模型最好从具体例子开始，用模型的默认值先&lt;/li&gt;&lt;li&gt;尝试不同类型的数据，用编码技巧，处理缺失值&lt;/li&gt;&lt;li&gt;用提前终止来防止过拟合&lt;/li&gt;&lt;li&gt;能画图就画图，一图胜千字&lt;/li&gt;&lt;li&gt;能并行就并行，时间就是生命&lt;/li&gt;&lt;li&gt;调参是门艺术，没有捷径只能积累，多看大师的推荐，从重要的参数开始，先粗调再细调&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes&lt;/p&gt;&lt;p&gt;http://archive.ics.uci.edu/ml/datasets/Iris&lt;/p&gt;&lt;p&gt;http://archive.ics.uci.edu/ml/datasets/Breast+Cancer&lt;/p&gt;&lt;p&gt;https://archive.ics.uci.edu/ml/datasets/Horse+Colic&lt;/p&gt;&lt;p&gt;https://www.kaggle.com/c/otto-group-product-classification-challenge/data&lt;/p&gt;&lt;p&gt;Greedy Function Approximation: A Gradient Boosting Machine, Jerome Friedman&lt;/p&gt;&lt;p&gt;Stochastic Gradient Boosting, Jerome Friedman&lt;/p&gt;&lt;p&gt;Gradient Boosting Machine Learning, Trevor Hastie&lt;/p&gt;&lt;p&gt;https://cran.r-project.org/web/packages/gbm/gbm.pdf&lt;/p&gt;&lt;p&gt;http://www.saedsayad.com/docs/gbm2.pdf&lt;/p&gt;&lt;p&gt;http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting&lt;/p&gt;&lt;p&gt;https://xgboost.readthedocs.io/en/latest/python/python_api.html&lt;/p&gt;&lt;p&gt;Winning Data Science Competitions, Owen Zhang&lt;/p&gt;&lt;p&gt;Open Source Tools and Data Science Competitions, Owen Zhang&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-24-33948430</guid>
<pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【顶级资源】掌握线性代数为机器学习打下坚实基础！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-22-33926699.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33926699&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a687d42513ea2550967a69eaecedeffe_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;原文：&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287475&amp;amp;idx=1&amp;amp;sn=39e41d221b8fca580825abea97b499a3&amp;amp;chksm=802e3626b759bf30d9bb761ad3fc02522531d746afa281e8a41a5da5df408e5b1610618397ce#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【顶级资源】掌握线性代数为机器学习打下坚实基础！&lt;/a&gt;&lt;p&gt;线性代数是数学领域，也是机器学习领域重要的支柱。对于初学者来说，要想学好机器学习，线性代数的掌握是必不可少的，也可以说是十分十分重要的。&lt;/p&gt;&lt;p&gt;春节后的第一天，公众号特此为大家分享一份这样的顶级学习清单。希望大家在新的一年里，学业有成，事业更旺。同时也感谢大家对公众号一直以来的支持与厚爱！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;维基百科&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一些&lt;b&gt;高层次&lt;/b&gt;的学习网页：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;线性代数&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Linear_algebra&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;矩阵（数学）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Matrix_(mathematics)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;矩阵分解&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Matrix_decomposition&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;线性代数主题列表&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/List_of_linear_algebra_topics&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;书籍教程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;所有书籍，编辑部费劲千辛万苦给大家都下载好了。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在文末获取。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Introduction to Linear Algebra, Fifth Edition, Gilbert Strang, 2016.&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-249687c6b0d02f7c57b32c600d0c0415_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;403&quot; data-rawheight=&quot;500&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra Done Right, Third Edition, 2015.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1a91b998d65a9d9d9d674f06fa388cd0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;872&quot; data-rawheight=&quot;1304&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;No Bullshit Guide To Linear Algebra, Ivan Savov, 2017.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-75c1eb06399ab9e84f48096274ec8931_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;874&quot; data-rawheight=&quot;1298&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Matrix Computations, Gene Golub and Charles Van Loan, 2012.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4b533353b62b768daa0ee5a21ec48c68_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1034&quot; data-rawheight=&quot;1502&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Numerical Linear Algebra with Applications: Using MATLAB.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7e81490146d8ee7eb0f6bb5186a77375_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;872&quot; data-rawheight=&quot;1326&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Numerical Linear Algebra with Applications Using MATLAB.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cf8d15ee18c005d0e20ae8606749845b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1318&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Applied Multivariate Statistical Analysis, Richard Johnson and Dean Wichern, 2012.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54d9bfa93329419a66aa5c9506432bce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;378&quot; data-rawheight=&quot;500&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebraand Its Applications David C. Lay, 2016.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-90bbd86eaf96c44cf0a92810480313c2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1343&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;ADVANCED LINEAR ALGEBRA NICHOLAS LOEHR, Virginia Polytechnic Institute and State University Blacksburg, USA, 2014.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8d8ebc7f7c0c440601209fc228f24668_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;1432&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Elementary Linear Algebra, 8e, Ron Larson, 2017.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-067e6be22032609e40d0d1dd69309f3a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1383&quot;&gt;&lt;ul&gt;&lt;li&gt;还有更多优秀的免费在线图书。 在维基百科上查看线性代数页面的末尾，可以看到更多的书籍列表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Linear_algebra#Further_reading&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7f1ef9839ca59d4a5e08a06f06aaed3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;706&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;大学课程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在许多大学课程提供PDF版本的讲义幻灯片，笔记和阅读材料。 有些甚至提供预先录制的视频讲座，这是非常宝贵的。&lt;/p&gt;&lt;p&gt;美国顶尖学校推荐的一些课程包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;MIT的线性代数——Gilbert Strang&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e306ddfee63efc35d8c3adaba5e43eb7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;849&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;The Matrix in Computer Science at Brown by Philip Klein&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://cs.brown.edu/courses/cs053/current/index.htm&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-113c6a675f47282b4ad3323bf5d18c54_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;781&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Computational Linear Algebra for Coders at University of San Francisco by Rachel Thomas.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://github.com/fastai/numerical-linear-algebra/&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-076f974480db3e6641bb1a4b94c78bbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;970&quot;&gt;&lt;h2&gt;&lt;b&gt;在线课程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra on Khan Academy&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.khanacademy.org/math/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5c161d361ec7f56b6d815147b032688d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;692&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra: Foundations to Frontiers on edX&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.edx.org/course/laff-linear-algebra-foundations-to-frontiers&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0456811498a219958c3939ab411e7605_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;757&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;问答平台&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;考虑到当前有大量的问答平台，有很多地方可以在线提出有关线性代数的问题。&lt;/p&gt;&lt;p&gt;以下是推荐的最热门的平台，供大家参考学习：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on the Mathematics Stack Exchange&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://math.stackexchange.com/?tags=linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-16596c2e55adf15f10d92370e7db02a5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;388&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on Cross Validated&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://stats.stackexchange.com/questions/tagged/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-12f62762aba07795a9c2dcc1be1672ad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;280&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on Stack Overflow&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://stackoverflow.com/questions/tagged/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5a0dfe7e08537514c473bef3f9023acc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;235&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra on Quora&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.quora.com/topic/Linear-Algebra&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-937b7d492f28e424a412dcf31e04f0d8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Math Subreddit&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.reddit.com/r/math/&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-774fe1d8508ea88587a0e2b10fc56c78_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;435&quot;&gt;&lt;h2&gt;&lt;b&gt;NumPy资源&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;在Python中实现线性代数时，您可能需要NumPy的帮助。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Reference&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Array Creation Routines&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Array Manipulation Routines&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Linear Algebra&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.linalg.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;SciPy Linear Algebra&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/scipy/reference/linalg.html&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;链接:https://pan.baidu.com/s/1daPb7C  &lt;/p&gt;&lt;p&gt;密码:1bqf&lt;/p&gt;&lt;p&gt;&lt;b&gt;开工大吉，新年旺旺旺&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287475&amp;amp;idx=1&amp;amp;sn=39e41d221b8fca580825abea97b499a3&amp;amp;chksm=802e3626b759bf30d9bb761ad3fc02522531d746afa281e8a41a5da5df408e5b1610618397ce#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【顶级资源】掌握线性代数为机器学习打下坚实基础！&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-22-33926699</guid>
<pubDate>Thu, 22 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>将海外量化资源一网打尽！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644454.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644454&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e13a551a257f4ad1e1d2f115f466f451_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;公众号开通了&lt;/p&gt;&lt;p&gt;&lt;b&gt;微博官方账号&lt;/b&gt;&lt;/p&gt;&lt;p&gt;取名&lt;/p&gt;&lt;h2&gt;&lt;b&gt;宽客的后花园&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;大家赶紧粉一波吧&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-58318d82ca17477b7a42c6888025eee9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;348&quot;&gt;&lt;p&gt;此举&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;一是弥补微信公众号平台不能调用外部链接的一个不足。&lt;/b&gt;&lt;br&gt;&lt;b&gt;二是给一些不能上外网的朋友带来福利。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;同时，在国内，我们还没有看到很好的量化海外分享平台。为此，我们决定做这件事。希望能给大家带来全新的阅读体验。&lt;/p&gt;&lt;p&gt;在这里，公众号还要要感谢一个人，他就是北邮的陈光老师。他的微博@爱可可，爱生活影响了很多人，公众号也从中受益很多。同时，也给了公众号新的途径和思路，让我们可以把国外优秀的资源搬进国内。为国内量化爱好者提供更多的视角和资讯！&lt;/p&gt;&lt;p&gt;之所以把公众号官方微博起名为：&lt;b&gt;宽客的后花园&lt;/b&gt;，也是希望大家能在这个量化的后花园发现更多意想不到的惊喜！&lt;/p&gt;&lt;p&gt;当然，大家也要知道公众号的&lt;b&gt;官方微博&lt;/b&gt;与&lt;b&gt;微信公众号&lt;/b&gt;有&lt;b&gt;很大不同&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;1、内容与微信公众号完全不同；&lt;/b&gt;&lt;br&gt;&lt;b&gt;2、主要分享海外关于量化投资方向的各种资源，机器学习方向为辅。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;还在等什么&lt;/p&gt;&lt;p&gt;赶紧去关注我们的官方微博吧&lt;/p&gt;&lt;h2&gt;&lt;b&gt;@宽客的后花园&lt;/b&gt;&lt;/h2&gt;&lt;a href=&quot;https://login.sina.com.cn/crossdomain2.php?action=login&amp;amp;entry=miniblog&amp;amp;r=https%3A%2F%2Fpassport.weibo.com%2Fwbsso%2Flogin%3Fssosavestate%3D1549453878%26url%3Dhttps%253A%252F%252Fweibo.com%252Flhtzjqxx%26display%3D0%26ticket%3DST-MTM1MDg5MzIzMw%3D%3D-1517917878-gz-4AB0E5FA2BEA75DAC3BCFD5DD32AC61A-1%26retcode%3D0&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;新浪通行证&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644454</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>『量化投资』冲顶大会第一场！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644277.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644277&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-affc81430b6d731cab77ccd3969899bf_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;最近、冲顶大会、百万赢家、百万英雄、芝士超人等众多在线答题平台，直接刷爆了朋友圈。作为专注于量化投资方向的我们，怎么能坐视不管呢！&lt;/p&gt;&lt;p&gt;从今天开始，量化投资与机器学习公众号将不定期举办&lt;b&gt;『量化投资』&lt;/b&gt;冲顶大会！&lt;/p&gt;&lt;p&gt;今天是第一场，后面还会有更多场次。我们也会邀请一些机构和行业内的大咖为大家出题，丰富知识的多元化和专业性。同时在后几期我们还会设立奖金或者赠送图书。&lt;/p&gt;&lt;p&gt;我们也欢迎有兴趣的个人或者机构来为我们出题。具体联系方式可以添加小编的微信：&lt;b&gt;zxlgglr&lt;/b&gt;【备注：出题】&lt;/p&gt;&lt;p&gt;总计12题，答案在这里&lt;b&gt;（留言第一条）&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287343&amp;amp;idx=1&amp;amp;sn=b9d1c18380213d0cf097aacdd4ac24b0&amp;amp;chksm=802e31bab759b8ac67886dc3d16d9bb409db8dbaa7dee3e9ed2a8925b979fef1f9969475358c#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;『量化投资』冲顶大会第一场！&lt;/a&gt;&lt;p&gt;&lt;b&gt;题目范围（金融+数学+计算机）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;好了，答题开始！&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;第一题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1．相对价值策略的特点是（）。  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．低收益、低风险、大容量   &lt;/p&gt;&lt;p&gt;B．高收益、低风险、小容量    &lt;/p&gt;&lt;p&gt;C．高收益、高风险、大容量    &lt;/p&gt;&lt;p&gt;D．高收益、高风险、小容量     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第二题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2．著名的Chern-Simons定理是由（）与数学家陈省身共同创立。    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．詹姆斯·西蒙斯    &lt;/p&gt;&lt;p&gt;B．大卫·肖    &lt;/p&gt;&lt;p&gt;C．伊曼纽尔·德曼    &lt;/p&gt;&lt;p&gt;D．Ray Dalio&lt;/p&gt;&lt;p&gt;&lt;b&gt;第三题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3．第十五届新财富最佳分析师金融工程组第一名是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．广发证券    &lt;/p&gt;&lt;p&gt;B．长江证券 &lt;/p&gt;&lt;p&gt;C．国泰君安证券 &lt;/p&gt;&lt;p&gt;D．申万宏源证券&lt;/p&gt;&lt;p&gt;&lt;b&gt;第四题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4．关于金融市场的数学定义，下列说法正确的是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．数学可以用来描述金融市场&lt;/p&gt;&lt;p&gt;B．把金融市场看成是函数逼近问题时，可以用贝叶斯分类进行计算&lt;/p&gt;&lt;p&gt;C．把金融市场看成是分类问题时，可以用回归分析的方式进行数据分析&lt;/p&gt;&lt;p&gt;D．把金融市场看成是概率问题时，可利用小波分析理论计算概率&lt;/p&gt;&lt;p&gt;&lt;b&gt;第五题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5．当随机误差项存在自相关时，进行单位根检验是由（）来实现。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．DF检验                   &lt;/p&gt;&lt;p&gt;B．ADF检验&lt;/p&gt;&lt;p&gt;C．EG检验                    &lt;/p&gt;&lt;p&gt;D．DW检验&lt;/p&gt;&lt;p&gt;&lt;b&gt;第六题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;6．目前在线量化平台使用MATLAB语言的平台是（）。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．优矿                     &lt;/p&gt;&lt;p&gt;B．聚宽&lt;/p&gt;&lt;p&gt;C．米筐                &lt;/p&gt;&lt;p&gt;D．点宽&lt;/p&gt;&lt;p&gt;&lt;b&gt;第七题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;7．A股第一次熔断是什么时候（）。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．2016年1月1日                     &lt;/p&gt;&lt;p&gt;B．2016年1月2日     &lt;/p&gt;&lt;p&gt;C．2016年1月3日     &lt;/p&gt;&lt;p&gt;D．2016年1月4日     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第八题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;8．下面这段代码的输出结果将是什么（）。 &lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def multipliers():
   return [lambda x : i * x for i in range(4)]
   
print [m(2) for m in multipliers()]&lt;/code&gt;&lt;p&gt;A．[2, 4, 6, 8]      &lt;/p&gt;&lt;p&gt;B．[0, 2, 4, 6]     &lt;/p&gt;&lt;p&gt;C．[6, 6, 6, 6]  &lt;/p&gt;&lt;p&gt;D．[2, 2, 2, 2]     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第九题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;9．峰度和偏度分别是概率分布的几阶矩（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．三、四   &lt;/p&gt;&lt;p&gt;B．四、三&lt;/p&gt;&lt;p&gt;C．二、三&lt;/p&gt;&lt;p&gt;D．三、二&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;10．对冲基金中的贝塔与杠杆成（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．正比&lt;/p&gt;&lt;p&gt;B．反比&lt;/p&gt;&lt;p&gt;C．不成比&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十一题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;11．CAPM、Mean-Variance model、APT三大理论提出的时间顺序是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．CAPM、Mean-Variance model、APT&lt;/p&gt;&lt;p&gt;B．APT、CAP、MMean-Variance model&lt;/p&gt;&lt;p&gt;C．Mean-Variance model、CAPM、APT&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十二题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;12．量化投资与机器学习公众号成立（）年了。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．2&lt;/p&gt;&lt;p&gt;B．1&lt;/p&gt;&lt;p&gt;C．3&lt;/p&gt;&lt;p&gt;D．4&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;公众号的&lt;b&gt;微博官方账号&lt;/b&gt;已开通&lt;/p&gt;&lt;p&gt;&lt;b&gt;@宽客的后花园&lt;/b&gt;&lt;/p&gt;&lt;p&gt;赶紧去关注我们吧&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;1、内容与微信公众号完全不同；&lt;/b&gt;&lt;br&gt;&lt;b&gt;2、主要分享海外关于量化投资方向的各种资源，机器学习方向为辅。&lt;/b&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-58318d82ca17477b7a42c6888025eee9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;348&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644277</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>请收下这份榜单：2017年最热门的50篇券商金工研报（附下载）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644167.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644167&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-177a92bd3d38bd1a6fc8ae99b1117c82_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;还有几天就要过年啦！在此，公众号特地为大家准备了第一份新年礼物。&lt;/p&gt;&lt;p&gt;回顾2017年，公众号分享了全年所有券商的金工研报（&lt;u&gt;&lt;b&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287361&amp;amp;idx=1&amp;amp;sn=b659d13209d14bfc2cc9865ca1c42042&amp;amp;chksm=802e31d4b759b8c2548c5f128f0a7b6d59dbc3a89b2f8f49fcdf197736ad2cb1c25182928e51&amp;amp;scene=21#wechat_redirect&quot;&gt;这里下载&lt;/a&gt;&lt;/b&gt;&lt;/u&gt;），但是在这么多的研报中，最受大家喜爱，排名靠前的是哪些呢？想必大家可能不是很了解。&lt;/p&gt;&lt;p&gt;今天，公众号就为大家公布：&lt;b&gt;2017年最热门的50篇券商金工研报&lt;/b&gt;。让我们一睹为快吧！（数据来源于多方媒体）&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-723c09380cd3c5f3ddd81e7fa857f69d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;802&quot; data-rawheight=&quot;883&quot;&gt;&lt;p&gt;我们对前50名做了简单的统计，排在前三的是：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、华泰证券&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、国泰君安&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、海通证券&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其他券商统计如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-177a92bd3d38bd1a6fc8ae99b1117c82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1925&quot; data-rawheight=&quot;1114&quot;&gt;&lt;p&gt;具体的文章，大家在过去一年的研报里，自行去找吧。多动手哦！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;金融工程研报汇总&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283257&amp;amp;idx=2&amp;amp;sn=49c78925e7f3535b9cad95bf91574519&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【每周研报干货】各大券商研报免费分享（附下载链接）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283773&amp;amp;idx=1&amp;amp;sn=d4604682da0c5563be9da16717d11bf9&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【干货】各大券商研究报告！&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283257&amp;amp;idx=2&amp;amp;sn=49c78925e7f3535b9cad95bf91574519&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【每周研报干货】各大券商研报免费分享（附下载链接）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284202&amp;amp;idx=1&amp;amp;sn=f94bdefe70ddcb538ca463ba1c5e5205&amp;amp;chksm=802e257fb759ac69899d8544937600c22637697591fce25d1ed1b72414d975eeeba7cc58c9d8&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;新财富金融工程前三名【海通证券】 研报大放送（百篇）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284199&amp;amp;idx=1&amp;amp;sn=4ec9cac078f8057744349c9c953decb2&amp;amp;chksm=802e2572b759ac6438362451289132ab4bb631da5b41e9f2b2545eb5efe50e0d14d6bd3d3015&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;新财富金融工程前三名【广发证券】 研报大放送（最全）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284196&amp;amp;idx=1&amp;amp;sn=85245caf9148fb965df1c56c963984ba&amp;amp;chksm=802e2571b759ac6772582aea40781bddd6f148f144edc9b8b08606749f3c2c012b907441d59d&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;两度蝉联新财富金融工程第一名的【国泰君安】 研报大放送&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284668&amp;amp;idx=1&amp;amp;sn=1d099b61ac8a378f39ef99203cfb85af&amp;amp;chksm=802e2b29b759a23f1ce824e84ab55601f8da41ace7877cac3fe97900f1a7147c97a732481841&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第1部分&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284678&amp;amp;idx=1&amp;amp;sn=0c29d884ada86f565b5849057fe5cdb6&amp;amp;chksm=802e2b53b759a245db87fe77c211e8f987464d0d188305808b412fb2d36cbc9f4bb707fedde9&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第2、3、4部分&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284702&amp;amp;idx=1&amp;amp;sn=c150e541adb6f852459b085a086bf97f&amp;amp;chksm=802e2b4bb759a25de30c981d25e8db6c90e409e0c8ec5303ad0b3fa673abfc01fd4832842c16&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第5、6、7、8、9部分【完结】&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286032&amp;amp;idx=1&amp;amp;sn=f931e3de55ba425049553d524173b57e&amp;amp;chksm=802e2c85b759a5935002ab01161a92be5ba6c7a5ba64ad12d8be55490fa328973835008ab2dc&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【年度干货】2017上半年所有券商金融工程研究报告（一）&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286032&amp;amp;idx=1&amp;amp;sn=f931e3de55ba425049553d524173b57e&amp;amp;chksm=802e2c85b759a5935002ab01161a92be5ba6c7a5ba64ad12d8be55490fa328973835008ab2dc&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【年度干货】2017上半年所有券商金融工程研究报告（一）&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286262&amp;amp;idx=1&amp;amp;sn=8fe879fc4a5189cf027b7496da82681f&amp;amp;chksm=802e2d63b759a47535c7a0dfe279672f10821edcdeb49c6f099a7388feef39e8faeb2aaf30e3&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年8月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286383&amp;amp;idx=1&amp;amp;sn=7c6b9f54ee5727ede261042510daa401&amp;amp;chksm=802e2dfab759a4ec6a3eb346d6e27fceae852aefae361bd93320ba4ffab7a2859899b28ace19&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年9月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286510&amp;amp;idx=1&amp;amp;sn=b64aab20dc1ba2e56776aa34090d361d&amp;amp;chksm=802e327bb759bb6d558caf6a2aaf4e86bfaf31a3558573f58c7f5f24d1526756ec0ac1d3a820&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年10月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286772&amp;amp;idx=1&amp;amp;sn=f8ca457e87587ed73aa3d81903336db5&amp;amp;chksm=802e3361b759ba7775e1879e1c8a0b9b917d9ff43649e68c85b17b434d516acfc0ec758968a7&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年11月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287361&amp;amp;idx=1&amp;amp;sn=b659d13209d14bfc2cc9865ca1c42042&amp;amp;chksm=802e31d4b759b8c2548c5f128f0a7b6d59dbc3a89b2f8f49fcdf197736ad2cb1c25182928e51&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年12月全部券商金融工程研报汇总（第十一期免费赠书活动来啦！）&lt;/a&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-20e147d1234f759b40d3358135abec1e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644167</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>基于Python预测股价的那些人那些坑，请认真看完！【系列52】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-25-33297567.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33297567&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7d01d8496ba2c0389083b675d7ef92d1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;基于Python预测股价的那些人那些坑，请认真看完！【系列52】&lt;/a&gt;&lt;p&gt;来源：AI科技大本营（ID：rgznai100）&lt;/p&gt;&lt;p&gt;编辑部：&lt;/p&gt;&lt;p&gt;前几天我们已经看过此文，@爱可可发过一次，今天AI科技大本营又翻译了此文。这篇文章真的很不错，不是在于模型的好坏，而是在于作者对预测股价的一些心得和体会，编辑部觉得大家应该好好的看一下这篇文章，它告诉我们所有高大上的东西，想在资本市场去赚钱，还是有难度的，预测股价的方法千奇百出，但是要正真理市场解去做，这才是最重要的。&lt;/p&gt;&lt;p&gt;不要跟风，不要盲目，不要虚荣。&lt;/p&gt;&lt;p&gt;B是给别人装的！钱是给自己挣的！&lt;/p&gt;&lt;p&gt;对数据科学家来说，预测证券市场走势是一项非常有诱惑力的工作，当然，他们这样做的目的很大程度上并不是为了获取物质回报，而是为了挑战自己。证券市场起起伏伏、变幻莫测，试想一下，如果在这个市场里存在一些我们或者我们的模型可以学习到的既定模式，让我们可以打败那些商科毕业的操盘手，将是多么美妙。当然，当我一开始使用加性模型（additive model）来做时间序列预测时，我不得不先用模拟盘来验证我的模型在股票市场上的表现。&lt;/p&gt;&lt;p&gt;一众挑战者们都希望在每日收益率上能够跑赢市场，但是大多数都失败了，我也未能幸免。不过，在这个过程中也学到了大量Python相关知识，包括面向对象编程、数据处理、建模、以及可视化等等。同时，我也认清了一个道理，不要在每日收益率上锱铢必较，学会容忍适当的短期亏损，放长线才能钓大鱼。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d7309ceb05c08d311844ee353152b408_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;590&quot; data-rawheight=&quot;304&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4f1c75c11cb0aba584f6f10cf7b30740_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;586&quot; data-rawheight=&quot;301&quot;&gt;&lt;p&gt;一天与三十年对比结果：你宁愿把钱投在哪里？&lt;/p&gt;&lt;p&gt;在任何任务中（不只是数据科学），当我们没有取得立竿见影的成效时，我们都有三个选择：&lt;/p&gt;&lt;p&gt;1. 调整结果，让我们看起来像是成功了&lt;/p&gt;&lt;p&gt;2. 隐藏结果，所以没有人会注意到&lt;/p&gt;&lt;p&gt;3. 公开我们所有的结果和方法，以便其他人（以及我们自己）可以从中吸取经验和教训&lt;/p&gt;&lt;p&gt;显然，不管站在个人还是社会层面，方案三都是最佳选择，但它同时也是最需要勇气去实践的。我可以选择性地公布结果，比如当我的模型能够带来丰厚的利润回报时，我也可以掩盖失败的事实，假装自己从来没有在这项工作上花过时间。这似乎是很天真的想法！我们之所以能够进步是因为不断重复失败——学习这个过程，而不仅仅是之前的成功。而且，为有难度的任务编写Python代码而付出的努力也并不应该白费！&lt;/p&gt;&lt;p&gt;这篇文章记录了我使用Python开发的“stock explorer”工具——Stocker的预测功能。此前，我曾展示了如何使用Stocker进行分析，并且将完整的代码贴在GitHub上，以方便大家。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;&lt;b&gt;Github代码地址：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;i&gt;&lt;b&gt;https://github.com/WillKoehrsen/Data-Analysis/tree/master/stocker&lt;/b&gt;&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌实现预测的Stocker工具&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Stocker是一款用于探索股票情况的Python工具。一旦我们安装了所需的库（查看文档），我们可以在脚本的同一文件夹中启动一个Jupyter Notebook，并导入Stocker类：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from stocker import Stocker &lt;/code&gt;&lt;p&gt;现在可以访问这个类了。我们通过传递任一有效的股票代码（粗体是输出）来创建一个Stocker类的对象：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon = Stocker(&#39;AMZN&#39;) &lt;/code&gt;&lt;p&gt;AMZN Stocker Initialized. Data covers 1997-05-16 to 2018-01-18.&lt;/p&gt;&lt;p&gt;根据上面的输出结果，我们有20年的亚马逊每日股票数据可以用来探索！ Stocker对象是建立在Quandl金融库上，而且拥有3000多只股票可以使用。我们可以使用plot_stock函数来绘制一个简单的历史股价图：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;amazon.plot_stock()
Maximum Adj. Close = 1305.20 on 2018-01-12.
Minimum Adj. Close = 1.40 on 1997-05-22.
Current Adj. Close = 1293.32.&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ab399e66b1cc5794e1ac422232e5a690_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;498&quot;&gt;&lt;p&gt;Stocker的分析功能可以用来发现数据中的整体趋势和模式，但我们将重点关注预测股票未来的价格上。Stocker中的预测功能是使用一个加性模型来实现的，该模型将时间序列视为季节性（如每日、每周和每月）的整体趋势组合。Stocker使用Facebook开发的智能软件包进行加性建模，用一行代码就可以创建模型并进行预测：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model, model_data = amazon.create_prophet_model(days=90)&lt;/code&gt;&lt;p&gt;Predicted Price on 2018-04-18 = $1336.98&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e6bcf8dbb79938ac9d7eabb82a4b8104_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;注意，表示预测结果的绿线包含了相对应的置信区间，这代表在模型预测的不确定性。在这种情况下，如果将置信区间宽度设置为80％，这意味着我们预计这个范围将包含实际值的可能性为80%。置信区间将随着时间进一步扩大，这是因为随着预测时间距离现有数据的时间越来越远，预测值将面临更多的不确定性。任何时候我们做这样的预测，都必须包含一个置信区间。尽管大多数人倾向于一个确定的值，但我们的预测结果必须反映出我们生活在一个充满不确定性的世界！&lt;/p&gt;&lt;p&gt;任何人都可以做股票预测：简单地选择一个数字，而这就是你的估测（我可能是错的，但我敢肯定，这是华尔街所有人都会做的）。为了让我们的模型具有可信度，我们需要评估它的准确性。Stocker工具中有许多用于评估模型准确度的方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌评估预测结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了计算准确率，我们需要一个测试集和一个训练集。我们需要知道测试集的答案，也就是实际的股价，所以我们将使用过去一年的历史数据（本例中为2017年）。训练时，我们不选用2014-2016的数据来作为训练集。监督学习的基本思想是模型从训练集中学习到数据中的模式和关系，然后能够在测试数据上正确地重现结果。&lt;/p&gt;&lt;p&gt;我们需要量化我们的准确率，所以我们使用了测试集的预测结果和实际值，我们计算的指标包括测试集和训练集的美元平均误差、正确预测价格变化趋势的时间百分比、以及实际价格落在预测结果80％置信区间内的时间百分比。所有这些计算都由Stocker自动完成，而且可视化效果很好：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction() &lt;/code&gt;&lt;p&gt;Prediction Range: 2017-01-18 to 2018-01-18.&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Predicted price on 2018-01-17 = $814.77.
Actual price on    2018-01-17 = $1295.00.

Average Absolute Error on Training Data = $18.21.
Average Absolute Error on Testing  Data = $183.86.

When the model predicted an increase, the price increased 57.66% of the time.
When the model predicted a  decrease, the price decreased  44.64% of the time.

The actual value was within the 80% confidence interval 20.40% of the time.&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4de85d9dc40918f9b6988c686a699367_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;可以看到，预测结果真是糟糕透了，还不如直接抛硬币。如果我们根据这个预测结果来投资，那么我们最好是买买彩票，这样比较明智。但是，不要放弃这个模型，第一个模型通常比较糟糕，因为我们使用的是默认参数（称为超参数）。如果我们最初的尝试不成功，那么我们可以调整这些参数来获得一个更好的模型。在Prophet模型中有许多不同的参数设置需要调整，最重要的是变点先验尺度（changepoint prior scale），它控制着模型在数据趋势上的偏移量。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;▌变点先验（Changepoint Prior）的选择&lt;/b&gt;&lt;/p&gt;&lt;p&gt;变点代表时间序列从增加到减少，或者从缓慢增加到越来越快（反之亦然）。它们出现在时间序列变化率最大的地方。变点先验尺度表示在模型中给予变点的偏移量。这是用来控制过度拟合与欠拟合的（也被称为偏差与方差间的权衡）。&lt;/p&gt;&lt;p&gt;一个更高的先验能创造一个更多变点权重和更具弹性的模型，但这可能会导致过拟合，因为该模型将严格遵守训练数据的规律，而不能将它泛化到新的测试数据中。降低先验会减少模型的灵活性，而这又可能会导致相反的问题：欠拟合，当我们的模型没有完全遵循训练数据，而没有学习到底层模式时，这种情况就会发生。如何找出适当的参数以达到正确的平衡，这更多的是一个工程问题而不是理论问题，在这里，我们只能依靠经验结果。Stocker类有两种不同的方式来选择适当的先验：可视化和量化。 我们可以从可视化方法开始：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_analysis(changepoint_priors=[0.001, 0.05, 0.1, 0.2]) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-aaa6c8fb67f9bb09fbc0b953106b373b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;498&quot;&gt;&lt;p&gt;在这里，我们使用三年的数据进行训练，然后显示了六个月的预测结果。我们没有量化这里的预测结果，因为我们只是试图去理解变点先验值的作用。这个图表很好地说明了过拟合与欠拟合！代表最小先验的蓝线与代表训练数据的黑线值并不是非常接近，就好像它有自己的一套模式，并在数据的附近随便选了一条路线。相比之下，代表最大先验的黄线，则与训练观察结果非常贴近。变点先验的默认值是0.5，它落在两个极值之间的某处。&lt;/p&gt;&lt;p&gt;我们还要注意先验值不同带来的不确定性（阴影区间）方面的差异。最小的先验值在训练数据上表现有最大的不确定性，但在测试数据上的不确定性却是最小。相比之下，最大的先验值在训练数据上具有最小的不确定性，但在测试数据上却有最大的不确定性。先验值越高，对训练数据的拟合就越好，因为它紧跟每次的观察值。但是，当使用测试数据时，过拟合模型就会因为没有任何数据点来定位而迷失掉。由于股票具有相当多的变化性，我们可能需要比默认模型更灵活的模型，这样才能够捕捉尽可能多的模式信息。&lt;/p&gt;&lt;p&gt;现在我们对先验值带来的影响有了一个概念，我们可以使用训练集和验证集对数值进行评估：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_validation(start_date=&#39;2016-01-04&#39;, 
end_date=&#39;2017-01-03&#39;, changepoint_priors=[0.001, 0.05, 0.1, 0.2])&lt;/code&gt;&lt;p&gt;Validation Range 2016-01-04 to 2017-01-03.&lt;br&gt;&lt;br&gt;     cps  train_err  train_range    test_err  test_range&lt;br&gt;0  0.001  44.475809   152.600078  149.373638  152.564766&lt;br&gt;1  0.050  11.203019    35.820696  152.033810  139.505624&lt;br&gt;2  0.100  10.722908    34.593207  152.903481  172.654255&lt;br&gt;3  0.200   9.725255    31.895204  127.604543  324.376524&lt;/p&gt;&lt;p&gt;在这里，我们必须注意到，我们的验证集和测试集是不一样的数据。如果它们是一样的，那么我们会得到在测试数据上效果最好的模型，但是它只是在测试数据上过拟合了，而我们的模型也不能用于现实世界的数据。总的来说，就像在数据科学中通常所做的那样，我们正在使用三组不同的数据：训练集（2013-2015）、验证集（2016）和测试集（2017）。&lt;/p&gt;&lt;p&gt;我们用四个指标来评估四个先验值：训练误差、训练范围（置信区间）、测试误差和测试范围（置信区间），所有的值都以美元为单位。正如我们在图中看到的那样，先验值越高，训练误差越低，训练数据的不确定性越低。我们也可以看到，更高的先验能降低我们的测试错误。为了在测试集上获得更高的准确率，作为交换，随着先验的增长，我们在测试数据上得到了更大范围的不确定性。&lt;/p&gt;&lt;p&gt;Stocker先验验证还可以通过两条线来阐述这些点：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-390f2b8a3502e56f41d2eb66843a3b00_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1ec797fc4d29f742eb1ad2971cae8c6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;基于不同变点先验尺度下，训练和测试准确性曲线和不确定性曲线&lt;/p&gt;&lt;p&gt;既然最高的先验值产生了最低的测试误差率，我们应该尝试再增加先验值来看看是否能得到更好的结果。我们可以通过在验证中加入其它值的方法来优化我们的搜索：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_validation(start_date=&#39;2016-01-04&#39;, 
end_date=&#39;2017-01-03&#39;, changepoint_priors=[0.15, 0.2, 0.25,0.4, 0.5, 0.6]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-942a5b07e6ee04a7126cf1f88a5ae101_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;改进后的训练和测试曲线&lt;/p&gt;&lt;p&gt;当先验值为0.5时，测试集的错误率将最小化。因此我们将重新设置Stocker对象的变点先验值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_scale = 0.5 &lt;/code&gt;&lt;p&gt;我们可以调整模型的其他参数，比如我们期望看到的模式，或者模型使用的训练数据。找到最佳组合只需要重复上述过程，并使用一些不同的值。请随意尝试任意的参数！&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌评估改进的模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在我们的模型已经优化好了，我们可以再次评估它：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction() 

Prediction Range: 2017-01-18 to 2018-01-18.

Predicted price on 2018-01-17 = $1160.43.
Actual price on    2018-01-17 = $1295.00.

Average Absolute Error on Training Data = $10.21.
Average Absolute Error on Testing  Data = $99.99.

When the model predicted an increase, the price increased 56.90% of the time.
When the model predicted a  decrease, the price decreased  44.00% of the time.

The actual value was within the 80% confidence interval 95.20% of the time.
&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-50863ca4da67ec63292b26a51724ba4d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;710&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;现在看起来好多了！ 这显示了模型优化的重要性。使用默认值可以提供第一次合理猜测，但是我们需要确定，我们正在使用正确的模型“设置”，就像我们试图通过调整平衡和淡入淡出来优化立体声的声音那样（很抱歉引用了一个过时的例子）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌玩转股票市场&lt;/b&gt;&lt;/p&gt;&lt;p&gt;股票预测是一个有趣的实践，但真正的乐趣在于观察这些预测结果在实际市场中会发挥多好的作用。使用evaluate_prediction函数，我们可以在评估期间使用我们的模型“玩一玩”股票市场。我们将使用模型预测给出的策略，与我们在整个期间简单地购买和持有股票的策略进行一个对比。&lt;/p&gt;&lt;p&gt;我们的策略规则很简单，如下：&lt;/p&gt;&lt;p&gt;1、当模型预测股价会上涨的那一天，我们开始买入，并在一天结束时卖出。当模型预测股价下跌时，我们就不买入任何股票；&lt;/p&gt;&lt;p&gt;2、如果我们购买股票的价格在当天上涨，那么我们就把股票上涨的幅度乘以我们购买的股票的数量；&lt;/p&gt;&lt;p&gt;3、如果我们购买的股票价格下跌，我们就把下跌的幅度乘以股票的数量，计作我们的损失。&lt;/p&gt;&lt;p&gt;在整个评估期间，也就是2017年，我们每天以这样的方式进行股票操作。将股票的数量添加进模型回馈里面，Stocker就会以数字和图表显示的方式告诉我们这个策略是如何进行的： &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction(nshares=1000)&lt;/code&gt;&lt;p&gt;You played the stock market in AMZN from 2017-01-18 to 2018-01-18 with 1000 shares.&lt;br&gt;&lt;br&gt;When the model predicted an increase, the price increased 57.99% of the time.&lt;br&gt;When the model predicted a  decrease, the price decreased  46.25% of the time.&lt;br&gt;&lt;br&gt;The total profit using the Prophet model = $299580.00.&lt;br&gt;The Buy and Hold strategy profit =         $487520.00.&lt;br&gt;&lt;br&gt;Thanks for playing the stock market!&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1caad3bee2cb73725b36396098199e8c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;746&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;上图告诉了我们一个非常宝贵的策略：买入并持有！虽然我们可以在策略上再作出相当大的调整，但更好的选择是长期投资。&lt;/p&gt;&lt;p&gt;我们可以尝试其他的测试时间段，看看有没有什么时候我们的模型给出的策略能胜过买入和持有的方法。我们的策略是比较保守的，因为当我们预测市场下跌的时候我们不进行操作，所以当股票下跌的时候，我们期待有比持有策略更好的方法。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eea7fc5e42cd9d1c19cecbba9e8fbf71_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;742&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;一直用虚拟货币实验&lt;/p&gt;&lt;p&gt;我就知道我们的模型可以做到这一点！不过，我们的模型只有在已经有了当天的数据时才能战胜市场，也就是说还只是事后诸葛亮。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌对股票未来价格的预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在我们有了一个像样的模型，然后就可以使用predict_future（）函数来对股票未来价格的进行预测。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.predict_future(days=10)

amazon.predict_future(days=100) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9c540994ff97f59ac8f39f28158f5c1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;768&quot; data-rawheight=&quot;603&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17dc7025767d3e01fb16eac14f813c22_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;768&quot; data-rawheight=&quot;603&quot;&gt;&lt;p&gt;预测接下来10天和100天的股票价格趋势&lt;/p&gt;&lt;p&gt;这个模型和大多数“专业人士”一样，总体上看好Amazon这支股票。另外，我们按照预期做出的估计，不确定性会进一步增加。实际上，如果我们使用这个模型策略进行交易，那我们每天都可以训练一个新的模型，并且提前预测最多一天的价格。&lt;/p&gt;&lt;p&gt;虽然我们可能没有从Stocker工具中获得丰厚的收益，但是重点在于开发过程而不是最终结果！ 在我们尝试之前，我们实际上不知道自己是否能解决这样一个问题，就算最终失败，也好过从不尝试！任何有兴趣检查代码或使用Stocker工具的人，都可以在GitHub上找到代码。（https://github.com/WillKoehrsen/Data-Analysis/tree/master/stocker）&lt;/p&gt;&lt;blockquote&gt;作者 | William Koehrsen&lt;br&gt;原文 | &lt;a href=&quot;https://towardsdatascience.com/stock-prediction-in-python-b66555171a2&quot;&gt;https://towardsdatascience.com/stock-prediction-in-python-b66555171a2&lt;/a&gt;&lt;/blockquote&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;基于Python预测股价的那些人那些坑，请认真看完！【系列52】&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-25-33297567</guid>
<pubDate>Thu, 25 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-22-33192877.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33192877&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ef12834aa1b065c4cd9f9cd305044a1d_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;公众号今天为大家带来机器学习应用区块链系列的第二篇文章。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287287&amp;amp;idx=1&amp;amp;sn=a4efd248d74afa7360f2e32751066b5d&amp;amp;chksm=802e3162b759b8748907f4d889d648d89acd54837041a8f94cde00740b5a867012ae48fa1f5d#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】&lt;/a&gt;&lt;p&gt;这篇文章的作者是公众号之前推过一位原作者，具体文章&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284793&amp;amp;idx=1&amp;amp;sn=76c954a5a8006c815565d8669411f983&amp;amp;chksm=802e2bacb759a2ba4dd2ad122fe7cd99ab85ed29900b212189ab0af36749123c9e39b422363b&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【量化精品】通过LSTM神经网络进行时序预测针对股票市场（附Python源码）&lt;/a&gt;&lt;p&gt;虽然有一段时了，但是，我们觉得这篇文章的结论很有用，希望大家可以认真阅读。&lt;/p&gt;&lt;p&gt;&lt;b&gt;结论很精彩，见文末。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;也侧面告诉我们使用一些机器学习算法做股价预测，效果并没有那么好！大家不要觉得什么高大上就一定好，有时候真实的交易其实并没有那么复杂！&lt;/p&gt;&lt;p&gt;至于怎么预测的过程大家可以下载代码自行去研究。&lt;/p&gt;&lt;p&gt;写这篇文章，主要是为了预测比特币的价格和张量，使用一个不只是看价格而且还看BTC交易量和货币（在这种情况下为美元）的多维LSTM神经网络，并创建一个多变量序列机器学习模型。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;时间数据集&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Kaggle上有一个数据集，包含了7种要素的比特币历史数据。&lt;/p&gt;&lt;p&gt;链接：&lt;b&gt;https://www.kaggle.com/mczielinski/bitcoin-historical-data&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-88501ee596eab8df850b510c9e8fa1cb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;971&quot; data-rawheight=&quot;692&quot;&gt;&lt;p&gt;具体处理的过程大家自行查看内容，只把clean_data（）函数的核心代码贴出来： &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;num_rows = len(data)
x_data = []
y_data = []
i = 0
while((i+x_window_size+y_window_size) &amp;lt;= num_rows):
 x_window_data = data[i:(i+x_window_size)]
 y_window_data = data[(i+x_window_size):(i+x_window_size+y_window_size)]

 #Remove any windows that contain NaN
 if(x_window_data.isnull().values.any() or y_window_data.isnull().values.any()):
   i += 1
   continue
 
 if(normalise):
   abs_base, x_window_data = self.zero_base_standardise(x_window_data)
   _, y_window_data = self.zero_base_standardise(y_window_data, abs_base=abs_base)

 #Average of the desired predicter y column
 y_average = np.average(y_window_data.values[:, y_col])
 x_data.append(x_window_data.values)
 y_data.append(y_average)
 i += 1&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;预测结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们将尝试做两种类型的预测：&lt;/p&gt;&lt;p&gt;第一种：逐点预测，即预测t+1点，然后移动真实数据的窗口并继续预测下一个点。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model = lstm.load_network(&#39;data/model_saved.h5&#39;)
predictions = model.predict_generator(
   generator_strip_xy(data_gen_test, true_values),
   steps=steps_test
)

#Save our predictions
with h5py.File(configs[&#39;model&#39;][&#39;filename_predictions&#39;], &#39;w&#39;) as hf:
   dset_p = hf.create_dataset(&#39;predictions&#39;, data=predictions)
   dset_y = hf.create_dataset(&#39;true_values&#39;, data=true_values)
   
plot_results(predictions[:800], true_values[:800]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-16e749c9969f4a1c4847a9d2016b1490_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1177&quot; data-rawheight=&quot;762&quot;&gt;&lt;p&gt;第二种：是t+n的多步超前预测，我们在移动窗口填充真实数据窗口预置的预测，并绘制N个步骤。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;#Reload the data-generator
data_gen_test = dl.generate_clean_data(
   configs[&#39;data&#39;][&#39;filename_clean&#39;],
   batch_size=800,
   start_index=ntrain
)
data_x, true_values = next(data_gen_test)
window_size = 50 #numer of steps to predict into the future

#We are going to cheat a bit here and just take the next 400 steps from the testing generator and predict that data in its whole
predictions_multiple = predict_sequences_multiple(
   model,
   data_x,
   data_x[0].shape[0],
   window_size
)

plot_results_multiple(predictions_multiple, true_values, window_size) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c0b7bffff414db7f177baf7ee9a34632_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1177&quot; data-rawheight=&quot;762&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;结论（我们认为很有用的结论）&lt;/b&gt; &lt;/p&gt;&lt;p&gt;不以人工智能的角度，而是从投资的角度来解释一些事实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测回报是一项相当没有意义的行为&lt;/b&gt;。我的意思是说，预测回报是预测的圣杯，而一些顶级对冲基金视图通过在现实中找到新的alpha指标来做到这一点，这是一件非常困难的事情，因为复杂的外部因素会影响到价格的走动。实际上，它可以看作是试图预测随机的下一步。&lt;/p&gt;&lt;p&gt;但是，我们做的也并不是完全没有意义。有限的时间序列数据，即使有多个维度，也很难预测回报，我们可以看到，特别是从第二个图表看到，是有一个预测波动的方法。而不仅仅是波动，而且我们也可以通过扩张它来预测市场环境，使我们能够了解我们目前所在的市场环境。&lt;/p&gt;&lt;p&gt;我们可以看到，通过了解我们当前的市场环境，预测未来的市场环境是在任何时候将正确的策略分配到市场的关键。虽然这更多是传统市场的一般投资方式，但同样适用于比特币市场。 &lt;/p&gt;&lt;p&gt;所以，&lt;b&gt;预测比特币的长期价格目前相当的困难&lt;/b&gt;，没有人可以只是通过时间序列数据技术做到，因为有很多因素加入使价格发生了变动。在这样的数据集上使用LSTM神经网络的另一个问题是我们将整个时间序列数据集作为一个固定的时间序列。&lt;b&gt;也就是说，时间序列的属性在整个时间内都是不变的。然而这不可能，因为影响价格变化的因素也会随时间而变化，所以假设网络发现的属性或模式在现在仍然使用是一种不合理的做法。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当然，可以通过一些方法来克服这个非平稳性问题，&lt;b&gt;目前前沿的研究方向是利用贝叶斯方法和LSTM一起克服时间序列非平稳性的问题。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;代码链接: https://pan.baidu.com/s/1qZJaoxA &lt;/p&gt;&lt;p&gt;密码: kghx&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287287&amp;amp;idx=1&amp;amp;sn=a4efd248d74afa7360f2e32751066b5d&amp;amp;chksm=802e3162b759b8748907f4d889d648d89acd54837041a8f94cde00740b5a867012ae48fa1f5d#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-22-33192877</guid>
<pubDate>Mon, 22 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>2018年「金融平均脸」出炉，是不是要长这样才能做期货？</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153911.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153911&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5fd69dc772d93269f198ba59246108a9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;对比各个金融领域公司的“平均样貌”，你眼中的“颜值代表”是哪家？&lt;/p&gt;&lt;p&gt;期货脸长啥样？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;金融行业&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最受热捧的脸&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-35476841a978e223078a0dcaa83d559f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;高盛&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9971fe42cfe51be34b8be0efeb234ac1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;65&quot; data-rawheight=&quot;45&quot;&gt;&lt;p&gt;世界上最可怕的事&lt;/p&gt;&lt;p&gt;是比你美的人还比你有脑子&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5c898afd266deaea0cbe92ac4f5fd6c8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;摩根大通&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9973210c31280929e25313a96e49612_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;178&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;听说很多人叫我爸爸&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9134a4e26e1083d862d646850bc027ae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中金&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6c837db1ea6b924e0885de5e9a7f96b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;149&quot; data-rawheight=&quot;102&quot;&gt;&lt;p&gt;我们有很多个小目标 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1250204b41f23ce7971882c804e6709_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中信&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55a2452a23ff0e371dca78ef63eb1112_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;175&quot; data-rawheight=&quot;50&quot;&gt;&lt;p&gt;几十个亿，真的好难花&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2ae16cc8f64bd1023f438395c9ea8502_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四大会计师事务所&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;风彩与黑眼圈并存&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aecb9c29dd345b1b1e64d5ac6eae597b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;安永&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9ecb3dce198252ac82d8382a16704f67_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;压力大又怎样，软妹多就好&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-990e351a3e9d217e174da157f08e479b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;普华永道&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-56f748ddacb95f4f63f1d870eb249e7f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;119&quot; data-rawheight=&quot;75&quot;&gt;&lt;/b&gt;&lt;p&gt;来自行业老大的微笑&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-86640f3058a89ac9a5ea17a80c0b78a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;毕马威&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有一种脸叫数据脸&lt;/p&gt;&lt;p&gt;&lt;b&gt;德勤&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5433aaa9de1e9c1230cb8244d28c7b68_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;91&quot;&gt;&lt;p&gt;金丝镜框后，是美图也盖不住的细纹&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3a93d9be2dc5ba5dc97b476ed913683b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;咨询行业&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;br&gt;做PPT，我们是最认真的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-567c4905d9fd12c102af578b3388673a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;麦肯锡&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1a52a74c8dfe09d2f8ce93a51a3f1f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;300&quot; data-rawheight=&quot;154&quot;&gt;&lt;p&gt;盛名在外的，除了专业还有颜值&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f5ac1f8e0bcd9c7b009bb437df32c6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;波士顿&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a7fc2b65f764767d6b75dba88352c70e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;104&quot;&gt;&lt;p&gt;没有用数字解释不了的事儿 &lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-312aabaeb2efa2cd8d306a8427bef24d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;贝恩&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-077b7999edd5e4dec5268847c209b9ba_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;169&quot;&gt;&lt;p&gt;好不好看不重要&lt;/p&gt;&lt;p&gt;会汇（bi）报（bi）最重要&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e96e9da41ebca94ea46c0588752af10_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;罗兰贝格&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fe72ebb22890c6adae89f32732a73999_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;224&quot; data-rawheight=&quot;120&quot;&gt;&lt;p&gt;150一天的餐补，果然不是盖的。&lt;/p&gt;&lt;p&gt;有图为证！&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-75304830a152a09ab90fc9b4dc35258e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;银行&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;算起账来，我们是最无辜的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;招商银行&lt;/b&gt;&lt;/p&gt;&lt;p&gt;招行的人一定要苗条漂亮，&lt;/p&gt;&lt;p&gt;如果太胖了请去对面的工行。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1a3b278e5d14cef8ce6d2c4034a79fd2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;工商银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d9c3047e1c0e889495b0c9f22c9ac08c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;466&quot; data-rawheight=&quot;113&quot;&gt;&lt;p&gt;我们宇宙行家大业大，实力雄厚。&lt;/p&gt;&lt;p&gt;楼上就是矫情！&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;建设银行&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c5f44fda1e311917046628090a8c44f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;454&quot; data-rawheight=&quot;106&quot;&gt;&lt;/b&gt;&lt;p&gt;我就静静看着楼上不说话，&lt;/p&gt;&lt;p&gt;反正我也没空说话。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-11f3df96d86dcc363093bb498b3b760b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;中国银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-449e5df1bdf3e70d36adc4102f81ad39_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;232&quot; data-rawheight=&quot;90&quot;&gt;&lt;p&gt;谢谢，这是四大行里最international的脸&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-16f1c79b4419f6a9bd1015713008b04e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;期货行业&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b7ae886a05284c6f583d9077432339d3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;657&quot; data-rawheight=&quot;430&quot;&gt;&lt;p&gt;咳咳...金融民工中的战斗机...灰头土脸...&lt;/p&gt;&lt;p&gt;本文纯属搞笑，如有雷同，纯属巧合&lt;/p&gt;&lt;p&gt;素材来源：UniCareer、瞭望智库&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-17c3d959b2db4c1b3a90a54e5af7f4ee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153911</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>模式识别在金融时间序列中的应用【系列五十】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153489.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153489&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-42b57b00a759ca6bdc258a8f967aa0b2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天编辑部诶大家带来Kathryn Dover的一篇文章。主要讲模式识别在股票数据中的应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;p&gt;Finding patterns in high dimensional data can be difficult because it cannot be easily visualized. Many different machine learning methods are able to fit this high dimensional data in order to predict and classify future data but there is typically a large expense on having the machine learn the fit for a certain part of the dataset. This thesis proposes a geometric way of defining different patterns in data that is invariant under size and rotation so it is not so dependent on the input data. Using a Gaussian Process, the pattern is found within stock market data and predictions are made from it.&lt;/p&gt;&lt;p&gt;主要是用一种不依赖于输入数据的方法。 使用高斯过程，在股票市场数据中发现该模式，并从中进行预测。&lt;/p&gt;&lt;p&gt;前面几部分都是背景介绍，大家可以自己去查阅，我们主要把后面几部分解读一下。&lt;/p&gt;&lt;p&gt;作者定义了三种大家常见的指标形态：W底、M顶、头肩形。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Standard Double-Bottom Pattern (A Standard W)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c020094c0c4880baf508c0d835417f7a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;249&quot; data-rawheight=&quot;186&quot;&gt;&lt;p&gt;&lt;b&gt;2. Standard Double-Top Pattern (A Standard M)&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-82f9bed60e9e9bf18b159f75b2186f1f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;277&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;&lt;b&gt;3. Standard Head and Shoulder Pattern&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d40dea7c724458261ce2e642a8bf61e3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;270&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;When we actually apply our definition of shapes to the data, we will find that there is no perfect match to our standard definition. Therefore, we will want to define a “fuzzy shape” that will be considered an approximate form of the standard shape. These fuzzy shapes will not only allow us to approximate how far away a shape is from the standard shape but also how far fuzzy shapes are from each other. The way that we define our fuzzy shapes is discussed below. &lt;/p&gt;&lt;p&gt;用数据去定义这些指标形态时，我们会发现与我们定义的形态并不是完全匹配。 作者做了一个“模糊形状”的处理。将被视为标准形状的近似形式。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;1. The Fuzzy W&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-41f9aac637fb0e8b131d3deed4a8f237_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;392&quot; data-rawheight=&quot;264&quot;&gt;&lt;p&gt;&lt;b&gt;2. The Fuzzy M&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d7a46b860d17b137f5ae32b6e318670_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;122&quot; data-rawheight=&quot;29&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-741fbac4dff294df4eb9fb2718b060b1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;221&quot;&gt;&lt;p&gt;&lt;b&gt;3. The Fuzzy Head and Shoulder&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-29da43f4bd4cced4f3a8eb0c80aad1a2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;436&quot; data-rawheight=&quot;270&quot;&gt;&lt;p&gt;Now that we have defined both our standard and fuzzy shapes, we want to be able to define some actions we can take on these shapes. We discuss the change of basis and flipping a shape and how they can be used to compare different shapes to each other. We also discuss how the slopes and lengths of the vectors can be used to categorize the shape that can be used for prediction.&lt;/p&gt;&lt;p&gt;接下来作者对这三种形态的一些变化进行了讨论。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Change of Basis&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Flipping a Shape&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f0bef6b065aa075213159f8630772bf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;265&quot; data-rawheight=&quot;203&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;b&gt;3. Symmetric Representation&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-134d9ad67dd4bab3f595d961c6b8a9d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;255&quot; data-rawheight=&quot;175&quot;&gt;&lt;p&gt;Once the shapes have been found in the data, we can categorize these shapes using their slopes and lengths. By categorizing certain shapes by their slopes and lengths, we can group similar shapes together and run traditional machine learning techniques on these specific groups. This is to determine indicators of these types of shapes, which can ultimately be used for predictive purposes. We discuss how shapes can be categorized using slopes and lengths and how we can use those categories to do rough predictions of stock market data.&lt;/p&gt;&lt;p&gt;开始对图形的斜率和长度进行定量的分析结合机器学习方法，然后开始预测。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Saving the Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cd6ee3a8a3a0a8c9734743d863aaa136_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;37&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91d72b91259f958235e5e1119fb0407b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;548&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-efcb1788e41e6f795bc849b750afd3d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;370&quot; data-rawheight=&quot;199&quot;&gt;&lt;p&gt;两种预测方法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Predictions with Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Prediction with Direction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;创建一个算法来寻找模式：&lt;/p&gt;&lt;p&gt;Now that we have defined what our shapes are and how to approximate shapes, we will be discussing the algorithm that finds these shapes and how it handles the data. I used a Gaussian Process to fit the data with a function that the local minimums and maximums could be extracted from. These local extrema are used to define the vectors of the shapes and subsequently the algorithm checks these vectors using the definitions from the previous sections.&lt;/p&gt;&lt;p&gt;关键词：&lt;b&gt;Gaussian Process&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最终过程如下：&lt;/p&gt;&lt;p&gt;1. Given a certain variance, a Gaussian Process is used to fit the data.&lt;/p&gt;&lt;p&gt;2. From this fit of the data, the algorithm identifies all the local extrema in the data.&lt;/p&gt;&lt;p&gt;3. The algorithm goes through the local extrema in the data sequentially and identifies sequences that could possibly be a predefined shape.For example, a W has a following sequence of minium and maximum:max/min/max/min/max.&lt;/p&gt;&lt;p&gt;4. If a sequence is valid for a certain shape, then the algorithm uses the definitions of the shapes defined earlier in order to figure out if they qualify.&lt;/p&gt;&lt;p&gt;5. If a shape is found, the lengths and slopes of the vectors in the shape are stored (as well as the following segment) for predictive purposes.&lt;/p&gt;&lt;p&gt;6. The categorizing constants k` i and ks i are calculated and stored.&lt;/p&gt;&lt;p&gt;7. Once the algorithm has gone through all the extrema, it uses the stored slopes, lengths, and categorizing constants and uses them to calculate the rough prediction from the weighted average.&lt;/p&gt;&lt;p&gt;在下图中，我们可以看到算法的运行步骤。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-42b53e58ffb2513c0e272d052520200f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;508&quot; data-rawheight=&quot;219&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7424028c97ee1280007620a30e1a50af_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;507&quot; data-rawheight=&quot;223&quot;&gt;&lt;p&gt;实际的应用：&lt;br&gt;In this section, I describe the results of the algorithm I developed in the last chapter when it was run on stock price history for different companies. The algorithm was run for variances 0.001 and 4 over the entire stock price history for the companies Apple, Disney, Microsoft, Walmart and the NASDAQ index. For testing the prediction part of the algorithm, the entire stock price history for the Dow Jones index was used. The reason for choosing this dataset was twofold. First, there was between 20-30 years worth of data for each company, which would mean I could potentially find a variety of shapes in different sizes. Second, I chose these specific companies by looking through the stock history of companies and choose those that had many turbulent years that offer enough fluctuation that can create the shapes I am looking for. All stock data was found at yahoo.finance.com. The below subsections discuss how I fit my data, found the shapes, and calculated predictions for different shapes. &lt;/p&gt;&lt;p&gt;用了两种方法&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Finding Different Sized Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-00c7953c6f670571bd80a643ecb1e04a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;468&quot;&gt;&lt;p&gt;The original data (upper le), the fit of the data when the variance is 0.001 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Finding Multiple Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c4eb07203dd4c838e90704938b5ac68a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;471&quot;&gt;&lt;p&gt;is 4 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ws were found&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ms were found&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;I used the data from Apple, Disney, Microsoft, Walmart and the NASDAQ as my learning set for both of my prediction methods and I used the Dow Jones index as my test data. The results from each of the methods is given below.&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bb403d155288803c73adfd2aca4f62b5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;620&quot; data-rawheight=&quot;268&quot;&gt;&lt;p&gt;&lt;b&gt;算法存在的问题：&lt;/b&gt;&lt;br&gt;While the algorithm was successful in finding the shapes and doing a rough prediction, there were still some issues with it. For example, finding a good tolerance for finding a shape was not exact. I had to run the algorithm many times to determine at what point the algorithm would find identify a pattern was not the correct shape. This usually happened because the tolerance was too high and the patterns found did not look like the shape. Likewise, when the tolerance was too low, it would identify no shape. Finding an appropriate tolerance (somewhere between 0.7 and 1) took a while and the tolerance for each shape was different as the Ws had a lower tolerance while the head and shoulder pattern needed a higher tolerance.&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Finally, the predictive step of the algorithm has a long way to go. Using the slopes and lengths method did not work well which might be explained by having no good way of measuring the difference between slopes. Addi tionally, the weighted average method might not be enough to get a good understanding unless I had a lot more data. I had about 55 shapes for each type of shape when I ran the algorithm on the data. If I want to get a better average, I would need to run my code on a lot more data in order for the average to not be swayed as heavily by outliers. The directional vector method worked much better than the slopes and lengths method, but there was still a error of about 20-28 degrees. We could potentially decrease this error by simply getting more data. This prediction also indicated that there actually might be a correlation between the shapes and the directional vector, so perhaps we could use more traditional machine learning methods on these directional vector values to see if we can learn anything extra from it.For example, is there some sort of correlation between the k values we found in the slope/lengths predictions and the directional vector we calculated? A question like this might be answered by using a neural network or a support vector machine.&lt;/p&gt;&lt;p&gt;同时作者还提出了一些改进模型的方法。我们认为此部分最为有探究性：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Improving Shape Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Extending into Multiple Dimensions&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文链接：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://scholarship.claremont.edu/hmc_theses/105/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;&quot;Pattern Recognition in Stock Data&quot; by Kathryn Dover&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153489</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
