<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>公众号主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Mon, 12 Feb 2018 16:25:53 +0800</lastBuildDate>
<item>
<title>将海外量化资源一网打尽！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644454.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644454&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e13a551a257f4ad1e1d2f115f466f451_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;公众号开通了&lt;/p&gt;&lt;p&gt;&lt;b&gt;微博官方账号&lt;/b&gt;&lt;/p&gt;&lt;p&gt;取名&lt;/p&gt;&lt;h2&gt;&lt;b&gt;宽客的后花园&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;大家赶紧粉一波吧&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-58318d82ca17477b7a42c6888025eee9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;348&quot;&gt;&lt;p&gt;此举&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;一是弥补微信公众号平台不能调用外部链接的一个不足。&lt;/b&gt;&lt;br&gt;&lt;b&gt;二是给一些不能上外网的朋友带来福利。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;同时，在国内，我们还没有看到很好的量化海外分享平台。为此，我们决定做这件事。希望能给大家带来全新的阅读体验。&lt;/p&gt;&lt;p&gt;在这里，公众号还要要感谢一个人，他就是北邮的陈光老师。他的微博@爱可可，爱生活影响了很多人，公众号也从中受益很多。同时，也给了公众号新的途径和思路，让我们可以把国外优秀的资源搬进国内。为国内量化爱好者提供更多的视角和资讯！&lt;/p&gt;&lt;p&gt;之所以把公众号官方微博起名为：&lt;b&gt;宽客的后花园&lt;/b&gt;，也是希望大家能在这个量化的后花园发现更多意想不到的惊喜！&lt;/p&gt;&lt;p&gt;当然，大家也要知道公众号的&lt;b&gt;官方微博&lt;/b&gt;与&lt;b&gt;微信公众号&lt;/b&gt;有&lt;b&gt;很大不同&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;1、内容与微信公众号完全不同；&lt;/b&gt;&lt;br&gt;&lt;b&gt;2、主要分享海外关于量化投资方向的各种资源，机器学习方向为辅。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;还在等什么&lt;/p&gt;&lt;p&gt;赶紧去关注我们的官方微博吧&lt;/p&gt;&lt;h2&gt;&lt;b&gt;@宽客的后花园&lt;/b&gt;&lt;/h2&gt;&lt;a href=&quot;https://login.sina.com.cn/crossdomain2.php?action=login&amp;amp;entry=miniblog&amp;amp;r=https%3A%2F%2Fpassport.weibo.com%2Fwbsso%2Flogin%3Fssosavestate%3D1549453878%26url%3Dhttps%253A%252F%252Fweibo.com%252Flhtzjqxx%26display%3D0%26ticket%3DST-MTM1MDg5MzIzMw%3D%3D-1517917878-gz-4AB0E5FA2BEA75DAC3BCFD5DD32AC61A-1%26retcode%3D0&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;新浪通行证&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644454</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>『量化投资』冲顶大会第一场！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644277.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644277&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-affc81430b6d731cab77ccd3969899bf_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;最近、冲顶大会、百万赢家、百万英雄、芝士超人等众多在线答题平台，直接刷爆了朋友圈。作为专注于量化投资方向的我们，怎么能坐视不管呢！&lt;/p&gt;&lt;p&gt;从今天开始，量化投资与机器学习公众号将不定期举办&lt;b&gt;『量化投资』&lt;/b&gt;冲顶大会！&lt;/p&gt;&lt;p&gt;今天是第一场，后面还会有更多场次。我们也会邀请一些机构和行业内的大咖为大家出题，丰富知识的多元化和专业性。同时在后几期我们还会设立奖金或者赠送图书。&lt;/p&gt;&lt;p&gt;我们也欢迎有兴趣的个人或者机构来为我们出题。具体联系方式可以添加小编的微信：&lt;b&gt;zxlgglr&lt;/b&gt;【备注：出题】&lt;/p&gt;&lt;p&gt;总计12题，答案在这里&lt;b&gt;（留言第一条）&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287343&amp;amp;idx=1&amp;amp;sn=b9d1c18380213d0cf097aacdd4ac24b0&amp;amp;chksm=802e31bab759b8ac67886dc3d16d9bb409db8dbaa7dee3e9ed2a8925b979fef1f9969475358c#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;『量化投资』冲顶大会第一场！&lt;/a&gt;&lt;p&gt;&lt;b&gt;题目范围（金融+数学+计算机）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;好了，答题开始！&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;第一题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1．相对价值策略的特点是（）。  &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．低收益、低风险、大容量   &lt;/p&gt;&lt;p&gt;B．高收益、低风险、小容量    &lt;/p&gt;&lt;p&gt;C．高收益、高风险、大容量    &lt;/p&gt;&lt;p&gt;D．高收益、高风险、小容量     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第二题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2．著名的Chern-Simons定理是由（）与数学家陈省身共同创立。    &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．詹姆斯·西蒙斯    &lt;/p&gt;&lt;p&gt;B．大卫·肖    &lt;/p&gt;&lt;p&gt;C．伊曼纽尔·德曼    &lt;/p&gt;&lt;p&gt;D．Ray Dalio&lt;/p&gt;&lt;p&gt;&lt;b&gt;第三题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3．第十五届新财富最佳分析师金融工程组第一名是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．广发证券    &lt;/p&gt;&lt;p&gt;B．长江证券 &lt;/p&gt;&lt;p&gt;C．国泰君安证券 &lt;/p&gt;&lt;p&gt;D．申万宏源证券&lt;/p&gt;&lt;p&gt;&lt;b&gt;第四题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4．关于金融市场的数学定义，下列说法正确的是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．数学可以用来描述金融市场&lt;/p&gt;&lt;p&gt;B．把金融市场看成是函数逼近问题时，可以用贝叶斯分类进行计算&lt;/p&gt;&lt;p&gt;C．把金融市场看成是分类问题时，可以用回归分析的方式进行数据分析&lt;/p&gt;&lt;p&gt;D．把金融市场看成是概率问题时，可利用小波分析理论计算概率&lt;/p&gt;&lt;p&gt;&lt;b&gt;第五题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;5．当随机误差项存在自相关时，进行单位根检验是由（）来实现。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．DF检验                   &lt;/p&gt;&lt;p&gt;B．ADF检验&lt;/p&gt;&lt;p&gt;C．EG检验                    &lt;/p&gt;&lt;p&gt;D．DW检验&lt;/p&gt;&lt;p&gt;&lt;b&gt;第六题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;6．目前在线量化平台使用MATLAB语言的平台是（）。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．优矿                     &lt;/p&gt;&lt;p&gt;B．聚宽&lt;/p&gt;&lt;p&gt;C．米筐                &lt;/p&gt;&lt;p&gt;D．点宽&lt;/p&gt;&lt;p&gt;&lt;b&gt;第七题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;7．A股第一次熔断是什么时候（）。 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．2016年1月1日                     &lt;/p&gt;&lt;p&gt;B．2016年1月2日     &lt;/p&gt;&lt;p&gt;C．2016年1月3日     &lt;/p&gt;&lt;p&gt;D．2016年1月4日     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第八题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;8．下面这段代码的输出结果将是什么（）。 &lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def multipliers():
   return [lambda x : i * x for i in range(4)]
   
print [m(2) for m in multipliers()]&lt;/code&gt;&lt;p&gt;A．[2, 4, 6, 8]      &lt;/p&gt;&lt;p&gt;B．[0, 2, 4, 6]     &lt;/p&gt;&lt;p&gt;C．[6, 6, 6, 6]  &lt;/p&gt;&lt;p&gt;D．[2, 2, 2, 2]     &lt;/p&gt;&lt;p&gt;&lt;b&gt;第九题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;9．峰度和偏度分别是概率分布的几阶矩（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．三、四   &lt;/p&gt;&lt;p&gt;B．四、三&lt;/p&gt;&lt;p&gt;C．二、三&lt;/p&gt;&lt;p&gt;D．三、二&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;10．对冲基金中的贝塔与杠杆成（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．正比&lt;/p&gt;&lt;p&gt;B．反比&lt;/p&gt;&lt;p&gt;C．不成比&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十一题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;11．CAPM、Mean-Variance model、APT三大理论提出的时间顺序是（）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．CAPM、Mean-Variance model、APT&lt;/p&gt;&lt;p&gt;B．APT、CAP、MMean-Variance model&lt;/p&gt;&lt;p&gt;C．Mean-Variance model、CAPM、APT&lt;/p&gt;&lt;p&gt;&lt;b&gt;第十二题&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;12．量化投资与机器学习公众号成立（）年了。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A．2&lt;/p&gt;&lt;p&gt;B．1&lt;/p&gt;&lt;p&gt;C．3&lt;/p&gt;&lt;p&gt;D．4&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;公众号的&lt;b&gt;微博官方账号&lt;/b&gt;已开通&lt;/p&gt;&lt;p&gt;&lt;b&gt;@宽客的后花园&lt;/b&gt;&lt;/p&gt;&lt;p&gt;赶紧去关注我们吧&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;1、内容与微信公众号完全不同；&lt;/b&gt;&lt;br&gt;&lt;b&gt;2、主要分享海外关于量化投资方向的各种资源，机器学习方向为辅。&lt;/b&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-58318d82ca17477b7a42c6888025eee9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;348&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644277</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>请收下这份榜单：2017年最热门的50篇券商金工研报（附下载）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-06-33644167.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33644167&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-177a92bd3d38bd1a6fc8ae99b1117c82_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;还有几天就要过年啦！在此，公众号特地为大家准备了第一份新年礼物。&lt;/p&gt;&lt;p&gt;回顾2017年，公众号分享了全年所有券商的金工研报（&lt;u&gt;&lt;b&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287361&amp;amp;idx=1&amp;amp;sn=b659d13209d14bfc2cc9865ca1c42042&amp;amp;chksm=802e31d4b759b8c2548c5f128f0a7b6d59dbc3a89b2f8f49fcdf197736ad2cb1c25182928e51&amp;amp;scene=21#wechat_redirect&quot;&gt;这里下载&lt;/a&gt;&lt;/b&gt;&lt;/u&gt;），但是在这么多的研报中，最受大家喜爱，排名靠前的是哪些呢？想必大家可能不是很了解。&lt;/p&gt;&lt;p&gt;今天，公众号就为大家公布：&lt;b&gt;2017年最热门的50篇券商金工研报&lt;/b&gt;。让我们一睹为快吧！（数据来源于多方媒体）&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-723c09380cd3c5f3ddd81e7fa857f69d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;802&quot; data-rawheight=&quot;883&quot;&gt;&lt;p&gt;我们对前50名做了简单的统计，排在前三的是：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、华泰证券&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、国泰君安&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、海通证券&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其他券商统计如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-177a92bd3d38bd1a6fc8ae99b1117c82_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1925&quot; data-rawheight=&quot;1114&quot;&gt;&lt;p&gt;具体的文章，大家在过去一年的研报里，自行去找吧。多动手哦！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;金融工程研报汇总&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283257&amp;amp;idx=2&amp;amp;sn=49c78925e7f3535b9cad95bf91574519&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【每周研报干货】各大券商研报免费分享（附下载链接）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283773&amp;amp;idx=1&amp;amp;sn=d4604682da0c5563be9da16717d11bf9&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【干货】各大券商研究报告！&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283257&amp;amp;idx=2&amp;amp;sn=49c78925e7f3535b9cad95bf91574519&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;【每周研报干货】各大券商研报免费分享（附下载链接）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284202&amp;amp;idx=1&amp;amp;sn=f94bdefe70ddcb538ca463ba1c5e5205&amp;amp;chksm=802e257fb759ac69899d8544937600c22637697591fce25d1ed1b72414d975eeeba7cc58c9d8&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;新财富金融工程前三名【海通证券】 研报大放送（百篇）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284199&amp;amp;idx=1&amp;amp;sn=4ec9cac078f8057744349c9c953decb2&amp;amp;chksm=802e2572b759ac6438362451289132ab4bb631da5b41e9f2b2545eb5efe50e0d14d6bd3d3015&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;新财富金融工程前三名【广发证券】 研报大放送（最全）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284196&amp;amp;idx=1&amp;amp;sn=85245caf9148fb965df1c56c963984ba&amp;amp;chksm=802e2571b759ac6772582aea40781bddd6f148f144edc9b8b08606749f3c2c012b907441d59d&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;两度蝉联新财富金融工程第一名的【国泰君安】 研报大放送&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284668&amp;amp;idx=1&amp;amp;sn=1d099b61ac8a378f39ef99203cfb85af&amp;amp;chksm=802e2b29b759a23f1ce824e84ab55601f8da41ace7877cac3fe97900f1a7147c97a732481841&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第1部分&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284678&amp;amp;idx=1&amp;amp;sn=0c29d884ada86f565b5849057fe5cdb6&amp;amp;chksm=802e2b53b759a245db87fe77c211e8f987464d0d188305808b412fb2d36cbc9f4bb707fedde9&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第2、3、4部分&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284702&amp;amp;idx=1&amp;amp;sn=c150e541adb6f852459b085a086bf97f&amp;amp;chksm=802e2b4bb759a25de30c981d25e8db6c90e409e0c8ec5303ad0b3fa673abfc01fd4832842c16&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【干货分享】2016年全年所有券商金融工程研究报告（共600篇）- 第5、6、7、8、9部分【完结】&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286032&amp;amp;idx=1&amp;amp;sn=f931e3de55ba425049553d524173b57e&amp;amp;chksm=802e2c85b759a5935002ab01161a92be5ba6c7a5ba64ad12d8be55490fa328973835008ab2dc&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【年度干货】2017上半年所有券商金融工程研究报告（一）&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286032&amp;amp;idx=1&amp;amp;sn=f931e3de55ba425049553d524173b57e&amp;amp;chksm=802e2c85b759a5935002ab01161a92be5ba6c7a5ba64ad12d8be55490fa328973835008ab2dc&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【年度干货】2017上半年所有券商金融工程研究报告（一）&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286262&amp;amp;idx=1&amp;amp;sn=8fe879fc4a5189cf027b7496da82681f&amp;amp;chksm=802e2d63b759a47535c7a0dfe279672f10821edcdeb49c6f099a7388feef39e8faeb2aaf30e3&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年8月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286383&amp;amp;idx=1&amp;amp;sn=7c6b9f54ee5727ede261042510daa401&amp;amp;chksm=802e2dfab759a4ec6a3eb346d6e27fceae852aefae361bd93320ba4ffab7a2859899b28ace19&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年9月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286510&amp;amp;idx=1&amp;amp;sn=b64aab20dc1ba2e56776aa34090d361d&amp;amp;chksm=802e327bb759bb6d558caf6a2aaf4e86bfaf31a3558573f58c7f5f24d1526756ec0ac1d3a820&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年10月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286772&amp;amp;idx=1&amp;amp;sn=f8ca457e87587ed73aa3d81903336db5&amp;amp;chksm=802e3361b759ba7775e1879e1c8a0b9b917d9ff43649e68c85b17b434d516acfc0ec758968a7&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年11月全部券商金融工程研报汇总&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287361&amp;amp;idx=1&amp;amp;sn=b659d13209d14bfc2cc9865ca1c42042&amp;amp;chksm=802e31d4b759b8c2548c5f128f0a7b6d59dbc3a89b2f8f49fcdf197736ad2cb1c25182928e51&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【每月系列】2017年12月全部券商金融工程研报汇总（第十一期免费赠书活动来啦！）&lt;/a&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-20e147d1234f759b40d3358135abec1e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-06-33644167</guid>
<pubDate>Tue, 06 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>基于Python预测股价的那些人那些坑，请认真看完！【系列52】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-25-33297567.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33297567&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7d01d8496ba2c0389083b675d7ef92d1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;基于Python预测股价的那些人那些坑，请认真看完！【系列52】&lt;/a&gt;&lt;p&gt;来源：AI科技大本营（ID：rgznai100）&lt;/p&gt;&lt;p&gt;编辑部：&lt;/p&gt;&lt;p&gt;前几天我们已经看过此文，@爱可可发过一次，今天AI科技大本营又翻译了此文。这篇文章真的很不错，不是在于模型的好坏，而是在于作者对预测股价的一些心得和体会，编辑部觉得大家应该好好的看一下这篇文章，它告诉我们所有高大上的东西，想在资本市场去赚钱，还是有难度的，预测股价的方法千奇百出，但是要正真理市场解去做，这才是最重要的。&lt;/p&gt;&lt;p&gt;不要跟风，不要盲目，不要虚荣。&lt;/p&gt;&lt;p&gt;B是给别人装的！钱是给自己挣的！&lt;/p&gt;&lt;p&gt;对数据科学家来说，预测证券市场走势是一项非常有诱惑力的工作，当然，他们这样做的目的很大程度上并不是为了获取物质回报，而是为了挑战自己。证券市场起起伏伏、变幻莫测，试想一下，如果在这个市场里存在一些我们或者我们的模型可以学习到的既定模式，让我们可以打败那些商科毕业的操盘手，将是多么美妙。当然，当我一开始使用加性模型（additive model）来做时间序列预测时，我不得不先用模拟盘来验证我的模型在股票市场上的表现。&lt;/p&gt;&lt;p&gt;一众挑战者们都希望在每日收益率上能够跑赢市场，但是大多数都失败了，我也未能幸免。不过，在这个过程中也学到了大量Python相关知识，包括面向对象编程、数据处理、建模、以及可视化等等。同时，我也认清了一个道理，不要在每日收益率上锱铢必较，学会容忍适当的短期亏损，放长线才能钓大鱼。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d7309ceb05c08d311844ee353152b408_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;590&quot; data-rawheight=&quot;304&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4f1c75c11cb0aba584f6f10cf7b30740_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;586&quot; data-rawheight=&quot;301&quot;&gt;&lt;p&gt;一天与三十年对比结果：你宁愿把钱投在哪里？&lt;/p&gt;&lt;p&gt;在任何任务中（不只是数据科学），当我们没有取得立竿见影的成效时，我们都有三个选择：&lt;/p&gt;&lt;p&gt;1. 调整结果，让我们看起来像是成功了&lt;/p&gt;&lt;p&gt;2. 隐藏结果，所以没有人会注意到&lt;/p&gt;&lt;p&gt;3. 公开我们所有的结果和方法，以便其他人（以及我们自己）可以从中吸取经验和教训&lt;/p&gt;&lt;p&gt;显然，不管站在个人还是社会层面，方案三都是最佳选择，但它同时也是最需要勇气去实践的。我可以选择性地公布结果，比如当我的模型能够带来丰厚的利润回报时，我也可以掩盖失败的事实，假装自己从来没有在这项工作上花过时间。这似乎是很天真的想法！我们之所以能够进步是因为不断重复失败——学习这个过程，而不仅仅是之前的成功。而且，为有难度的任务编写Python代码而付出的努力也并不应该白费！&lt;/p&gt;&lt;p&gt;这篇文章记录了我使用Python开发的“stock explorer”工具——Stocker的预测功能。此前，我曾展示了如何使用Stocker进行分析，并且将完整的代码贴在GitHub上，以方便大家。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;&lt;b&gt;Github代码地址：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;i&gt;&lt;b&gt;https://github.com/WillKoehrsen/Data-Analysis/tree/master/stocker&lt;/b&gt;&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌实现预测的Stocker工具&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Stocker是一款用于探索股票情况的Python工具。一旦我们安装了所需的库（查看文档），我们可以在脚本的同一文件夹中启动一个Jupyter Notebook，并导入Stocker类：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from stocker import Stocker &lt;/code&gt;&lt;p&gt;现在可以访问这个类了。我们通过传递任一有效的股票代码（粗体是输出）来创建一个Stocker类的对象：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon = Stocker(&#39;AMZN&#39;) &lt;/code&gt;&lt;p&gt;AMZN Stocker Initialized. Data covers 1997-05-16 to 2018-01-18.&lt;/p&gt;&lt;p&gt;根据上面的输出结果，我们有20年的亚马逊每日股票数据可以用来探索！ Stocker对象是建立在Quandl金融库上，而且拥有3000多只股票可以使用。我们可以使用plot_stock函数来绘制一个简单的历史股价图：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;amazon.plot_stock()
Maximum Adj. Close = 1305.20 on 2018-01-12.
Minimum Adj. Close = 1.40 on 1997-05-22.
Current Adj. Close = 1293.32.&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ab399e66b1cc5794e1ac422232e5a690_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;498&quot;&gt;&lt;p&gt;Stocker的分析功能可以用来发现数据中的整体趋势和模式，但我们将重点关注预测股票未来的价格上。Stocker中的预测功能是使用一个加性模型来实现的，该模型将时间序列视为季节性（如每日、每周和每月）的整体趋势组合。Stocker使用Facebook开发的智能软件包进行加性建模，用一行代码就可以创建模型并进行预测：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model, model_data = amazon.create_prophet_model(days=90)&lt;/code&gt;&lt;p&gt;Predicted Price on 2018-04-18 = $1336.98&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e6bcf8dbb79938ac9d7eabb82a4b8104_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;注意，表示预测结果的绿线包含了相对应的置信区间，这代表在模型预测的不确定性。在这种情况下，如果将置信区间宽度设置为80％，这意味着我们预计这个范围将包含实际值的可能性为80%。置信区间将随着时间进一步扩大，这是因为随着预测时间距离现有数据的时间越来越远，预测值将面临更多的不确定性。任何时候我们做这样的预测，都必须包含一个置信区间。尽管大多数人倾向于一个确定的值，但我们的预测结果必须反映出我们生活在一个充满不确定性的世界！&lt;/p&gt;&lt;p&gt;任何人都可以做股票预测：简单地选择一个数字，而这就是你的估测（我可能是错的，但我敢肯定，这是华尔街所有人都会做的）。为了让我们的模型具有可信度，我们需要评估它的准确性。Stocker工具中有许多用于评估模型准确度的方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌评估预测结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了计算准确率，我们需要一个测试集和一个训练集。我们需要知道测试集的答案，也就是实际的股价，所以我们将使用过去一年的历史数据（本例中为2017年）。训练时，我们不选用2014-2016的数据来作为训练集。监督学习的基本思想是模型从训练集中学习到数据中的模式和关系，然后能够在测试数据上正确地重现结果。&lt;/p&gt;&lt;p&gt;我们需要量化我们的准确率，所以我们使用了测试集的预测结果和实际值，我们计算的指标包括测试集和训练集的美元平均误差、正确预测价格变化趋势的时间百分比、以及实际价格落在预测结果80％置信区间内的时间百分比。所有这些计算都由Stocker自动完成，而且可视化效果很好：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction() &lt;/code&gt;&lt;p&gt;Prediction Range: 2017-01-18 to 2018-01-18.&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;Predicted price on 2018-01-17 = $814.77.
Actual price on    2018-01-17 = $1295.00.

Average Absolute Error on Training Data = $18.21.
Average Absolute Error on Testing  Data = $183.86.

When the model predicted an increase, the price increased 57.66% of the time.
When the model predicted a  decrease, the price decreased  44.64% of the time.

The actual value was within the 80% confidence interval 20.40% of the time.&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4de85d9dc40918f9b6988c686a699367_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;700&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;可以看到，预测结果真是糟糕透了，还不如直接抛硬币。如果我们根据这个预测结果来投资，那么我们最好是买买彩票，这样比较明智。但是，不要放弃这个模型，第一个模型通常比较糟糕，因为我们使用的是默认参数（称为超参数）。如果我们最初的尝试不成功，那么我们可以调整这些参数来获得一个更好的模型。在Prophet模型中有许多不同的参数设置需要调整，最重要的是变点先验尺度（changepoint prior scale），它控制着模型在数据趋势上的偏移量。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;▌变点先验（Changepoint Prior）的选择&lt;/b&gt;&lt;/p&gt;&lt;p&gt;变点代表时间序列从增加到减少，或者从缓慢增加到越来越快（反之亦然）。它们出现在时间序列变化率最大的地方。变点先验尺度表示在模型中给予变点的偏移量。这是用来控制过度拟合与欠拟合的（也被称为偏差与方差间的权衡）。&lt;/p&gt;&lt;p&gt;一个更高的先验能创造一个更多变点权重和更具弹性的模型，但这可能会导致过拟合，因为该模型将严格遵守训练数据的规律，而不能将它泛化到新的测试数据中。降低先验会减少模型的灵活性，而这又可能会导致相反的问题：欠拟合，当我们的模型没有完全遵循训练数据，而没有学习到底层模式时，这种情况就会发生。如何找出适当的参数以达到正确的平衡，这更多的是一个工程问题而不是理论问题，在这里，我们只能依靠经验结果。Stocker类有两种不同的方式来选择适当的先验：可视化和量化。 我们可以从可视化方法开始：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_analysis(changepoint_priors=[0.001, 0.05, 0.1, 0.2]) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-aaa6c8fb67f9bb09fbc0b953106b373b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;777&quot; data-rawheight=&quot;498&quot;&gt;&lt;p&gt;在这里，我们使用三年的数据进行训练，然后显示了六个月的预测结果。我们没有量化这里的预测结果，因为我们只是试图去理解变点先验值的作用。这个图表很好地说明了过拟合与欠拟合！代表最小先验的蓝线与代表训练数据的黑线值并不是非常接近，就好像它有自己的一套模式，并在数据的附近随便选了一条路线。相比之下，代表最大先验的黄线，则与训练观察结果非常贴近。变点先验的默认值是0.5，它落在两个极值之间的某处。&lt;/p&gt;&lt;p&gt;我们还要注意先验值不同带来的不确定性（阴影区间）方面的差异。最小的先验值在训练数据上表现有最大的不确定性，但在测试数据上的不确定性却是最小。相比之下，最大的先验值在训练数据上具有最小的不确定性，但在测试数据上却有最大的不确定性。先验值越高，对训练数据的拟合就越好，因为它紧跟每次的观察值。但是，当使用测试数据时，过拟合模型就会因为没有任何数据点来定位而迷失掉。由于股票具有相当多的变化性，我们可能需要比默认模型更灵活的模型，这样才能够捕捉尽可能多的模式信息。&lt;/p&gt;&lt;p&gt;现在我们对先验值带来的影响有了一个概念，我们可以使用训练集和验证集对数值进行评估：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_validation(start_date=&#39;2016-01-04&#39;, 
end_date=&#39;2017-01-03&#39;, changepoint_priors=[0.001, 0.05, 0.1, 0.2])&lt;/code&gt;&lt;p&gt;Validation Range 2016-01-04 to 2017-01-03.&lt;br&gt;&lt;br&gt;     cps  train_err  train_range    test_err  test_range&lt;br&gt;0  0.001  44.475809   152.600078  149.373638  152.564766&lt;br&gt;1  0.050  11.203019    35.820696  152.033810  139.505624&lt;br&gt;2  0.100  10.722908    34.593207  152.903481  172.654255&lt;br&gt;3  0.200   9.725255    31.895204  127.604543  324.376524&lt;/p&gt;&lt;p&gt;在这里，我们必须注意到，我们的验证集和测试集是不一样的数据。如果它们是一样的，那么我们会得到在测试数据上效果最好的模型，但是它只是在测试数据上过拟合了，而我们的模型也不能用于现实世界的数据。总的来说，就像在数据科学中通常所做的那样，我们正在使用三组不同的数据：训练集（2013-2015）、验证集（2016）和测试集（2017）。&lt;/p&gt;&lt;p&gt;我们用四个指标来评估四个先验值：训练误差、训练范围（置信区间）、测试误差和测试范围（置信区间），所有的值都以美元为单位。正如我们在图中看到的那样，先验值越高，训练误差越低，训练数据的不确定性越低。我们也可以看到，更高的先验能降低我们的测试错误。为了在测试集上获得更高的准确率，作为交换，随着先验的增长，我们在测试数据上得到了更大范围的不确定性。&lt;/p&gt;&lt;p&gt;Stocker先验验证还可以通过两条线来阐述这些点：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-390f2b8a3502e56f41d2eb66843a3b00_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1ec797fc4d29f742eb1ad2971cae8c6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;基于不同变点先验尺度下，训练和测试准确性曲线和不确定性曲线&lt;/p&gt;&lt;p&gt;既然最高的先验值产生了最低的测试误差率，我们应该尝试再增加先验值来看看是否能得到更好的结果。我们可以通过在验证中加入其它值的方法来优化我们的搜索：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_validation(start_date=&#39;2016-01-04&#39;, 
end_date=&#39;2017-01-03&#39;, changepoint_priors=[0.15, 0.2, 0.25,0.4, 0.5, 0.6]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-942a5b07e6ee04a7126cf1f88a5ae101_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;693&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;改进后的训练和测试曲线&lt;/p&gt;&lt;p&gt;当先验值为0.5时，测试集的错误率将最小化。因此我们将重新设置Stocker对象的变点先验值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.changepoint_prior_scale = 0.5 &lt;/code&gt;&lt;p&gt;我们可以调整模型的其他参数，比如我们期望看到的模式，或者模型使用的训练数据。找到最佳组合只需要重复上述过程，并使用一些不同的值。请随意尝试任意的参数！&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌评估改进的模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在我们的模型已经优化好了，我们可以再次评估它：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction() 

Prediction Range: 2017-01-18 to 2018-01-18.

Predicted price on 2018-01-17 = $1160.43.
Actual price on    2018-01-17 = $1295.00.

Average Absolute Error on Training Data = $10.21.
Average Absolute Error on Testing  Data = $99.99.

When the model predicted an increase, the price increased 56.90% of the time.
When the model predicted a  decrease, the price decreased  44.00% of the time.

The actual value was within the 80% confidence interval 95.20% of the time.
&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-50863ca4da67ec63292b26a51724ba4d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;710&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;现在看起来好多了！ 这显示了模型优化的重要性。使用默认值可以提供第一次合理猜测，但是我们需要确定，我们正在使用正确的模型“设置”，就像我们试图通过调整平衡和淡入淡出来优化立体声的声音那样（很抱歉引用了一个过时的例子）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌玩转股票市场&lt;/b&gt;&lt;/p&gt;&lt;p&gt;股票预测是一个有趣的实践，但真正的乐趣在于观察这些预测结果在实际市场中会发挥多好的作用。使用evaluate_prediction函数，我们可以在评估期间使用我们的模型“玩一玩”股票市场。我们将使用模型预测给出的策略，与我们在整个期间简单地购买和持有股票的策略进行一个对比。&lt;/p&gt;&lt;p&gt;我们的策略规则很简单，如下：&lt;/p&gt;&lt;p&gt;1、当模型预测股价会上涨的那一天，我们开始买入，并在一天结束时卖出。当模型预测股价下跌时，我们就不买入任何股票；&lt;/p&gt;&lt;p&gt;2、如果我们购买股票的价格在当天上涨，那么我们就把股票上涨的幅度乘以我们购买的股票的数量；&lt;/p&gt;&lt;p&gt;3、如果我们购买的股票价格下跌，我们就把下跌的幅度乘以股票的数量，计作我们的损失。&lt;/p&gt;&lt;p&gt;在整个评估期间，也就是2017年，我们每天以这样的方式进行股票操作。将股票的数量添加进模型回馈里面，Stocker就会以数字和图表显示的方式告诉我们这个策略是如何进行的： &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.evaluate_prediction(nshares=1000)&lt;/code&gt;&lt;p&gt;You played the stock market in AMZN from 2017-01-18 to 2018-01-18 with 1000 shares.&lt;br&gt;&lt;br&gt;When the model predicted an increase, the price increased 57.99% of the time.&lt;br&gt;When the model predicted a  decrease, the price decreased  46.25% of the time.&lt;br&gt;&lt;br&gt;The total profit using the Prophet model = $299580.00.&lt;br&gt;The Buy and Hold strategy profit =         $487520.00.&lt;br&gt;&lt;br&gt;Thanks for playing the stock market!&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1caad3bee2cb73725b36396098199e8c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;746&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;上图告诉了我们一个非常宝贵的策略：买入并持有！虽然我们可以在策略上再作出相当大的调整，但更好的选择是长期投资。&lt;/p&gt;&lt;p&gt;我们可以尝试其他的测试时间段，看看有没有什么时候我们的模型给出的策略能胜过买入和持有的方法。我们的策略是比较保守的，因为当我们预测市场下跌的时候我们不进行操作，所以当股票下跌的时候，我们期待有比持有策略更好的方法。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eea7fc5e42cd9d1c19cecbba9e8fbf71_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;742&quot; data-rawheight=&quot;467&quot;&gt;&lt;p&gt;一直用虚拟货币实验&lt;/p&gt;&lt;p&gt;我就知道我们的模型可以做到这一点！不过，我们的模型只有在已经有了当天的数据时才能战胜市场，也就是说还只是事后诸葛亮。&lt;/p&gt;&lt;p&gt;&lt;b&gt;▌对股票未来价格的预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;现在我们有了一个像样的模型，然后就可以使用predict_future（）函数来对股票未来价格的进行预测。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;amazon.predict_future(days=10)

amazon.predict_future(days=100) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9c540994ff97f59ac8f39f28158f5c1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;768&quot; data-rawheight=&quot;603&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17dc7025767d3e01fb16eac14f813c22_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;768&quot; data-rawheight=&quot;603&quot;&gt;&lt;p&gt;预测接下来10天和100天的股票价格趋势&lt;/p&gt;&lt;p&gt;这个模型和大多数“专业人士”一样，总体上看好Amazon这支股票。另外，我们按照预期做出的估计，不确定性会进一步增加。实际上，如果我们使用这个模型策略进行交易，那我们每天都可以训练一个新的模型，并且提前预测最多一天的价格。&lt;/p&gt;&lt;p&gt;虽然我们可能没有从Stocker工具中获得丰厚的收益，但是重点在于开发过程而不是最终结果！ 在我们尝试之前，我们实际上不知道自己是否能解决这样一个问题，就算最终失败，也好过从不尝试！任何有兴趣检查代码或使用Stocker工具的人，都可以在GitHub上找到代码。（https://github.com/WillKoehrsen/Data-Analysis/tree/master/stocker）&lt;/p&gt;&lt;blockquote&gt;作者 | William Koehrsen&lt;br&gt;原文 | &lt;a href=&quot;https://towardsdatascience.com/stock-prediction-in-python-b66555171a2&quot;&gt;https://towardsdatascience.com/stock-prediction-in-python-b66555171a2&lt;/a&gt;&lt;/blockquote&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;基于Python预测股价的那些人那些坑，请认真看完！【系列52】&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-25-33297567</guid>
<pubDate>Thu, 25 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-22-33192877.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33192877&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ef12834aa1b065c4cd9f9cd305044a1d_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;公众号今天为大家带来机器学习应用区块链系列的第二篇文章。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287287&amp;amp;idx=1&amp;amp;sn=a4efd248d74afa7360f2e32751066b5d&amp;amp;chksm=802e3162b759b8748907f4d889d648d89acd54837041a8f94cde00740b5a867012ae48fa1f5d#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】&lt;/a&gt;&lt;p&gt;这篇文章的作者是公众号之前推过一位原作者，具体文章&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284793&amp;amp;idx=1&amp;amp;sn=76c954a5a8006c815565d8669411f983&amp;amp;chksm=802e2bacb759a2ba4dd2ad122fe7cd99ab85ed29900b212189ab0af36749123c9e39b422363b&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【量化精品】通过LSTM神经网络进行时序预测针对股票市场（附Python源码）&lt;/a&gt;&lt;p&gt;虽然有一段时了，但是，我们觉得这篇文章的结论很有用，希望大家可以认真阅读。&lt;/p&gt;&lt;p&gt;&lt;b&gt;结论很精彩，见文末。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;也侧面告诉我们使用一些机器学习算法做股价预测，效果并没有那么好！大家不要觉得什么高大上就一定好，有时候真实的交易其实并没有那么复杂！&lt;/p&gt;&lt;p&gt;至于怎么预测的过程大家可以下载代码自行去研究。&lt;/p&gt;&lt;p&gt;写这篇文章，主要是为了预测比特币的价格和张量，使用一个不只是看价格而且还看BTC交易量和货币（在这种情况下为美元）的多维LSTM神经网络，并创建一个多变量序列机器学习模型。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;时间数据集&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Kaggle上有一个数据集，包含了7种要素的比特币历史数据。&lt;/p&gt;&lt;p&gt;链接：&lt;b&gt;https://www.kaggle.com/mczielinski/bitcoin-historical-data&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-88501ee596eab8df850b510c9e8fa1cb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;971&quot; data-rawheight=&quot;692&quot;&gt;&lt;p&gt;具体处理的过程大家自行查看内容，只把clean_data（）函数的核心代码贴出来： &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;num_rows = len(data)
x_data = []
y_data = []
i = 0
while((i+x_window_size+y_window_size) &amp;lt;= num_rows):
 x_window_data = data[i:(i+x_window_size)]
 y_window_data = data[(i+x_window_size):(i+x_window_size+y_window_size)]

 #Remove any windows that contain NaN
 if(x_window_data.isnull().values.any() or y_window_data.isnull().values.any()):
   i += 1
   continue
 
 if(normalise):
   abs_base, x_window_data = self.zero_base_standardise(x_window_data)
   _, y_window_data = self.zero_base_standardise(y_window_data, abs_base=abs_base)

 #Average of the desired predicter y column
 y_average = np.average(y_window_data.values[:, y_col])
 x_data.append(x_window_data.values)
 y_data.append(y_average)
 i += 1&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;预测结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们将尝试做两种类型的预测：&lt;/p&gt;&lt;p&gt;第一种：逐点预测，即预测t+1点，然后移动真实数据的窗口并继续预测下一个点。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model = lstm.load_network(&#39;data/model_saved.h5&#39;)
predictions = model.predict_generator(
   generator_strip_xy(data_gen_test, true_values),
   steps=steps_test
)

#Save our predictions
with h5py.File(configs[&#39;model&#39;][&#39;filename_predictions&#39;], &#39;w&#39;) as hf:
   dset_p = hf.create_dataset(&#39;predictions&#39;, data=predictions)
   dset_y = hf.create_dataset(&#39;true_values&#39;, data=true_values)
   
plot_results(predictions[:800], true_values[:800]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-16e749c9969f4a1c4847a9d2016b1490_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1177&quot; data-rawheight=&quot;762&quot;&gt;&lt;p&gt;第二种：是t+n的多步超前预测，我们在移动窗口填充真实数据窗口预置的预测，并绘制N个步骤。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;#Reload the data-generator
data_gen_test = dl.generate_clean_data(
   configs[&#39;data&#39;][&#39;filename_clean&#39;],
   batch_size=800,
   start_index=ntrain
)
data_x, true_values = next(data_gen_test)
window_size = 50 #numer of steps to predict into the future

#We are going to cheat a bit here and just take the next 400 steps from the testing generator and predict that data in its whole
predictions_multiple = predict_sequences_multiple(
   model,
   data_x,
   data_x[0].shape[0],
   window_size
)

plot_results_multiple(predictions_multiple, true_values, window_size) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c0b7bffff414db7f177baf7ee9a34632_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1177&quot; data-rawheight=&quot;762&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;结论（我们认为很有用的结论）&lt;/b&gt; &lt;/p&gt;&lt;p&gt;不以人工智能的角度，而是从投资的角度来解释一些事实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测回报是一项相当没有意义的行为&lt;/b&gt;。我的意思是说，预测回报是预测的圣杯，而一些顶级对冲基金视图通过在现实中找到新的alpha指标来做到这一点，这是一件非常困难的事情，因为复杂的外部因素会影响到价格的走动。实际上，它可以看作是试图预测随机的下一步。&lt;/p&gt;&lt;p&gt;但是，我们做的也并不是完全没有意义。有限的时间序列数据，即使有多个维度，也很难预测回报，我们可以看到，特别是从第二个图表看到，是有一个预测波动的方法。而不仅仅是波动，而且我们也可以通过扩张它来预测市场环境，使我们能够了解我们目前所在的市场环境。&lt;/p&gt;&lt;p&gt;我们可以看到，通过了解我们当前的市场环境，预测未来的市场环境是在任何时候将正确的策略分配到市场的关键。虽然这更多是传统市场的一般投资方式，但同样适用于比特币市场。 &lt;/p&gt;&lt;p&gt;所以，&lt;b&gt;预测比特币的长期价格目前相当的困难&lt;/b&gt;，没有人可以只是通过时间序列数据技术做到，因为有很多因素加入使价格发生了变动。在这样的数据集上使用LSTM神经网络的另一个问题是我们将整个时间序列数据集作为一个固定的时间序列。&lt;b&gt;也就是说，时间序列的属性在整个时间内都是不变的。然而这不可能，因为影响价格变化的因素也会随时间而变化，所以假设网络发现的属性或模式在现在仍然使用是一种不合理的做法。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当然，可以通过一些方法来克服这个非平稳性问题，&lt;b&gt;目前前沿的研究方向是利用贝叶斯方法和LSTM一起克服时间序列非平稳性的问题。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;代码链接: https://pan.baidu.com/s/1qZJaoxA &lt;/p&gt;&lt;p&gt;密码: kghx&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287287&amp;amp;idx=1&amp;amp;sn=a4efd248d74afa7360f2e32751066b5d&amp;amp;chksm=802e3162b759b8748907f4d889d648d89acd54837041a8f94cde00740b5a867012ae48fa1f5d#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-22-33192877</guid>
<pubDate>Mon, 22 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>2018年「金融平均脸」出炉，是不是要长这样才能做期货？</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153911.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153911&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5fd69dc772d93269f198ba59246108a9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;对比各个金融领域公司的“平均样貌”，你眼中的“颜值代表”是哪家？&lt;/p&gt;&lt;p&gt;期货脸长啥样？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;金融行业&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最受热捧的脸&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-35476841a978e223078a0dcaa83d559f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;高盛&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9971fe42cfe51be34b8be0efeb234ac1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;65&quot; data-rawheight=&quot;45&quot;&gt;&lt;p&gt;世界上最可怕的事&lt;/p&gt;&lt;p&gt;是比你美的人还比你有脑子&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5c898afd266deaea0cbe92ac4f5fd6c8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;摩根大通&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9973210c31280929e25313a96e49612_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;178&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;听说很多人叫我爸爸&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9134a4e26e1083d862d646850bc027ae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中金&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6c837db1ea6b924e0885de5e9a7f96b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;149&quot; data-rawheight=&quot;102&quot;&gt;&lt;p&gt;我们有很多个小目标 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1250204b41f23ce7971882c804e6709_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;中信&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55a2452a23ff0e371dca78ef63eb1112_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;175&quot; data-rawheight=&quot;50&quot;&gt;&lt;p&gt;几十个亿，真的好难花&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2ae16cc8f64bd1023f438395c9ea8502_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四大会计师事务所&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;风彩与黑眼圈并存&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aecb9c29dd345b1b1e64d5ac6eae597b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;安永&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9ecb3dce198252ac82d8382a16704f67_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;75&quot;&gt;&lt;p&gt;压力大又怎样，软妹多就好&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-990e351a3e9d217e174da157f08e479b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;普华永道&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-56f748ddacb95f4f63f1d870eb249e7f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;119&quot; data-rawheight=&quot;75&quot;&gt;&lt;/b&gt;&lt;p&gt;来自行业老大的微笑&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-86640f3058a89ac9a5ea17a80c0b78a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;毕马威&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有一种脸叫数据脸&lt;/p&gt;&lt;p&gt;&lt;b&gt;德勤&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5433aaa9de1e9c1230cb8244d28c7b68_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;121&quot; data-rawheight=&quot;91&quot;&gt;&lt;p&gt;金丝镜框后，是美图也盖不住的细纹&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3a93d9be2dc5ba5dc97b476ed913683b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;咨询行业&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;br&gt;做PPT，我们是最认真的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-567c4905d9fd12c102af578b3388673a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;麦肯锡&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1a52a74c8dfe09d2f8ce93a51a3f1f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;300&quot; data-rawheight=&quot;154&quot;&gt;&lt;p&gt;盛名在外的，除了专业还有颜值&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f5ac1f8e0bcd9c7b009bb437df32c6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;波士顿&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a7fc2b65f764767d6b75dba88352c70e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;104&quot;&gt;&lt;p&gt;没有用数字解释不了的事儿 &lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-312aabaeb2efa2cd8d306a8427bef24d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;贝恩&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-077b7999edd5e4dec5268847c209b9ba_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;225&quot; data-rawheight=&quot;169&quot;&gt;&lt;p&gt;好不好看不重要&lt;/p&gt;&lt;p&gt;会汇（bi）报（bi）最重要&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e96e9da41ebca94ea46c0588752af10_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;419&quot;&gt;&lt;p&gt;&lt;b&gt;罗兰贝格&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fe72ebb22890c6adae89f32732a73999_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;224&quot; data-rawheight=&quot;120&quot;&gt;&lt;p&gt;150一天的餐补，果然不是盖的。&lt;/p&gt;&lt;p&gt;有图为证！&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-75304830a152a09ab90fc9b4dc35258e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;银行&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;行业平均脸&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;算起账来，我们是最无辜的&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;招商银行&lt;/b&gt;&lt;/p&gt;&lt;p&gt;招行的人一定要苗条漂亮，&lt;/p&gt;&lt;p&gt;如果太胖了请去对面的工行。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1a3b278e5d14cef8ce6d2c4034a79fd2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;工商银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d9c3047e1c0e889495b0c9f22c9ac08c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;466&quot; data-rawheight=&quot;113&quot;&gt;&lt;p&gt;我们宇宙行家大业大，实力雄厚。&lt;/p&gt;&lt;p&gt;楼上就是矫情！&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a369cc08cb10fe4523fe12ce233b155_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;建设银行&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c5f44fda1e311917046628090a8c44f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;454&quot; data-rawheight=&quot;106&quot;&gt;&lt;/b&gt;&lt;p&gt;我就静静看着楼上不说话，&lt;/p&gt;&lt;p&gt;反正我也没空说话。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-11f3df96d86dcc363093bb498b3b760b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;p&gt;&lt;b&gt;中国银行&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-449e5df1bdf3e70d36adc4102f81ad39_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;232&quot; data-rawheight=&quot;90&quot;&gt;&lt;p&gt;谢谢，这是四大行里最international的脸&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-16f1c79b4419f6a9bd1015713008b04e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;298&quot;&gt;&lt;h2&gt;&lt;b&gt;期货行业&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b7ae886a05284c6f583d9077432339d3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;657&quot; data-rawheight=&quot;430&quot;&gt;&lt;p&gt;咳咳...金融民工中的战斗机...灰头土脸...&lt;/p&gt;&lt;p&gt;本文纯属搞笑，如有雷同，纯属巧合&lt;/p&gt;&lt;p&gt;素材来源：UniCareer、瞭望智库&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-17c3d959b2db4c1b3a90a54e5af7f4ee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153911</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>模式识别在金融时间序列中的应用【系列五十】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33153489.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33153489&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-42b57b00a759ca6bdc258a8f967aa0b2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天编辑部诶大家带来Kathryn Dover的一篇文章。主要讲模式识别在股票数据中的应用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;来源：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;p&gt;Finding patterns in high dimensional data can be difficult because it cannot be easily visualized. Many different machine learning methods are able to fit this high dimensional data in order to predict and classify future data but there is typically a large expense on having the machine learn the fit for a certain part of the dataset. This thesis proposes a geometric way of defining different patterns in data that is invariant under size and rotation so it is not so dependent on the input data. Using a Gaussian Process, the pattern is found within stock market data and predictions are made from it.&lt;/p&gt;&lt;p&gt;主要是用一种不依赖于输入数据的方法。 使用高斯过程，在股票市场数据中发现该模式，并从中进行预测。&lt;/p&gt;&lt;p&gt;前面几部分都是背景介绍，大家可以自己去查阅，我们主要把后面几部分解读一下。&lt;/p&gt;&lt;p&gt;作者定义了三种大家常见的指标形态：W底、M顶、头肩形。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Standard Double-Bottom Pattern (A Standard W)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c020094c0c4880baf508c0d835417f7a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;249&quot; data-rawheight=&quot;186&quot;&gt;&lt;p&gt;&lt;b&gt;2. Standard Double-Top Pattern (A Standard M)&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-82f9bed60e9e9bf18b159f75b2186f1f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;277&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;&lt;b&gt;3. Standard Head and Shoulder Pattern&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d40dea7c724458261ce2e642a8bf61e3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;270&quot; data-rawheight=&quot;207&quot;&gt;&lt;p&gt;When we actually apply our definition of shapes to the data, we will find that there is no perfect match to our standard definition. Therefore, we will want to define a “fuzzy shape” that will be considered an approximate form of the standard shape. These fuzzy shapes will not only allow us to approximate how far away a shape is from the standard shape but also how far fuzzy shapes are from each other. The way that we define our fuzzy shapes is discussed below. &lt;/p&gt;&lt;p&gt;用数据去定义这些指标形态时，我们会发现与我们定义的形态并不是完全匹配。 作者做了一个“模糊形状”的处理。将被视为标准形状的近似形式。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;1. The Fuzzy W&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-41f9aac637fb0e8b131d3deed4a8f237_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;392&quot; data-rawheight=&quot;264&quot;&gt;&lt;p&gt;&lt;b&gt;2. The Fuzzy M&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d7a46b860d17b137f5ae32b6e318670_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;122&quot; data-rawheight=&quot;29&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-741fbac4dff294df4eb9fb2718b060b1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;221&quot;&gt;&lt;p&gt;&lt;b&gt;3. The Fuzzy Head and Shoulder&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-29da43f4bd4cced4f3a8eb0c80aad1a2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;436&quot; data-rawheight=&quot;270&quot;&gt;&lt;p&gt;Now that we have defined both our standard and fuzzy shapes, we want to be able to define some actions we can take on these shapes. We discuss the change of basis and flipping a shape and how they can be used to compare different shapes to each other. We also discuss how the slopes and lengths of the vectors can be used to categorize the shape that can be used for prediction.&lt;/p&gt;&lt;p&gt;接下来作者对这三种形态的一些变化进行了讨论。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Change of Basis&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Flipping a Shape&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f0bef6b065aa075213159f8630772bf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;265&quot; data-rawheight=&quot;203&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;b&gt;3. Symmetric Representation&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-134d9ad67dd4bab3f595d961c6b8a9d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;255&quot; data-rawheight=&quot;175&quot;&gt;&lt;p&gt;Once the shapes have been found in the data, we can categorize these shapes using their slopes and lengths. By categorizing certain shapes by their slopes and lengths, we can group similar shapes together and run traditional machine learning techniques on these specific groups. This is to determine indicators of these types of shapes, which can ultimately be used for predictive purposes. We discuss how shapes can be categorized using slopes and lengths and how we can use those categories to do rough predictions of stock market data.&lt;/p&gt;&lt;p&gt;开始对图形的斜率和长度进行定量的分析结合机器学习方法，然后开始预测。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Saving the Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cd6ee3a8a3a0a8c9734743d863aaa136_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;37&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-91d72b91259f958235e5e1119fb0407b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;548&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-efcb1788e41e6f795bc849b750afd3d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;370&quot; data-rawheight=&quot;199&quot;&gt;&lt;p&gt;两种预测方法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Predictions with Slopes and Lengths&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Prediction with Direction&lt;/b&gt;&lt;/p&gt;&lt;p&gt;创建一个算法来寻找模式：&lt;/p&gt;&lt;p&gt;Now that we have defined what our shapes are and how to approximate shapes, we will be discussing the algorithm that finds these shapes and how it handles the data. I used a Gaussian Process to fit the data with a function that the local minimums and maximums could be extracted from. These local extrema are used to define the vectors of the shapes and subsequently the algorithm checks these vectors using the definitions from the previous sections.&lt;/p&gt;&lt;p&gt;关键词：&lt;b&gt;Gaussian Process&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最终过程如下：&lt;/p&gt;&lt;p&gt;1. Given a certain variance, a Gaussian Process is used to fit the data.&lt;/p&gt;&lt;p&gt;2. From this fit of the data, the algorithm identifies all the local extrema in the data.&lt;/p&gt;&lt;p&gt;3. The algorithm goes through the local extrema in the data sequentially and identifies sequences that could possibly be a predefined shape.For example, a W has a following sequence of minium and maximum:max/min/max/min/max.&lt;/p&gt;&lt;p&gt;4. If a sequence is valid for a certain shape, then the algorithm uses the definitions of the shapes defined earlier in order to figure out if they qualify.&lt;/p&gt;&lt;p&gt;5. If a shape is found, the lengths and slopes of the vectors in the shape are stored (as well as the following segment) for predictive purposes.&lt;/p&gt;&lt;p&gt;6. The categorizing constants k` i and ks i are calculated and stored.&lt;/p&gt;&lt;p&gt;7. Once the algorithm has gone through all the extrema, it uses the stored slopes, lengths, and categorizing constants and uses them to calculate the rough prediction from the weighted average.&lt;/p&gt;&lt;p&gt;在下图中，我们可以看到算法的运行步骤。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-42b53e58ffb2513c0e272d052520200f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;508&quot; data-rawheight=&quot;219&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7424028c97ee1280007620a30e1a50af_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;507&quot; data-rawheight=&quot;223&quot;&gt;&lt;p&gt;实际的应用：&lt;br&gt;In this section, I describe the results of the algorithm I developed in the last chapter when it was run on stock price history for different companies. The algorithm was run for variances 0.001 and 4 over the entire stock price history for the companies Apple, Disney, Microsoft, Walmart and the NASDAQ index. For testing the prediction part of the algorithm, the entire stock price history for the Dow Jones index was used. The reason for choosing this dataset was twofold. First, there was between 20-30 years worth of data for each company, which would mean I could potentially find a variety of shapes in different sizes. Second, I chose these specific companies by looking through the stock history of companies and choose those that had many turbulent years that offer enough fluctuation that can create the shapes I am looking for. All stock data was found at yahoo.finance.com. The below subsections discuss how I fit my data, found the shapes, and calculated predictions for different shapes. &lt;/p&gt;&lt;p&gt;用了两种方法&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Finding Different Sized Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-00c7953c6f670571bd80a643ecb1e04a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;468&quot;&gt;&lt;p&gt;The original data (upper le), the fit of the data when the variance is 0.001 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Finding Multiple Shapes&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c4eb07203dd4c838e90704938b5ac68a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;509&quot; data-rawheight=&quot;471&quot;&gt;&lt;p&gt;is 4 (upper right), the algorithm identifying the local extrema (lower le), the W found by the algorithm (lower right)&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ws were found&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-930ee1fcbe7bb54af07ef173075902a4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;505&quot; data-rawheight=&quot;235&quot;&gt;&lt;p&gt;A fit with variance = 0.001 where multiple Ms were found&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测&lt;/b&gt;&lt;/p&gt;&lt;p&gt;I used the data from Apple, Disney, Microsoft, Walmart and the NASDAQ as my learning set for both of my prediction methods and I used the Dow Jones index as my test data. The results from each of the methods is given below.&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bb403d155288803c73adfd2aca4f62b5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;620&quot; data-rawheight=&quot;268&quot;&gt;&lt;p&gt;&lt;b&gt;算法存在的问题：&lt;/b&gt;&lt;br&gt;While the algorithm was successful in finding the shapes and doing a rough prediction, there were still some issues with it. For example, finding a good tolerance for finding a shape was not exact. I had to run the algorithm many times to determine at what point the algorithm would find identify a pattern was not the correct shape. This usually happened because the tolerance was too high and the patterns found did not look like the shape. Likewise, when the tolerance was too low, it would identify no shape. Finding an appropriate tolerance (somewhere between 0.7 and 1) took a while and the tolerance for each shape was different as the Ws had a lower tolerance while the head and shoulder pattern needed a higher tolerance.&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Finally, the predictive step of the algorithm has a long way to go. Using the slopes and lengths method did not work well which might be explained by having no good way of measuring the difference between slopes. Addi tionally, the weighted average method might not be enough to get a good understanding unless I had a lot more data. I had about 55 shapes for each type of shape when I ran the algorithm on the data. If I want to get a better average, I would need to run my code on a lot more data in order for the average to not be swayed as heavily by outliers. The directional vector method worked much better than the slopes and lengths method, but there was still a error of about 20-28 degrees. We could potentially decrease this error by simply getting more data. This prediction also indicated that there actually might be a correlation between the shapes and the directional vector, so perhaps we could use more traditional machine learning methods on these directional vector values to see if we can learn anything extra from it.For example, is there some sort of correlation between the k values we found in the slope/lengths predictions and the directional vector we calculated? A question like this might be answered by using a neural network or a support vector machine.&lt;/p&gt;&lt;p&gt;同时作者还提出了一些改进模型的方法。我们认为此部分最为有探究性：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Improving Shape Recognition&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Extending into Multiple Dimensions&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;论文链接：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://scholarship.claremont.edu/hmc_theses/105/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;&quot;Pattern Recognition in Stock Data&quot; by Kathryn Dover&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287106&amp;amp;idx=1&amp;amp;sn=93e506237376f0540ff5ede81f1d2343&amp;amp;chksm=802e30d7b759b9c1d9716776fade282acebae0e55a1f6fcd3897f183736c24b14fd23f08ce27#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;模式识别在金融时间序列中的应用【系列五十】&lt;/a&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33153489</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【代码+论文】通过ML、Time Series模型学习股价行为</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33152954.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33152954&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3ff2fee46d48ff47aed98c1cc9ce4c20_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天编辑部给大家带来的是来自Jeremy Jordan的论文，主要分析论文的建模步骤和方法，具体内容大家可以自行查看。&lt;/p&gt;&lt;p&gt;&lt;b&gt;（代码在文末下载。由于代码过长，有些代码部分略去了一些）&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;来源&lt;/h2&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;【代码+论文】通过ML、Time Series模型学习股价行为（第九期免费赠书活动来啦！）&lt;/a&gt;&lt;code lang=&quot;python&quot;&gt;# Standard imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&#39;seaborn-notebook&#39;)
import seaborn as sns
sns.set()
import matplotlib.cm as cm

# Enable logging
import logging
import sys
logging.basicConfig(level=logging.DEBUG, 
format=&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) &lt;/code&gt;&lt;p&gt;&lt;b&gt;步骤1：下载数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;获取S＆P 500公司名单&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从维基百科下载的S＆P 500公司信息。我们只使用股票代码列表，但GICS_sector和GICS_sub_industry等可能有用。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;column_names = [&#39;ticker&#39;, &#39;security&#39;, &#39;filings&#39;, 
&#39;GICS_sector&#39;, &#39;GICS_sub_industry&#39;, &#39;HQ_address&#39;, &#39;date_added&#39;, &#39;CIK&#39;]

sp500companies = pd.read_csv(&#39;data/S&amp;amp;P500.csv&#39;,
header = 0, names = column_names).drop([&#39;filings&#39;], axis=1)

sp500companies = sp500companies.set_index([&#39;ticker&#39;])
sp500companies.head(10) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-00eeda76a8541e935271cc8dacdf72af_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;486&quot;&gt;&lt;p&gt;&lt;b&gt;下载价格数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import pandas_datareader as pdr

start = pd.to_datetime(&#39;2009-01-01&#39;)
end = pd.to_datetime(&#39;2017-01-01&#39;)
source = &#39;google&#39;

for company in sp500companies.index:
   try:
       price_data = pdr.DataReader(company, source, start, end)
       price_data[&#39;Close&#39;].to_csv(&#39;data/company_prices/%s_adj_close.csv&#39; % company)
   except:
       logging.error(&quot;Oops! %s occured for %s. \nMoving on to next entry.&quot; % (sys.exc_info()[0], company))Transcripts are scraped from Seeking Alpha using the Python library Scrapy.&lt;/code&gt;&lt;p&gt;To fetch a company transcript, complete the following steps.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd data/
scrapy crawl transcripts -a symbol=$SYM &lt;/code&gt;&lt;p&gt;This will download all of the posted earnings call transcripts for company SYM and store it as a JSON lines file in data/company_transcripts/SYM.json.&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤2：加载数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Read in a collection of company transcripts
def load_company_transcripts(company):
   &#39;&#39;&#39;
   Reads a company&#39;s transcripts into a Pandas DataFrame. 
   
   Args:
       company: The ticker symbol for the desired company. 
   &#39;&#39;&#39;
   logging.debug(&#39;Reading company transcripts for {}&#39;.format(company))
   
   # Read file
   text_data = pd.read_json(&#39;data/company_transcripts/{}.json&#39;.format(company), lines=True)
    
   # Drop events that don&#39;t have an associated date
   text_data = text_data.dropna(axis = 0, subset= [&#39;date&#39;])
   
   # Reindex according to the date
   text_data = text_data.set_index(&#39;date&#39;).sort_index()
   
   # Check for possible duplicate entries from scraping
   if sum(text_data.duplicated([&#39;title&#39;])) &amp;gt; 0:
       logging.warning(&#39;{} duplicates removed from file&#39;.format(sum(text_data.duplicated([&#39;title&#39;]))))
        text_data = text_data.drop_duplicates([&#39;title&#39;])
              
   # Concatenate body into single body of text
   text_data[&#39;body&#39;] = text_data[&#39;body&#39;].apply(lambda x: &#39; &#39;.join(x))
   
   # Add a column to each transcript for word count, plot histogram
   text_data[&#39;count&#39;] = text_data[&#39;body&#39;].apply(lambda x: len(x.split()))
   
   # Check for empty transcripts
   if len(text_data[text_data[&#39;count&#39;] == 0]) &amp;gt; 0:
       logging.warning(&#39;{} empty transcripts removed from file&#39;.format(len(text_data[text_data[&#39;count&#39;] == 0])))
    text_data = text_data[text_data[&#39;count&#39;] != 0]
   
   return text_data&lt;/code&gt;&lt;p&gt;&lt;b&gt;加载价格数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def load_company_price_history(companies, normalize = False, fillna = True, dropna=True):
    &#39;&#39;&#39; 
   Builds a DataFrame where each column contains one company&#39;s adjusted closing price history.
    
   Args:
       companies: A list of company ticker symbols to load.
       normalize: Boolean flag to calculate the log-returns from the raw price data. 
       fillna: Boolean flag to fill null values. Limited fill up to 5 days forward and 1 day backward,
        for companies with long periods of null values, this prevents from creating a stagnant time series.
        Instead, those companies should be dropped using `dropna=true`.
       dropna: Boolean flag to drop companies that don&#39;t have a full price history. 
        
   &#39;&#39;&#39;
   prices = []
   for company in companies:
       logging.debug(&#39;Reading company prices for {}&#39;.format(company))
       price_history = pd.read_csv(&#39;data/company_prices/{}_adj_close.csv&#39;.format(company), 
                                    names=[company], index_col=0)
       prices.append(price_history)
       
   df = pd.concat(prices, axis=1)
   df.index = pd.to_datetime(df.index)
   df = df.asfreq(&#39;B&#39;, method=&#39;ffill&#39;)
  
   if normalize:
       df = np.log(df).diff() # Calculate the log-return, first value becomes null
       df.iloc[0] = df.iloc[1] # Forward fill the null value
       
   if fillna:
       df = df.fillna(method = &#39;ffill&#39;, limit=5) # First pass fill NAs as previous day price
        df = df.fillna(method = &#39;bfill&#39;, limit=1) # For NAs with no prev value (ie. first day), fill NA as next day price
    
   if dropna:
       # Validate quality of data (null values, etc)
       # Drop any companies that don&#39;t have a full 8-year history
       df = df.dropna(axis=1, how=&#39;any&#39;)
       assert df.isnull().values.any() == 0
       logging.debug(&#39;Null values found after cleaning: {}&#39;.format(df.isnull().values.any()))
        
   
   return df&lt;/code&gt;&lt;p&gt;&lt;b&gt;读取文件目录并加载可用数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Go to data folder and get list of all companies that have data
import glob
import re

# All files for price history
price_files = glob.glob(&#39;data/company_prices/*_adj_close.csv&#39;)
company_prices = [re.search(r&#39;(?&amp;lt;=data\/company_prices\/)(.*)(?=_adj_close.csv)&#39;, f).group(0) 
                  for f in price_files]
logging.info(&#39;{} company price histories available.&#39;.format(len(company_prices)))

# All files for transcripts
transcript_files = glob.glob(&#39;data/company_transcripts/*.json&#39;)
company_transcripts = [re.search(r&#39;(?&amp;lt;=data\/company_transcripts\/)(.*)(?=.json)&#39;, f).group(0) 
                       for f in transcript_files]
logging.info(&#39;{} company transcripts available.&#39;.format(len(company_transcripts)))

# Intersection of two datasets
company_both = list(set(company_prices) &amp;amp; set(company_transcripts))
logging.info(&#39;{} companies have both transcripts and price history available.&#39;.format(len(company_both)))
2017-10-04 09:52:44,215 - INFO - 502 company price histories available.
2017-10-04 09:52:44,219 - INFO - 173 company transcripts available.
2017-10-04 09:52:44,220 - INFO - 170 companies have both transcripts and price history available.&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load all pricing data into memory
company_price_df = load_company_price_history(company_both, normalize=True)
2017-10-04 09:52:45,680 - DEBUG - Reading company prices for AAL
2017-10-04 09:52:45,683 - DEBUG - Reading company prices for XRAY
2017-10-04 09:52:45,686 - DEBUG - Reading company prices for CVX
2017-10-04 09:52:45,689 - DEBUG - Reading company prices for ACN
2017-10-04 09:52:45,692 - DEBUG - Reading company prices for COF
2017-10-04 09:52:45,696 - DEBUG - Reading company prices for DISCK
2017-10-04 09:52:45,699 - DEBUG - Reading company prices for AXP
2017-10-04 09:52:45,702 - DEBUG - Reading company prices for CVS
2017-10-04 09:52:45,705 - DEBUG - Reading company prices for AMGN
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load all transcript data into memory
company_transcripts_dict = {}
failures= []

for company in company_transcripts:
   try:
       company_transcripts_dict[company] = load_company_transcripts(company)
   except:
       logging.error(&quot;Oops! {} occured for {}. \nMoving on to next entry.&quot;.format(sys.exc_info()[0], company))
        failures.append(company)
2017-10-04 09:52:51,910 - DEBUG - Reading company transcripts for A
2017-10-04 09:52:51,979 - DEBUG - Reading company transcripts for AAL
2017-10-04 09:52:52,080 - DEBUG - Reading company transcripts for AAP
2017-10-04 09:52:52,150 - DEBUG - Reading company transcripts for AAPL
2017-10-04 09:52:52,213 - DEBUG - Reading company transcripts for ABBV
2017-10-04 09:52:52,251 - DEBUG - Reading company transcripts for ABC
2017-10-04 09:52:52,314 - DEBUG - Reading company transcripts for ABT
2017-10-04 09:52:52,345 - WARNING - 1 duplicates removed from file
2017-10-04 09:52:52,422 - DEBUG - Reading company transcripts for ACN
2017-10-04 09:52:52,483 - DEBUG - Reading company transcripts for ADBE
2017-10-04 09:52:52,546 - DEBUG - Reading company transcripts for ADI
2017-10-04 09:52:52,548 - ERROR - Oops! &amp;lt;class &#39;KeyError&#39;&amp;gt; occured for ADI. 
Moving on to next entry.
2017-10-04 09:52:52,549 - DEBUG - Reading company transcripts for ADM
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;failures 
[&#39;ADI&#39;, &#39;BHF&#39;, &#39;CTHS&#39;, &#39;ED&#39;, &#39;ETN&#39;]&lt;/code&gt;&lt;p&gt;都是空的文件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;抽样检查&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Select companies to load and inspect
google_transcripts = company_transcripts_dict[&#39;GOOG&#39;]
amazon_transcripts = company_transcripts_dict[&#39;AMZN&#39;]
adobe_transcripts = company_transcripts_dict[&#39;ADBE&#39;]
apple_transcripts = company_transcripts_dict[&#39;AAPL&#39;]

transcript_samples = [google_transcripts, amazon_transcripts, adobe_transcripts, apple_transcripts]

google_prices = company_price_df[&#39;GOOG&#39;]
amazon_prices = company_price_df[&#39;AMZN&#39;]
adobe_prices = company_price_df[&#39;ADBE&#39;]
apple_prices = company_price_df[&#39;AAPL&#39;]

price_samples = load_company_price_history([&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;ADBE&#39;, &#39;AAPL&#39;], normalize=True)

google_transcripts &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-25d25c9f2426acd772f519e1bb69834e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1011&quot; data-rawheight=&quot;469&quot;&gt;&lt;p&gt;&lt;b&gt;步骤3：分析数据&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Calculate average word count across all company transcripts
companies_avg_word_count = []

for company in company_transcripts_dict:
   company_avg = company_transcripts_dict[company][&#39;count&#39;].mean()
   companies_avg_word_count.append(company_avg)
   
print(&#39;Average word count in transcripts: {}&#39;.format(np.mean(np.array(companies_avg_word_count))))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Average word count in transcripts: 9449.255561736534&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Plot histogram of word counts for company transcripts
def visualize_word_count(transcripts):
   &#39;&#39;&#39;
   Plots a histogram of a company&#39;s transcript word counts.
   
   Args:
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   
   company = transcripts[&#39;company&#39;][0]
   fig, ax = plt.subplots(figsize=(10,5))
   ax.hist(transcripts[&#39;count&#39;])
   plt.title(&quot;Word count distribution for {}&quot;.format(company))
   ax.set_xlabel(&#39;Word count&#39;)
   ax.set_ylabel(&#39;Number of occurrences&#39;)

for company in transcript_samples:
   visualize_word_count(company) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3a13f91bf87ce1dd41cf4aa7a193197d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;594&quot; data-rawheight=&quot;670&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b50533d8c3f312ecc0a14a4b4ef6387b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;669&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Visualize transcript dates
def visualize_dates(transcripts):
   &#39;&#39;&#39;
   Plots the dates of a company&#39;s earning calls.
   
   Args:
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   
   company = transcripts[&#39;company&#39;][0]
   fig, ax = plt.subplots(figsize=(22,5))
   ax.scatter(transcripts.index, np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.6, s=100)
    fig.autofmt_xdate()
   ax.set_title(&quot;Dates of earnings calls for {}&quot;.format(company))
   ax.set_yticklabels([])

for company in transcript_samples:
   visualize_dates(company) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f96f7ae28dabc0b84864c0e9b0f7d109_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;748&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a385201c4c6b4cbe725e2b0e988d6fa9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1008&quot; data-rawheight=&quot;243&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Average value of all S&amp;amp;P 500 companies
all_companies = load_company_price_history(company_both, normalize=False)
all_companies.mean(axis=1).plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Adjusted Closing Price&#39;)
plt.title(&#39;Average Daily Stock Price of all S&amp;amp;P 500 companies&#39;)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_true_prices = load_company_price_history([&#39;GOOG&#39;])
google_true_prices.plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Adjusted Closing Price&#39;)
plt.title(&#39;Google Stock Price&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0dbce695eec0d9b30e524d5d5b100b54_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;498&quot; data-rawheight=&quot;357&quot;&gt;&lt;code lang=&quot;python&quot;&gt;google_prices.plot()
plt.xlabel(&#39;Date&#39;)
plt.ylabel(&#39;Daily Log Return&#39;)
plt.title(&#39;Google Daily Log Returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3d9769eae850775e2f8cf6c2aea1fe9f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;507&quot; data-rawheight=&quot;357&quot;&gt;&lt;code lang=&quot;python&quot;&gt;price_samples.head(10) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9d00fd121f691308ae2be26022c3922d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;327&quot;&gt;&lt;code lang=&quot;python&quot;&gt;cum_returns = price_samples.cumsum()
cum_returns.plot()
plt.title(&#39;Cumulative log-returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a21031327837ddbe8d597ba505d6ae38_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;487&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tot_rel_returns = 100*(np.exp(price_samples.cumsum()) - 1)
tot_rel_returns.plot()
plt.title(&#39;Total relative returns&#39;) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bf7c34bcfca837e733fea0f34fdc623c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;490&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;google_price_sample = load_company_price_history([&#39;GOOG&#39;])[&#39;2012&#39;:&#39;2015&#39;]
google_returns_sample = load_company_price_history([&#39;GOOG&#39;], normalize=True)[&#39;2012&#39;:&#39;2015&#39;]
google_transcript_sample = load_company_transcripts(&#39;GOOG&#39;)[&#39;2012&#39;:&#39;2015&#39;]
2017-09-19 20:26:57,937 - DEBUG - Reading company prices for GOOG
2017-09-19 20:26:57,996 - INFO - Null values found after cleaning: False
2017-09-19 20:26:57,998 - DEBUG - Reading company prices for GOOG
2017-09-19 20:26:58,050 - INFO - Null values found after cleaning: False
2017-09-19 20:26:58,052 - DEBUG - Reading company transcripts for GOOG
2017-09-19 20:26:58,093 - WARNING - 1 duplicates removed from file&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Comparing price data with earnings call events&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;def plot_price_and_text(prices, transcripts):
   &#39;&#39;&#39;
   Plots the dates of a company&#39;s earning calls on top of a chart of the company&#39;s stock price.
    
   Args:
       prices: A Pandas DataFrame containing a company&#39;s price history. 
       transcripts: A Pandas DataFrame containing a company&#39;s history of earnings calls.
    &#39;&#39;&#39;
   # Plot the transcript events below the price, 10% offset from min price
   event_level = int(prices.min()*0.9) 
   fig, ax = plt.subplots(figsize=(22,8))
   ax.scatter(transcripts.index, event_level*np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.4, s=100)
    ax.plot(prices.index, prices)
   fig.autofmt_xdate()
   ax.set_title(&#39;Analyzing the connection between earnings call events and price movement&#39;)
    
plot_price_and_text(google_price_sample, google_transcript_sample) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c963935a1f0f8be6e152fa321ec13a55_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1270&quot; data-rawheight=&quot;453&quot;&gt;&lt;h2&gt;&lt;b&gt;Explore connection between text events and returns&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;def plot_returns_and_text(returns, transcripts):
   # Plot the transcript events below the price, 10% offset from min price
   event_level = int(returns.min()*0.9) 
   fig, ax = plt.subplots(figsize=(22,8))
   #ax.scatter(transcripts.index, 0.1*np.ones(len(transcripts.index)), marker = &#39;s&#39;, alpha=0.4, s=100)
    ax.plot(returns.index, returns)
   for date in transcripts.index:
       ax.axvspan(date - pd.to_timedelta(&#39;1 days&#39;), date + pd.to_timedelta(&#39;6 days&#39;), color=&#39;green&#39;, alpha=0.15)
    fig.autofmt_xdate()
   ax.set_title(&#39;Analyzing the connection between earnings call events and price movement&#39;)
    ax.set_ylabel(&#39;Daily return&#39;)

plot_returns_and_text(google_returns_sample, google_transcript_sample) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-01746fce48b6c1f4714190a8d89fc62e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;453&quot;&gt;&lt;code lang=&quot;python&quot;&gt;plot_returns_and_text(load_company_price_history([&#39;CMG&#39;], normalize=True)[&#39;2012&#39;:&#39;2015&#39;],
                      load_company_transcripts(&#39;CMG&#39;)[&#39;2012&#39;:&#39;2015&#39;]) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-28506d896c3c00dade2cdeab8df3c2d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1274&quot; data-rawheight=&quot;453&quot;&gt;&lt;p&gt;Clearly, we can see some examples of large price movements surrounding the time of quarterly earnings calls. The goal of this project is to develop an algorithm capable of learning the price movements associated with the content of an earnings call.&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤4：将文本转换为文字嵌入&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;with open(&#39;glove.6B/glove.6B.50d.txt&#39;) as words:
   w2v = {word.split()[0]: np.vectorize(lambda x: float(x))(word.split()[1:]) for word in words}

logging.info(&#39;{} words in word2vec dictionary.&#39;.format(len(w2v)))

# We&#39;ll later reduce the dimensionality from 50 to 2, let&#39;s go ahead and fit the entire corpus
# I&#39;ve opted to use PCA over t-SNE given that we can fit the transformer once and have deterministic results
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(list(w2v.values()))
w2v_reduced = dict(zip(list(w2v.keys()), reduced_embeddings.tolist()))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;2017-10-04 09:55:50,937 - INFO - 400000 words in word2vec dictionary.&lt;/code&gt;&lt;p&gt;我们创建了一个字典，其中的关键字代表了我们词汇表中的单词，值是给定单词的向量表示。可以通过查询字典来访问单词的50维向量。并将50维矢量投影到二维中，以便于观察。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;w2v[&#39;example&#39;]
array([ 0.51564  ,  0.56912  , -0.19759  ,  0.0080456,  0.41697  ,
        0.59502  , -0.053312 , -0.83222  , -0.21715  ,  0.31045  ,
        0.09352  ,  0.35323  ,  0.28151  , -0.35308  ,  0.23496  ,
        0.04429  ,  0.017109 ,  0.0063749, -0.01662  , -0.69576  ,
        0.019819 , -0.52746  , -0.14011  ,  0.21962  ,  0.13692  ,
       -1.2683   , -0.89416  , -0.1831   ,  0.23343  , -0.058254 ,
        3.2481   , -0.48794  , -0.01207  , -0.81645  ,  0.21182  ,
       -0.17837  , -0.02874  ,  0.099358 , -0.14944  ,  0.2601   ,
        0.18919  ,  0.15022  ,  0.18278  ,  0.50052  , -0.025532 ,
        0.24671  ,  0.10596  ,  0.13612  ,  0.0090427,  0.39962  ])



w2v_reduced[&#39;example&#39;]
[4.092878121172412, 1.785939893037579]&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Sample transcripts from collection
sample_text_google = google_transcripts[&#39;body&#39;][5]
sample_text_amazon = amazon_transcripts[&#39;body&#39;][5]
sample_text_adobe = adobe_transcripts[&#39;body&#39;][5]&lt;/code&gt;&lt;p&gt;Let&#39;s see what words were ignored when we translate the transcripts to word embeddings.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from keras.preprocessing.text import text_to_word_sequence

not_in_vocab = set([word for word in text_to_word_sequence(sample_text_google) if word not in w2v])
print(&#39;  --  &#39;.join(not_in_vocab))
motofone  --  segment&#39;s  --  we&#39;re  --  ml910  --  here&#39;s  --  we&#39;ve  --  doesn&#39;t  --  didn&#39;t  --  ed&#39;s  --  z6  --  asp&#39;s  --  vhub  --  p2k  --  weren&#39;t  --  you&#39;re  --  3gq  --  i&#39;ll  --  ray&#39;s  --  wasn&#39;t  --  they&#39;re  --  what&#39;s  --  i&#39;d  --  motowi4  --  world&#39;s  --  embracement  --  downish  --  broadbus  --  hereon  --  devices&#39;  --  shippable  --  w355  --  motorola&#39;s  --  w205  --  that’s  --  isn&#39;t  --  morning&#39;s  --  mw810  --  wimax&#39;s  --  that&#39;s  --  wouldn&#39;t  --  ounjian  --  let&#39;s  --  w215  --  dan&#39;s  --  motoming  --  organization&#39;s  --  krzr  --  reprioritizing  --  w510  --  mc50  --  greg&#39;s  --  today&#39;s  --  mc70  --  terry&#39;s  --  we&#39;ll  --  company&#39;s  --  don&#39;t  --  5ish  --  haven&#39;t  --  kvaal  --  you&#39;ve  --  you&#39;ll  --  can&#39;t  --  nottenburg  --  motorokr  --  what’s  --  mc35  --  i&#39;ve  --  metlitsky  --  there&#39;s  --  july&#39;s  --  w370  --  i&#39;m  --  it&#39;s  --  motorizr&lt;/code&gt;&lt;p&gt;词嵌入字典不支持连词。 然而，这应该是正确的，因为大多数会被视为停用词。&lt;br&gt;A note on stopwords, these are words that are very commonly used and their presence does little to convey a unique signature of a body of text. They&#39;re useful in everyday conversations, but when you&#39;re identifying text based on the frequency of words used, they&#39;re next to useless.&lt;/p&gt;&lt;p&gt;&lt;b&gt;TF-IDF权重&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

def get_tfidf_values(documents, norm=None):
   count_vec = CountVectorizer()
   counts = count_vec.fit_transform(documents)
   words = np.array(count_vec.get_feature_names())
   
   transformer = TfidfTransformer(norm=norm)
   tfidf = transformer.fit_transform(counts)
   tfidf_arr = tfidf.toarray()
   
   tfidf_documents = []
   for i in range(len(documents)):
       tfidf_doc = {}
       for word, tfidf in zip(words[np.nonzero(tfidf_arr[i, :])], tfidf_arr[i, :][np.nonzero(tfidf_arr[i, :])]):
            tfidf_doc[word] = tfidf
       tfidf_documents.append(tfidf_doc)
   return tfidf_documents

def docs_to_3D(tfidf_documents, w2v_reduced):
   text_docs_3D = []
   
   for i, doc in enumerate(tfidf_documents): # list of documents with word:tfidf
       data = []
       for k, v in tfidf_documents[i].items():
           try:
               item = w2v_reduced[k][:] # Copy values from reduced embedding dictionary
               item.append(v) # Append the TFIDF score
               item.append(k) # Append the word
               data.append(item) # Add [dim1, dim2, tfidf, word] to collection
           except: # If word not in embeddings dictionary
               continue 

       df = pd.DataFrame(data, columns=[&#39;dim1&#39;, &#39;dim2&#39;, &#39;tfidf&#39;, &#39;word&#39;])
       df = df.set_index([&#39;word&#39;])

       text_docs_3D.append(df)
       
   return text_docs_3D

from mpl_toolkits.mplot3d import Axes3D
``` &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5b36b5e161decc74ddf1cba44c2fe957_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;572&quot; data-rawheight=&quot;558&quot;&gt;&lt;code lang=&quot;python&quot;&gt;plt.hist(text_docs_3D[0][&#39;tfidf&#39;].apply(np.sqrt), range=(0,20))
(array([ 911.,  679.,   88.,   15.,   12.,    3.,    2.,    1.,    2.,    2.]),
 array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.]),

&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-eb461245551568b76e3c7eea101adcec_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;484&quot; data-rawheight=&quot;329&quot;&gt;&lt;p&gt;&lt;b&gt;Evolution of company transcripts over time&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Enhancement: rather than providing a list of word embedding vectors to plot, pass a dictionary of word:vector pairs so that user can hover mouse over points to see what words are.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from matplotlib import rc
# equivalent to rcParams[&#39;animation.html&#39;] = &#39;html5&#39;
rc(&#39;animation&#39;, html=&#39;html5&#39;)

from matplotlib.animation import FuncAnimation
from IPython.display import HTML

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


def animate_company_transcripts_3D(vis_docs):
   fig = plt.figure()
   ax = fig.add_subplot(111, projection=&#39;3d&#39;)
   ax.set_xlim([-3, 6])
   ax.set_ylim([-3, 6])
   ax.set_zlim([0, 20])
   ax.set_xlabel(&#39;Word Embedding&#39;)
   ax.set_ylabel(&#39;Word Embedding&#39;)
   ax.set_zlabel(&#39;Word Importance&#39;)

   text = vis_docs[0]
   scatter = ax.scatter(text[&#39;dim1&#39;], text[&#39;dim2&#39;], text[&#39;tfidf&#39;], alpha=0.1, 
                        zdir=&#39;z&#39;, s=20, c=None, depthshade=True, animated=True)

   def update(frame_number):
       text = vis_docs[frame_number]
       scatter._offsets3d = (text[&#39;dim1&#39;], text[&#39;dim2&#39;], text[&#39;tfidf&#39;].apply(np.sqrt))
       return scatter

   return FuncAnimation(fig, update, frames=len(vis_docs), interval=300, repeat=True)


tfidf_docs = get_tfidf_values(google_transcripts[&#39;body&#39;])
text_docs_3D = docs_to_3D(tfidf_docs, w2v_reduced)
animate_company_transcripts_3D(text_docs_3D) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-edf33f4a1c3f6f3262f7c4b3e5096660_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;440&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-edf33f4a1c3f6f3262f7c4b3e5096660_b.jpg&quot;&gt;&lt;p&gt;&lt;b&gt;Digitize input space for ConvNet&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def digitize_embedding_space(text_docs_3D, index, bins=250):
   binned_docs = []
   for frame, data in enumerate(text_docs_3D):
       doc = text_docs_3D[frame]

       # Sort collection of word embeddings in continous vector space to a 2D array of bins. Take square root of 
        # TF-IDF score as a means of scaling values to prevent a small number of terms from being too dominant.
        hist = np.histogram2d(doc[&#39;dim1&#39;], doc[&#39;dim2&#39;], weights=doc[&#39;tfidf&#39;].apply(np.sqrt), bins=bins)[0]
        binned_docs.append(hist)

   # Technically, you shouldn&#39;t store numpy arrays as a Series
   # Somehow, I was able to hack my way around that, but when you try to reindex the Series it throws an error
    # It was convenient to use the Series groupby function, though
   # NOTE: This should be revisited at some point using xarray or some other more suitable data store
    text_3D = pd.Series(binned_docs, index=index)

   # Combine same-day events
   if text_3D.index.duplicated().sum() &amp;gt; 0:
       logging.info(&#39;{} same-day events combined.&#39;.format(text_3D.index.duplicated().sum()))
    text_3D = text_3D.groupby(text_3D.index).apply(np.mean)
   
   # Now I&#39;ll convert the Series of numpy 2d arrays into a list of numpy 2d array (losing the date index)
    # and create another Series that ties the date to the list index of text_docs 
   text_docs = text_3D.values.tolist()
   lookup = pd.Series(range(len(text_docs)), index = text_3D.index)
   
   return text_docs, lookup&lt;/code&gt;&lt;p&gt;&lt;b&gt;Develop full text processing pipeline&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def process_text_for_input(documents, w2v_reduced, norm=None):
   index = documents.index
   tfidf_docs = get_tfidf_values(documents, norm=norm)
   text_docs_3D = docs_to_3D(tfidf_docs, w2v_reduced)
   text_docs, lookup = digitize_embedding_space(text_docs_3D, index)
   return text_docs, lookup

# Test out pre-processing pipeline
text_docs, lookup = process_text_for_input(google_transcripts[&#39;body&#39;], w2v_reduced)
2017-09-19 17:28:52,774 - INFO - 2 same-day events combined.&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤五：ARIMA 模型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;探索数据的统计属性&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Note: this cell was copied from source as cited. 

# TSA from Statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
from statsmodels.graphics.api import qqplot

def tsplot(y, lags=None, title=&#39;&#39;, figsize=(14, 8)):
   &#39;&#39;&#39;Examine the patterns of ACF and PACF, along with the time series plot and histogram.
    Original source: https://tomaugspurger.github.io/modern-7-timeseries.html
   &#39;&#39;&#39;
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load a few companies for inspection
company_price_ARIMA = load_company_price_history([&#39;GOOG&#39;, &#39;AAPL&#39;, &#39;AMZN&#39;, &#39;CA&#39;, &#39;MMM&#39;])

# Select a company and sample a two year time period, reindexing to have a uniform frequency
google_price_ARIMA = company_price_ARIMA[&#39;GOOG&#39;][&#39;2012&#39;:&#39;2013&#39;]

apple_price_ARIMA = company_price_ARIMA[&#39;AAPL&#39;][&#39;2012&#39;:&#39;2013&#39;]

logging.info(&quot;Index frequency: {}&quot;.format(google_price_ARIMA.index.freq))&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(google_price_ARIMA[&#39;2013&#39;], title=&#39;Google price for 2013&#39;, lags =40) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-932b8d1c0da650a5ac7f0a28c431c465_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;568&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(apple_price_ARIMA[&#39;2013&#39;], title=&#39;Apple price for 2013&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a5b8936a9df01f421711cae9015d82bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;559&quot;&gt;&lt;p&gt;看看自相关图，时间序列数据高度依赖于它的历史。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_returns_2013 = np.log(google_price_ARIMA[&#39;2013&#39;]).diff()[1:]
apple_returns_2013 = np.log(apple_price_ARIMA[&#39;2013&#39;]).diff()[1:]

tsplot(google_returns_2013, title=&#39;Google 2013 log returns&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-36e2f54e9f057ab0871cee0d5707e72a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;989&quot; data-rawheight=&quot;563&quot;&gt;&lt;code lang=&quot;python&quot;&gt;tsplot(apple_returns_2013, title=&#39;Apple 2013 log returns&#39;, lags = 40) &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1804ace2c71592c9f7a5d4a48f7ffc0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;995&quot; data-rawheight=&quot;567&quot;&gt;&lt;p&gt;注意，一阶差分产生时间序列的平稳性。 因此，我们应该执行d = 1。&lt;/p&gt;&lt;p&gt;返回大概遵循随机游走，其中当前时间步与先前的任何时间步不相关。 这表明一个ARIMA（0,1,0）模型，其中最好的预测可以使它成为一个常数值。&lt;/p&gt;&lt;p&gt;&lt;b&gt;网格搜索最佳的ARIMA参数&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import itertools

import warnings
warnings.filterwarnings(&quot;ignore&quot;) # Ignore convergence warnings

def grid_search_SARIMA(y, pdq_min, pdq_max, seasonal_period):
   p = d = q = range(pdq_min, pdq_max+1)
   pdq = list(itertools.product(p, d, q))
   seasonal_pdq = [(x[0], x[1], x[2], seasonal_period) for x in list(itertools.product(p, d, q))]
    
   best_params = []
   best_seasonal_params = []
   score = 1000000000000 # this is a bit of a hack
   
   for param in pdq:
       for param_seasonal in seasonal_pdq:
           try:
               mod = sm.tsa.statespace.SARIMAX(y,
                                               order=param,
                                               seasonal_order=param_seasonal,
                                               enforce_stationarity=False,
                                               enforce_invertibility=False)

               results = mod.fit()
               logging.info(&#39;ARIMA{}x{}12 - AIC:{}&#39;.format(param, param_seasonal, results.aic))
                if results.aic &amp;lt; score:
                   best_params = param
                   best_seasonal_params = param_seasonal
                   score = results.aic
           except:
               continue
   logging.info(&#39;\n\nBest ARIMA{}x{}12 - AIC:{}&#39;.format(best_params, best_seasonal_params, score))
    return best_params, best_seasonal_params, score


params, seasonal_params, score = grid_search_SARIMA(google_price_ARIMA, 0, 2, 12)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;ARIMA(0, 0, 0)x(0, 0, 1, 12)12 - AIC:6898.362293047701
ARIMA(0, 0, 0)x(0, 0, 2, 12)12 - AIC:6200.392463920813
ARIMA(0, 0, 0)x(0, 1, 1, 12)12 - AIC:4377.816871987012
ARIMA(0, 0, 0)x(0, 1, 2, 12)12 - AIC:4278.05126125979
```&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mod = sm.tsa.statespace.SARIMAX(google_price_ARIMA,
                               order=(0, 1, 2),
                               seasonal_order=(0, 1, 2, 12),
                               enforce_stationarity=False,
                               enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1]) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-59d5dd7eac6f21e6010f5344f1b436b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;560&quot; data-rawheight=&quot;151&quot;&gt;&lt;code lang=&quot;python&quot;&gt;pred = results.get_prediction(start = pd.to_datetime(&#39;2013-10-1&#39;), end = pd.to_datetime(&#39;2013-12-31&#39;), dynamic=False)
pred_ci = pred.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_price_ARIMA.index, google_price_ARIMA, 
       label=&#39;Observed stock price&#39;)
ax.plot(pred.predicted_mean.index, pred.predicted_mean, 
       label=&#39;One-step ahead forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_ci.index,
               pred_ci.iloc[:, 0],
               pred_ci.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Stock price&#39;)
ax.set_title(&#39;In sample one-step prediction&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-453b961c9b83ccc29ca1e9485e6afa1d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;code lang=&quot;python&quot;&gt;pred_dynamic = results.get_prediction(start = pd.to_datetime(&#39;2013-11-1&#39;), dynamic=True, full_results=True)
pred_ci_dynamic = pred_dynamic.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_price_ARIMA.index, google_price_ARIMA, 
       label=&#39;Observed stock price&#39;)
ax.plot(pred_dynamic.predicted_mean.index, pred_dynamic.predicted_mean, 
       label=&#39;Dynamic forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_ci_dynamic.index,
               pred_ci_dynamic.iloc[:, 0],
               pred_ci_dynamic.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Google stock price&#39;)
ax.set_title(&#39;In sample dynamic prediction&#39;)
ax.set_ylim(200,600)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-41aabc55f2ce74227232c04ffc4326e4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Get forecast 500 steps ahead in future
pred_future = results.get_forecast(steps=100)

# Get confidence intervals of forecasts
pred_future_ci = pred_future.conf_int()

fig, ax = plt.subplots(figsize=(20,10))
ax.plot(company_price_ARIMA[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014-05-01&#39;].index, company_price_ARIMA[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014-05-01&#39;], 
        label=&#39;Observed stock price&#39;)
ax.plot(pd.to_datetime(pred_future.predicted_mean.index), pred_future.predicted_mean, 
       label=&#39;Dynamic forecast&#39;, alpha=.7)
fig.autofmt_xdate()
ax.fill_between(pred_future_ci.index,
               pred_future_ci.iloc[:, 0],
               pred_future_ci.iloc[:, 1], color=&#39;k&#39;, alpha=.2)
ax.set_xlabel(&#39;Date&#39;)
ax.set_ylabel(&#39;Google stock price 2013&#39;)
ax.set_title(&#39;Out of sample prediction&#39;)
ax.set_ylim(200,800)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-79a740c4c456f5b333fd0f40e9ec5174_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1168&quot; data-rawheight=&quot;565&quot;&gt;&lt;p&gt;&lt;b&gt;训练&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除非时间序列数据是由同一过程生成的，否则ARIMA模型通常不会在时间序列上进行训练。 对于股票价格的情况，对个别公司的影响是独立的，对所有公司的影响并不一致。 因此，5种不同的ARIMA模型将针对不同的公司进行训练，分别评估每个模型并平均每个结果。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;google_arima_train = company_price_df[&#39;GOOG&#39;][&#39;2009&#39;:&#39;2012&#39;]
amazon_arima_train = company_price_df[&#39;AMZN&#39;][&#39;2009&#39;:&#39;2012&#39;]
mmm_arima_train = company_price_df[&#39;MMM&#39;][&#39;2009&#39;:&#39;2012&#39;]
chipotle_arima_train = company_price_df[&#39;CMG&#39;][&#39;2009&#39;:&#39;2012&#39;]
duke_arima_train = company_price_df[&#39;DUK&#39;][&#39;2009&#39;:&#39;2012&#39;]

companies_train = [google_arima_train, amazon_arima_train, mmm_arima_train, chipotle_arima_train, duke_arima_train]

google_arima_test = company_price_df[&#39;GOOG&#39;][&#39;2013&#39;:&#39;2014&#39;]
amazon_arima_test = company_price_df[&#39;AMZN&#39;][&#39;2013&#39;:&#39;2014&#39;]
mmm_arima_test = company_price_df[&#39;MMM&#39;][&#39;2013&#39;:&#39;2014&#39;]
chipotle_arima_test = company_price_df[&#39;CMG&#39;][&#39;2013&#39;:&#39;2014&#39;]
duke_arima_test = company_price_df[&#39;DUK&#39;][&#39;2013&#39;:&#39;2014&#39;]

companies_test = [google_arima_test, amazon_arima_test, mmm_arima_test, chipotle_arima_test, duke_arima_test]

company_results = []
for company_price in companies_train:
   model = sm.tsa.statespace.SARIMAX(company_price,
                               order=(0, 1, 2),
                               seasonal_order=(0, 1, 2, 12),
                               enforce_stationarity=False,
                               enforce_invertibility=False)

   results = model.fit()
   company_results.append(results)&lt;/code&gt;&lt;p&gt;&lt;b&gt;评估&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们将在三个时间尺度上评估每个模型：5天预测，20天预测和100天预测。 长期预测非常困难，特别是对于随机过程；因此，短期预测提供了一个更合理的业绩衡量标准。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;forecast_5_day = []
forecast_20_day = []
forecast_100_day = []

for result in company_results:
   forecast_5_day.append(result.get_forecast(steps=5))
   forecast_20_day.append(result.get_forecast(steps=20))
   forecast_100_day.append(result.get_forecast(steps=100))

from sklearn.metrics import mean_absolute_error

forecast_5_day_mae = []
for true, pred in zip(companies_test, forecast_5_day):
   forecast_5_day_mae.append(mean_absolute_error(true.iloc[0:5], pred.predicted_mean))
   
forecast_20_day_mae = []
for true, pred in zip(companies_test, forecast_20_day):
   forecast_20_day_mae.append(mean_absolute_error(true.iloc[0:20], pred.predicted_mean))
    
forecast_100_day_mae = []
for true, pred in zip(companies_test, forecast_100_day):
   forecast_100_day_mae.append(mean_absolute_error(true.iloc[0:100], pred.predicted_mean))
    
   
print(&#39;Average MAE across companies (5 day): {:.6f}&#39;.format(np.mean(forecast_5_day_mae)))
print(&#39;Average MAE across companies (20 day): {:.6f}&#39;.format(np.mean(forecast_20_day_mae)))
print(&#39;Average MAE across companies (100 day): {:.6f}&#39;.format(np.mean(forecast_100_day_mae)))
Average MAE across companies (5 day): 0.009500
Average MAE across companies (20 day): 0.009690
Average MAE across companies (100 day): 0.009965&lt;/code&gt;&lt;p&gt;让我们来看看长期预测是怎样的。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fig, ax = plt.subplots(figsize=(20,10))
ax.plot(google_arima_test.iloc[0:100], label=&#39;True&#39;)
ax.plot(forecast_100_day[0].predicted_mean, label=&#39;Pred&#39;)
ax.set_title(&#39;ARIMA: 100 Day Forecast of Google (starting Jan 1, 2013)&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b8f283b03c930910c7fd64443d954384_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1162&quot; data-rawheight=&quot;588&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤六：LSTM 模型&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;数据准备&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Load a test company for inspection
LSTM_company_prices = load_company_price_history([&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;MMM&#39;, &#39;CMG&#39;, &#39;DUK&#39;], normalize = True)

LSTM_prices_train = LSTM_company_prices[&#39;2009&#39;:&#39;2012&#39;]
LSTM_prices_test = LSTM_company_prices[&#39;2013&#39;:&#39;2014&#39;]
LSTM_prices_val = LSTM_company_prices[&#39;2015&#39;:&#39;2016&#39;]

def price_generator(data, window=180, batch_size=128):
   
   timesteps = data.shape[0]
   companies = data.shape[1]
   
   if window + batch_size &amp;gt; timesteps:
       logging.warning(&#39;Not enough data to fill a batch, forcing smaller batch size.&#39;)
       batch_size = timesteps - window
   
   # Index to keep track of place within price timeseries, corresponds with the last day of output
    i = window
   
   # Index to keep track of which company to query the data from
   j = 0
   
   while True:
       # If there aren&#39;t enough sequential days to fill a batch, go to next company
       if i + batch_size &amp;gt;= timesteps:
           i = window
           
           # If end of companies has been reached, start back at first company
           if j+1 &amp;gt;= companies:
               j=0
           else:
               j+=1
           
       
       samples = np.arange(i, i + batch_size)
       i += len(samples)
       
       # 代码太多略去&lt;/code&gt;&lt;p&gt;&lt;b&gt;建立模型&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from keras.models import Sequential
from keras.layers import Dense, TimeDistributed, LSTM


# define LSTM configuration
n_features = 1 # only price
window = 180 # look back 50 days
batch_size = 128

# create LSTM
price_only_model = Sequential()
price_only_model.add(LSTM(20, input_shape=(window, n_features), return_sequences=True))
price_only_model.add(LSTM(60, return_sequences=True))
price_only_model.add(TimeDistributed(Dense(1)))
print(price_only_model.summary()) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-63e452f69beaf301507bc9a4022137b7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;478&quot; data-rawheight=&quot;235&quot;&gt;&lt;code lang=&quot;python&quot;&gt;from keras.callbacks import ModelCheckpoint 

price_only_model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=&#39;adam&#39;)

checkpointer = ModelCheckpoint(filepath=&#39;saved_models/weights.best.price_only.hdf5&#39;, 
                              verbose=1, save_best_only=True)

# Train LSTM
history = price_only_model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=3, callbacks=[checkpointer], 
                                         validation_data=val_gen, validation_steps=val_steps) &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-09df422176fc1c3eb56e20ec0082fde5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1014&quot; data-rawheight=&quot;191&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Load weights from previous training
price_only_model.load_weights(&#39;saved_models/weights.best.price_only.hdf5&#39;)

def validation_curve(history):
   loss = history.history[&#39;loss&#39;]
   val_loss = history.history[&#39;val_loss&#39;]
   epochs = range(len(loss))

   plt.figure()
   plt.plot(epochs, loss, &#39;g&#39;, label=&#39;Training loss&#39;)
   plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation loss&#39;)
   plt.title(&#39;Training and validation loss&#39;)
   plt.legend()
   
   validation_curve(history) &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-059297d13a55ee14632f06b099bec6f0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;504&quot; data-rawheight=&quot;343&quot;&gt;&lt;p&gt;&lt;b&gt;评估模型&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def forecast(seed_data, forecast_steps, model):
   &#39;&#39;&#39;
   Forecast future returns by making day-by-day predictions.
   
   Args:
       seed_data: Initial input sequence.
       forecast_steps: Defines how many steps into the future to predict.
       model: Trained LSTM prediction model. 
   &#39;&#39;&#39;
   
   future = []
   
   for timestep in range(forecast_steps):
       pred = model.predict(seed_data)[0][-1][0]
       future.append(pred)
       seed_data = np.append(seed_data[0][1:], [pred]).reshape(1, seed_data.shape[1], 1)

   return future

    # 代码太多略去
&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fig, ax = plt.subplots(figsize=(20,10))
ax.plot(company_prices_test.iloc[0:100, 0].values, label=&#39;True&#39;)
ax.plot(forecast_100_day[0], label=&#39;Pred&#39;)
ax.set_title(&#39;LSTM: 100 Day Forecast of Google (starting Jan 1, 2013)&#39;)
plt.legend() &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-27581638674e73013b7cd3ec65bad300_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1162&quot; data-rawheight=&quot;588&quot;&gt;&lt;p&gt;&lt;b&gt;测试评估&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mae = price_only_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {:.6f}&#39;.format(mae))
Mean absolute error on test data: 0.009176&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤七：ConvNet特征提取&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本节中，我们将为ConvNet模型准备数据，开发一个能够根据文本信息预测价格变动的模型，并从网络的最后一层提取特征，以便在卷积网络中输入。&lt;/p&gt;&lt;p&gt;只贴出核心代码&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def text_generator(price_data, text_data, w2v_reduced, window=5, batch_size=1):
   companies = len(text_data)
   
   # Start with first transcript
   i = 0
   
   # Start with first company
   j = 0
   
   text_docs, lookup = process_text_for_input(text_data[j][&#39;body&#39;], w2v_reduced, norm=&#39;l2&#39;)
    
   while True:
       # If end of transcripts reached, go to next company
       if i &amp;gt;= len(lookup):
           i = 0
           if j+1 &amp;gt;= companies:
               j=0
           else:
               j+=1
               
           text_docs, lookup = process_text_for_input(text_data[j][&#39;body&#39;], w2v_reduced, norm=&#39;l2&#39;)
        
       
       event = lookup.index[i]
       text = text_docs[i].reshape(1, text_docs[i].shape[0], text_docs[i].shape[1], 1)
       price_target = company_prices[event : event + pd.to_timedelta(&#39;{} days&#39;.format(window+1))].iloc[:,j].sum()
        
       yield text, np.array(price_target).reshape(1)
       
       i+= 1
``` &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8cad8fd8a0868cf6ab7395b8f771634f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;493&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# Run this cell to inspect a new example
text, price = next(test_gen)

print(&#39;Predicted return:\t{}&#39;.format(text_model.predict(text)[0][0]))
print(&#39;True return:\t\t{}&#39;.format(price[0]))
Predicted return:	0.01737634837627411
True return:		0.06355231462064292&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;mae = text_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {}&#39;.forma
Mean absolute error on test data: 0.057885023705180616&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;步骤八：LSTM价格+测试模型&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dcc8a7970645f6ceda1a03d8b8995e8a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;735&quot; data-rawheight=&quot;479&quot;&gt;&lt;code lang=&quot;python&quot;&gt;companies = [&#39;GOOG&#39;, &#39;AMZN&#39;, &#39;MMM&#39;, &#39;CMG&#39;, &#39;DUK&#39;]

company_transcripts_train = [load_company_transcripts(company)[&#39;2009&#39;:&#39;2012&#39;] for company in companies]
company_prices_train = load_company_price_history(companies, normalize=True)[&#39;2009&#39;:&#39;2012&#39;]

```
window_size = 180
batch_size = 128
text_features = 10

train_gen = price_text_generator(company_prices_train, company_transcripts_train, w2v_reduced, extract_features, 
                                 text_features=text_features, window=window_size, batch_size=batch_size)
train_steps = (company_prices_train.shape[0] // batch_size)*company_prices_train.shape[1]

```

def price_text_generator(price_data, text_data, w2v_reduced, extract_features_model, text_features, 
                         window=180, batch_size=128):
```

# Train LSTM
history = price_text_model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=3, callbacks=[checkpointer], 
                                         validation_data=val_gen, validation_steps=val_steps)

# Load weights from previous training
price_text_model.load_weights(&#39;saved_models/weights.best.price_with_text_features.hdf5&#39;)

validation_curve(history)

mae = price_text_model.evaluate_generator(test_gen, steps=test_steps)
print(&#39;Mean absolute error on test data: {:.6f}&#39;.format(mae)) &lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-46a85f3ee5bb9e9895a6157344783e00_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;498&quot; data-rawheight=&quot;343&quot;&gt;&lt;code lang=&quot;python&quot;&gt;2017-09-22 14:29:52,468 - INFO - 1 same-day events combined.
2017-09-22 14:29:55,605 - INFO - 1 same-day events combined.
Mean absolute error on test data: 0.008930&lt;/code&gt;&lt;p&gt;链接: https://pan.baidu.com/s/1pMLdP1L &lt;/p&gt;&lt;p&gt;密码: x6mj&lt;/p&gt;&lt;p&gt;谢谢大家对公众号的一直以来厚爱！&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;【代码+论文】通过ML、Time Series模型学习股价行为（第九期免费赠书活动来啦！）&lt;/a&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2593c949ae5b55159619364a33f21f8e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33152954</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器学习应用区块链系列（一）——如何开发一套自己的智能合约系统</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-21-33152738.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33152738&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5627788616a10bf169cd136fc01f8e65_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;从今天开始&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;编辑部将带来机器学习应用区块链系列&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于是第一期，我们想解读一些国外已有的文献和研究。故带来了START-Summit-2017-Blockchain-Machine-Learning-Workshop的演讲稿和示例代码，希望能够给大家带来一些启迪。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;来源&lt;/b&gt;&lt;/h2&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287256&amp;amp;idx=2&amp;amp;sn=483e71c66e863aec424ff0c1936411c6&amp;amp;chksm=802e314db759b85b505f7fd1ed0cb0cacf10ed4bbbf86cc50c7bbc9d07d4ac0fed3b783b2278##&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;机器学习应用区块链系列（一）--如何开发一套自己的智能合约系统&lt;/a&gt;&lt;h2&gt;&lt;b&gt;介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;代码的目的是用一个简单的例子来演示如何把区块链技术，智能合约和机器学习结合在一起。 &lt;b&gt;（代码在文末下载）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码文件&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;- runTestnet.sh: launches a local development Blockchain for easy testing
- contract.sol: contains the smart contract code in solidity language
- installContract.py: Python script for sending our contract to the Blockchain
- user.py: Python script that contains the actual chat client
- classify.py: Python script to classifiy an image file &lt;/code&gt;&lt;p&gt;&lt;b&gt;流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推荐使用Linux系统。 安装将集中在Ubuntu Linux 16.04 LTS上，但对于其他发行版应该是类似的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们需要先安装Python 2.7，pip，curl和git：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;open a Linux Terminal and enter the following command:
sudo apt-get install python2.7 python-pip curl git build-essential libssl-dev &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装当前nodejs版本：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -
sudo apt-get install -y nodejs &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装区块链测试环境：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;sudo npm install -g ethereumjs-testrpc solc &lt;/code&gt;&lt;p&gt;&lt;b&gt;安装其他Python包：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;sudo pip install -U pip
sudo pip install -U numpy keras==2.0.0 tensorflow ethjsonrpc h5py Pillow scipy &lt;/code&gt;&lt;p&gt;&lt;b&gt;克隆workshop代码：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
git clone https://github.com/thoschm/START-Summit-2017-Blockchain-Machine-Learning-Workshop.git workshop_code &lt;/code&gt;&lt;p&gt;&lt;b&gt;克隆现成的深度学习模型：&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
git clone https://github.com/fchollet/deep-learning-models.git
cp deep-learning-models/resnet50.py workshop_code/code/
cp deep-learning-models/imagenet_utils.py workshop_code/code/ &lt;/code&gt;&lt;p&gt;&lt;b&gt;简单的图像分类示例&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;cd
cd workshop_code/code
[get some image file in there]
python classify.py image.jpg &lt;/code&gt;&lt;p&gt;当您第一次运行它时，它会从互联网下载预先训练的网络权重。 不需要自己训练。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Using TensorFlow backend.
content:

Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json
&amp;gt;  tree_frog  &amp;lt; &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;PPT展示&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9553ce4cd6ddf45121cb09b8928926f0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4a66954e4b5191556dd53003475639fa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a18be2bc915f4cd672ff0c65cac58ff8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2cdba7dc48e60dd3cf13857f18bb1841_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eb1e6b5979731993475b18d5e5a3722e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b44e2a489704bc45616954846f651d4c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7e2d553ec4188126abf85e28f8af58e8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6726013442b8dff8728900b3bd993e86_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cccc6288f0f41e48bfee42d8b3e0927f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5d61ef34da06495ce41e2fed1a75cf18_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2c604e5a811289f7c164aaed1f473f2f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e91b95db74e92d8fbf366f2afd2a7d6e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d934053a159a1219e856bb9d57575b3e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7efad18817c8d68d3c55db27a8d67c5b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7bd65405ef1a92e2e312c6983eda9a39_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa8c91696ab177e2fbe45e5108286610_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-06299f07baedd00070b1fc451c6ed6aa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-64c52c167ae10b21b47224532770d4d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f3f0f280b9468853573b885ad5393a1f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b519b1f09183f20a84060828af2a59c0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-41b2f56f78c42716f059e396d820b6d9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-22a71c4175f77f6c6669cc1ba18a45e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bc224e010d93ff7af782e81b47e7dc9b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1577966f716f5a6c7b6a5296c404d6df_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-009e6c99b633554b6a5c26558bf69c63_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b3b657e89a99bcc9220253ddd8accb98_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3c762149ec52d81aaac91b870ad39337_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5f888cf0c5ef24df32cf399479906c8c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7b2649e9a4f6205386910f82aebdfe1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3c65b615ad5962d99faf3beff9da5730_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b40d386932966cd50008dc3e2ed7d981_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54e64e234265287ca24237eef498511f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4309cff7fbd215f8c9e21cd5ad0a61d6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1957d87efb17e050a30e88eb69086209_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7ff555c6cf4d3db258e467b5d8baf174_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1d78dde1a6c7b2d610ab24846a4ed673_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-17195cae25e59965f4e9a7f1c7c48357_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ee684c83aeb183a1f4e5f7ecdf13c5e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb9679b1b88cb2897745bd2fb2070118_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;4000&quot; data-rawheight=&quot;2250&quot;&gt;&lt;p&gt;链接: https://pan.baidu.com/s/1o9Zteiu &lt;/p&gt;&lt;p&gt;密码: 95qd&lt;/p&gt;&lt;p&gt;谢谢大家对公众号的一直以来厚爱！&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287256&amp;amp;idx=2&amp;amp;sn=483e71c66e863aec424ff0c1936411c6&amp;amp;chksm=802e314db759b85b505f7fd1ed0cb0cacf10ed4bbbf86cc50c7bbc9d07d4ac0fed3b783b2278##&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2e9f227f80bf31b439e47581a6b43f2b&quot; data-image-width=&quot;640&quot; data-image-height=&quot;640&quot; data-image-size=&quot;ipico&quot;&gt;机器学习应用区块链系列（一）--如何开发一套自己的智能合约系统&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ecd27e25f4fe3285f8f1fe63024bb19b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1096&quot; data-rawheight=&quot;372&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-21-33152738</guid>
<pubDate>Sun, 21 Jan 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【拥抱量化】当AI遇上智能投顾——公众号受邀参加万得3C中国财经会议</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-01-10-32821542.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32821542&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c9134639a91c9f3da75369d6232e8dc4_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;收听会议&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1192a9ff8a55aece3650f2362f87427f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;755&quot; data-rawheight=&quot;1355&quot;&gt;&lt;p&gt;量化投资与机器学习公众号成立2年多来，一直秉持着无偿分享，共同成长的精神。努力为中国的量化投资事业贡献一份自己微薄的力量。&lt;/p&gt;&lt;p&gt;2年来从0到1，公众号成长很多，但是在大家的鼓励和支持下，我们不忘初心，赢得了众多媒体和机构的认可。&lt;/p&gt;&lt;p&gt;近期，公众号作为全网受 &lt;b&gt;Wind（万得）&lt;/b&gt;邀请的自媒体平台&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f1f30251631ad520d49ad3cb18bd606_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;201&quot;&gt;&lt;p&gt;参加了&lt;b&gt;中国最专业的机构路演平台&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-11fe70780bc01cff5e51ae37f032ad2a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;180&quot;&gt;&lt;p&gt;我们很荣幸，能够和各大金融机构、金融大咖在此平台上分享我们的研究心得。这既是对公众号的认可，也是对公众号编辑部所有人的肯定。&lt;/p&gt;&lt;p&gt;在未来，公众号会继续秉持知识分享、坚持原创的精神。为更多的量化爱好者提供学习交流的平台。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;下面，大家快来看看我们这次线上路演的内容吧！&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8a2e0a9f24c66c5d70d57138b04b4560_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1113&quot; data-rawheight=&quot;157&quot;&gt;&lt;p&gt;&lt;b&gt;部分PPT内容&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-61278fbbcb2079559d11f69c324c3bb0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1378&quot; data-rawheight=&quot;770&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c1d6604b67a8087dfdffaf1ec6d374d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1363&quot; data-rawheight=&quot;767&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何参加？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;非万得用户&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;万得用户&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;PC端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1、点击Wind金融终端首页右侧的&lt;b&gt;“3C会议”&lt;/b&gt;或在右下角按键精灵&lt;b&gt;输入“3C”&lt;/b&gt;，进入3C会议模块。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fe6ca35155ce6f20c49389e67278c636_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;771&quot;&gt;&lt;p&gt;2、选中一场会议，点击&lt;b&gt;“立即报名”&lt;/b&gt;。 &lt;/p&gt;&lt;p&gt;3、会议开始后，进入会议详情页，点击&lt;b&gt;“立即参会”&lt;/b&gt;，无需拨打电话，立即在线收听。您还可以在互动处留言，交流、提问。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;移动端&lt;/b&gt;&lt;/p&gt;&lt;p&gt;没有Wind金融终端的客户，请扫面下方二维码下载。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b8d3084c36db5e9bc4b14a70d80bf6bc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;790&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ca6de73d4a23d38fc1079568cb067c3d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;1003&quot;&gt;&lt;p&gt;1、进入APP后，先点击右下角&lt;b&gt;“发现”&lt;/b&gt;，再进入&lt;b&gt;“3C会议平台”&lt;/b&gt;。&lt;br&gt;&lt;/p&gt;&lt;p&gt;2、报名及参会步骤，与PC端相同。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：&lt;/b&gt;请确认APP已升级至最新版。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;收听会议&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;a href=&quot;http://3c-share.wind.com.cn/3CWeb/3CMobile/html/sharedMeeting.html?meetingId=52235&quot;&gt;会议详情&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a3cf7ad8b603ed25923764c15c446691_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1715&quot; data-rawheight=&quot;444&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-01-10-32821542</guid>
<pubDate>Wed, 10 Jan 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
