<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Wed, 24 Oct 2018 14:36:39 +0800</lastBuildDate>
<item>
<title>一招鲜，判断哪些输入特征对神经网络是重要的！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47475888.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47475888&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f07bd4bd990b9118b93d7a50e51c6bf5_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d0adadbc7e374710cc6f20b574638ffb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d0adadbc7e374710cc6f20b574638ffb&quot; data-watermark-src=&quot;v2-e96895b3f9d106aef40902f9660c108d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Muhammad Ryan&lt;/p&gt;&lt;p&gt;本期编译：Peter&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289289&amp;amp;idx=1&amp;amp;sn=7c6cf3ed7f4f2859a0e95bc87914814c&amp;amp;chksm=802e395cb759b04adcee6afc1d44ffa2f3ceac2137796dad4ee9c51cdb165ce3463258647a3c&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【ML系列】一招鲜，判断哪些输入特征对神经网络是重要的！&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=2020206794&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;7、使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289274&amp;amp;idx=1&amp;amp;sn=f40be8372658c2c79fdd47c03d62e037&amp;amp;chksm=802e392fb759b039435fc6700ef5d45142cdfe72234586bd8de9b8dfabcc3264f2ae826def80&amp;amp;token=1003651614&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;8、手把手教你用Numpy构建神经网络(附代码)&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;正文&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果你看到这个题目，可能会马上回答&lt;/p&gt;&lt;p&gt;主成分分析（PCA），因为冗余的输入是无用的。&lt;/p&gt;&lt;p&gt;没错，但这不是今天的重点。我们想知道的是输入特征对神经网络的预测计算有多重要。例如，通过学习时间、年龄、身高和缺席人数等几个预测因素来预测谁会通过考试。直觉上，决定学生是否通过考试的最重要的因素是学习时间。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e9e14454680947af05adac3d88a04cfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-e9e14454680947af05adac3d88a04cfc&quot; data-watermark-src=&quot;v2-49065bcacadfa623c6b3bc8df99c6106&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在一个简单的线性回归中，我们可以通过看它的线性方程的权重来测量它。当然，假设预测器(X)已经标准化(X &#39;)所以数据的量纲是相同的。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-732d6954009b21d9c0296fb9c6b8d151_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;562&quot; data-rawheight=&quot;170&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-732d6954009b21d9c0296fb9c6b8d151&quot; data-watermark-src=&quot;v2-40e2e1ff16bd5b4837fa1736b96842bc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6df5e22be3ab1a5ec28d7c7e10de6d20_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;688&quot; data-rawheight=&quot;160&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6df5e22be3ab1a5ec28d7c7e10de6d20&quot; data-watermark-src=&quot;v2-b864c7239b970257bf1e661c598d807d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;你可以在这两个函数中选择一个来归一化你的预测器。为了理解为什么只有使用权重我们才能衡量一个预测器相对于其他预测器的重要性，这里有一个例子。假设我们有一个线性方程。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9937466fdccb75067e8d73753ae3f966_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;598&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们把所有的x用5代替：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d019039c29cf24b3edf3d8e010aaa6e9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;758&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这是它贡献的部分，直观上来说，如果这个部分很大，当输入出错时，输出就会出错。例如，当我把x3从5换成1，我们得到：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c598d476fa2bf0150cf886de0ab4bf7d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;78&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果把x2换成1，得到的是：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-db8d134f22a232d9e30ecfd5bf12a04b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;758&quot; data-rawheight=&quot;80&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这里我们可以看到，由于权重的不同，x2值的变化比x3值的变化影响更大。这很明显，但我想强调的是，除了权重之外，我们可以从输出值与参考值的偏差来看我们的输入有多重要。&lt;/p&gt;&lt;p&gt;在神经网络中，输入的权重不是直接连接到输出层，而是连接到隐藏层。此外，与线性回归不同，神经网络是非线性的。为了看到输入的显著水平，我们寻找我们之前找到的第二个参数，如果我们随机改变输入值，它与神经网络输出值的偏差有多大。这里我们使用的参考值是原始错误值。为什么我称之为“original”。&lt;/p&gt;&lt;p&gt;让我们来看看真实的数据和真实的神经网络。预测学生在考试中的表现。&lt;/p&gt;&lt;p&gt;数据下载地址：&lt;i&gt;https://archive.ics.uci.edu/ml/datasets/student+performance&lt;/i&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-279572efbfce56c3c8b6a91f9c60e34c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;396&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-279572efbfce56c3c8b6a91f9c60e34c&quot; data-watermark-src=&quot;v2-622255806ae8777e1a22b8b8198fc078&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下面是逐步来实现到在神经网络中输入显著水平：&lt;/p&gt;&lt;p&gt;1、使用下面的代码&lt;b&gt;构建&lt;/b&gt;、&lt;b&gt;训练&lt;/b&gt;和&lt;b&gt;保存神经网络&lt;/b&gt;。在训练神经网络之后，我们不会直接使用它来预测，而是将训练过的模型保存到一个文件中。我们为什么要这么做？因为我们需要一个稳定的模型（记住，每次对模型进行训练，每次得到的权重和偏差都会不同）来计算每个输入的显著水平。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import csv
#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
def generatemodel(totvar):
# create and fit the LSTM network
model = Sequential()
model.add(Dense(3, batch_input_shape=(batch_size, totvar), activation=&#39;sigmoid&#39;))
model.add(Dense(hidden_neuron, activation=&#39;sigmoid&#39;))
model.add(Dense(1))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
return model
#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]
#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]
testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]
trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]
testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]
###############
#normalization#
###############
trainparamnorm = np.zeros(np.shape(trainparam))
trainlabelnorm = np.zeros(np.shape(trainlabel))
testparamnorm = np.zeros(np.shape(testparam))
testlabelnorm = np.zeros(np.shape(testlabel))
print &#39;shape label adalah&#39;, np.shape(testlabelnorm)
#for param
for i in xrange(len(trainparam[0])-2):
trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
for i in xrange(2):
trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)
#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
######################
#build and save model#
######################
mod = generatemodel(len(trainparamnorm[0]))
mod.fit(trainparamnorm, trainlabelnorm, epochs=iterasi, batch_size=batch_size, verbose=2, shuffle=True)
#save trained model
mod.save(studentmodel)&lt;/code&gt;&lt;p&gt;2、加载模型并计算其误差：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import csv
import random
#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]
#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]
testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]
trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]
testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]
###############
#normalization#
###############
trainparamnorm = np.zeros(np.shape(trainparam)).astype(&#39;float32&#39;)
trainlabelnorm = np.zeros(np.shape(trainlabel)).astype(&#39;float32&#39;)
testparamnorm = np.zeros(np.shape(testparam)).astype(&#39;float32&#39;)
testlabelnorm = np.zeros(np.shape(testlabel)).astype(&#39;float32&#39;)
#for param
for i in xrange(len(trainparam[0])-2):
trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
for i in xrange(2):
trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)
#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
#load trained model
mod = load_model(studentmodel)
G3pred = mod.predict(testparamnorm, batch_size=batch_size)
G3real = G3pred*20.0
err = mean_squared_error(testlabel, G3real)
print &#39;our error value is&#39;, err&lt;/code&gt;&lt;p&gt;在自己的电脑上，错误是3.44525143751。这是初始误差。&lt;/p&gt;&lt;p&gt;3、&lt;b&gt;为随机改变每个输入值&lt;/b&gt;。我们将随机生成0到1之间的数字，替换测试数据测中的归一化输入参数，并立即将修改后的输入数据应用到刚刚加载的神经网络中。为什么在0和1之间随机生成值呢？因为我们在上面一段使用了第二个归一化函数（使用最大值和最小值）来归一化我们的输入。对每个归一化输入进行迭代，随机改变其值，反复进行，得到大量的样本，从而得到误差的平均值和标准差。这样可以消除偶然因素（记住，我们随机产生值）。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed
from keras.models import load_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.svm import SVR
import random
import os
import csv

#setting
datafile = &#39;studentperform.csv&#39;
studentmodel = &#39;studentmodel.h5&#39;
batch_size = 10
hidden_neuron = 10
trainsize = 900
iterasi = 200
randsample = 100

#read data
alldata = np.genfromtxt(datafile,delimiter=&#39;,&#39;)[1:]

#separate between training and test
trainparam = alldata[:900, :-1]
trainlabel = alldata[:900, -1]

testparam = alldata[900:, :-1]
testlabel = alldata[900:, -1]

trainparam = trainparam[len(trainparam)%10:]
trainlabel = trainlabel[len(trainlabel)%10:]

testparam = testparam[len(testparam)%10:]
testlabel = testlabel[len(testlabel)%10:]


###############
#normalization#
###############

trainparamnorm = np.zeros(np.shape(trainparam)).astype(&#39;float32&#39;)
trainlabelnorm = np.zeros(np.shape(trainlabel)).astype(&#39;float32&#39;)

testparamnorm = np.zeros(np.shape(testparam)).astype(&#39;float32&#39;)
testlabelnorm = np.zeros(np.shape(testlabel)).astype(&#39;float32&#39;)

#for param
for i in xrange(len(trainparam[0])-2):
 trainparamnorm[:,i] = (trainparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))
 testparamnorm[:,i] = (testparam[:,i] - np.min(trainparam[:,i])) / (np.max(trainparam[:,i]) - np.min(trainparam[:,i]))

for i in xrange(2):
 trainparamnorm[:,-2+i] = (trainparam[:,-2+i] - 0.0) / (20.0 - 0.0)
 testparamnorm[:,-2+i] = (testparam[:,-2+i] - 0.0) / (20.0 - 0.0)

#for label
trainlabelnorm = (trainlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))
testlabelnorm = (testlabel - np.min(trainlabel)) / (np.max(trainlabel) - np.min(trainlabel))


#load trained model
mod = load_model(studentmodel)

G3pred = mod.predict(testparamnorm, batch_size=batch_size)
G3real = G3pred*20.0

errreal = mean_squared_error(testlabel, G3real)
print &#39;our error value is&#39;, errreal

################################
#permutation importance session#
################################

permutsample = np.zeros((randsample, len(testparamnorm[0])))
for trying in xrange(randsample):
 randval = np.zeros((len(testlabelnorm)))
 for i in xrange(len(testlabelnorm)):
   randval[i] = random.uniform(0,1)

 for i in xrange(len(testparamnorm[0])):
   permutinput = np.zeros(np.shape(testparamnorm))
   permutinput[:] = testparamnorm
   permutinput[:,i] = randval
   G3pred = mod.predict(permutinput, batch_size=batch_size)
   G3real = G3pred*20.0
   err = mean_squared_error(testlabel, G3real)
   permutsample[trying, i] = err

print permutsample
#print testparamnorm

#print mean and standard deviation of error
errperformance = np.zeros((len(testparamnorm[0]), 2))
for i in xrange(len(testparamnorm[0])):
 errperformance[i,0] = np.mean(permutsample[:,i])
 errperformance[i,1] = np.std(permutsample[:,i])
errperformance[:,0] = errreal - errperformance[:,0]

print errperformance&lt;/code&gt;&lt;p&gt;代码输出示例：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9efba29f4c31f95bdc6157d69bc19352_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;674&quot; data-rawheight=&quot;274&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9efba29f4c31f95bdc6157d69bc19352&quot; data-watermark-src=&quot;v2-6d2adb237ef461da3799a0ea8fdefa66&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;4、解释结果。我们得到了一些有趣的结果。首先是第二行，从随机输入值结果中得到的误差变化较小。这表明，参数“出行时间”对学生期末考试的成绩根本没有影响。在最后一行（G2）中，我们得到了一个非常高的误差。这说明第二阶段考试的成绩与期末考试成绩高度相关。从这个结果中，我们得到了输入的显著水平，即G2、G1、考试不及格、空闲时间、缺勤、学习时间和上学时间。另一个有趣的结果是学习时间对期末考试的价值没有明显的影响。这个结果非常违反直觉。在现实生活研究中，必须进一步研究。&lt;/p&gt;&lt;p&gt;这就是一种简单的方法来测量神经网络输入的显著水平。该技术可应用于神经网络、支持向量机和随机森林等其他机器学习算法。&lt;/p&gt;&lt;p&gt;来源：https://medium.com/datadriveninvestor/a-simple-way-to-know-how-important-your-input-is-in-neural-network-86cbae0d3689&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗！&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47475888</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>手把手教你用Numpy构建神经网络！（附代码）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47475047.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47475047&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e3996db44efbd7026f85736c5f4937ab_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d0adadbc7e374710cc6f20b574638ffb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d0adadbc7e374710cc6f20b574638ffb&quot; data-watermark-src=&quot;v2-e96895b3f9d106aef40902f9660c108d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Piotr Skalski&lt;/p&gt;&lt;p&gt;本期编译：1+1=3&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289274&amp;amp;idx=1&amp;amp;sn=f40be8372658c2c79fdd47c03d62e037&amp;amp;chksm=802e392fb759b039435fc6700ef5d45142cdfe72234586bd8de9b8dfabcc3264f2ae826def80#rd&quot;&gt;【ML系列】手把手教你用Numpy构建神经网络！（附代码）&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=2020206794&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;7、使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;比如使用Keras，TensorFlow或PyTorch这样的高级框架，我们可以快速构建非常复杂的模型。但是，需要花时间去了解其内部结构并理解基本原理。今天，将尝试利用现有知识，并仅使用Numpy去构建一个完全可操作的神经网络。最后，我们还将测试我们的模型，并将其性能与Keras构建的NN进行比较。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fbc3feb1f21134e08a59b3d75110e69f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;600&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fbc3feb1f21134e08a59b3d75110e69f&quot; data-watermark-src=&quot;v2-a2bdf0d6dba1a06ec247a7cbbf0b8d82&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;准备操作&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们开始编程之前，先准备一个基本的路线图。 我们的目标是创建一个程序，该程序能够创建具有指定体系结构（层的数量和大小以及适当的激活函数）的密集连接的神经网络。 下图给出了这种网络的例子。最重要的是，我们必须能够训练该网络并使用它进行预测。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2971d973dcafd7df30aea49b9cf88d5a_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;585&quot; data-rawheight=&quot;545&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-2971d973dcafd7df30aea49b9cf88d5a_b.jpg&quot;&gt;&lt;p&gt;上图展示了在训练NN期间必须执行的操作。 它还显示了在不同阶段单次迭代，我们需要更新和读取的参数数量。 构建正确的数据结构并巧妙地管理其状态是我们任务中最困难的部分之一。 &lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2fa2d45da50e231cc9d401d7d1b0a8b9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2fa2d45da50e231cc9d401d7d1b0a8b9&quot; data-watermark-src=&quot;v2-5e3c2c7f16f6f2befea95df90c3d1a67&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;创建神经网络层&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从每一层启动权重矩阵W和偏置向量b开始。上标[l]表示当前层的索引（从1开始计数），值n表示给定层中的单元数。 假设描述NN架构的信息将以类似于Snippet 1中所示的列表的形式传递给我们的程序。列表中的每个项目都是描述单个网络层的基本参数的字典：input_dim——作为输入层提供信号矢量的大小，output_dim - 作为输出层获得激活矢量的大小。activation - 在层内使用的激活函数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np

NN_ARCHITECTURE = [
   {&quot;input_dim&quot;: 2, &quot;output_dim&quot;: 25, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 25, &quot;output_dim&quot;: 50, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 50, &quot;output_dim&quot;: 50, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 50, &quot;output_dim&quot;: 25, &quot;activation&quot;: &quot;relu&quot;},
   {&quot;input_dim&quot;: 25, &quot;output_dim&quot;: 1, &quot;activation&quot;: &quot;sigmoid&quot;},
]&lt;/code&gt;&lt;p&gt;最后，让我们关注在这一部分中必须完成的主要任务——层参数的初始化。那些已经看过上面代码并对Numpy有一定经验的人注意到，矩阵W和向量b已经被小的随机数填充了。这种做法并非偶然。权重值不能用相同的数字初始化，因为这会导致破坏对称问题。基本上，如果所有的权值都是一样的，不管输入X是多少，隐藏层中的所有单位也是一样的。在某种程度上，我们陷入了最初的状态，没有任何逃脱的希望，无论训练我们的模型多长时间，我们的网络有多深。线性代数是不会原谅你的。&lt;/p&gt;&lt;p&gt;在第一次迭代中，使用小值可以提高算法的效率。下图中的sigmoid函数，我们可以看到，对于较大的值，它几乎是平的，这对NN的学习速度有显著的影响。总之，使用小随机数进行参数初始化是一种简单的方法，但它保证了我们的算法有足够好的起点。准备好的参数值存储在python字典中，带有唯一标识其父层的键。字典在函数末尾返回，因此我们将在算法的下一个阶段访问它的内容。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def init_layers(nn_architecture, seed = 99):
   # random seed initiation
   np.random.seed(seed)
   # number of layers in our neural network
   number_of_layers = len(nn_architecture)
   # parameters storage initiation
   params_values = {}
   
   # iteration over network layers
   for idx, layer in enumerate(nn_architecture):
       # we number network layers from 1
       layer_idx = idx + 1
       
       # extracting the number of units in layers
       layer_input_size = layer[&quot;input_dim&quot;]
       layer_output_size = layer[&quot;output_dim&quot;]
       
       # initiating the values of the W matrix
       # and vector b for subsequent layers
       params_values[&#39;W&#39; + str(layer_idx)] = np.random.randn(
           layer_output_size, layer_input_size) * 0.1
       params_values[&#39;b&#39; + str(layer_idx)] = np.random.randn(
           layer_output_size, 1) * 0.1
       
   return params_values&lt;/code&gt;&lt;h2&gt;&lt;b&gt;激活函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们将要使用的所有函数中，有一些非常简单但功能强大的函数。激活函数可以写在一行代码中，但是它们提供了他们所需要的神经网络非线性和表现力。&lt;b&gt;“没有它们，我们的神经网络就会变成线性函数的组合，所以它本身就是一个线性函数”&lt;/b&gt;。它有很多激活功能，但在这个项目中，使用其中两种功能——sigmoid和ReLU。为了能够运行一个完整的循环并同时向前和向后传播，我们还需要准备它们的导数。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f60fc77b2f2f43ff83d6539e3ac42275_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;320&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-f60fc77b2f2f43ff83d6539e3ac42275_b.jpg&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def sigmoid(Z):
   return 1/(1+np.exp(-Z))

def relu(Z):
   return np.maximum(0,Z)

def sigmoid_backward(dA, Z):
   sig = sigmoid(Z)
   return dA * sig * (1 - sig)

def relu_backward(dA, Z):
   dZ = np.array(dA, copy = True)
   dZ[Z &amp;lt;= 0] = 0;
   return dZ;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;向前传播&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这部分代码可能是最直观、最容易理解的。给定上一层输入信号，计算仿射变换Z，然后应用选定的激活函数。通过使用Numpy，我们可以利用向量化执行矩阵操作。这样做消除了迭代，大大加快了计算速度。除了计算出的矩阵A，我们的函数还返回中间值Z。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0e917d27c37135252c2978e126c3dee5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;220&quot; data-rawheight=&quot;58&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0e917d27c37135252c2978e126c3dee5&quot; data-watermark-src=&quot;v2-33d02233463183e2351ab7940307fff8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=&quot;relu&quot;):
   Z_curr = np.dot(W_curr, A_prev) + b_curr
   
   if activation is &quot;relu&quot;:
       activation_func = relu
   elif activation is &quot;sigmoid&quot;:
       activation_func = sigmoid
   else:
       raise Exception(&#39;Non-supported activation function&#39;)
       
   return activation_func(Z_curr), Z_curr&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f34aad10434a9609a0118a8509991781_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f34aad10434a9609a0118a8509991781&quot; data-watermark-src=&quot;v2-26426c7a10e3798415cef206644c05c1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;随着single_layer_forward_propagation函数的完成，我们可以轻松地向前构建整个步骤。这是一个稍微复杂一点的函数，它的作用不仅是执行预测，还包含中间值的集合。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def full_forward_propagation(X, params_values, nn_architecture):
   memory = {}
   A_curr = X
   
   for idx, layer in enumerate(nn_architecture):
       layer_idx = idx + 1
       A_prev = A_curr
       
       activ_function_curr = layer[&quot;activation&quot;]
       W_curr = params_values[&quot;W&quot; + str(layer_idx)]
       b_curr = params_values[&quot;b&quot; + str(layer_idx)]
       A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)
       
       memory[&quot;A&quot; + str(idx)] = A_prev
       memory[&quot;Z&quot; + str(layer_idx)] = Z_curr
      
   return A_curr, memory&lt;/code&gt;&lt;h2&gt;&lt;b&gt;损失函数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了监控项目进展并确保我们正在朝着期望的方向前进，我们也应该计算损失函数的值。“一般来说，损失函数是用来表示我们离‘理想的’解决方案还有多远”。它是根据我们计划解决的问题进行选择的，像Keras这样的框架有很多选择。因为我打算测试我们的NN在两个类之间的点的分类，我们决定使用二进制交叉熵，它是由下面的公式定义的。还有，我们还决定实现一个函数来计算我们的准确性。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19363a3c73098c10c2b444742e477beb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;656&quot; data-rawheight=&quot;156&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-19363a3c73098c10c2b444742e477beb&quot; data-watermark-src=&quot;v2-fb8686f3e8098e506c5b0955697b86c5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def get_cost_value(Y_hat, Y):
   m = Y_hat.shape[1]
   cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))
   return np.squeeze(cost)

def convert_prob_into_class(probs):
   probs_ = np.copy(probs)
   probs_[probs_ &amp;gt; 0.5] = 1
   probs_[probs_ &amp;lt;= 0.5] = 0
   return probs_

def get_accuracy_value(Y_hat, Y):
   Y_hat_ = convert_prob_into_class(Y_hat)
   return (Y_hat_ == Y).all(axis=0).mean()&lt;/code&gt;&lt;h2&gt;&lt;b&gt;单层反向传播步骤&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;遗憾的是，许多缺乏经验的深度学习爱好者认为反向传播是一种令人生畏且难以理解的算法。微积分和线性代数的结合常常使那些没有受过扎实的数学训练的人望而却步。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=&quot;relu&quot;):
   m = A_prev.shape[1]
   
   if activation is &quot;relu&quot;:
       backward_activation_func = relu_backward
   elif activation is &quot;sigmoid&quot;:
       backward_activation_func = sigmoid_backward
   else:
       raise Exception(&#39;Non-supported activation function&#39;)
   
   dZ_curr = backward_activation_func(dA_curr, Z_curr)
   dW_curr = np.dot(dZ_curr, A_prev.T) / m
   db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m
   dA_prev = np.dot(W_curr.T, dZ_curr)

   return dA_prev, dW_curr, db_curr&lt;/code&gt;&lt;p&gt;人们常常把反向宣传播与梯度下降混淆，但实际上这是两个独立的问题。第一种方法的目的是有效地计算梯度，而第二种方法是利用计算得到的梯度进行优化。在NN中，我们计算代价函数关于参数的梯度，但是反向传播可以用来计算任何函数的导数。该算法的本质是递归使用微分学中已知的链式法则——计算集合其他函数而得到的函数的导数，我们已经知道这些函数的导数。这个过程对于一个网络层可以用下面的公式来描述。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1cfe28d2da4e0bb672eabd440edc9580_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;562&quot; data-rawheight=&quot;398&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1cfe28d2da4e0bb672eabd440edc9580&quot; data-watermark-src=&quot;v2-f905eff7e0e46b508fa8185843c811fb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6ad0d6a59d04b349a86636044252a487_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;772&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6ad0d6a59d04b349a86636044252a487&quot; data-watermark-src=&quot;v2-140252dd0494cd1927b8dec7d4bf692c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;就像前向传播一样，将计算分为两个独立的函数。第一个侧重于一个单独的层，可以归结为用Numpy重写上面的公式。第二个表示完全反向传播，主要处理在三个字典中读取和更新值的关键值。我们从计算成本函数对正向传播的预测向量结果的导数开始。这很简单，因为它只包括重写下面的公式。然后遍历网络的各个层，从最后开始，根据上图所示的图计算关于所有参数的导数。最后，函数返回一个Python字典，其中包含我们要查找的梯度。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-efe79cb3b79d26219a9b6b041de1edcc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;298&quot; data-rawheight=&quot;74&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-efe79cb3b79d26219a9b6b041de1edcc&quot; data-watermark-src=&quot;v2-a7901c7204da9afccd6fe56ed2d580f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):
   grads_values = {}
   m = Y.shape[1]
   Y = Y.reshape(Y_hat.shape)
  
   dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));
   
   for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):
       layer_idx_curr = layer_idx_prev + 1
       activ_function_curr = layer[&quot;activation&quot;]
       
       dA_curr = dA_prev
       
       A_prev = memory[&quot;A&quot; + str(layer_idx_prev)]
       Z_curr = memory[&quot;Z&quot; + str(layer_idx_curr)]
       W_curr = params_values[&quot;W&quot; + str(layer_idx_curr)]
       b_curr = params_values[&quot;b&quot; + str(layer_idx_curr)]
       
       dA_prev, dW_curr, db_curr = single_layer_backward_propagation(
           dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)
       
       grads_values[&quot;dW&quot; + str(layer_idx_curr)] = dW_curr
       grads_values[&quot;db&quot; + str(layer_idx_curr)] = db_curr
   
   return grads_values&lt;/code&gt;&lt;h2&gt;&lt;b&gt;更新参数值&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;该方法的目标是使用梯度优化更新网络参数。通过这种方式，我们试图使我们的目标函数更接近最小值。为了完成这项任务，我们将使用两个字典作为函数参数：params_values（存储参数的当前值）和grads_values（存储相对于这些参数计算的成本函数导数）。现在你只需要对每一层应用下面的方程。这是一个非常简单的优化算法，我们决定使用它，因为它是更高级优化器的一个很好的起点。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cb4b0f0369b81201f0a2729c1bf58072_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;358&quot; data-rawheight=&quot;94&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-cb4b0f0369b81201f0a2729c1bf58072&quot; data-watermark-src=&quot;v2-95847b50ac2c7cc10a90aae425866a89&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;def update(params_values, grads_values, nn_architecture, learning_rate):
   for idx, layer in enumerate(nn_architecture):
       layer_idx = idx + 1
       params_values[&quot;W&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;dW&quot; + str(layer_idx)]        
       params_values[&quot;b&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;db&quot; + str(layer_idx)]

   return params_values;&lt;/code&gt;&lt;h2&gt;&lt;b&gt;综合讨论&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最难的部分已经在前面全部概述完毕。我们已经准备好了所有必要的功能，现在我们只需要把它们按正确的顺序放在一起。为了进行预测，我们只需要使用接收到的权重矩阵和一组测试数据运行完整的正向传播。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def train(X, Y, nn_architecture, epochs, learning_rate):
   params_values = init_layers(nn_architecture, 2)
   cost_history = []
   accuracy_history = []
   
   for i in range(epochs):
       Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)
       cost = get_cost_value(Y_hat, Y)
       cost_history.append(cost)
       accuracy = get_accuracy_value(Y_hat, Y)
       accuracy_history.append(accuracy)
       
       grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)
       params_values = update(params_values, grads_values, nn_architecture, learning_rate)
       
   return params_values, cost_history, accuracy_history&lt;/code&gt;&lt;h2&gt;&lt;b&gt;与Keras比较&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在是时候看看我们的模型能否解决一个简单的分类问题了。我们生成了一个由两个类的组成的数据集，如下图所示。让我们试着训练我们的模型去分类这个数据集。为了便于比较，还使用Keras编写了一个框架进行比较。两种模型具有相同的体系结构和学习速率。最终，Numpy模型和Keras模型在测试集上的准确率都达到了95%，但是我们的模型需要几十倍的时间才能达到这样的准确率。在我看来，这种状态主要是由于缺乏适当的优化。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;N_SAMPLES = 1000
TEST_SIZE = 0.1

X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)

def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):
   if (dark):
       plt.style.use(&#39;dark_background&#39;)
   else:
       sns.set_style(&quot;whitegrid&quot;)
   plt.figure(figsize=(16,12))
   axes = plt.gca()
   axes.set(xlabel=&quot;$X_1$&quot;, ylabel=&quot;$X_2$&quot;)
   plt.title(plot_name, fontsize=30)
   plt.subplots_adjust(left=0.20)
   plt.subplots_adjust(right=0.80)
   if(XX is not None and YY is not None and preds is not None):
       plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)
       plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=&quot;Greys&quot;, vmin=0, vmax=.6)
   plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors=&#39;black&#39;)
   if(file_name):
       plt.savefig(file_name)
       plt.close()
make_plot(X, y, &quot;Dataset&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bbcb694cad85b9a6dba5bd05273bfe7c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;651&quot; data-rawheight=&quot;633&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bbcb694cad85b9a6dba5bd05273bfe7c&quot; data-watermark-src=&quot;v2-e4c8c6426b0d3613280789dbf607efcc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;NN模型测试&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;# 训练
params_values = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01)
# 预测
Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)

acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))
print(&quot;Test set accuracy: {:.2f}&quot;.format(acc_test))
Test set accuracy: 0.98 &lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8c73432bca699946ef2e702e6af86de4_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;224&quot; data-rawheight=&quot;231&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-8c73432bca699946ef2e702e6af86de4_b.jpg&quot;&gt;&lt;h2&gt;&lt;b&gt;Keras模型测试&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;model = Sequential()
model.add(Dense(25, input_dim=2,activation=&#39;relu&#39;))
model.add(Dense(50, activation=&#39;relu&#39;))
model.add(Dense(50, activation=&#39;relu&#39;))
model.add(Dense(25, activation=&#39;relu&#39;))
model.add(Dense(1, activation=&#39;sigmoid&#39;))

model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&quot;sgd&quot;, metrics=[&#39;accuracy&#39;])

# 训练
history = model.fit(X_train, y_train, epochs=200, verbose=0)

Y_test_hat = model.predict_classes(X_test)
acc_test = accuracy_score(y_test, Y_test_hat)
print(&quot;Test set accuracy: {:.2f} - Goliath&quot;.format(acc_test))
Test set accuracy: 0.98 &lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-496d82d8a82b4190d0f68954dd5613b8_r.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;309&quot; data-rawheight=&quot;311&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-496d82d8a82b4190d0f68954dd5613b8_b.jpg&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗！&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47475047</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>使用LSTM预测股票市场基于Tensorflow</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47473497.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47473497&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8cc95b7ae2a14c2c7311e1798ace8010_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f389a4f62cb51f3ae4c0db4468b7ecfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f389a4f62cb51f3ae4c0db4468b7ecfc&quot; data-watermark-src=&quot;v2-91940980656c229c6cb060d187a770fd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;本期作者：Thushan Ganegedara&lt;/p&gt;&lt;p&gt;本期编辑：1+1=3&lt;/p&gt;&lt;p&gt;原来链接：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289238&amp;amp;idx=1&amp;amp;sn=3144f5792f84455dd53c27a78e8a316c&amp;amp;chksm=802e3903b759b015da88acde4fcbc8547ab3e6acbb5a0897404bbefe1d8a414265d5d5766ee4&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【年度系列】使用LSTM预测股票市场基于Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本文开始前，作者并没有提倡LSTM是一种高度可靠的模型，它可以很好地利用股票数据中的内在模式，或者可以在没有任何人参与的情况下使用。写这篇文章，纯粹是出于对机器学习的热爱。在我看来，该模型已经观察到了数据中的某些模式，因此它可以在大多数时候正确预测股票的走势。但是，这个模型是否可以用于实际，有待用更多回测和实践去验证。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;为什么需要时间序列模型?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你想要正确地模拟股票价格，因此作为股票买家，你可以合理地决定什么时候买股票，什么时候卖股票。这就是时间序列建模的切入点。你需要良好的机器学习模型，可以查看数据序列的历史记录，并正确地预测序列的未来元素是什么。&lt;/p&gt;&lt;p&gt;提示：股市价格高度不可预测且不稳定。这意味着在数据中没有一致的模式可以让你近乎完美地模拟股票价格。就像普林斯顿大学经济学家Burton Malkiel在他1973年的书中写到的：“随机漫步华尔街”，如果市场确实是有效的，那么当股票价格反应的所有因素一旦被公开时，那我们闭着眼睛都可以做的和专业投资者一样好。&lt;/p&gt;&lt;p&gt;但是，我们不要一直相信这只是一个随机过程，觉得机器学习是没有希望的。你不需要预测未来的股票确切的价格，而是股票价格的变动。做到这点就很不错了！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据准备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;使用以下数据源：&lt;/p&gt;&lt;p&gt;地址：https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3ba5be7bb5480c118b1bb0b50e1575f6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;567&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3ba5be7bb5480c118b1bb0b50e1575f6&quot; data-watermark-src=&quot;v2-3de8059f0031a3e442345642c2e844c5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;当然你也可基于Wind数据库去研究。因为Wind数据相对于其他平台和数据商而言，总体上在国内算是比较全面和准确的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从Kaggle获得数据&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在Kaggle上找到的数据是csv文件，所以，你不需要再进行任何的预处理，因此你可以直接将数据加载到DataFrame中。同时你还应该确保数据是按日期排序的，因为数据的顺序在时间序列建模中至关重要。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;df = df.sort_values(&#39;Date&#39;)
df.head()&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c85bc3652efa2f6c80364df245e32d26_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;692&quot; data-rawheight=&quot;362&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c85bc3652efa2f6c80364df245e32d26&quot; data-watermark-src=&quot;v2-5b38149d68e6483f322e0de7759079a2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;数据可视化&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;plt.figure(figsize = (18,9))
plt.plot(range(df.shape[0]),(df[&#39;Low&#39;]+df[&#39;High&#39;])/2.0)
plt.xticks(range(0,df.shape[0],500),df[&#39;Date&#39;].loc[::500],rotation=45)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4ed665620beacacc51f8acf2477cd650_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1064&quot; data-rawheight=&quot;584&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4ed665620beacacc51f8acf2477cd650&quot; data-watermark-src=&quot;v2-789653dddc505399eae48474899df116&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上图已经说明了很多东西。我选择这家公司而不是其他公司的具体原因是，随着时间的推移，这张图中展现了不同的股价行为。这将使学习更加稳健，并且可以更改以便测试各种情况下预测的好坏程度。&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据拆分训练集和测试集&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;计算一天中最高和最低价的平均值来计算的中间价格。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;high_prices = df.loc[:,&#39;High&#39;].as_matrix()
low_prices = df.loc[:,&#39;Low&#39;].as_matrix()
mid_prices = (high_prices+low_prices)/2.0&lt;/code&gt;&lt;p&gt;现在你可以分离训练数据和测试数据。训练数据是时间序列的前11000个数据，其余的是测试数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_data = mid_prices[:11000]
test_data = mid_prices[11000:]&lt;/code&gt;&lt;p&gt;现在需要定义一个标准对数据进行归一化。MinMaxScalar方法将所有数据归到0和1之间。你还可以将训练和测试数据重新组为[data_size, num_features]。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;scaler = MinMaxScaler()
train_data = train_data.reshape(-1,1)
test_data = test_data.reshape(-1,1)&lt;/code&gt;&lt;p&gt;根据之前得数据，可以看出不同时间段有不同的取值范围，你可以将整个序列拆分为窗口来进行归一化。如果不这样做，早期的数据接近于0，并且不会给学习过程增加太多价值。这里你选择的窗口大小是2500。&lt;/p&gt;&lt;p&gt;当选择窗口大小时，确保它不是太小，因为当执行窗口规范化时，它会在每个窗口的末尾引入一个中断，因为每个窗口都是独立规范化的。&lt;/p&gt;&lt;p&gt;在本例中，4个数据点将受此影响。但假设你有11000个数据点，4个点不会引起任何问题。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;smoothing_window_size = 2500
for di in range(0,10000,smoothing_window_size):
   scaler.fit(train_data[di:di+smoothing_window_size,:])
   train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])

# You normalize the last bit of remaining data 
scaler.fit(train_data[di+smoothing_window_size:,:])
train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])&lt;/code&gt;&lt;p&gt;将数据重新塑造为[data_size]的Shape：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_data = train_data.reshape(-1)
test_data = scaler.transform(test_data).reshape(-1)&lt;/code&gt;&lt;p&gt;现在可以使用指数移动平均平滑数据。可以帮助你避免股票价格数据的杂乱，并产生更平滑的曲线。&lt;/p&gt;&lt;p&gt;我们只使用训练数据来训练MinMaxScaler，&lt;b&gt;通过将MinMaxScaler与测试数据进行匹配来规范化测试数据是错误的。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：你应该只平滑训练数据。&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;EMA = 0.0
gamma = 0.1
for ti in range(11000):
 EMA = gamma*train_data[ti] + (1-gamma)*EMA
 train_data[ti] = EMA

all_mid_data = np.concatenate([train_data,test_data],axis=0)&lt;/code&gt;&lt;p&gt;下面是平均结果。它非常接近股票的实际行为。接下来您将看到一个更精确的一步预测方法：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d6807ad52a0984a928ee20d42035b0e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;538&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d6807ad52a0984a928ee20d42035b0e5&quot; data-watermark-src=&quot;v2-a915d2dff38a71304da508f595170f2b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;上面的图（和MSE）说明了什么呢？对于非常短的predictiosn（一天之后）来说，这个模型似乎不算太坏。考虑到股票价格在一夜之间不会从0变化到100，这种行为是明智的。接下来我们来看一种更有趣的平均技术，称为指数移动平均。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;指数移动平均线&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你可能在互联网上看到过一些文章使用非常复杂的模型来预测股票市场的行为。但是要小心！我所看到的这些都只是视觉错觉，不是因为学习了有用的东西。下面你将看到如何使用简单的平均方法复制这种行为。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;window_size = 100
N = train_data.size

run_avg_predictions = []
run_avg_x = []

mse_errors = []

running_mean = 0.0
run_avg_predictions.append(running_mean)

decay = 0.5

for pred_idx in range(1,N):
   
   running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]
   run_avg_predictions.append(running_mean)
   mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)
   run_avg_x.append(date)

print(&#39;MSE error for EMA averaging: %.5f&#39;%(0.5*np.mean(mse_errors)))

MSE error for EMA averaging: 0.00003&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-64d809481eb42a34b13fddf7c90d1d37_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;538&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-64d809481eb42a34b13fddf7c90d1d37&quot; data-watermark-src=&quot;v2-66a21d911e4ca21d6d2c2d47769a9968&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如果指数移动平均线很好，为什么需要更好的模型呢？&lt;/p&gt;&lt;p&gt;可以看到，它符合遵循真实分布的完美直线（通过非常低的MSE证明了这一点）。实际上，仅凭第二天的股票市值，你就做不了什么。就我个人而言，我想要的不是第二天股市的确切价格，而是未来30天股市的价格会上涨还是下跌&lt;/p&gt;&lt;p&gt;让我们试着在窗口中进行预测（假设你预测接下来两天的窗口，而不是第二天）。然后你就会意识到EMA会有多么的失败。让我们通过一个例子来理解这一点。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d3ca79365a08fc602ab3b34c0a6bdfd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;848&quot; data-rawheight=&quot;370&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5d3ca79365a08fc602ab3b34c0a6bdfd&quot; data-watermark-src=&quot;v2-3b2cfd64eb4c384341489cedfb2d39ee&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;不管你预测未来的步骤是多少，你都会得到相同的答案。&lt;/p&gt;&lt;p&gt;输出有用信息的一种解决方案是查看基于动量算法。他们的预测是基于过去的近期值是上升还是下降（而不是精确的数值）。例如，如果过去几天的价格一直在下降，第二天的价格可能会更低。这听起来很合理。然而，我们将使用更复杂的模型：LSTM。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;评价结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们将使用均值平方误差来计算我们的模型有多好。均值平方误差(MSE)的计算方法是先计算真实值与预测值之间的平方误差，然后对所有的预测进行平均。但是：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;平均预测是一种很好的预测方法（这对股票市场的预测不是很有用），但对未来的预测并不是很有用。&lt;/b&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;LSTM简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;长短时记忆模型是非常强大的时间序列模型。它们可以预测未来任意数量的步骤。LSTM模块(或单元)有5个基本组件，可以对长期和短期数据进行建模。&lt;/p&gt;&lt;p&gt;LSTM单元格如下所示：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b468b958243ed3c233a36fe3cf07c519_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;372&quot; data-rawheight=&quot;374&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b468b958243ed3c233a36fe3cf07c519&quot; data-watermark-src=&quot;v2-1af93db67e4b64692505034900e4e131&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;计算方程如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-24c5135b5db25ff4eed06f710156aa96_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;460&quot; data-rawheight=&quot;256&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-24c5135b5db25ff4eed06f710156aa96&quot; data-watermark-src=&quot;v2-af18212fe95ba20c7aa7394a7132c424&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Tensorflow为实现时间序列模型提供了一个很好的子API。后面我们会使用到它。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;LSTM数据生成器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先要实现一个数据生成器来训练LSTM。这个数据生成器将有一个名为&lt;b&gt;unroll_batch(…)&lt;/b&gt;的方法，该方法将输出一组按顺序批量获取num_unrollings的输入数据，其中批数据的大小为[batch_size, 1]。然后每批输入数据都有相应的输出数据。&lt;br&gt;&lt;/p&gt;&lt;p&gt;例如，如果num_unrollings=3和batch_size=4则看起来像一组展开的批次。&lt;/p&gt;&lt;p&gt;输入数据： [x0,x10,x20,x30],[x1,x11,x21,x31],[x2,x12,x22,x32]&lt;/p&gt;&lt;p&gt;输出数据： [x1,x11,x21,x31],[x2,x12,x22,x32],[x3,x13,x23,x33]&lt;/p&gt;&lt;h2&gt;&lt;b&gt;数据生成器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面将演示如何可视化创建一批数据。基本思想是将数据序列划分为N / b段，使每个段的大小为b，然后定义游标每段为1。然后对单个数据进行抽样，我们得到一个输入（当前段游标索引）和一个真实预测（在[当前段游标+1，当前段游标+5]之间随机抽样）。请注意，我们并不总是得到输入旁边的值，就像它的预测一样。这是一个减少过拟合的步骤。在每次抽样结束时，我们将光标增加1。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-06baea34f8a276d4a60233a04c934c90_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;771&quot; data-rawheight=&quot;471&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-06baea34f8a276d4a60233a04c934c90&quot; data-watermark-src=&quot;v2-f5e9d7802d185e1e6ab51fd634cf538b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;定义超参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在本节中，将定义几个超参数。D是输入的维数。很简单，你以之前的股票价格作为输入并预测下一个应该为1。&lt;/p&gt;&lt;p&gt;然后是num_unrollings，它表示单个优化步骤需要考虑多少连续时间步骤。越大越好。&lt;/p&gt;&lt;p&gt;然后是batch_size。批量处理大小是在单个时间步骤中考虑的数据样本的数量。越大越好，因为在给定的时间内数据的可见性越好。&lt;/p&gt;&lt;p&gt;接下来定义num_nodes，它表示每个单元格中隐藏的神经元数量。在这个示例中，你可以看到有三层LSTM。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;D = 1 
num_unrollings = 50 
batch_size = 500 
num_nodes = [200,200,150] 
n_layers = len(num_nodes) 
dropout = 0.2 

tf.reset_default_graph()&lt;/code&gt;&lt;h2&gt;&lt;b&gt;定义输入和输出&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;接下来为训练输入和标签定义占位符。这非常简单，因为你有一个输入占位符列表，其中每个占位符包含一批数据。 该列表包含num_unrollings占位符，它将用于单个优化步骤。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;train_inputs, train_outputs = [],[]
for ui in range(num_unrollings):
   train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name=&#39;train_inputs_%d&#39;%ui))
   train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = &#39;train_outputs_%d&#39;%ui))&lt;/code&gt;&lt;h2&gt;&lt;b&gt;定义LSTM和回归层的参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;用三个LSTM层和一个线性回归层，用w和b表示，该层提取最后一个长短期内存单元的输出并输出对下一个时间步骤的预测。你可以使用TensorFlow中的MultiRNNCell来封装创建的三个LSTMCell对象。此外，还可以使用dropout实现LSTM单元格，因为它们可以提高性能并减少过拟合。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;lstm_cells = [
   tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],
                           state_is_tuple=True,
                           initializer= tf.contrib.layers.xavier_initializer()
                          )
for li in range(n_layers)]

drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(
   lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout
) for lstm in lstm_cells]
drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)
multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)

w = tf.get_variable(&#39;w&#39;,shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())
b = tf.get_variable(&#39;b&#39;,initializer=tf.random_uniform([1],-0.1,0.1))&lt;/code&gt;&lt;h2&gt;&lt;b&gt;计算LSTM输出并将其输入回归层，得到最终预测结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先创建TensorFlow变量（c和h），它将保持单元状态和长短期记忆单元的隐藏状态。 然后将train_input列表转换为[num_unrollings, batch_size, D]，使用tf.nn.dynamic_rnn计算所需输出。然后使用tf.nn.dynamic_rnn计算LSTM输出。并将输出分解为一列num_unrolling的张量。预测和真实股价之间的损失。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;c, h = [],[]
initial_state = []
for li in range(n_layers):
 c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))
 h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))
 initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))

all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)

all_lstm_outputs, state = tf.nn.dynamic_rnn(
   drop_multi_cell, all_inputs, initial_state=tuple(initial_state),
   time_major = True, dtype=tf.float32)

all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])

all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)

split_outputs = tf.split(all_outputs,num_unrollings,axis=0)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;损失计算和优化器&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在，要计算损失。然而，在计算损失时，你应该注意到有一个独特的特征。对于每一批预测和真实输出，计算均方误差。然后把所有这些均方损失加起来（不是平均值）。最后，定义要用来优化神经网络的优化器在这种情况下，您可以使用Adam，这是一个非常新且性能良好的优化器。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print(&#39;Defining training Loss&#39;)
loss = 0.0
with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+
                            [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):
 for ui in range(num_unrollings):
   loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)

print(&#39;Learning rate decay operations&#39;)
global_step = tf.Variable(0, trainable=False)
inc_gstep = tf.assign(global_step,global_step + 1)
tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)
tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)

learning_rate = tf.maximum(
   tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),
   tf_min_learning_rate)

# Optimizer.
print(&#39;TF Optimization operations&#39;)
optimizer = tf.train.AdamOptimizer(learning_rate)
gradients, v = zip(*optimizer.compute_gradients(loss))
gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
optimizer = optimizer.apply_gradients(
   zip(gradients, v))

print(&#39;\tAll done&#39;)&lt;/code&gt;&lt;p&gt;这里定义了与预测相关的TensorFlow操作。首先，为输入（sample_inputs）定义一个占位符，然后与训练阶段类似，定义预测的状态变量（sample_c和sample_h）。最后用tf.nn.dynamic_rnn计算预测。后通过回归层（w和b）发送输出。 还应该定义reset_sample_state操作，该操作将重置单元状态和隐藏状态。 每次进行一系列预测时，都应该在开始时执行此操作。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print(&#39;Defining prediction related TF functions&#39;)

sample_inputs = tf.placeholder(tf.float32, shape=[1,D])

sample_c, sample_h, initial_sample_state = [],[],[]
for li in range(n_layers):
 sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))
 sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))
 initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))

reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],
                              *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])

sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),
                                  initial_state=tuple(initial_sample_state),
                                  time_major = True,
                                  dtype=tf.float32)

with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+
                             [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  
 sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)

print(&#39;\tAll done&#39;)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;运行LSTM&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这里，你将训练和预测几个时期的股票价格走势，看看这些预测是否会随着时间的推移而变得更好或更糟。按照以下步骤操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在时间序列上定义一组测试起点（test_points_seq）来计算LSTM&lt;/li&gt;&lt;li&gt;对于每一个epoch&lt;/li&gt;&lt;ul&gt;&lt;li&gt;用于训练数据的完整序列长度&lt;/li&gt;&lt;ul&gt;&lt;li&gt;展开一组num_unrollings批次&lt;/li&gt;&lt;li&gt;使用展开的批次LSTM进行训练&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;计算平均训练损失&lt;/li&gt;&lt;li&gt;对于测试集中的每个起点&lt;/li&gt;&lt;ul&gt;&lt;li&gt;通过迭代在测试点之前找到的以前的num_unrollings数据点来更新LSTM状态&lt;/li&gt;&lt;li&gt;使用先前的预测作为当前输入，连续预测n_predict_once步骤&lt;/li&gt;&lt;li&gt;计算预测到的n_predict_once点与当时股票价格之间的MSE损失&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;部分代码&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;epochs = 30
valid_summary = 1 
n_predict_once = 50
train_seq_length = train_data.size
train_mse_ot = [] 
test_mse_ot = [] 
predictions_over_time = []
session = tf.InteractiveSession()
tf.global_variables_initializer().run()
loss_nondecrease_count = 0
loss_nondecrease_threshold = 2 

print(&#39;Initialized&#39;)
average_loss = 0
data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings) 
x_axis_seq = []
test_points_seq = np.arange(11000,12000,50).tolist() 

for ep in range(epochs):       
   
   # ========================= Training =====================================
   for step in range(train_seq_length//batch_size):
       
       u_data, u_labels = data_gen.unroll_batches()

       feed_dict = {}
       for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            
           feed_dict[train_inputs[ui]] = dat.reshape(-1,1)
           feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)
       
       feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})

       _, l = session.run([optimizer, loss], feed_dict=feed_dict)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;可视化预测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;可以看到MSE损失是如何随着训练量的减少而减少的。这是一个好迹象，表明模型正在学习一些有用的东西。你可以看到LSTM比标准平均值做得更好。标准平均（虽然不完美）合理地跟随真实的股票价格运动。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;best_prediction_epoch = 28 
plt.figure(figsize = (18,18))
plt.subplot(2,1,1)
plt.plot(range(df.shape[0]),all_mid_data,color=&#39;b&#39;)

predictions with high alpha
start_alpha = 0.25
alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))
for p_i,p in enumerate(predictions_over_time[::3]):
   for xval,yval in zip(x_axis_seq,p):
       plt.plot(xval,yval,color=&#39;r&#39;,alpha=alpha[p_i])

plt.title(&#39;Evolution of Test Predictions Over Time&#39;,fontsize=18)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.xlim(11000,12500)

plt.subplot(2,1,2)

plt.plot(range(df.shape[0]),all_mid_data,color=&#39;b&#39;)
for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):
   plt.plot(xval,yval,color=&#39;r&#39;)
   
plt.title(&#39;Best Test Predictions Over Time&#39;,fontsize=18)
plt.xlabel(&#39;Date&#39;,fontsize=18)
plt.ylabel(&#39;Mid Price&#39;,fontsize=18)
plt.xlim(11000,12500)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-622bf3317cdbb3fd1f5dab2040f38541_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1067&quot; data-rawheight=&quot;1051&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-622bf3317cdbb3fd1f5dab2040f38541&quot; data-watermark-src=&quot;v2-bbb5d3febe3d4c1005d52102564453c4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;尽管LSTM并不完美，但它似乎在大多数情况下都能正确预测股价走势。请注意，你的预测大致在0和1之间（也就是说，不是真实的股票价格）。这是可以的，因为你预测的是股价的走势，而不是股价本身。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结论&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;股票价格/移动预测是一项极其困难的任务。就我个人而言，我认为任何股票预测模型都不应该被视为理所当然，并且盲目地依赖它们。然而，模型在大多数情况下可能能够正确预测股票价格的变动，但并不总是如此。&lt;/p&gt;&lt;p&gt;&lt;b&gt;不要被那些预测曲线完全与真实股价重叠的文章所迷惑。这可以用一个简单的平均技术来复制，但实际上它是无用的。更明智的做法是预测股价走势。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;模型的超参数对你得到的结果非常敏感。因此，一个非常好的事情是在超参数上运行一些&lt;b&gt;超参数优化技术&lt;/b&gt;（例如，网格搜索/随机搜索）。这里列出了一些最关键的超参数：&lt;b&gt;优化器的学习率&lt;/b&gt;、&lt;b&gt;层数&lt;/b&gt;、&lt;b&gt;每层的隐藏单元数&lt;/b&gt;，优化器Adam表现最佳，模型的类型（GRU / LSTM / LSTM with peepholes）。&lt;/p&gt;&lt;p&gt;由于本文由于数据量小，我们用测试损耗来衰减学习速率。这间接地将测试集的信息泄露到训练过程中。处理这个问题更好的方法是有一个单独的验证集（除了测试集）与验证集性能相关的衰减学习率。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47473497</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>股市风起云涌，我用Python分析周期之道</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47472815.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47472815&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a760b103d78fca935c14352aa61472f6_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f389a4f62cb51f3ae4c0db4468b7ecfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f389a4f62cb51f3ae4c0db4468b7ecfc&quot; data-watermark-src=&quot;v2-91940980656c229c6cb060d187a770fd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;原文：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289211&amp;amp;idx=1&amp;amp;sn=d4ce13fa79cb6309f1676ab906136712&amp;amp;chksm=802e38eeb759b1f866b5935a85c010bd2af8c5db7c46a0e577c889b30f3dc9e6c3b268c0336f&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【年度系列】股市风起云涌，我用Python分析周期之道&lt;/a&gt;&lt;/p&gt;&lt;p&gt;本期作者：Yin-Ta Pan&lt;/p&gt;&lt;p&gt;本期编辑：Wally&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;正文&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;股票市场周期是股票市场长期的价格模式，通常与商业周期有关。 它是技术分析的关键，其中投资方法基于周期或重复的价格模式。 如果我们对股市周期有了更好的理解，我们总能以相对低的价格买入并在每个周期以相对较高的价格卖出，将始终获得正的回报。当然，股票市场没有什么策略可以永远赚钱，但我们基于Python，可以帮助我们更深入、快速地了解隐藏在股市中的周期。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;fbprophet简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Fbprophet是Facebook发布的一个开源软件，旨在为大规模预测提供一些有用的指导。 默认情况下，它会将时间序列划分为趋势和季节性，可能包含年度，周度和每日。 但是，分析师可以定义自己的季节性。 为了更好地理解该库，先导文件是非常有用的。&lt;/p&gt;&lt;p&gt;该库的一个特点是简单性、灵活性。 由于我们想要计算的股票市场周期不限于每年，每周或每日，我们应该定义自己的周期，找出哪些更适合数据。 此外，由于周末没有交易，我们不应该使用每周季节性。 我们还可以通过add_seasonality函数定义&#39;self_define_cycle&#39;。 所有设置只需两行代码即可完成。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;m = Prophet(weekly_seasonality=False,yearly_seasonality=False)
m.add_seasonality(&#39;self_define_cycle&#39;,period=8,fourier_order=8,mode=&#39;additive&#39;)&lt;/code&gt;&lt;h2&gt;&lt;b&gt;以Costco为例&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们可以使用Costco标的从2015/10/1到2018/10/1， 使用pandas_datareader，我们可以快速读取股票价格。如下图：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-be5e9470a60f1fbc24f00e9e3d74ea50_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;723&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-be5e9470a60f1fbc24f00e9e3d74ea50&quot; data-watermark-src=&quot;v2-a3b5dd7ba92522c03cf97cf5e5b113f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;地址：&lt;i&gt;https://pandas-datareader.readthedocs.io/en/latest/remote_data.html&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在下图中，我们可以看到从2015年开始有一个强劲的价格增长趋势。然而，在中途仍然存在很多上下周期波动，这些周期都是我们的赚钱点。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;ticker = &quot;COST&quot;
start_date = &#39;2015-10-01&#39;
end_date = &#39;2018-10-01&#39;
stock_data = data.DataReader(ticker, &#39;iex&#39;, start_date, end_date)
stock_data[&#39;close&#39;].plot(figsize=(16,8),color=&#39;#002699&#39;,alpha=0.8)
plt.xlabel(&quot;Date&quot;,fontsize=12,fontweight=&#39;bold&#39;,color=&#39;gray&#39;)
plt.ylabel(&#39;Price&#39;,fontsize=12,fontweight=&#39;bold&#39;,color=&#39;gray&#39;)
plt.title(&quot;Stock price for Costco&quot;,fontsize=18)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ce3fd5df900c5a96eb4ca324cc0655ee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;953&quot; data-rawheight=&quot;503&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ce3fd5df900c5a96eb4ca324cc0655ee&quot; data-watermark-src=&quot;v2-5f7a1275d9ce39784715020d027d92bb&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;对于预测模型，评估它们的一种方法是样本均方误差。 我们可以使用2015/10/1至2018/3/31进行训练，并保留最后6个月的数据进行测试和计算样本均方误差。 在每个周期内，我们可以通过以最低价格买入并以最高价格卖出的方式来优化我们的回报。 为了简化过程，我们使用自定义函数cycle_analysis。 输出是一个列表，其中包含每个周期的预计回报和样本均方误差。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;data：&lt;/b&gt;带有时间索引的Pandas数据&lt;/li&gt;&lt;li&gt;&lt;b&gt;split_date：&lt;/b&gt;分割训练和测试数据的日期&lt;/li&gt;&lt;li&gt;&lt;b&gt;cycle：&lt;/b&gt;每个周期的间隔（天）&lt;/li&gt;&lt;li&gt;&lt;b&gt;mode：&lt;/b&gt;季节性的加法或乘法（可选）&lt;/li&gt;&lt;li&gt;&lt;b&gt;forecast_plot：&lt;/b&gt;是否打印预测图（可选，默认为False）&lt;/li&gt;&lt;li&gt;&lt;b&gt;print_ind：&lt;/b&gt;是否打印每个周期的预计回报和是否采样均方误差（可选，默认为False）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;代码&lt;/b&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def cycle_analysis(data,split_date,cycle,mode=&#39;additive&#39;,forecast_plot = False,print_ind=False):
   training = data[:split_date].iloc[:-1,]
   testing = data[split_date:]
   predict_period = len(pd.date_range(split_date,max(data.index)))
   df = training.reset_index()
   df.columns = [&#39;ds&#39;,&#39;y&#39;]
   m = Prophet(weekly_seasonality=False,yearly_seasonality=False,daily_seasonality=False)
   m.add_seasonality(&#39;self_define_cycle&#39;,period=cycle,fourier_order=8,mode=mode)
   m.fit(df)
   future = m.make_future_dataframe(periods=predict_period)
   forecast = m.predict(future)
   if forecast_plot:
       m.plot(forecast)
       plt.plot(testing.index,testing.values,&#39;.&#39;,color=&#39;#ff3333&#39;,alpha=0.6)
       plt.xlabel(&#39;Date&#39;,fontsize=12,fontweight=&#39;bold&#39;,color=&#39;gray&#39;)
       plt.ylabel(&#39;Price&#39;,fontsize=12,fontweight=&#39;bold&#39;,color=&#39;gray&#39;)
       plt.show()
   ret = max(forecast.self_define_cycle)-min(forecast.self_define_cycle)
   model_tb = forecast[&#39;yhat&#39;]
   model_tb.index = forecast[&#39;ds&#39;].map(lambda x:x.strftime(&quot;%Y-%m-%d&quot;))
   out_tb = pd.concat([testing,model_tb],axis=1)
   out_tb = out_tb[~out_tb.iloc[:,0].isnull()]
   out_tb = out_tb[~out_tb.iloc[:,1].isnull()]
   mse = mean_squared_error(out_tb.iloc[:,0],out_tb.iloc[:,1])
   rep = [ret,mse]
   if print_ind:
       print &quot;Projected return per cycle: {}&quot;.format(round(rep[0],2))
       print &quot;MSE: {}&quot;.format(round(rep[1],4))
   return rep&lt;/code&gt;&lt;p&gt;在下面两个图中，我们将两种不同cycle（30和300）分别应用于Costco股票价格，并将2018/4/1作为训练和测试的分割日期。 正如我们所看到的，如果我们选择一个较短的长度（例如30天），则一个周期内的回报是很小的，我们需要经常进行交易，如果我们选择较长的长度，它会延长我们的预测（例如300天）。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-07c0facb095a22e724c759ba3973f509_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;715&quot; data-rawheight=&quot;427&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-07c0facb095a22e724c759ba3973f509&quot; data-watermark-src=&quot;v2-fc34632f5019eba9898223d5c46ad8ee&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b6258602954336dde1fe9032bc8c7741_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;715&quot; data-rawheight=&quot;427&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b6258602954336dde1fe9032bc8c7741&quot; data-watermark-src=&quot;v2-9581cdf64111eb9a45595fd21330556f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们可以在cycle_analysis函数上应用一个循环来计算不同循环长度的预计回报和样本均方误差，并且我们在下图中显示了结果。正如我们所看到的，长度越长，每个周期的预计回报和样本均方误差会增加。 考虑到交易成本，每个周期内的预计回报应该大于10元。 在这种约束下，我们可以选择最小样本均方误差的周期，并且它是252天。 每个周期的预计回报为17.12元，样本均方误差为15.936。 两者都很不错！&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;testing_box = range(10,301)
return_box = []
mse_box = []
for c in testing_box:
   f = cycle_analysis(stock_data[&#39;close&#39;],&#39;2018-04-01&#39;,c)
   return_box.append(f[0])
   mse_box.append(f[1])&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b301c79e11679b6d920438e5e8d34e30_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;959&quot; data-rawheight=&quot;1046&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b301c79e11679b6d920438e5e8d34e30&quot; data-watermark-src=&quot;v2-d300406f90f65973a912792fa7f67410&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;report = pd.DataFrame({&#39;cycle&#39;:testing_box,&#39;return&#39;:return_box,&#39;mse&#39;:mse_box})
possible_choice = report[report[&#39;return&#39;] &amp;gt;10]
possible_choice[possible_choice[&#39;mse&#39;]==min(possible_choice[&#39;mse&#39;])]&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a51146cbc5ae8f9a6c06b0d6906236e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;460&quot; data-rawheight=&quot;136&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5a51146cbc5ae8f9a6c06b0d6906236e&quot; data-watermark-src=&quot;v2-c6f13aecaed98bc43f97301be3115660&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;c = possible_choice[possible_choice[&#39;mse&#39;]==min(possible_choice[&#39;mse&#39;])][&#39;cycle&#39;].values[0]
ycle_analysis(stock_data[&#39;close&#39;],&#39;2018-04-01&#39;,c,forecast_plot=True,print_ind=True)&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-867ea5176c3f50e41fd7d8aea170cb70_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;715&quot; data-rawheight=&quot;427&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-867ea5176c3f50e41fd7d8aea170cb70&quot; data-watermark-src=&quot;v2-a30ae85d490266869862caa1db33a7f7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;Projected return per cycle: 17.12
MSE: 15.9358
[17.120216439034987, 15.93576020351612]&lt;/code&gt;&lt;p&gt;为了进一步说明投资策略，我们可以看到2015/10/1和2018/10/1之间的买入和卖出日期。 Return_Dates函数可以将所有买入和卖出日期作为输出返回，输入：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;forecast：&lt;/b&gt;fbprophet预测对象&lt;/li&gt;&lt;li&gt;&lt;b&gt;stock_data：&lt;/b&gt;带有时间索引的Pandas数据&lt;/li&gt;&lt;li&gt;&lt;b&gt;cycle：&lt;/b&gt;周期长度&lt;/li&gt;&lt;li&gt;&lt;b&gt;cycle_name：&lt;/b&gt;预测对象中循环列的名称&lt;/li&gt;&lt;li&gt;&lt;b&gt;time_name：&lt;/b&gt;预测对象中时间列的名称&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;def Return_Dates(forecast,stock_data,cycle,cycle_name = &#39;self_define_cycle&#39;,time_name = &#39;ds&#39;):
   # find out the highest and lowest dates in the first cycle 
   # We cannot simply search for all highest and lowest point since there is slightly difference for high and low values in different cycles
   high = forecast.iloc[:cycle,]
   high = high[high[cycle_name]==max(high[cycle_name])][time_name]
   high = datetime.strptime(str(high.values[0])[:10],&quot;%Y-%m-%d&quot;)
   low = forecast.iloc[:cycle,]
   low = low[low[cycle_name]==min(low[cycle_name])][time_name]
   low = datetime.strptime(str(low.values[0])[:10],&quot;%Y-%m-%d&quot;)
   end_dt = datetime.strptime(stock_data.index[-1],&quot;%Y-%m-%d&quot;)
   find_list = stock_data.index.map(lambda x:datetime.strptime(x,&quot;%Y-%m-%d&quot;))
   # Finding selling and buying dates with loop
   sell_dt = []
   sell_dt.append(high)
   # Looking for new cycle until it goes beyond the last date in stock_data
   while high&amp;lt;end_dt:
       high = high+timedelta(days=cycle)
       dif = (find_list-high).days
       high = find_list[abs(dif)==min(abs(dif))][0] # In order to avoid the non-trading dates
       sell_dt.append(high)
   buy_dt = []
   buy_dt.append(low)
   # Looking for new cycle until it goes beyond the last date in stock_data
   while low&amp;lt;end_dt:
       low = low+timedelta(days=cycle)
       dif = (find_list-low).days
       low = find_list[abs(dif)==min(abs(dif))][0] # In order to avoid the non-trading dates
       buy_dt.append(low)
   if buy_dt[0] &amp;gt; sell_dt[0]:
       sell_dt = sell_dt[1:]
   buy_dt = buy_dt[:-1]
   sell_dt = sell_dt[:-1]
   return [buy_dt,sell_dt]&lt;/code&gt;&lt;p&gt;在2015/10/1和2018/10/1期间，我们买卖Costco四次。3年内的回报率为23.2％。 可能不是很吸引人，但至少它是比较乐观的回报。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2bba1057151a13b4faf5624080fe1c63_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;338&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2bba1057151a13b4faf5624080fe1c63&quot; data-watermark-src=&quot;v2-b880a105af9c9af392201e890ab921f8&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;更多股票的应用&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当然，这种方法可以应用于尽可能多的股票。 我们列出了Costco，Apple，Microsoft，Home Depot和Nike的平均购买价格，平均销售价格，周期长度，样本均方误差，购买数量，销售数量和每个周期内的预计回报。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;Analysis_ticks = [&#39;COST&#39;,&#39;AAPL&#39;,&#39;MSFT&#39;,&#39;HD&#39;,&#39;NKE&#39;]
start_date = &#39;2015-10-01&#39;
end_date = &#39;2018-10-01&#39;
opt_cycle = []
prot_return = []
MSE = []
buy_times = []
sell_times = []
avg_buy_price = []
avg_sell_price = []
# Loop over each stock
for ticker in Analysis_ticks:
   stock_data = data.DataReader(ticker, &#39;iex&#39;, start_date, end_date)
   testing_box = range(50,301)
   return_box = []
   mse_box = []
   for cc in testing_box:
       f = cycle_analysis(stock_data[&#39;close&#39;],&#39;2018-04-01&#39;,cc)
       return_box.append(f[0])
       mse_box.append(f[1])
   report = pd.DataFrame({&#39;cycle&#39;:testing_box,&#39;return&#39;:return_box,&#39;mse&#39;:mse_box})
   possible_choice = report[report[&#39;return&#39;] &amp;gt;10]
   # If we cannot find a cycle with return greater than 10, give 0
   if possible_choice.shape[0]&amp;gt;0:
       c = possible_choice[possible_choice[&#39;mse&#39;]==min(possible_choice[&#39;mse&#39;])][&#39;cycle&#39;].values[0]
       rp = possible_choice[possible_choice[&#39;mse&#39;]==min(possible_choice[&#39;mse&#39;])][&#39;return&#39;].values[0]
       mse = possible_choice[possible_choice[&#39;mse&#39;]==min(possible_choice[&#39;mse&#39;])][&#39;mse&#39;].values[0]
       df = stock_data[:&#39;2018-04-01&#39;].iloc[:-1,][&#39;close&#39;].reset_index()
       df.columns = [&#39;ds&#39;,&#39;y&#39;]
       predict_period = len(pd.date_range(&#39;2018-04-01&#39;,&#39;2018-10-01&#39;))
       m = Prophet(weekly_seasonality=False,yearly_seasonality=False,daily_seasonality=False)
       m.add_seasonality(&#39;self_define_cycle&#39;,period=c,fourier_order=8,mode=&#39;additive&#39;)
       m.fit(df)
       future = m.make_future_dataframe(periods=predict_period)
       forecast = m.predict(future)
       dt_list = Return_Dates(forecast,stock_data,c)
       buy_price = stock_data.loc[map(lambda x: x.strftime(&quot;%Y-%m-%d&quot;),dt_list[0])][&#39;close&#39;]
       sell_price = stock_data.loc[map(lambda x: x.strftime(&quot;%Y-%m-%d&quot;),dt_list[1])][&#39;close&#39;]
       bt = buy_price.shape[0]
       st = sell_price.shape[0]
       bp = np.mean(buy_price)
       sp = np.mean(sell_price)
   else:
       c = 0
       rp = 0
       mse = 0
       bt = 0
       st = 0
       bp = 0
       sp = 0
   opt_cycle.append(c)
   prot_return.append(rp)
   MSE.append(mse)
   buy_times.append(bt)
   sell_times.append(st)
   avg_buy_price.append(bp)
   avg_sell_price.append(sp)
   print &quot;{} Finished&quot;.format(ticker)&lt;/code&gt;&lt;p&gt;对于微软和耐克，我们找不到符合我们要求每个周期超过10元回报的周期。 对于Costco，Apple和Home Depot，我们可以找到大约250天的周期，并做出良好的预测和良好的回报。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;stock_report = pd.DataFrame({&#39;Stock&#39;:Analysis_ticks,&#39;Cycle&#39;:opt_cycle,&#39;Projected_Return_per_Cycle&#39;:prot_return,
                           &#39;MSE&#39;:MSE,&#39;Num_of_Buy&#39;:buy_times,&#39;Num_of_Sell&#39;:sell_times,
                           &#39;Average_Buy_Price&#39;:avg_buy_price,&#39;Average_Sell_Price&#39;:avg_sell_price})
stock_report&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1a21cd95b8a7ef1422a44d1c8d89c525_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;155&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;借助Python和fbprophet包，我们可以更好地了解股市。 根据我们发现的周期，我们可以在3年内获得大约23％的回报。 也许这种投资策略无法满足所有人的需求，但你始终可以根据自己的知识和经验设定自己的方法。 强大的fbprophet软件包可以让你对股票市场的分析更加深入和轻松。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47472815</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【精选】卡尔曼滤波及其在配对交易中的应用</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47471981.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47471981&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-79b42484586f17c15d0d9b30cbec73b4_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f389a4f62cb51f3ae4c0db4468b7ecfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f389a4f62cb51f3ae4c0db4468b7ecfc&quot; data-watermark-src=&quot;v2-91940980656c229c6cb060d187a770fd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;来源：人工智能与量化交易&lt;/p&gt;&lt;p&gt;原文：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289194&amp;amp;idx=1&amp;amp;sn=bab6eb5cb22272e8b11335548b9b813f&amp;amp;chksm=802e38ffb759b1e9b447b7a8fd83617b77a2eab21f88d7fc7fed7eb3f67d51f0824f99e3ace0&amp;amp;token=680616212&amp;amp;lang=zh_CN#rd&quot;&gt;【精选】卡尔曼滤波及其在配对交易中的应用&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前沿&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;听过卡尔曼滤波的差不多有两年的时间了，虽然大致上明白其原理，但是也是直到现在才能够彻底掌握下来。主要是卡尔曼滤波算法涉及到比较复杂的数学公式推导。在很多博客上都有写卡尔曼滤波的相关文章，但都是花非常大的篇幅来通过一些例子来通俗地讲解卡尔曼滤波，对于不知道其数学原理的读者来说，看完之后依然是一知半解。&lt;/p&gt;&lt;p&gt;本文会先讲解最简单的单变量卡尔曼滤波，让大家知道卡尔曼滤波大致是什么样的，然后再详细地给出公式的推导过程，最后展示卡尔曼滤波在配对交易中的应用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;卡尔曼滤波&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;卡尔曼滤波(Kalman filtering)一种利用线性系统状态方程，通过系统输入输出观测数据，对系统状态进行最优估计的算法。由于观测数据中包括系统中的噪声和干扰的影响，所以最优估计也可看作是滤波过程。&lt;/p&gt;&lt;p&gt;最简单的单变量卡尔曼滤波，可以认为，我们观测的时间序列是存在噪声的，而我们可以通过卡尔曼滤波，过滤掉噪声，而得到了去除噪声之后的状态序列。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5e03720d025f04c3df03b6c4402bd5db_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;372&quot; data-rawheight=&quot;243&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5e03720d025f04c3df03b6c4402bd5db&quot; data-watermark-src=&quot;v2-8c46b3a401a9f3185e669bf5ab512d0c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21cdfaade0f9d4ec0d6ff9e4abb10f8f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;318&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-21cdfaade0f9d4ec0d6ff9e4abb10f8f&quot; data-watermark-src=&quot;v2-6450abe900b92da71042a7c081da5c84&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-da545d25389142a9710f3078c7687a38_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;373&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-da545d25389142a9710f3078c7687a38&quot; data-watermark-src=&quot;v2-60aa43691efadccd43d23585d5efb2ec&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a9a6e1d6a75c45c07347e8a292d0e5ba_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;741&quot; data-rawheight=&quot;484&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a9a6e1d6a75c45c07347e8a292d0e5ba&quot; data-watermark-src=&quot;v2-fa4d68df336752b198bae4415ba00c0c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-979e02aef2d5e953b7823c6d8e7b8a90_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;736&quot; data-rawheight=&quot;363&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-979e02aef2d5e953b7823c6d8e7b8a90&quot; data-watermark-src=&quot;v2-13db4d00eda4512e6c13d47623150e9e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ab77e1d0cbf30bc41789041bdefff492_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;734&quot; data-rawheight=&quot;329&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ab77e1d0cbf30bc41789041bdefff492&quot; data-watermark-src=&quot;v2-eaa269c2fff556f5e0b7f303aff963ef&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bf5c034c8dfec167eb4141b188701105_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;739&quot; data-rawheight=&quot;326&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bf5c034c8dfec167eb4141b188701105&quot; data-watermark-src=&quot;v2-55b61b967b93eb04e355b649a85cd0e0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0ffad80375c223dd2cdaf31461e34049_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;727&quot; data-rawheight=&quot;503&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0ffad80375c223dd2cdaf31461e34049&quot; data-watermark-src=&quot;v2-dbaed2336a998ce6d88ce6c76e551f93&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7e0e22167e7e43ce364394eedf9e1714_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;738&quot; data-rawheight=&quot;487&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7e0e22167e7e43ce364394eedf9e1714&quot; data-watermark-src=&quot;v2-db6448c21aaf2312459184c798b995fc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4b24d979c6eb446e25b42cbb3b6c2aca_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;329&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4b24d979c6eb446e25b42cbb3b6c2aca&quot; data-watermark-src=&quot;v2-0b156df51a0c695514984f386ca7ac86&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d938450512745cb15602b198aca34978_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;742&quot; data-rawheight=&quot;537&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d938450512745cb15602b198aca34978&quot; data-watermark-src=&quot;v2-0bf8605943b446e5e4afd389b88dade3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ffa29391e67f5e2932a242ff003e9763_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;730&quot; data-rawheight=&quot;341&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ffa29391e67f5e2932a242ff003e9763&quot; data-watermark-src=&quot;v2-3e4a2d77d87ec65c7b8f9f30ba6481f4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-69a1db86f4ab18a7148a2db0b91d3548_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;745&quot; data-rawheight=&quot;379&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-69a1db86f4ab18a7148a2db0b91d3548&quot; data-watermark-src=&quot;v2-655175c57c4c59ba8cf6a15217d20655&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-dfc2ba26c71fdf44809da097d8d93faf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;734&quot; data-rawheight=&quot;265&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-dfc2ba26c71fdf44809da097d8d93faf&quot; data-watermark-src=&quot;v2-b068366fbe23dc09e203085a7a9ee949&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7c2357cf02d76b165087cb5dce3992f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;739&quot; data-rawheight=&quot;423&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7c2357cf02d76b165087cb5dce3992f9&quot; data-watermark-src=&quot;v2-5b694b53a025459d9cc815efdcabe162&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1bc5b317f6bd3ee872ecea67abf6ccf1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;439&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1bc5b317f6bd3ee872ecea67abf6ccf1&quot; data-watermark-src=&quot;v2-8dbf00aad691cb361d65e63f57072074&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c13c36c9201a4bcbb6fa349247f084e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;170&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c13c36c9201a4bcbb6fa349247f084e7&quot; data-watermark-src=&quot;v2-d36ff040c96d82689a4b93b81dc97988&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;卡尔曼滤波在配对交易的应用&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;关于什么配对交易，什么是统计套利中的协整，知乎上有非常好的回答，在这里我们只讨论卡尔曼滤波在配对交易中的应用。&lt;/p&gt;&lt;p&gt;在配对交易中，我们构造了如下回归方程&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-cd16031f9176460bb6d3b1011abedb1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;108&quot; data-rawheight=&quot;24&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;然后利用该方程在样本外进行套利。那么，假如我们这里的a和B是会改变的，那么我们如何动态地去调整回归方程的系数？我们可以使用如下滤波的方式。&lt;/p&gt;&lt;p&gt;建立观测方程&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b7440764e902d93bd455651052956dc9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;134&quot; data-rawheight=&quot;51&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;建立状态方程&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-67c9f84e23f22de0c1833cc707a38d30_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;114&quot; data-rawheight=&quot;51&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们需要估计的状态为&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b14ec9378392899b2eee8b1629957d28_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;34&quot; data-rawheight=&quot;51&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下面以焦炭和螺纹为例，采用焦炭和螺纹主力连续合约的收盘价数据：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fa28d0956ed52ba5a9830ae595727080_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;276&quot; data-rawheight=&quot;204&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fa28d0956ed52ba5a9830ae595727080&quot; data-watermark-src=&quot;v2-791d27406dd0ac1389a7a7fdd025228f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-49aee8882a8b4d19af90ba8dfdbc664a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;484&quot; data-rawheight=&quot;328&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-49aee8882a8b4d19af90ba8dfdbc664a&quot; data-watermark-src=&quot;v2-3712f723db86c33a67412dbde9878dc9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# 以焦炭的收盘价数据作为x，螺纹的收盘价数据作为y
# 螺纹价格 = alpha + beta * 焦炭价格 + 随机误差 
from pykalman import KalmanFilter

#建立观测矩阵
observation_matrices = np.vstack(( np.ones(len(df[:&#39;2013&#39;])),
                                  df.loc[:&#39;2013&#39;,&#39;焦炭&#39;].values )).T
Shape = observation_matrices.shape
observation_matrices = observation_matrices.reshape(Shape[0],1,Shape[1])

#定义卡尔曼滤波的方程
kf = KalmanFilter(transition_matrices=np.array([[1,0],[0,1]]), #转移矩阵为单位阵
                  observation_matrices=observation_matrices)
np.random.seed(0)

# 使用2013年以前的数据，采用EM算法，估计出初始状态，
# 初始状态的协方差，观测方程和状态方程误差的协方差
kf.em(df.loc[:&#39;2013&#39;,&#39;螺纹&#39;])


#对2013年的数据做滤波
filter_mean,filter_cov = kf.filter(df.loc[:&#39;2013&#39;,&#39;螺纹&#39;])#观测值为螺纹

#从2014年开始滚动
start_index = np.where(df.index.year==2014)[0][0]

for i in range(start_index,len(df)):
    observation_matrix = np.array([[1,df[&#39;焦炭&#39;].values[i]]])
    observation = df[&#39;螺纹&#39;].values[i]

    #以上一个时刻的状态，状态的协方差以及当前的观测值，得到当前状态的估计
    next_filter_mean,next_filter_cov = kf.filter_update(
            filtered_state_mean = filter_mean[-1],
            filtered_state_covariance = filter_cov[-1],
            observation = observation,
            observation_matrix = observation_matrix)

    filter_mean = np.vstack((filter_mean,next_filter_mean))
    filter_cov = np.vstack((filter_cov,next_filter_cov.reshape(1,2,2)))

#得到alpha和beta
alpha = pd.Series(filter_mean[start_index:,0], index = df.index[start_index:])
beta = pd.Series(filter_mean[start_index:,1], index = df.index[start_index:])&lt;/code&gt;&lt;p&gt;得到alpha和beta的值如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0527c4fa6584115cb5e6d13bf62454e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;479&quot; data-rawheight=&quot;352&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0527c4fa6584115cb5e6d13bf62454e5&quot; data-watermark-src=&quot;v2-2d073636c05abe1a7bf33b15102705a5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47471981</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>从Seq2seq到Attention模型到Self Attention（二）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-23-47470866.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47470866&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e44677ecf27711a9ef591182543488c1_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;传送门：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289147&amp;amp;idx=1&amp;amp;sn=09f954d3e4d74c102ef47a930b0f565b&amp;amp;chksm=802e38aeb759b1b89dd53aa556ca5ea7844c717376ac794f05489aae3d69c5997155775c49c3&amp;amp;token=1768501699&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;从Seq2seq到Attention模型到Self Attention（一）&lt;/a&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f389a4f62cb51f3ae4c0db4468b7ecfc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f389a4f62cb51f3ae4c0db4468b7ecfc&quot; data-watermark-src=&quot;v2-91940980656c229c6cb060d187a770fd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;作者：Bgg&lt;/p&gt;&lt;p&gt;系列一介绍了Seq2seq和 Attention model。这篇文章将重点摆在Google於2017年发表论文“Attention is all you need”中提出的 “”The transformer模型。”The transformer”模型中主要的概念有2项：1. Self attention 2. Multi-head，此外，模型更解决了传统attention model中无法平行化的缺点，并带来优异的成效。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;系列一中，我们学到attention model是如何运作的，缺点就是不能平行化，且忽略了输入句中文字间和目标句中文字间的关係。&lt;/p&gt;&lt;p&gt;为了解决此问题，2017年，Self attention诞生了。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-64ce31e18fba67bf495ac8ef79f8d5b9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;491&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-64ce31e18fba67bf495ac8ef79f8d5b9&quot; data-watermark-src=&quot;v2-45499f507ceca0ad33e80212f6c3cb1d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Self Attention&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Self attention是Google在 “Attention is all you need”论文中提出的”The transformer”模型中主要的概念之一，我们可以把”The transformer”想成是个黑盒子，将输入句输入这个黑盒子，就会產生目标句。&lt;/p&gt;&lt;p&gt;最特别的地方是，”The transformer”完全捨弃了RNN、CNN的架构。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2d481af91772b7698c1ace8e1c9fc19c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;285&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2d481af91772b7698c1ace8e1c9fc19c&quot; data-watermark-src=&quot;v2-a4acb707917fe55816a454c62c610d5f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;The transformer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;“The transformer”和Seq2seq模型皆包含两部分：Encoder和Decoder。比较特别的是，”The transformer”中的Encoder是由6个Encoder堆积而成(paper当中N=6)，Deocder亦然，这和过去的attention model只使用一个encoder/decoder是不同的。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-06b562023475450e8d058ec51d5f9987_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;563&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-06b562023475450e8d058ec51d5f9987&quot; data-watermark-src=&quot;v2-cdc2f3fc02385a98b9a46986d40576d3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Query, Key, Value&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;进入”The transformer”前，我们重新复习attention model，attention model是从输入句&amp;lt;X1,X2,X3…Xm&amp;gt;產生h1,h2,h….hm的hidden state，透过attention score α 乘上input 的序列加权求和得到Context vector c_{i}，有了context vector和hidden state vector，便可计算目标句&amp;lt;y1…yn&amp;gt;。换言之，就是将输入句作为input而目标句作为output。&lt;/p&gt;&lt;p&gt;如果用另一种说法重新詮释：&lt;/p&gt;&lt;p&gt;输入句中的每个文字是由一系列成对的 &amp;lt;地址Key, 元素Value&amp;gt;所构成，而目标中的每个文字是Query，那麼就可以用Key, Value, Query去重新解释如何计算context vector，透过计算Query和各个Key的相似性，得到每个Key对应Value的权重係数，权重係数代表讯息的重要性，亦即attention score；Value则是对应的讯息，再对Value进行加权求和，得到最终的Attention/context vector。&lt;/p&gt;&lt;p&gt;笔者认为这概念非常创新，特别是从attention model到”The transformer”间，鲜少有论文解释这种想法是如何连结的，间接导致”attention is all you need”这篇论文难以入门，有兴趣可以参考key、value的起源论文 Key-Value Memory Networks for Directly Reading Documents。&lt;/p&gt;&lt;p&gt;在NLP的领域中，Key, Value通常就是指向同一个文字隐向量(word embedding vector)。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3158c35dea23bd8c6751ab8f161055b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;532&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c3158c35dea23bd8c6751ab8f161055b&quot; data-watermark-src=&quot;v2-e98a6d20b3bb090646b1f467e424d83e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;有了Key, Value, Query的概念，我们可以将attention model中的Decoder公式重新改写。1. score e_{ij}= Similarity(Query, Key_{i})，上一篇有提到3种计算权重的方式，而我们选择用内积。2. 有了Similarity(Query, Key_{i})，便可以透过softmax算出Softmax(sim_{i})=a_{i}，接著就可以透过attention score a_{i}乘上Value_{i}的序列和加总所得 = Attention(Query, Source)，也就是context/attention vector。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-85ca78bd2c1b782aa31955f6b40684b2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;509&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-85ca78bd2c1b782aa31955f6b40684b2&quot; data-watermark-src=&quot;v2-5a87a396dd37dbac285e0dec512bd3a9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在了解Key, Value, Query的概念后，我们可以进入”the transformer”的世界了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Scaled Dot-Product Attention&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果仔细观察，其实“The transformer”计算 attention score的方法和attention model如出一辙，但”The transformer”还要除上分母=根号d_{k}，目的是避免内积过大时，softmax產出的结果非0即1。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0711b287eac158fbfb24c9a9911c5eaf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;470&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0711b287eac158fbfb24c9a9911c5eaf&quot; data-watermark-src=&quot;v2-48d5c89102a38d7c53e6fded06e399c0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Three kinds of Attention&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;“The transformer”在计算attention的方式有三种，1. encoder self attention，存在於encoder间. 2. decoder self attention，存在於decoder间，3. encoder-decoder attention, 这种attention算法和过去的attention model相似。&lt;/p&gt;&lt;p&gt;接下来我们透过encoder和decoder两部份，来分别介绍encoder/decoder self attention。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1a5122d57c761a69b608322831f5d20f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;559&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1a5122d57c761a69b608322831f5d20f&quot; data-watermark-src=&quot;v2-ff4f81601c0191724016f56281ce54f6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Encoder&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们将”The transformer”模型分为左右两部分，左边是Encoder，如前述，”Attention is all you need”当中N=6，代表Encoder部分是由6个encoder堆积而成的。其中在计算encoder self attention时，更透过multi-head的方式去学习不同空间的特徵，在后续内容会探讨multi-head的部分。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cb933d6e8282c17d9ebcccb52b90b79f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;541&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-cb933d6e8282c17d9ebcccb52b90b79f&quot; data-watermark-src=&quot;v2-6b8541cbc24a1acecc808ac13e1bb145&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;如何计算encoder self attention?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们先用微观的角度来观察Attention(q_{t}, K, V)，也就是输入句中的某个文字，再将所有输入句中的文字一次用矩阵Attention(Q,K,V)来解决。&lt;/p&gt;&lt;p&gt;第一步是创造三个encoder的输入向量Q,K,V，举例来说，“Are you very big?”中的每一个字的隐向量都有各自的Q,K,V，接著我们会乘上一个初始化矩阵，论文中输出维度d_{model}=512。&lt;/p&gt;&lt;p&gt;第二步是透过内积来计算score &amp;lt;q_{t}, k_{s}&amp;gt;，类似attention model 中的score e_{ij}。假设我们在计算第一个字”Are”的self-attention，我们可能会将输入句中的每个文字”Are”, ”you”, ‘very’, ‘big’分别和”Are”去做比较，这个分数决定了我们在encode某个特定位置的文字时，应该给予多少注意力(attention)。所以当我们在计算#位置1的self-attention，第一个分数是q1、k1的内积 (“Are vs Are”)，第二个分数则是q1、k2 (“Are vs you”)，以此类推。&lt;/p&gt;&lt;p&gt;第三步是将算出的分数除以根号d_{k}，论文当中假定d_{k}=64，接著传递至exponential函数中并乘上1/Z，其实这结果就是attention/softmax score，我们可以把1/Z看成是softmax时，所除上的exponential总和，最终的总分数就是attention score，代表我们应该放多少注意力在这个位置上，也就是attention model的概念，有趣的是，怎麼算一定都会发现自己位置上的分数永远最高，但有时候可以发现和其他位置的文字是有关联的。&lt;/p&gt;&lt;p&gt;最后一步就是把attention score再乘上value，然后加总得到attention vector(z_{I})，这就是#位置1的attention vector z1，概念都和以往的attention model类似。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-53a6ad48339b51ac3f4f29433ea23161_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;517&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-53a6ad48339b51ac3f4f29433ea23161&quot; data-watermark-src=&quot;v2-4d1e4b23607433517d28b9e485a66986&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;以上就是self-attention的计算，算出来的向量我们可以往前传递至feed-forward neural network，实际的运作上，是直接将每个文字同时处理，因此会变成一个矩阵，而非单一词向量，计算后的结果attention vector也会变成attention matrix Z。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1adb9286b5038665ce5eaa5324ac2a6f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;574&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1adb9286b5038665ce5eaa5324ac2a6f&quot; data-watermark-src=&quot;v2-56d6c60cfccdf584bdb4a0ef05b0eae3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Multi-head attention&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;有趣的是，如果我们只计算一个attention，很难捕捉输入句中所有空间的讯息，为了优化模型，论文当中提出了一个新颖的做法：Multi-head attention，概念是不要只用d_{model}维度的key, value, query们做单一个attention，而是把key, value, query们线性投射到不同空间h次，分别变成维度d_{q}, d_{k} and d_{v}，再各自做attention，其中，d_{k}=d_{v}=d_{model}/h=64，概念就是投射到h个head上。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-87a9f744f97169c19a2d75c6aafd17b6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;450&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-87a9f744f97169c19a2d75c6aafd17b6&quot; data-watermark-src=&quot;v2-4ae4dc579babad5e4f7aff2e5ac37fcd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;此外，”The transformer”用了8个attention head，所以我们会產生8组encoder/decoder，每一组都代表将输入文字的隐向量投射到不同空间，如果我们重复计算刚刚所讲的self-attention，我们就会得到8个不同的矩阵Z，可是呢，feed-forward layer期望的是一个矩阵而非8个，所以我们要把这8个矩阵併在一起，透过乘上一个权重矩阵，还原成一个矩阵Z。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ade09169d813e5ac7ca4b232a5465b55_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;518&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ade09169d813e5ac7ca4b232a5465b55&quot; data-watermark-src=&quot;v2-009dba8c7218aee2af35fc59d07c7932&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Residual Connections&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Encoder还有一个特别的架构，Multihead-attention完再接到feed-forward layer中间，还有一个sub-layer，会需要经过residual connection和layer normalization。&lt;/p&gt;&lt;p&gt;Residual connection 就是构建一种新的残差结构，将输出改写成和输入的残差，使得模型在训练时，微小的变化可以被注意到，这种架构很常用在电脑视觉(computer vision)，有兴趣可以参考神人Kaiming He的Deep Residual Learning for Image Recognition。&lt;/p&gt;&lt;p&gt;Layer normalization则是在深度学习领域中，其中一种正规化方法，最常和batch normalization进行比较，layer normalization的优点在於它是独立计算的，也就是针对单一样本进行正规化，batch normalization则是针对各维度，因此和batch size有所关联，可以参考layer normalization。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9af7d37dbe3e54d9f82bad749166b96e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;518&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9af7d37dbe3e54d9f82bad749166b96e&quot; data-watermark-src=&quot;v2-8c9e1320f67fde9fbca8c687d23e0ce3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Position-wise Feed-Forward Networks&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Encoder/Decoder中的attention sublayers都会接到一层feed-forward networks(FFN)：两层线性转换和一个RELU，论文中是根据各个位置(输入句中的每个文字)分别做FFN，举例来说，如果输入文字是&amp;lt;x1,x2…xm&amp;gt;，代表文字共有m个。&lt;/p&gt;&lt;p&gt;其中，每个位置进行相同的线性转换，这边使用的是convolution1D，也就是kernel size=1，原因是convolution1D才能保持位置的完整性，可参考CNN，模型的输入/输出维度d_{model}=512，但中间层的维度是2048，目的是为了减少计算量，这部分一样参考神人Kaiming He的Deep Residual Learning for Image Recognition。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fcfa48d5522d09260fcbbf97baac7427_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;514&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fcfa48d5522d09260fcbbf97baac7427&quot; data-watermark-src=&quot;v2-7991cc25cd59a66dc986fccc5c439175&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Positional Encoding&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;和RNN不同的是，multi-head attention不能学到输入句中每个文字的位置，举例来说，“Are you very big?” and “Are big very you?”，对multi-head而言，是一样的语句，因此，”The transformer”透过positional encoding，来学习每个文字的相对/绝对位置，最后再和输入句中文字的隐向量相加。&lt;/p&gt;&lt;p&gt;论文使用了方程式PE(pos, 2i)=sin(pos/10000^{2i/d_{model}})、PE(pos, 2i+1)=cos(pos/10000^{2i/d_{model}})来计算positional encoding，pos代表的是位置，i代表的是维度，偶数位置的文字会透过sin函数进行转换，奇数位置的文字则透过cos函数进行转换，藉由三角函数，可以发现positional encoding 是个有週期性的波长；举例来说，[pos+k]可以写成PE[pos]的线性转换，使得模型可以学到不同位置文字间的相对位置。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-00df2117cfcfa5f40ddbb4a49aa61516_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;524&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-00df2117cfcfa5f40ddbb4a49aa61516&quot; data-watermark-src=&quot;v2-7c7fe7a87f6b1ef0343eb933fbdff13e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;如下图，假设embedding 的维度为4：&lt;/p&gt;&lt;p&gt;每列对应的是经过positional encoding后的向量，以第一列而言，就是输入句中第一个文字隐向量和positioncal encoding后的向量和，所以每列维度都是d_{model}，总共有pos列，也就是代表输入句中有几个文字。&lt;/p&gt;&lt;p&gt;下图为含有20字的输入句，文字向量维度为512，可以发现图层随著位置產生变化。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d965e5e145506aff2fb2f3aeebab638a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;483&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d965e5e145506aff2fb2f3aeebab638a&quot; data-watermark-src=&quot;v2-18c86efa1ebfb38cacb9a9d9833cb057&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Encoder内容告一段落，接下来让我们看Decoder的运作模式。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Decoder&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-059a4fbfb62171f8fafeb12a3f19e71f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;563&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-059a4fbfb62171f8fafeb12a3f19e71f&quot; data-watermark-src=&quot;v2-b71147a45fb01380fde461d5b5db7981&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Masked multi-head attention&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Decoder的运作模式和Encoder大同小异，也都是经过residual connections再到layer normalization。Encoder中的self attention在计算时，key, value, query都是来自encoder前一层的输出，Decoder亦然。&lt;/p&gt;&lt;p&gt;不同的地方是，为了避免在解码的时后，还在翻译前半段时，就突然翻译到后半段的句子，会在计算self-attention时的softmax前先mask掉未来的位置(设定成-∞)。这个步骤确保在预测位置i的时候只能根据i之前位置的输出，其实这个是因应Encoder-Decoder attention 的特性而做的配套措施，因为Encoder-Decoder attention可以看到encoder的整个句子，&lt;/p&gt;&lt;p&gt;Encoder-Decoder Attention&lt;/p&gt;&lt;p&gt;“Encoder-Decoder Attention”和Encoder/Decoder self attention不一样，它的Query来自於decoder self-attention，而Key、Value则是encoder的output。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4101cd93a98ac5eab9162ccf6be57c97_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;821&quot; data-rawheight=&quot;832&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-4101cd93a98ac5eab9162ccf6be57c97&quot; data-watermark-src=&quot;v2-5ab8a3246f713bbfc2f080cb6c189680&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;至此，我们讲完了三种attention，接著看整体运作模式。&lt;/p&gt;&lt;p&gt;从输入文字的序列给Encoder开始，Encoder的output会变成attention vectors的Key、Value，接著传送至encoder-decoder attention layer，帮助Decoder该将注意力摆在输入文字序列的哪个位置进行解码。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The Final Linear and Softmax Layer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Decoder最后会產出一个向量，传到最后一层linear layer后做softmax。Linear layer只是单纯的全连接层网络，并產生每个文字对应的分数，softmax layer会将分数转成机率值，最高机率的值就是在这个时间顺序时所要產生的文字。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fa8f4a9b88166c07314da305844d17a6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;524&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fa8f4a9b88166c07314da305844d17a6&quot; data-watermark-src=&quot;v2-2e0a4143814d769968284ca08918460c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Why self attention?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;过去，Encoder和Decoder的核心架构都是RNN，RNN把输入句的文字序列 (x1…, xn)一个个有序地转成hidden encodings (h1…hn)，接著在產出目标句的文字序列(y1…yn)。然而，RNN的序列性导致模型不可能平行计算，此外，也导致计算复杂度很高，而且，很难捕捉长序列中词语的依赖关係(long-range dependencies)。&lt;/p&gt;&lt;p&gt;透过 “the transformer”，我们可以用multi-head attention来解决平行化和计算复杂度过高的问题，依赖关係也能透过self-attention中词语与词语比较时，长度只有1的方式来克服。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Future&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在金融业，企业可以透过客户歷程，深入了解客户行为企业，进而提供更好的商品与服务、提升客户满意度，藉此创造价值。然而，和以往的基本特徵不同，从序列化的客户歷程资料去萃取资讯是非常困难的，在有了self-attention的知识后，我们可以将这种处理序列资料的概念应用在复杂的客户歷程上，探索客户潜在行为背后无限的商机。&lt;/p&gt;&lt;p&gt;笔者也推荐有兴趣钻研self-attention概念的读者，可以参考阿里巴巴所提出的论文ATrank，此篇论文将self-attention应用在產品推荐上，并带来更好的成效。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参考&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;[1] Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translationr. arXiv:1406.1078v3 (2014).&lt;/p&gt;&lt;p&gt;[2] Sequence to Sequence Learning with Neural Networks. arXiv:1409.3215v3 (2014).&lt;/p&gt;&lt;p&gt;[3] Neural machine translation by joint learning to align and translate. arXiv:1409.0473v7 (2016).&lt;/p&gt;&lt;p&gt;[4] Effective Approaches to Attention-based Neural Machine Translation. arXiv:1508.0402v5 (2015).&lt;/p&gt;&lt;p&gt;[5] Convolutional Sequence to Sequence learning. arXiv:1705.03122v3(2017).&lt;/p&gt;&lt;p&gt;[6] Attention Is All You Need. arXiv:1706.03762v5 (2017).&lt;/p&gt;&lt;p&gt;[7] ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. arXiv:1711.06632v2 (2017).&lt;/p&gt;&lt;p&gt;[8] Key-Value Memory Networks for Directly Reading Documents. arXiv:1606.03126v2 (2016).&lt;/p&gt;&lt;p&gt;[9] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv:1502.03044v3 (2016).&lt;/p&gt;&lt;p&gt;[10] Deep Residual Learning for Image Recognition. arXiv:1512.03385v1 (2015).&lt;/p&gt;&lt;p&gt;[11] Layer Normalization. arXiv:1607.06450v1 (2016).&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;来源：https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;推荐阅读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289074&amp;amp;idx=1&amp;amp;sn=e859d363eef9249236244466a1af41b6&amp;amp;chksm=802e3867b759b1717f77e07a51ee5671e8115130c66562577280ba1243cba08218add04f1f00&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;1、经过多年交易之后你应该学到的东西（深度分享）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289050&amp;amp;idx=1&amp;amp;sn=60043a5c95b877dd329a5fd150ddacc4&amp;amp;chksm=802e384fb759b1598e500087374772059aa21b31ae104b3dca04331cf4b63a233c5e04c1945a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;2、监督学习标签在股市中的应用（代码+书籍）&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289028&amp;amp;idx=1&amp;amp;sn=631cbc728b0f857713fc65841e48e5d1&amp;amp;chksm=802e3851b759b147dc92afded432db568d9d77a1b97ef22a1e1a376fa0bc39b55781c18b5f4f&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;3、2018年学习Python最好的5门课程&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289018&amp;amp;idx=1&amp;amp;sn=8c411f676c2c0d92b0dd218f041bee4b&amp;amp;chksm=802e382fb759b139ffebf633ac14cdd0f21938e4613fe632d5d9231dab3d2aca95a11628378a&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;4、全球投行顶尖机器学习团队全面分析&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289014&amp;amp;idx=1&amp;amp;sn=3762d405e332c599a21b48a7dc4df587&amp;amp;chksm=802e3823b759b135928d55044c2729aea9690f86752b680eb973d1a376dc53cfa18287d0060b&amp;amp;token=449379994&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;5、使用Tensorflow预测股票市场变动&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653289110&amp;amp;idx=1&amp;amp;sn=538d00046a15fb2f70a56be79f71e6b9&amp;amp;chksm=802e3883b759b1950252499ea9a7b1fadaa4748ec40b8a1a8d7da0d5c17db153bd86548060fb&amp;amp;token=1336933869&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot;&gt;6、被投资圈残害的清北复交学生们&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;知识在于分享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-23-47470866</guid>
<pubDate>Tue, 23 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>从Seq2seq到Attention模型到Self Attention（一）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-10-08-46250529.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/46250529&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-25a90441225b333156d2ac9c2c34ca3b_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：Bgg&lt;/p&gt;&lt;p&gt;&lt;br&gt;近一两年，注意力模型（Attention Model）是深度学习领域最受瞩目的新星，用来处理与序列相关的数据，特别是2017年Google提出后，模型成效、复杂度又取得了更大的进展。以金融业为例，客户的行为代表一连串的序列，但要从串行化的客户历程数据去萃取信息是非常困难的，如果能够将self-attention的概念应用在客户历程并拆解分析，就能探索客户潜在行为背后无限的商机。然而，笔者从Attention model读到self attention时，遇到不少障碍，其中很大部分是后者在论文提出的概念，鲜少有文章解释如何和前者做关联，笔者希望藉由这系列文，解释在机器翻译的领域中，是如何从Seq2seq演进至Attention model再至self attention，使读者在理解Attention机制不再这么困难。&lt;/p&gt;&lt;p&gt;为此，系列文分为两篇，第一篇着重在解释Seq2seq、Attention模型，第二篇重点摆在self attention，希望大家看完后能有所收获。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-03537e4ae1fdd02bfd24581018eb2895_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-03537e4ae1fdd02bfd24581018eb2895&quot; data-watermark-src=&quot;v2-3e8078520617f443e85504f60d3269ec&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你可能很常听到Seq2seq这词，却不明白是什么意思。Seq2seq全名是Sequence-to-sequence，也就是从序列到序列的过程，是近年当红的模型之一。Seq2seq被广泛应用在机器翻译、聊天机器人甚至是图像生成文字等情境。如下图：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-356dee819a59fb40c5428c8ac4ddd3dc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;555&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-356dee819a59fb40c5428c8ac4ddd3dc&quot; data-watermark-src=&quot;v2-b51c99e9ddd9bf0d9c06dd63df790741&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;其中，Seq2seq常见情境为机器翻译，因此接下来的内容都会以情境进行说明。&lt;/p&gt;&lt;p&gt;图（3）是个典型的Seq2seq模型，包含了编码器（Encoder）和解码器（Decoder）.只要输入句子至Encoder，即可从Decoder获得目标句。&lt;/p&gt;&lt;p&gt;举例来说，如果我们将“Are you very big”作为输入句（source sentence），即可得到目标句（target sentence）“你很大？”。机器翻译就是这么简单，然而，如果想了解它如何组成，会发现其中充斥着各种难以咀嚼的RNN/LSTM等概念。&lt;/p&gt;&lt;p&gt;接下来，让我们快速回味一下RNN/LSTM，方便后续模型理解。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-de3abdac68adab2377373f12c9fce191_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;538&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-de3abdac68adab2377373f12c9fce191&quot; data-watermark-src=&quot;v2-3e06187789edd0a83215c46f71b3e5c0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;RNN/LSTM&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;RNN是DNN模型的变种，不同之处在于它可以储存过去的行为记忆，进行更准确的预测，然而，就像人脑一样，一旦所需记忆量太大，就会比较健忘。我们可以把隐藏状态（hidden state）h_{t}认为是记忆单元，h_{t}可通过前一步的hidden state和当前时刻的输入（input）得到，因为是记忆单元，h_{t}可以捕捉到之前所有时刻产生的信息，而输出（output）o_{t}仅依赖于t时刻的记忆，也就是h_{t}。&lt;br&gt;RNN在反向训练误差时，都会乘上参数，参数乘上误差的结果，大则出现梯度爆炸；小则梯度消失，导致模型成效不佳，如图4。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-98002584fa7ee32eff8c95161e2f488f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;508&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-98002584fa7ee32eff8c95161e2f488f&quot; data-watermark-src=&quot;v2-123ba8754c207178acdf5669f0003a0c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;为了解决健忘、训练误差的问题，LSTM有了像是遗忘/输入/输出门（forget/input/output gate），隐藏状态（hidden state），记忆单元（cell memory）等概念，带来了更好的结果。在2014年，论文Learning Phrase Representations除了提出Seq2seq的概念，更提出了LSTM的简化版GRU，此后，LSTM和GRU便取代RNN成为深度学习当中的主流。&lt;/p&gt;&lt;p&gt;下图是LSTM的各种应用，在此不深入描述。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2a9dd66534c5fba825bba269c02793b3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;632&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2a9dd66534c5fba825bba269c02793b3&quot; data-watermark-src=&quot;v2-4550479d7e7a3be85af858f9feab777f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Seq2seq&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b608d60d46a2fe693f0d41d817814e28_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;545&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b608d60d46a2fe693f0d41d817814e28&quot; data-watermark-src=&quot;v2-e6415f75b1ebce43236b8c3298f48010&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;回到正题，所以Seq2seq是怎么组成的？我们可以看到Seq2seq包含两部分：Encoder和Decoder。一旦将句子输入至Encoder，即可从Decoder获得目标句。本篇文章着墨在Decoder生成过程，Encoder就是个单纯的RNN/ LSTM，读者若有兴趣可再自行研究，此外RNN/LSTM可以互相代替，以下仅以RNN作为解释。&lt;/p&gt;&lt;p&gt;现在我们具备RNN/LSTM的知识，可以发现Seq2seq中，Decoder的公式和RNN根本就是同一个模子出来的，差别在于Decoder多了一个C — 图（6），这个C是指context vector/thought vector。context vector可以想成是一个含有所有输入句信息的向量，也就是Encoder当中，最后一个hidden state。简单来说，Encoder将输入句压缩成固定长度的context vector，context vector即可完整表达输入句，再透过Decoder将context vector内的信息产生输出句，如图7。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2b67c891f6793820f6820be49c2832c8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;423&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-2b67c891f6793820f6820be49c2832c8&quot; data-watermark-src=&quot;v2-91e83b003990ba88a34e266c85dc0185&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;但是，在Seq2seq模型中，Encoder将输入句压缩成固定长度的context vector真的好吗？如果句子今天很长，固定长度的context vector效果就会不好。怎么办呢？&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-408d9f559910422ea73911e1c145d4dc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;489&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-408d9f559910422ea73911e1c145d4dc&quot; data-watermark-src=&quot;v2-73098770ec3784cf5ce0308acbbf04cd&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在2015年，有个救星诞生了，叫作注意力模型（attention model）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Attention model&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为什么要用attention model？&lt;/p&gt;&lt;p&gt;The attention model用来帮助解决机器翻译在句子过长时效果不佳的问题。&lt;/p&gt;&lt;p&gt;这种新的构架替输入句的每个文字都创造一个context vector，而非仅仅替输入句创造一个从最终的hidden state得来的context vector，举例来说，如果一个输入句有N个文字，就会产生N个context vector，好处是，每个context vector能够被更有效的译码。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-059327c86cff49fbc09b29b618207f4b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;332&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-059327c86cff49fbc09b29b618207f4b&quot; data-watermark-src=&quot;v2-141cc67c99d8723abcce514a4185e86f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在Attention model中，Encoder和Seq2seq概念一样，一样是从输入句&amp;lt;X1，X2，X3…Xm&amp;gt;产生&amp;lt;h1，h2，h….hm&amp;gt;的hidden state，再计算目标句&amp;lt;y1…yn&amp;gt;。换言之，就是将输入句作为input而目标句作为output，所以差别就在于context vector c_{i}是怎么计算？&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-55e0c4c3218ae3f518913eb370c617ce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;487&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-55e0c4c3218ae3f518913eb370c617ce&quot; data-watermark-src=&quot;v2-7f37fc0bf34bef674a3ebad37be4bd70&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-cb93e5b5f9e9a3fed1be9b6d2f3d2c36_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;491&quot; data-rawheight=&quot;254&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-cb93e5b5f9e9a3fed1be9b6d2f3d2c36&quot; data-watermark-src=&quot;v2-e6dbe3aea3c883230422d3c0229ccd13&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Context vector c_{i}是透过attention scoreα乘上input的序列加权求和.Attention/Alignment score是attention model中提出一个很重要的概念，可以用来衡量输入句中的每个文字对目标句中的每个文字所带来重要性的程度。由公式可知，attention score藉由score e_{ij}所计算得到，所以先来看看score e_{ij}是什么。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b45c71dbe814c4a76cb0ee57b4475320_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;209&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b45c71dbe814c4a76cb0ee57b4475320&quot; data-watermark-src=&quot;v2-0bbece720a9cb2aa6001c0fb3c56528b&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a7edcfb5e6d18ec5f1245a44425f07ae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;828&quot; data-rawheight=&quot;86&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在计算score中，a代表Alignment model会根据输入字位置j和输出字位置i这两者的关联程度，计算出一个score e_{ij}。换言之，e_{i，j}是衡量RNN decoder中的hidden state s_{i-1}和输入句中的第j个文字hidden state h_{j}的关系所计算出的权重 — 如方程式3，那权重怎么算呢？&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-800a0ab1c7a1c33f30bbaba863ca089a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;329&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-800a0ab1c7a1c33f30bbaba863ca089a&quot; data-watermark-src=&quot;v2-79c9ce8e4ca27fdd07b0e4b46dc2cc83&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Neural Machine Translation发表之后，接续的论文Effective approaches of the NMT、Show，Attend and Tell提出了global/local attention和soft/hard attention的概念，而score e_{ij}的计算方式类似global和soft attention。细节在此不多说，图11可以看到3种计算权重的方式，我们把刚才的公式做些改变，将score e_{ij}改写成score（h_{t}，\bar {h_{s}}），h_{t}代表s_{i-1}而\bar {h_{s}}代表h_{j}，为了计算方便，我们采用内积（dot）计算权重。&lt;/p&gt;&lt;p&gt;有了score e_{ij}，即可透过softmax算出attention score，context vector也可得到，在attention model中，context vector又称为attention vector。我们可以将attention score列为矩阵，透过此矩阵可看到输入端文字和输出端文字间的对应关系，也就是论文当中提出align的概念。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f9555bd16b7b9cb1944738b19f43a778_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;728&quot; data-rawheight=&quot;400&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f9555bd16b7b9cb1944738b19f43a778&quot; data-watermark-src=&quot;v2-cc0e50cee25b09b691c43f6bd3bc562f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我们知道如何计算context vector后，回头看encoder。&lt;/p&gt;&lt;p&gt;attention model中的encoder用的是改良版RNN：双向RNN（Bi-directional RNN），以往单向RNN的问题在于t时刻时，只能透过之前的信息进行预测，但事实上，模型有时候可能也需要利用未来时刻的信息进行预测，其运作模式为，一个hidden layer用来由左到右，另一个由右到左，透过双向RNN，我们可以对词语进行更好的预测。&lt;/p&gt;&lt;p&gt;举例来说，”我喜欢苹果，因为它很好吃”？和”我喜欢苹果，因为他比安卓稳定”这两个句子当中，如果只看”我喜欢苹果”，你可能不知道苹果指的是水果还是手机，但如果可以根据后面那句得到信息，答案就很显而易见，这就是双向RNN运作的方式。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54c469e0dd303eb141dd7c4537283281_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;601&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-54c469e0dd303eb141dd7c4537283281&quot; data-watermark-src=&quot;v2-5fe62f701c3e1e5244fc05211aae37ac&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;Attention model虽然解决了输入句仅有一个context vector的缺点，但依旧存在不少问题。1.context vector计算的是输入句、目标句间的关联，却忽略了输入句中文字间的关联，和目标句中文字间的关联性，2.不管是Seq2seq或是Attention model，其中使用的都是RNN，RNN的缺点就是无法平行化处理，导致模型训练的时间很长，有些论文尝试用CNN去解决这样的问题，像是Facebook提出的Convolutional Seq2seq learning，但CNN实际上是透过大量的layer去解决局部信息的问题，在2017年，Google提出了一种叫做”The transformer”的模型，透过self attention、multi-head的概念去解决上述缺点，完全舍弃了RNN、CNN的构架。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6d99bba176cee5afc070da9f20ad14ad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;491&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6d99bba176cee5afc070da9f20ad14ad&quot; data-watermark-src=&quot;v2-7561cc63f7c7acc41019b370d22ee836&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;让我们复习一下Seq2seq、Attention model，差别在于计算context vector的方式。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1097ef24b8791b86499bd3c8ed68ba1b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;261&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1097ef24b8791b86499bd3c8ed68ba1b&quot; data-watermark-src=&quot;v2-8bb86e339d355020a9334c6a89a4b90d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;透过上述内容，我们快速的了解Seq2seq、Attention model运作、计算方式，我强烈建议有兴趣的读者可以参考图1中的论文，会有很多收获。&lt;/p&gt;&lt;p&gt;系列二将着重在Google于论文“Attention is all you need“所提出的self attention、multi-head等概念。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;参考&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;[1] Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translationr. arXiv:1406.1078v3 (2014).&lt;br&gt;&lt;/p&gt;&lt;p&gt;[2] Sequence to Sequence Learning with Neural Networks. arXiv:1409.3215v3 (2014).&lt;/p&gt;&lt;p&gt;[3] Neural machine translation by joint learning to align and translate. arXiv:1409.0473v7 (2016).&lt;/p&gt;&lt;p&gt;[4] Effective Approaches to Attention-based Neural Machine Translation. arXiv:1508.0402v5 (2015).&lt;/p&gt;&lt;p&gt;[5] Convolutional Sequence to Sequence learning. arXiv:1705.03122v3(2017).&lt;/p&gt;&lt;p&gt;[6] Attention Is All You Need. arXiv:1706.03762v5 (2017).&lt;/p&gt;&lt;p&gt;[7] ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. arXiv:1711.06632v2 (2017).&lt;/p&gt;&lt;p&gt;[8] Key-Value Memory Networks for Directly Reading Documents. arXiv:1606.03126v2 (2016).&lt;/p&gt;&lt;p&gt;[9] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv:1502.03044v3 (2016).&lt;/p&gt;&lt;p&gt;[10] Deep Residual Learning for Image Recognition. arXiv:1512.03385v1 (2015).&lt;/p&gt;&lt;p&gt;[11] Layer Normalization. arXiv:1607.06450v1 (2016).&lt;/p&gt;&lt;p&gt;来源：&lt;/p&gt;&lt;p&gt;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-10-08-46250529</guid>
<pubDate>Mon, 08 Oct 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【深度分享】经过多年交易之后你应该学到的东西</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-09-27-45499811.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45499811&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6a7ff8c24eb99fad3875a4042d1075e9_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;公众号推送了很多技术类的文章，今天为大家带来一篇软文，直指交易实战。所有策略、算法等，可能都需要经过实践的检验和不断的改进才有可能为你带来一定的财富，但也不是永远的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;真的很好哦，希望你们能读完&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;我在6个月内获得了20％的投资回报率，并又在一个月内全部亏损。 然后在接下来的一个月中，我在10笔交易中又损失了30％。 几乎差点强平了我的帐户，追加了十几次保证金。我尝试了多种机器学习算法，交易了多个市场等等。 我犯了任何可能的错误，但不知怎的，我活了下来并且学到了很多东西。&lt;/p&gt;&lt;p&gt;在软件工程行业工作了4年后，我意识到我的职业道路很顺畅。 我很擅长处理与数据科学相关的项目。 我对算法交易的热情永远不会褪去，所以我决定放弃我的数据科学职业生涯并以交易为生。 以下是我所犯的错误以及如何避免这些错误：&lt;/p&gt;&lt;h2&gt;&lt;b&gt;机器学习的炒作&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现如今，机器学习算法几乎在我们的生活中无处不在。 媒体喜欢它，投资者称之为“buzz words”，因为他们永远不会用到它。 DeepMind的AlphaGo进展令人兴奋，但是只是一个开始。&lt;/p&gt;&lt;p&gt;机器学习的问题在于，在交易中使用十分困难。 它更像是一种过滤方法，而不是一种决策制定工具。 大多数回测都很棒，在实际交易中则是失败的，因为它们容易过拟合。 你将通过交叉验证来对修正它，并挑选出最佳样本中的最佳模型，你认为你是对的，以某种方式增加自己的想法和 leaking数据。 这样的方法是不对的。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-24d74407bc94ff03c360367ef75a26ec_r.jpg&quot; data-caption=&quot;市场有不同的训练/测试分布&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;370&quot; data-rawheight=&quot;261&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-24d74407bc94ff03c360367ef75a26ec&quot; data-watermark-src=&quot;v2-6624b1af7c589714c7d87ba685891eb6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;通过仔细拉平价格和评估不同的资产、时间范围或期间来避免过度拟合。 使用非传统的训练/测试并添加随机噪声来评估模型的泛化能力。 一定要非常小心，因为你连自己都不知道会发生什么，但蒙特卡罗模拟是你最好的朋友，因为你至少可以模拟很多场景。&lt;/p&gt;&lt;p&gt;&lt;b&gt;总结：机器学习在量化投资的应用需要根据自己的实力和能力适可而止。复杂的不一定是最好的，模型一定要可解释，错在哪里要一清二楚。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;风险&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我在交易期间多次感到十分安全，以为我已经搞定了。我觉得没有什么能让我惊讶，但我一次又一次被市场打脸。1笔交易出现了问题，消除了之前的10笔盈利，波动率飙升和你违反了常规的操作，市场流动性变成了你投资中的恶魔，继而引发了一场追加保证金和你的经纪人。伙计们，这是现实，没有免费的午餐。一切都涉及风险，关键是你对自己的赔率和赔率多少有深刻的了解。&lt;/p&gt;&lt;p&gt;作为一名期权交易员，我的优势在于卖出定价过高的期权，并在价格下跌时买回来。目前有几种定价模型，如布莱克-斯科尔斯-默顿模型、二叉树模型和蒙特卡洛模型。它们都提供了资产在预定义的时间范围内的定价估计。通常IV(隐含波动率)夸大了市场的恐惧。但有时恐惧是真实的。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c083d56bbf88050876a667ee4e03a231_r.jpg&quot; data-caption=&quot;2018-02-06 TripAdvisor上线&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1033&quot; data-rawheight=&quot;574&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-c083d56bbf88050876a667ee4e03a231&quot; data-watermark-src=&quot;v2-effcce691e755e755e458a96c97c7f15&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;正如我们在2018年2月看到的那样，市场担忧有时是真实存在的。指数开始抛售，人们纷纷逃离ETF和股票，转向较安全的现金和黄金，因为现金才是真正的王者。加密货币之所以被抛弃，是因为人们意识到，与数字货币相比，劫掠者显然更喜欢现金和黄金。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Alpha&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Kaggle的所有比赛都是通过疯狂的分类器组合和平均法赢得的。在很长一段时间里，我试图通过运用尖端技术、算法和工具，试图成为一个聪明的人。这些方法和工具有时工作，有时不工作。通常情况下，你要花上几周或几个月的时间才能弄清楚哪里出了问题。正如前面提到的，这是一个非确定性的过程，只会增加噪音和leaking数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;事实是，你只需会简单的统计、蒙特卡洛模拟和会一些Python就足够了。通过简单的假设和较好的模拟验证，就可以发现定价过高和过低的出价和要价，仅此而已。这些花哨的模型对你的自我意识和理解都是有好处的。但在市场中，本科和研究生所学的的概率和统计知识足以支撑你实现一个盈利策略。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;KISS (Keep It Simple Stupid)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果没有交易经验的人问你如何赚钱，你必须能用几句话来解释清楚，否则，你就不会赚钱。在此之前，我常常向人们解释我的机器学习流程是多么的花哨，却无法解释Alpha。现在很简单：我出售定价过高的期权溢价，仅此而已。资产价格边界方法非常复杂，但Alpha来源很清楚。&lt;/p&gt;&lt;p&gt;对大多数人来说，交易似乎是一项艰巨的任务。事实上，人们每天都在不知不觉中扮演着交易者的角色。你去超市买东西。这是你的市场（exchange），价格是你的买卖价差（level 1），你只能买（ask），但供应商也可以卖（bid）。超市后面的供应是 level 2。收银员是你的订单簿。现在把它推广到你买卖的所有东西，都是交易，每个人都是日内交易者。&lt;/p&gt;&lt;p&gt;说真的，我的算法越复杂，我的损失就越大。被花哨的Spark工作、Lambda表达式和漂亮的Jupyter notebook所笼罩，实际上赚的钱却更少了。事实上，一开始我使用简单的多线程流和几个简单的脚本来计算Alpha值。当我开始专注于性能时，我失去了对Alpha本身的追踪。最重要的是要跟踪一个简单有效的工作流程，然后你可以在一个靠谱的框架上添加各种想法。性能和易用性固然重要，但对于散户来说，一致性和简单性更为重要。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;市场 Roller Coaster&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;6个月的盈利固然不错，但你可能比前几个月损失更多。有时候市场就像鳄鱼一样残酷而快速。我犯的最大的错误之一就是过度下注。虽然凯利标准是重要的考虑因素，少投注总是比多投注好。风险评估和仓位调整是你成功的关键。拥有一个高胜率的策略与正确的头寸规模和保证金要求，是同样重要。&lt;/p&gt;&lt;p&gt;巴菲特引用合作伙伴芒格的话说：&lt;/p&gt;&lt;blockquote&gt; “liquor, ladies and leverage”&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;事后诸葛亮&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;人们会告诉你应该做什么。我犯的每一个错误都有人告诉我这是可以避免的。你的家人、朋友和同事会怀疑你，怀疑你的能力、你的技能和你的想法。Robert Kiyosaki说的很对：人们太害怕尝试一些东西，所以评估别人总是很容易，但自己不去做。大多数时候，你会对周围的环境感到沮丧：从社交网络上的到你的潜在投资者，每个人都认为这很容易，而且他们可以做到。但话说回来，他们都是事后诸葛亮。&lt;b&gt;你一定要跟随你的直觉，这是你的战场，不要被观众误导。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;偶尔的疯狂&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这也会发生在你身上。大多数专业交易都需要心理稳健性。在市场波动的时，你会随着市场而心跳加速，保证金要求会突然提高。你会绝望和痛苦，因为你做了非常大的交易，有无限的风险，没有考虑到雨天。有些日子会下雨，总是要做好准备。&lt;/p&gt;&lt;p&gt;此外，我失去了我的灵魂。我的朋友们反复批评我“冷漠”、“咄咄逼人”和“粗鲁”。有一会儿，我差点忘了怎么弹吉他。对我来说，每一个社交活动都突然变得烦人、耗时，或者浪费了宝贵的编程时间。长时间的工作，饮食失调导致体重减轻等。这太极端了，不要这样做。我在什么地方读到过，这实际上是我在努力工作中表现出色的标志。但是这对你的交易不利。你必须避免，不要夸大。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;重生&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一次又一次搞砸之后，大大降低了我的期望。我突然明白了一个说法，关于你能从市场上拿走多少钱。&lt;/p&gt;&lt;p&gt;最重要的是，我突然变得无所畏惧，再也没有什么能吓到我了。我不再天真，完全理解其中的风险。在某种程度上，我意识到这个行业是多么脆弱和危险。就像走在雷区。&lt;/p&gt;&lt;p&gt;我开始较小资金的交易。首先调查公司，加上通常会出现多个交易想法，所以不会出现FoMO(Fear of Missing Out)。此外，我还大幅减少了自己的观察名单，专注于流动性和数量。这是最重要的时刻。进出交易是强制性的。我所有的亏损交易都是低流动性资产和糟糕的基本面，现在需要花很少时间去评估。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;佣金&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我遭受重大损失之前，我过去每个月都会赚取4％的ROC。佣金似乎无关紧要。当我爆了超过30％的仓位并减少头寸时，我突然意识到低佣金的重要性。与IB（盈透证券交易经纪人）的交易，考虑到强大的平台和灵活的API，我支付的佣金在业内是最低的。另一方面，通过减少头寸规模，我发现自己支付了5-10%的佣金，IB建议“提供流动性”，因为他们“不讨论利率”，即使你一个月做200笔交易。此外，我还需要历史价格，所以Quandl在这里是至关重要的。经纪人对你一点也不在乎，做一个零售交易员真糟糕。&lt;/p&gt;&lt;p&gt;避免佣金流失的唯一方法是交易规模。人们倾向于谈论多样化等等。位置应该如何小等等。事实证明，交易规模太小会害死你。这是一个个人参数，是你账户大小、风险规避等的函数。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;耐心&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在日间交易中最难完成的事情之一就是耐心。有时候最好的交易是不交易，就像象棋中的祖格王。做短线交易员意味着做一个市场瘾君子，这意味着在开盘前肾上腺素会飙升。有时现金为王，不交易或等待合适的交易时机是你能做的最好的状态，特别是当市场在抛售或危机时期疯狂的时候。另一方面，交易就像空气，如果你不交易，你就不存在。控制这种冲动是你成功的关键。很多次我都在增加亏损头寸，或者试图保存头寸，而不是等待和保留现金。&lt;/p&gt;&lt;p&gt;就像Charlie Munger说的：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;大笔的钱不是通过买卖赚来的……但在等待中。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;耐心也与进入和退出有关。你会看到你想要的价格，但是他们会改变LMT的订单，而你没有得到它。然后你会调整并追逐价格，价格会再次移动。不要急于求成，这需要时间。最终你会坚持自己的观点，等待对方接受。根据我的经验，如果基础是流动性的，全天中间价格的交易就会被填满。所以有时候等待是有帮助的。永远不要光看你的亏损头寸，市场总是可以反弹的，即使它离到期还有一天。&lt;/p&gt;&lt;h2&gt;&lt;br&gt;&lt;b&gt;掌握艺术&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;交易绝对是艺术而非科学。你可能马上就开始学习一种新乐器，大概过了几个星期，可以演奏一些简单的乐谱。一个月后，你希望演奏一首歌。努力工作是关键。&lt;/p&gt;&lt;p&gt;同样，交易需要大量的实践。专业人士和业余交易者之间的区别就在于毫不费力地达到交易水平。不幸的是，人们不明白这个概念，因为短线交易员总是看起来像有钱可花的人。人们似乎忽略了这条路，因为这是这场游戏中最困难的部分。&lt;/p&gt;&lt;p&gt;我读了几十本书，读了数百篇文章，看了数百小时的视频相关内容。&lt;b&gt;学习永远不会停止。你的交易策略必须不断地改进，否则你的优势就会消失。&lt;/b&gt;最好的内容可以在线获取，而且主要是免费的。如果你正在进行期权交易，你必须检查一下tastytrade，OptionAlpha和OIC。它们会教你所有你需要知道的东西。这取决于你有多积极。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;像做市商一样思考&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在进行了数百次手动交易之后，你就会开始注意到这些事情，尤其是像新手一样被被骗的事件。除了佣金之外，交易期权中最令人沮丧的概念之一就是做市商。做市商本质上是操纵市场的参与者。众所周知，像Citadel，Final和许多其他HFT（高频交易）这样的公司完全控制了市场上的价格低效。试图买卖的大型机构是被敲诈的主要目标，因为快速的玩家可以很容易地抢到你之前，以更高的价格卖给你同样的东西，或者以更低的价格从你那里买，然后卖给/从别人那里买。&lt;/p&gt;&lt;p&gt;如前所述，佣金是问题的一部分，但没有佣金，就没有交易场所。真正的问题是做市商在虚张声势。所有的基础设施都是自动化的，快速的玩家到处都在追逐你的交易，在买入时给你提供高价，在卖出时给你提供低价。击败它的唯一方法是使用限价订单，并尽量预测中间价。所以这就像支付两次佣金一样！永远不要使用市场订单或原始价格，总是以中间价或更好的价格为目标。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;了解你的设备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我花了6个月的时间来充分利用我的交易软件。我在我的PC，笔记本电脑和AWS实例上完全克隆了我的交易设置。如果发生故障，我可以很容易地恢复我的交易和需要的软件。我的网络连接非常稳定。在恶劣的天气或罕见的事件中，我有多个网络适配器，使我的智能手机成为热点。我添加了多个自动化层，使我的交易尽可能稳健。分析瘫痪是不好的，特别是在交易中。尝试尽可能地消除手动询问。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b9c7483c5edc807c7d4e814621e058a8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;712&quot; data-rawheight=&quot;434&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-b9c7483c5edc807c7d4e814621e058a8&quot; data-watermark-src=&quot;v2-f8d6493ea10f82ef12ae0ce20accb4d3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我犯了几十个快捷键错误，可能因为错误的价格和多种交易组合而损失了几千个。匆忙和缺乏知识会导致愚蠢的错误和资本的损失。不要运行，你是一个零售交易员，而不是一个算法交易计算机。要彻底了解你的交易软件。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;分析瘫痪&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;过度交易是十分糟糕的。交易是非常令人兴奋的，你会变成一个瘾君子。你太急于交易、改进和修改，最终你被困住了，弊大于利。一开始我无法离开屏幕。我在一个屏幕显示放期货和Tastytrade广播，在另一个屏幕上显示我的头寸。最终，我会坐上6个小时，修改、交易、改进或通常“触摸”我的交易。我从惨痛的教训中认识到，期权交易只在开盘和收盘时进行。其他的都不好。&lt;/p&gt;&lt;p&gt;在这个游戏中生存的唯一方法就是像机器人一样交易。尝试新东西是可以的，也是学习的一部分，但是在你的live账户上尝试新东西可能是一场灾难。我把利润门槛和风险的概率转换得太快了。最终我能够收敛并找到最优的比率。在我的live账户上做这件事花费了我数千美元，我可以通过事先评估来避免这样的错误，至少模拟回测交易一个月。一致性和持久性是必须的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;PDT&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;PDT（模式日交易者）规则要求每天至少2万5千美元的交易：每周进行3天以上的交易。我讨厌它，每个人都讨厌它，认为它是愚蠢的。这很烦人，但一旦你像机器一样在交易后被迫接受了PDT限制，这个逻辑是合理的。&lt;/p&gt;&lt;p&gt;我一直在用一个不错的账户进行交易，这个限制对我来说似乎无关紧要。我从来没有把我的交易按优先级排序，因为我可以把它们都做出来。当我失去一半账户的那一刻，我突然意识到每笔交易都是多么珍贵。我不得不用激光聚焦我的交易，而不是到处射击。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;交易日志&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;你会听到很多关于交易日志如何重要的信息，但是老实说，没有人会保留日志。通常你认为这是浪费时间。你只对你赚了多少钱感兴趣。最终你会成长为一名交易员，你会意识到交易日志的重要性。&lt;/p&gt;&lt;p&gt;保持一个最新的交易日志将会改善一切。从佣金和赔率到你交易的资产。你会学到比你想象的更多的东西，并且会以不同的方式提高你的纪律性。我开始用一份谷歌表格作为交易日志。在一周后，我意识到自己的风险太高，交易规模太小。实际上，我冒着超过1到4的风险，实际情况接近1比5，因为我的交易规模太小了。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a15995a5431747ec5744fd25a931b1d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;551&quot; data-rawheight=&quot;216&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-a15995a5431747ec5744fd25a931b1d1&quot; data-watermark-src=&quot;v2-af3f884d9db611647c1e6529bc1d64d0&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;例如，在我的期权策略中，我通常会卖出至少0.5个信用额度，或者50美元，然后分析你最多能赚到的利润（期权合约有100个标准乘数）。假设你拿100美元去赌25美元也就是1比4概率对你有利，但实际上你赚了19美元，风险还是100美元。从25美元跌至19美元，这是普遍建议的固定汇率中间价为50%，加上经纪商在每次交易往返中（进出）收取3至10美元。&lt;/p&gt;&lt;p&gt;解决这个问题的唯一办法是提高最低入场价格。从那一刻起，我的保费不少于100美元。我用75%的利润代替了50%。实际上，风险是75到200美元也就是1到2.67美元。这个比例很糟糕，但很现实。75%的交易成功了，我们还好，而且比80%的交易容易。虽然使用长距离期权策略很常见，但日志对我来说很有用，所以你必须调整你的获利者，并相应地停止亏损水平。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;SPX是国王，VIX是皇后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;SPX@CBOE或标普500指数是市场。所有移动的东西和所有有趣的东西都反映在这些索引中。每一次崩溃、高峰、炒作和恐惧都在那里。VIX@CBOE或SPX波动指数是市场中恐惧和贪婪的纯粹反映。当市场波动时，波动性也会波动，反之亦然。相关性长期有效，但当波动性飙升时，一切都是相关的。&lt;/p&gt;&lt;p&gt;波动率指数的高值对期权卖家有利，而低值则是不好的。请注意，长时间的低波动率最终会导致大规模爆发。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ba0230831d8c6dd38b1a8812110e8a75_r.jpg&quot; data-caption=&quot;波动率指数历史数据&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;869&quot; data-rawheight=&quot;314&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ba0230831d8c6dd38b1a8812110e8a75&quot; data-watermark-src=&quot;v2-a8440986d7b76bb84818a3bd2696d75e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;看清楚当天最移动的资产。随着溢价的上升，资产价值的下降会更加有趣。作为优质卖家，这是行动迅速，出价丰厚的时候。随着期权溢价上升，市场担忧对期权交易有利。要有耐心抓住那些瞬间并立即行动。这就是你的优势。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;No Fill is a Kill&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;无法填补你的交易将使你发疯。我多次追逐价格直到我得到它，但弊大于利。做市商总是会在你入市的时候给你一个更好的填充，并且很少会为您提供中等价格或更好的Fill。在进入之前，只要找到满足你的风险回报比的出价。两件事几乎总会发生：1.你会得到Fill。2.你会马上看到更好的价格。但至少你可能获得回报。进行大量交易你会没事的。很难意识到，这将导致两种： 1. Almost no fills 2. Negative expectancy in terms of risk to reward due to commissions and your target exit price (which is seldom 0).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;永远不要重复你的错误&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;每次交易前记得检查自己。我曾多次犯过交易错误，并一次又一次地犯同样的错误。&lt;/p&gt;&lt;p&gt;记住，交易需要数年时间才能掌握，这是一个艰难的过程。学习永远不会停止。市场是动态的。重新评估和补充你的策略是很重要的。&lt;/p&gt;&lt;p&gt;最后送大家一句话：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;Be careful as we are small retail traders and the sharks love us fat stupid snacks.&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;Andrew Kreimer | Traveling Trader&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;知识在于分享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-09-27-45499811</guid>
<pubDate>Thu, 27 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>基于阻力支撑相对强度（RSRS）的市场择时</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-09-11-44280303.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/44280303&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ccf4ea61444220e4c47f62d44b8fb29f_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：量化小白H&lt;/p&gt;&lt;p&gt;本文是对光大证券研究报告《基于阻力支撑相对强度（RSRS）的市场择时》前四种择时方法的复现。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-911c159c4c179bf412afb4be114934fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;187&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-911c159c4c179bf412afb4be114934fe&quot; data-watermark-src=&quot;v2-7b4cac3a7b3b818bd0fcc9376d16f45c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;报告认为，常用的均线系统和MACD等指标滞后性较高，阻力支撑指标RSRS领先性较好，可以以此为依据构造择时策略，具体原理见报告。&lt;/p&gt;&lt;p&gt;（获取研报和代码&lt;b&gt;查看文末&lt;/b&gt;）&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;文章仅出于个人对于报告的理解，不一定正确，有问题请指出。回测结果除方法一以外，均与报告差异较大，&lt;b&gt;如果有大佬作出了跟研报差不多的结果，求指导&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;语言：python3.6&lt;/p&gt;&lt;p&gt;数据来源：&lt;b&gt;tushare（免费）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;回测区间：两个时间段，第一个与研报一致，2005年3月-2017年3月，通过对比来看复制是否准确，第二个为2005年初到当前，看策略现在的表现如何。&lt;/p&gt;&lt;p&gt;文章结构如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;各种阻力支撑指标的构造及分析&lt;/li&gt;&lt;li&gt;斜率策略回测&lt;/li&gt;&lt;li&gt;标准分策略回测&lt;/li&gt;&lt;li&gt;修正标准分策略回测&lt;/li&gt;&lt;li&gt;右偏标准分策略回测&lt;/li&gt;&lt;li&gt;回测结果汇总&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;b&gt;1. 阻力支撑指标&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;研报一共构造了4种阻力支撑指标，指标背后的逻辑不过多分析，见研报，仅给出原理和代码实现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;当日斜率指标&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3f454435a20bcdced885a4533acbec69_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;678&quot; data-rawheight=&quot;43&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-66ef2405b2d9d10355019fb61c61bb9a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;585&quot; data-rawheight=&quot;183&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-66ef2405b2d9d10355019fb61c61bb9a&quot; data-watermark-src=&quot;v2-3de36ef82b65e3a88198ace6bf8c65a3&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;当日标准分&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-75af397ac0f1d37910876693eaca977b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;608&quot; data-rawheight=&quot;176&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-75af397ac0f1d37910876693eaca977b&quot; data-watermark-src=&quot;v2-ba948e0b507ccc3b24aff4e408824e91&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;修正标准分&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fcff57ddc08a0473027216ad31069c7d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;770&quot; data-rawheight=&quot;309&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-fcff57ddc08a0473027216ad31069c7d&quot; data-watermark-src=&quot;v2-f847d6ab784341cfda3c307c2507999c&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;右偏标准分&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b0aa87942d01e93a6f74e335d44d4d06_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;796&quot; data-rawheight=&quot;112&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;为了后续策略回测方便，我们构造函数&lt;b&gt;getdata&lt;/b&gt;从&lt;b&gt;tushare&lt;/b&gt;中取出原始价格序列，并一次性计算出所有需要用的指标，其中N为回归用的天数，M为计算标准分所用的时间长度，&lt;b&gt;与研报一致，对于前三种策略，N=18,M=600,对于第四种策略，N=16,M=300&lt;/b&gt;。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-defbdd3524a3e2cbe52a7056c002a9f3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1364&quot; data-rawheight=&quot;974&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-defbdd3524a3e2cbe52a7056c002a9f3&quot; data-watermark-src=&quot;v2-b688c75c658ce7258f97ba57bb75cab5&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;斜率指标分布如下&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f94565db44aa27137ef888e94d60f08e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1020&quot; data-rawheight=&quot;410&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-f94565db44aa27137ef888e94d60f08e&quot; data-watermark-src=&quot;v2-5248752f82d58ee38f265d566cedf656&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8558bb00b3983481c7a554fbfffa021d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;236&quot; data-rawheight=&quot;70&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8558bb00b3983481c7a554fbfffa021d&quot; data-watermark-src=&quot;v2-e04c6dcdae1d20803a9e028c282ec487&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;与研报的对比：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3a0817868ff09fbb447821b63a46e8cc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;659&quot; data-rawheight=&quot;576&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3a0817868ff09fbb447821b63a46e8cc&quot; data-watermark-src=&quot;v2-9f61af68e8fa96fc5d1913e7d3525b06&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;斜率指标看起来是基本一致的，但其余指标统计量差异较大，见代码。&lt;/p&gt;&lt;p&gt;斜率250天的滚动均值&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a7abda9262b7be0e032fd13bddc2003_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1023&quot; data-rawheight=&quot;461&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-5a7abda9262b7be0e032fd13bddc2003&quot; data-watermark-src=&quot;v2-9419a9e6877adcf33864f4000ebd7ab7&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;报告结果&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3e3f56ebac048c48de5eaf6a42bace3c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;685&quot; data-rawheight=&quot;301&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-3e3f56ebac048c48de5eaf6a42bace3c&quot; data-watermark-src=&quot;v2-0c57f92ab057b8e7c417df1d1136032e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;整体趋势是一致的，只有前期不足250的部分，可能这部分的处理方法与报告有差异，所以结果有差异。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. 斜率策略回测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;策略逻辑：&lt;b&gt;斜率指标超出阈值S1 = 1时买入，斜率指标超出阈值S2 = 0.8时卖出&lt;/b&gt;。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ab3a46ed02de43bd9b85fcbcb45da112_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1370&quot; data-rawheight=&quot;844&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ab3a46ed02de43bd9b85fcbcb45da112&quot; data-watermark-src=&quot;v2-5949b2ffee01eecda6c6285bd49ab11d&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7179f5cf8248ad1c61ce041fbcc57335_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;891&quot; data-rawheight=&quot;239&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-7179f5cf8248ad1c61ce041fbcc57335&quot; data-watermark-src=&quot;v2-ed1844f7c24f15886d93f00e7c4ecf49&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;策略净值为10.59，交易次数45次，报告结果为10.57，41次，基本相近，并且相比于沪深300指数（黄色），有明显超额收益，且回撤较小。&lt;/p&gt;&lt;p&gt;但之后的策略都与报告结果差异较大，可能是指标计算上有一定差异。（之后部分策略回测结果中交易次数含0.5次，是因为买卖合计一次，最后一次开仓后未平仓。）&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3. 标准分策略回测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;之后三部分只给出逻辑和代码，回测结果汇总在最后。&lt;/p&gt;&lt;p&gt;策略逻辑：&lt;b&gt;标准分大于S=0.7买入，小于-0.7卖出。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0db3b6f755e766ef048e32b8a1d54c67_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1362&quot; data-rawheight=&quot;840&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0db3b6f755e766ef048e32b8a1d54c67&quot; data-watermark-src=&quot;v2-168b38109ece6517e2dabcc5ea8ee293&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;4. 修正标准分策略回测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;策略逻辑同3。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-df8cead218b93a137e01ee47079184b7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1370&quot; data-rawheight=&quot;840&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-df8cead218b93a137e01ee47079184b7&quot; data-watermark-src=&quot;v2-64f48f4a7e80d14e3e1437388b361da4&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;5. 右偏标准分策略回测&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;策略逻辑同3。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0db3b6f755e766ef048e32b8a1d54c67_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1362&quot; data-rawheight=&quot;840&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-0db3b6f755e766ef048e32b8a1d54c67&quot; data-watermark-src=&quot;v2-168b38109ece6517e2dabcc5ea8ee293&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;6. 回测结果汇总&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;回测区间：同研报&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ef136a4b751dbfa0d225faf55cd0009_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;885&quot; data-rawheight=&quot;240&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-9ef136a4b751dbfa0d225faf55cd0009&quot; data-watermark-src=&quot;v2-9857a37bc4314c173709c84f4f5e0cb2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;与研报结果相对比，斜率指标回测结果基本相同，其余均差异较大。此外，右偏标准分策略表现最好与研报一致，但标准分策略表现较差与报告结果不符。&lt;/p&gt;&lt;p&gt;回测区间：2005年3月-当前&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8724c6450b63f1435b423843fbc54de9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;882&quot; data-rawheight=&quot;241&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-8724c6450b63f1435b423843fbc54de9&quot; data-watermark-src=&quot;v2-0a84fd5ce2df8c9e8bf3850108c87f91&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;从净值曲线来看，右偏标准分策略依然表现最好，btw，2018年后所有策略都在出现明显回撤后保持空仓，所以，入市需谨慎呐。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;参考资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 20170501-光大证券-光大证券技术择时系列报告之一：基于阻力支撑相对强度（RSRS）的市场择时&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;代码+报告获取方式：&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;tempkey=OTczX2MxYUl0dUlHbkszMWdHUUZrNnN6S2NKNTJ4UjRud1VmSEpFay1FNGdFeGh5UE1vUnVnXzU5T0VpSm5nZUVjb055UVIxcGF5clVMNEp2SlZWRUlWdG5JVHdERkZaVVoteTJfYjFqMXkyU0V5bHh0MEIyZTNWLWJwUmJ2YW5wTkxXaDF1VzNsbFlabkdVaGhVdjJkMmN0LXZ5NlJGeHhnSC1DSHMxS1F%2Bfg%3D%3D&amp;amp;chksm=002e3bde3759b2c82337b9414bbe7f93e6a401598a95915e912f1dc97e14733271167e8914f9#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;基于阻力支撑相对强度（RSRS）的市场择时&lt;/a&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;知识在于分享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-09-11-44280303</guid>
<pubDate>Tue, 11 Sep 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【价值投资逻辑】高质量、低估值选股模型（代码+报告）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-09-11-44279385.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/44279385&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e5dbda7178159694e796b38db8d65d8f_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;本文参考：东兴证券《价值投资的逻辑：高质量+低估值》&lt;/p&gt;&lt;p&gt;作者：WindQuant | 001&lt;/p&gt;&lt;p&gt;&lt;b&gt;点击图片了解详情&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5ODQ4MjgyMQ==&amp;amp;mid=2651124201&amp;amp;idx=4&amp;amp;sn=e3b75bae3eaa6d34ac660507e9c20aba&amp;amp;scene=21&amp;amp;token=1339406744&amp;amp;lang=zh_CN#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;这次，Wind免费带你玩量化&lt;/a&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;内容直接开门见山&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、因子提取&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1、回测区间&lt;/b&gt;：2007年1月30日至2018年6月29日；&lt;/p&gt;&lt;p&gt;&lt;b&gt;2、调仓日：&lt;/b&gt;每月最后一个交易日为调仓日，以每月最后一个交易日的收盘价买入卖出；&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、股票池：&lt;/b&gt;全部A股；剔除选股日的ST/PT 股票；剔除上市不满一年的股票；剔除选股日由于停牌等原因而无法买入的股票；&lt;/p&gt;&lt;p&gt;&lt;b&gt;4、因子选取如下所示：&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6697b6c5048bcbe2533fbc7b8ebd857e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1032&quot; data-rawheight=&quot;514&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6697b6c5048bcbe2533fbc7b8ebd857e&quot; data-watermark-src=&quot;v2-a7a4f17165efb88d8fdc4af0816807cc&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;部分代码展示，全部代码请见文末&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-97593fe98568cadf49fb2b4c50b52011_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;350&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-97593fe98568cadf49fb2b4c50b52011&quot; data-watermark-src=&quot;v2-783a51ae58afde76389ac347cfe60f88&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ba30d02c7c988030188ff3f3c0d2ddb0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;284&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ba30d02c7c988030188ff3f3c0d2ddb0&quot; data-watermark-src=&quot;v2-ff14eceffd62ebcd4c9ac91363705b0f&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;二、投资组合构建&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;选股逻辑&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;1、扣除有ST标示、调仓日近一年内上市、单季度净利润为负、近12个月净利润为负、单季净利润同比增长率为负或则大于1000%的股票；&lt;br&gt;2、再取估值因子EP_DEDUCTED_TTM排在股票池前30%的股票；&lt;br&gt;3、单季度扣除非经常性损益ROE排在股票池前30%的股票；&lt;br&gt;4、单季净利润同比增长率排在股票池前30%的股票。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;三、投资组合表现分析&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;3.1选出的股票数量分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;部分调仓日的股票代码如下所示：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa3bd1a844552fdd3f812ac6b3689069_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;246&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-aa3bd1a844552fdd3f812ac6b3689069&quot; data-watermark-src=&quot;v2-ab35859816eb914d765af41e7c57a6ea&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6a727106d4735e50fe5e96a761ffd34a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;118&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-64e69f038b6ad183c97725be8c2ce115_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;536&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-64e69f038b6ad183c97725be8c2ce115&quot; data-watermark-src=&quot;v2-c94af3ed9a7f7aa1fc1aaef5dec93dc2&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;在回测区间的各个调仓日选出的股票数在18只-86只之间，2009年4月-2009年7月、2014年10月-2015年2月、2015年10月-2016年3月三个时间段的股票数较少，从2017年后半年至2018年前半年的一年时间里选出的股票数较多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2 持仓股票市值分析&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ebeb32430f48ca94ccf3a728fe450913_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;569&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-ebeb32430f48ca94ccf3a728fe450913&quot; data-watermark-src=&quot;v2-32683a4225e31a09c84e70770a8e3309&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-99c6e008f93331b4b715e503f9daaeab_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;502&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-99c6e008f93331b4b715e503f9daaeab&quot; data-watermark-src=&quot;v2-117507a66933f5cf3479996a695b598e&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;该组合的市值与中证500指数较为接近。&lt;br&gt;&lt;b&gt;3.3 收益率分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.1 投资组合与沪深300指数、中证500指数、上证50指数、上证综指净值对比&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-caa6e245f3dfdc7fbc488860b4d0f58a_r.jpg&quot; data-caption=&quot;部分代码展示，全部代码请见文末&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-caa6e245f3dfdc7fbc488860b4d0f58a&quot; data-watermark-src=&quot;v2-8e7b99a3e0e39c318002a4795acd74ff&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bad7396686e9e3d62c28c7d5068b3a0f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;469&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-bad7396686e9e3d62c28c7d5068b3a0f&quot; data-watermark-src=&quot;v2-3a411128e39cd79dff226bef575f2eb6&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;从投资组合与各大指数的净值时序图可以看出，投资组合的回测净值在整个回测周期内基本大于各大指数的回测净值，其中在2011年之前，投资组合与各大指数的净值相差不大，2011年至2014年末，投资组合的净值基本上为各大指数净值的2倍，从2014年末以后，投资组合的净值与各大指数的净值的差距逐渐拉大，投资组合要远高于各大指数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.2 投资组合与沪深300指数、中证500指数、上证50指数、上证综指收益率对比&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过对比投资组合与沪深300指数、中证500指数、上证50指数、上证综指的年华收益率、收益波动率、夏普比率、最大回撤等指标，来考察投资组合的收益和风险情况。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d9062fca46cdca06f701e5822e99496b_r.jpg&quot; data-caption=&quot;部分代码展示，全部代码请见文末&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;435&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-d9062fca46cdca06f701e5822e99496b&quot; data-watermark-src=&quot;v2-0138bcb1f99fb411b982ab0dc05b00d1&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1c24c304f2411532d6e47801e87db90e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1016&quot; data-rawheight=&quot;390&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-1c24c304f2411532d6e47801e87db90e&quot; data-watermark-src=&quot;v2-f7287622cc869665ab4e1b5dc1e64534&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;从投资组合与各大指数的收益情况来看，投资组合的年化收益远高于各大指数的年化收益，达27.0，夏普比率也最高，达0.85,但收益波动率与最大回撤最不理想，说明投资组合在高风险下能带来高收益，但面临的投资风险也较大。&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码+报告&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://www.windquant.com/qntcloud/v?b57c0b89-fc11-4856-aff9-ed80ca379fb6=&amp;amp;share=y&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;价值投资的逻辑--高质量、低估值选股模型&lt;/a&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;知识在于分享&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在量化投资的道路上&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你不是一个人在战斗&lt;/b&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-09-11-44279385</guid>
<pubDate>Tue, 11 Sep 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
