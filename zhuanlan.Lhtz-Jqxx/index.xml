<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>【量化投资与机器学习】微信公众号</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/</link>
<description>公众号主要介绍关于量化投资和机器学习的知识和应用。通过研报，论坛，博客，程序等途径全面的为大家带来知识食粮。版块语言分为：Python、Matlab、R，涉及领域有：量化投资、机器学习、深度学习、综合应用、干货分享等。</description>
<language>zh-cn</language>
<lastBuildDate>Sun, 08 Apr 2018 00:57:08 +0800</lastBuildDate>
<item>
<title>【深入解读】为什么机器学习在投资领域并不是那么好用（系列57）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-04-06-35353081.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35353081&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-544eb569857cbbf6a02b328a609f3d6e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者 | 石川 公众号特约作者&lt;/p&gt;&lt;p&gt;前六期传送门：&lt;/p&gt;&lt;p&gt;【系列56】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287672&amp;amp;idx=1&amp;amp;sn=9f59afb57b99cab6692367578c0aaa70&amp;amp;chksm=802e36edb759bffbd644791bb16bbe1ee0042ac3118421976e13141b807f90ae1f3d752c3450&amp;amp;scene=21#wechat_redirect&quot;&gt;特征重要性在量化投资中的深度应用&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列55】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287642&amp;amp;idx=1&amp;amp;sn=a7c71f89c3ad6f60590585b1bf016780&amp;amp;chksm=802e36cfb759bfd9a629eae57fc430ff457753a138d053cb7666d55cd41f91809d821285a521&amp;amp;scene=21#wechat_redirect&quot;&gt;机器学习应用量化投资必须要踩的那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列54】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287512&amp;amp;idx=1&amp;amp;sn=14ee62549dab3c64468f78b3dbfd39a5&amp;amp;chksm=802e364db759bf5bb5abffc6a50f72d0e31722c178e01ce11a3d48fb28386055e741c9ecce8d&amp;amp;scene=21#wechat_redirect&quot;&gt;因子的有效性分析基于7种机器学习算法&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列53】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287481&amp;amp;idx=1&amp;amp;sn=dcb1dda1e2362d8297ae1a97845cf02e&amp;amp;chksm=802e362cb759bf3a3aaea75af824451a3dba7345ecc73e27facc4b917792835fdd2878403c8c&amp;amp;scene=21#wechat_redirect&quot;&gt;基于XGBoost的量化金融实战&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;今天，继续我们的机器学习应用量化投资系列。本期我们再介绍一篇公众号特约作者石川的文章，为大家深入讲解机器学习和投资之间的关系。&lt;/p&gt;&lt;p&gt;“ &lt;i&gt;The essence of data snooping is that focusing on interesting events is quite different from trying to figure out which events are interesting.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;译：关注有趣的事件与弄清楚哪些事件是有趣的是两码事，这就是数据迁就的本质。&lt;/i&gt;”&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最近，一条新闻引爆了投资圈：世界上最大的投资管理公司贝莱德（BlackRock）宣布将使用机器（确切的说是人工智能 artificial intelligence 或机器学习算法 machine learning algorithm）来取代一些基金经理进行选股。近年来，随着其在人脸识别，信用反欺诈乃至国际象棋和围棋领域的应用和杰出表现，人工智能被越来越多的人所熟悉。很多人开始看好在不久的将来机器学习算法在二级市场投资上将会比人取得更加优异的成绩。而贝莱德的这一宣布无疑将人工智能又一次推上了风口浪尖。这其中最根本的观点是：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;机器学习通过可以使用复杂的各种非线性算法（比如神经网络、决策树、遗传算法）来从大量的历史交易数据中挖掘出人类无法看到的投资模式。根据这些模式来选股就可以取得丰厚收益。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;虽然身处并坚定地看好量化投资领域，但我对“机器学习在选股上能取代人类”这个观点上持保守和谨慎的态度。这是因为&lt;b&gt;金融分析属于非实验性科学（nonexperimental science），因此无法进行对照实验（scientific control或controlled experiments）&lt;/b&gt;。这意味着虽然存在大量的金融交易数据，但是无法通过设计实验来控制自变量的变化、通过重复性试验来检验提出的假设（比如说机器学习发现的某种选股模式）。&lt;b&gt;如此的数据分析得到的大多是看似显著但实际上是欺骗式的模式（尤其对样本外数据），这个现象称作数据迁就（data snooping）。&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;数据迁就（data snooping）：从数据中挖掘子虚乌有的模式（finding patterns in the data that do not exist）。&lt;/b&gt;数据迁就问题存在于所有的非实验性研究中，而当我们把复杂的机器学习算法用于选股时，这种问题尤甚。这是因为复杂的非线性算法中包含大量的参数，通过这些参数的配合总能发现一些人类无法理解的、可以获得超额收益的选股模式。如果不能正确地理解并从业务上解释这些模式，数据迁就将使复杂的机器学习算法成为从历史数据中发现&lt;b&gt;无效巧合&lt;/b&gt;的高效工具，正如本文开头的引用所说的那样。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;使用伪素数选股&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;来看一个和股票八竿子打不着的选股算法。传统的基金经理恐怕绞尽脑汁也想不出这么个模式，但是机器学习算法可以轻易地（但是错误地）找出它。这个算法利用了素数（质数）的一个性质，它来自费马小定理的一个变种：除了 2 之外，任何一个素数 x 满足“2 的 x-1 次方被它自身除的余数为 1”。举个例子，13 是一个素数，2 的 13-1（即 12）次方等于 4096。用它除以 13 得到 315，余数为 1。可以证明，所有 2 以外的素数都满足这个性质。但是满足这个性质的数不一定都是素数，它们被称为伪素数（又称为卡迈克尔数）。一万以内的伪素数有七个：561，1105，1729，2465，2821，6601，以及 8911。我们利用这些伪素数来对美股进行选股：选择股票编号中包含上述伪素数的股票进行投资。按照这个规则，Ametek公司（一个制造企业，股票编号03110510）脱颖而出。更令人称奇的是，它在过去 40 年取得了 95 倍的累计收益，远超道琼斯工业或标普 500 指数。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f402b4ad4570a0cf5f529a7e32fe27bd_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;499&quot; data-rawheight=&quot;547&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;毫无疑问，这是一支非凡的股票，而我们的伪素数策略取得了巨大的成功。然而， 先别急着激动。我们需要好好审视一下：伪素数和选股到底有什么关系？答案是没有关系。那么这个策略是否真正找到了有效的选股模式？答案也是否定的。&lt;/p&gt;&lt;p&gt;有些人会马上跳出来说&lt;b&gt;“只要管用就行，为什么有用不重要！”。这种认知是非常危险的。对于选股这种非实验性问题，由于无法通过对照实验来检验假设，那么至少从业务上明白机器学习的算法为什么有效就显得格外重要。&lt;/b&gt;因此，“只要管用就行”是非常不负责任的态度。&lt;/p&gt;&lt;p&gt;这个例子代表了很多机器学习算法的问题：我们总可以使用复杂的非线性算法（比如神经网络）、通过过度优化参数发现回测中无敌的选股模式。在这个过程中，我们已然落入了数据迁就的陷阱。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;认知偏差加剧数据迁就&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在以下这些条件下很容易发生数据迁就问题，很显然它们都存在于二级市场投资中&lt;/p&gt;&lt;p&gt;1. 存在大量的数据。&lt;/p&gt;&lt;p&gt;2. 很多人都在使用同样的数据进行分析。&lt;/p&gt;&lt;p&gt;3. 缺乏业务理论或者无法控制变量。&lt;/p&gt;&lt;p&gt;4. 认知偏差“只要管用就行，为什么好使不重要”。&lt;/p&gt;&lt;p&gt;这其中前三条是市场的客观条件，而最后一条则植根于人们的认知错误。人类认知中总是倾向于追寻不同寻常的事件。只有当一些“不同寻常”的巧合发生时，我们才往往能关注到。瑞士心理学家荣格将人们对巧合的过度关注称为&lt;b&gt;共时性（synchronicity）&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;共时性：指“有意义的巧合”，用于解释因果律无法解释的现象，如梦境成真，想到某人某人便出现等（“说曹操、曹操到”）。荣格认为，这些表面上无因果关系的事件之间有着非因果性、有意义的联系，这些联系常取决于人的主观经验。当两者同时发生时，便称为“共时性”现象。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;通俗的说，当在时间和空间上毫无联系的两件事同时发生时，人们便会认为有一种超自然的神秘力量把它们联系在一起，并认为这种巧合具备某种意义。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;比如在上面的例子中，股票标码含有伪素数和股票获得了巨大的超额收益就是一个纯粹的巧合，这样的巧合被机器学习算法发现并呈现给使用者。如果使用者不试图去理解这两者到底是否真的有关系，便会由于共时性而将这种错误的巧合赋予某种意义，即机器学习发现了一个牛逼哄哄的选股模式。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;运气还是实力&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;前面说了这么多，目的当然不是为了否定人工智能和机器学习在二级市场的应用前景。&lt;/p&gt;&lt;p&gt;但我想说，对于人工智能发现的任何模式，它有效的前提是我们能够明白无误的理解它的含义。不能以此为基础便无法分辨出好的结果到底是来自运气还是实力。&lt;/p&gt;&lt;p&gt;我们使用顺序统计量（order statistic）解释了这样一个道理：&lt;/p&gt;&lt;p&gt;&lt;b&gt;在众多股票中，最好的那支总会有非常优秀的收益率；在众多的策略中，最厉害的那一个总会带来令人称奇的回报率。然而，通过计算独立样本的极值（顺序统计量）分布可知，这种结果实属必然。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;假设一个股票投资策略的年化收益率 X 符合均值为 10%，标准差为 20% 的正态分布。假设市场中有 m 个不同的策略，则它们中最好的那个的收益率 Y 是 X 的函数，Y = max(X1, X2, …, Xm)。下图是当 m = 3000 时，最好的那个的收益率分布和单一策略收益率分布的比较：最优策略的收益率分布在横坐标上向右移动且变的更窄。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4f0034fb8575884cfc7ecefd5a8bdf58_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;561&quot; data-rawheight=&quot;414&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;下图为 prob(Y≥0.7) 随策略个数 m 变化的结果。同时也给出了 Y 的均值和标准差随 m 的变化。随着 m 的增大，我们越来越确定总会有一些策略脱颖而出，年化收益率超过 70%。这种判断也同样可以被 Y 的均值和方差来证明：随着策略个数的增大，最优策略的年化收益率的均值在增加，且标准差在减小。 &lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-24fefba8753717f1b4d9a3dc133eb49e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;477&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这个结果说明，当存在大量不同的策略时，最好的那一个总会异常非凡。&lt;b&gt;但我们真正关心的问题是：这个策略到底是在茫茫历史数据中找到了虚假的模式，还是发现了一套真正的科学投资模式？我们必须从业务层面弄清楚它是如何工作的。&lt;/b&gt; &lt;/p&gt;&lt;p&gt;&lt;i&gt;&quot;As with any black box, if you don&#39;t know why it works, you won&#39;t realize when it&#39;s stopped working. Even a broken watch is right twice a day&quot;.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;译：机器学习算法犹如一个黑匣子，如果你不知道它为什么好使，你就不会知道它何时回失效。就连一块停摆的手表每天也能正确两次。&lt;/i&gt; &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;人工智能前路漫漫&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;其实，人们使用算法来选股并不是什么新鲜事。风险多因子模型就可以算是一个算法选股的策略。当然，它之所以有效是因为它使用的因子，比如成长因子、规模因子、动量因子等，都有着清晰的业务基础。近几年，很多人使用机器学习的复杂算法，比如支持向量机，来改进多因子选股。这些非线性算法构建了很多非线性的因子。比如，如果算法告诉我们&lt;b&gt;“雄安概念板块，且对数市值 ÷ 三个月动量的 e 次方大于 π”&lt;/b&gt;是一个好的模式，那我们就得好好琢磨琢磨了。&lt;/p&gt;&lt;p&gt;对于人工智能在二级市场投资的应用，一位具有丰富实战经验的量化投资前辈阐述过如下的观点，我对此十分认可：&lt;/p&gt;&lt;blockquote&gt;&quot;我们可以相信它（人工智能）能够捕获到那些人类根本无法察觉到的细微模式。&lt;b&gt;但是这些模式能够持续吗？这些模式会不会只是一些不会重复的随机噪声？&lt;/b&gt;人工智能领域的专家向我们保证他们有许多防范措施用以过滤那些瞬间噪声。并且，这些工具确实在消费者营销和信用卡欺诈检测上效果显著。消费者行为和诈骗行为的模式显然都具有较长的持续期，这使得这些人工智能算法即使包含大量参数也能有效运行。然而，以我的经验来看，要对金融市场进行预测，这种防范措施是远远不够的，并且对历史数据噪声的过度拟合还会带来严重后果。……相对于可以获取的大量相互独立的消费者行为和信用交易数据，&lt;b&gt;我们能够获取的在统计学意义上相互独立的金融数据是非常有限的&lt;/b&gt;。你可能会说，我们拥有大量分时金融数据可供使用。但实际上，&lt;b&gt;这些数据是序列相关的，并不是相互独立的。&lt;/b&gt;&quot; &lt;/blockquote&gt;&lt;p&gt;这位前辈对于人工智能何时有效给出了自己的见解：&lt;/p&gt;&lt;p&gt;1. 基于正确的计量经济学或理论基础，而不是随机发现的模式。 &lt;/p&gt;&lt;p&gt;2. 所需的参数用到历史数据较少。&lt;/p&gt;&lt;p&gt;3. 只用到线性回归，并未使用复杂的非线性函数。&lt;/p&gt;&lt;p&gt;4. 概念上很简单。&lt;/p&gt;&lt;p&gt;5. 所有优化都必须在不含未来未知数据的移动窗口中实现，并且这种优化的效果必须不断地被未来未知的数据所证实。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;策略的规则越多，模型的参数越多，就越有可能发生数据迁就。能经得起时间考验的往往是简单的模型。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;再来看贝莱德的决定&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;作为全球最大的资产管理公司，贝莱德宣布使用人工智能代替基金经理无法令人忽视，且必然会一石激起千层浪。有机构预测，到 2025 年，全球金融机构将有 10% 的人工会被机器取代。这恐怕和越来越高昂的 alpha 不无关系。毕竟，从长期来看，绝大多数基金经理都跑不赢指数，那么要这些基金经理还有什么用呢？&lt;/p&gt;&lt;p&gt;引用我的合伙人高老板的话也许可以更好的理解贝莱德的这个决定：&lt;/p&gt;&lt;p&gt;“ &lt;b&gt;超额收益越来越贵，开源不行，就想办法节流。最终投资市场的均衡状态是超额收益的边际成本恰好等于超额收益。这样成本高的投资基金终将不断被成本低的基金挤出市场。&lt;/b&gt;” &lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-04-06-35353081</guid>
<pubDate>Fri, 06 Apr 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>比较13种算法在165个数据集上的表现，你猜哪个最好？</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-04-03-35263566.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35263566&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3ccc7edf53b6d8026e3c2f61196993ba_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;原文：&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/VflRgUx563AVnMsEmf36FA&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-93ed78ce63d4ac6a675d8313d790b2a9&quot; data-image-width=&quot;888&quot; data-image-height=&quot;500&quot; data-image-size=&quot;180x120&quot;&gt;比较13种算法在165个数据集上的表现，你猜哪个最好？&lt;/a&gt;&lt;p&gt;作者 |  Jason Brownlee &lt;/p&gt;&lt;p&gt;编译 | 公众号编辑部&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;你应该使用哪种机器学习算法？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;这是应用机器学习中的一个让大家很捉急的问题。&lt;/p&gt;&lt;p&gt;在Randal Olson和其他人最近的一篇论文中，他们试图去回答它，并给出一个指导关于算法和参数。&lt;/p&gt;&lt;p&gt;在这篇文章中，你将展开一项研究和评估许多机器学习算法通过大量的机器学习数据集。并且得到对这项研究的一些意见。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;论文&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2017年，Randal Olson等人发表了一篇标题为&lt;b&gt;&lt;i&gt;“Data-driven Advice for Applying Machine Learning to Bioinformatics Problems” &lt;/i&gt;&lt;/b&gt;的论文。&lt;/p&gt;&lt;p&gt;论文下载地址：&lt;i&gt;https://arxiv.org/abs/1708.05070&lt;/i&gt;&lt;/p&gt;&lt;p&gt;他们的工作目标是解决每个从业人员在开始预测建模问题时所面临的问题，即：&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;我应该使用什么算法？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;作者将此问题描述为choice overload，如下所示：&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;Although having several readily-available ML algorithm implementations is advantageous to bioinformatics researchers seeking to move beyond simple statistics, many researchers experience “choice overload” and find difficulty in selecting the right ML algorithm for their problem at hand.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;他们通过在大量机器学习数据集的样本上运行其算法样本来解决这个问题，以了解通常哪些算法和参数最适合。&lt;/p&gt;&lt;p&gt;论文描述为：&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;… a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;机器学习算法&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A total of 13 different algorithms were chosen for the study.Algorithms were chosen to provide a mix of types or underlying assumptions.The goal was to represent the most common classes of algorithms used in the literature, as well as recent state-of-the-art algorithms The complete list of algorithms is provided below.&lt;/p&gt;&lt;p&gt;下面提供了完整的13种算法列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Gaussian Naive Bayes (GNB)&lt;/li&gt;&lt;li&gt;Bernoulli Naive Bayes (BNB)&lt;/li&gt;&lt;li&gt;Multinomial Naive Bayes (MNB)&lt;/li&gt;&lt;li&gt;Logistic Regression (LR)&lt;/li&gt;&lt;li&gt;Stochastic Gradient Descent (SGD)&lt;/li&gt;&lt;li&gt;Passive Aggressive Classifier (PAC)&lt;/li&gt;&lt;li&gt;Support Vector Classifier (SVC)&lt;/li&gt;&lt;li&gt;K-Nearest Neighbor (KNN)&lt;/li&gt;&lt;li&gt;Decision Tree (DT)&lt;/li&gt;&lt;li&gt;Random Forest (RF)&lt;/li&gt;&lt;li&gt;Extra Trees Classifier (ERF)&lt;/li&gt;&lt;li&gt;AdaBoost (AB)&lt;/li&gt;&lt;li&gt;Gradient Tree Boosting (GTB)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;scikit-learn库被用来实现这些算法。&lt;/p&gt;&lt;p&gt;每个算法具有零个或多个参数，并且针对每个算法执行合理参数值的网格搜索。&lt;/p&gt;&lt;p&gt;对于每种算法，使用固定的网格搜索来调整超参数。&lt;/p&gt;&lt;p&gt;下面列出了算法和超参数评估表：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8e5827d860c739e80e73521765186785_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1606&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;使用10倍交叉验证和平衡准确性度量来评估算法。&lt;/p&gt;&lt;p&gt;交叉验证没有重复，可能会在结果中引入一些统计噪音。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;机器学习数据集&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;研究选择了165种标准机器学习问题。&lt;/p&gt;&lt;p&gt;许多问题来自生物信息学领域，尽管并非所有数据集都属于这一研究领域。&lt;/p&gt;&lt;p&gt;所有的预测问题都是两类或更多类的分类问题。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;The algorithms were compared on 165 supervised classification datasets from the Penn Machine Learning Benchmark (PMLB). […] PMLB is a collection of publicly available classification problems that have been standardized to the same format and collected in a central location with easy access via Python.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;数据集来自Penn机器学习基准（PMLB）集合，你可以在GitHub项目中了解关于此数据集的更多信息。地址：&lt;i&gt;https://github.com/EpistasisLab/penn-ml-benchmarks&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在拟合模型之前，所有数据集均已标准化。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结果分析&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;The entire experimental design consisted of over 5.5 million ML algorithm and parameter evaluations in total, resulting in a rich set of data that is analyzed from several viewpoints…&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;对每个数据集对算法性能进行排名，然后计算每个算法的平均排名。&lt;/p&gt;&lt;p&gt;这提供了一个粗略和容易理解每一种算法在平均情况下好或不好活的方法。&lt;/p&gt;&lt;p&gt;结果表明，&lt;b&gt;梯度提升&lt;/b&gt;（Gradient boosting）和&lt;b&gt;随机森林&lt;/b&gt;（random forest ）的排名最低（&lt;b&gt;表现最好&lt;/b&gt;），&lt;b&gt;朴素贝叶斯&lt;/b&gt;（Naive Bayes）平均得分最高（&lt;b&gt;表现最差&lt;/b&gt;）。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;The post-hoc test underlines the impressive performance of Gradient Tree Boosting, which significantly outperforms every algorithm except Random Forest at the p &amp;lt; 0.01 level.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;通过这张图，展示了所有算法的结果，摘自论文。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fd89f383b60e8e51dc96657237ce775d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;481&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;没有单一的算法表现最好或最差。&lt;/p&gt;&lt;p&gt;这是机器学习实践者所熟知的，但对于该领域的初学者来说很难掌握。&lt;/p&gt;&lt;p&gt;你必须在一个给定的数据集上测试一套算法，看看什么效果最好。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;… it is worth noting that no one ML algorithm performs best across all 165 datasets. For example, there are 9 datasets for which Multinomial NB performs as well as or better than Gradient Tree Boosting, despite being the overall worst- and best-ranked algorithms, respectively. Therefore, it is still important to consider different ML algorithms when applying ML to new datasets.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;此外，选择正确的算法是不够的。你还必须为数据集选择正确的算法配置。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;选择正确的ML算法并调整其参数对于大多数问题是至关重要的。&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;结果发现，根据算法和数据集的不同，调整算法可将该方法的性能从提高至3%——50％。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;The results demonstrate why it is unwise to use default ML algorithm hyperparameters: tuning often improves an algorithm’s accuracy by 3-5%, depending on the algorithm. In some cases, parameter tuning led to CV accuracy improvements of 50%.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;本图表展示了参数调整对每种算法的改进情况。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c7d8e5b6e06560538d230ca1f7bf6497_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1078&quot; data-rawheight=&quot;1086&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;并非所有算法都是必需的。&lt;/p&gt;&lt;p&gt;结果发现，在165个测试数据集中的106个中，五种算法和特定参数的性能达到Top1％。&lt;/p&gt;&lt;p&gt;推荐这五种算法:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Gradient Boosting&lt;/li&gt;&lt;li&gt;Random Forest&lt;/li&gt;&lt;li&gt;Support Vector Classifier&lt;/li&gt;&lt;li&gt;Extra Trees&lt;/li&gt;&lt;li&gt;Logistic Regression&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;i&gt;The paper provides a table of these algorithms, including the recommend parameter settings and the number of datasets covered, e.g. where the algorithm and configuration achieved top 1% performance.&lt;/i&gt;&lt;/blockquote&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a70cb366b0e862f1f3ba24b8251a93e2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;811&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;实际结果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本文有两个重要的发现对于从业者是有价值的，尤其是对那些刚开始学习机器学习算法或者对此有困惑的人。&lt;/p&gt;&lt;p&gt;1、使用Ensemble Trees&lt;/p&gt;&lt;p&gt;The analysis demonstrates the strength of state-of-the-art, tree-based ensemble algorithms, while also showing the problem-dependent nature of ML algorithm performance.&lt;/p&gt;&lt;p&gt;2、Spot Check and Tune&lt;/p&gt;&lt;p&gt;没有人可以看到你的问题，并告诉你使用什么算法。&lt;/p&gt;&lt;p&gt;你必须为每种算法测试一套参数，以查看哪些方法更适合你的特定问题。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;In addition, the analysis shows that selecting the right ML algorithm and thoroughly tuning its parameters can lead to a significant improvement in predictive accuracy on most problems, and is there a critical step in every ML application.&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;这个问题的讨论请查看这个链接：&lt;i&gt;https://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;进一步阅读&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果你希望深入了解类似问题，这里提供了有关该主题的更多资源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Data-driven Advice for Applying Machine Learning to Bioinformatics Problems（&lt;i&gt;https://arxiv.org/abs/1708.05070&lt;/i&gt;）&lt;/li&gt;&lt;li&gt;scikit-learn benchmarks on GitHub（&lt;i&gt;https://github.com/rhiever/sklearn-benchmarks&lt;/i&gt;）&lt;/li&gt;&lt;li&gt;Penn Machine Learning Benchmarks（&lt;i&gt;https://github.com/EpistasisLab/penn-ml-benchmarks&lt;/i&gt;）&lt;/li&gt;&lt;li&gt;Quantitative comparison of scikit-learn’s predictive models on a large number of machine learning datasets: A good start（&lt;i&gt;https://crossinvalidation.com/2017/08/22/quantitative-comparison-of-scikit-learns-predictive-models-on-a-large-number-of-machine-learning-datasets-a-good-start/&lt;/i&gt;）&lt;/li&gt;&lt;li&gt;Use Random Forest: Testing 179 Classifiers on 121 Datasets（&lt;i&gt;https://machinelearningmastery.com/use-random-forest-testing-179-classifiers-121-datasets/&lt;/i&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原文：&lt;i&gt;https://machinelearningmastery.com/start-with-gradient-boosting/&lt;/i&gt;&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-04-03-35263566</guid>
<pubDate>Tue, 03 Apr 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>小猪佩奇 × 全网量化平台 | 圣杯的开始！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-04-02-35228649.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35228649&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6d0351de8bf2e491b53e167b74bb1b9d_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;原文：&lt;/p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s/OC7pIswmKz9ER3IFB1u6kg&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-c122cb5b7e1da58ce5ed0b6d9f03b659&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;722&quot; data-image-size=&quot;180x120&quot;&gt;小猪佩奇 × 全网量化平台 | 圣杯的开始！&lt;/a&gt;&lt;p&gt;最近网上流行起了小猪佩奇和国际大牌的联名系列图片（虽然是恶搞）&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-db913f6a0e2266efa1a22617b8761893_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1078&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;网上都说小猪佩奇是社会人，那社会人怎么能少了和我们投资界的明星——&lt;b&gt;量化投资&lt;/b&gt;合作呢？&lt;/p&gt;&lt;p&gt;今天公众号推出佩奇合作系列之——&lt;b&gt;全网量化平台深深度&lt;/b&gt;合作小猪佩奇。带你领略我们量化界最活跃的一个组织。&lt;/p&gt;&lt;p&gt;这个组织为国内的量化事业做出了突出贡献。为那些寻找圣杯的人搭设了一条便捷的道路，助力那些有理想、有想法的宽客实现自己的量化之梦！&lt;/p&gt;&lt;p&gt;下面我们就来看看这些组织和小猪佩奇的联名吧&lt;b&gt;（全网的量化平台都在这里了！）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;没有先后顺序，随机排版&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;万得——万矿&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fbc5ed2ad82a8990f945cb851be304e0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;通联数据——优矿&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-04bdda801094be5dc82a9f17d0b09e7a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;聚宽&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e5662cabc0a6e90e436ba4b8c0ad0a29_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;米筐&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d28631d2d5e7398f61564c13bfea2ba3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;BigQuant&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d2ada88cec647293ee6995716674924b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;京东金融——JD Quant&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f9a9290b8627bc12fdf1b911e3f21df1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;点宽网&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-95d990945bd9ed32d0cf8644c1cbf595_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;东方财富——Choice&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2946af35a449038cbedde3ce574e84c7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;同花顺——Mind GO&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dfa9e91dc2fed3df249f6031fb04a740_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;MT4&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-499ed2ec280774d88578e5b51ed522a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;VN.PY&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ae56c51e9571abdc0c88a708887cc568_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;国信证券——TS&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3b1579760ede599eedeb81c6606568b8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;掘金量化&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a77a7cde6109f12a1856c7228a9a9c29_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;天软&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9e0c71fe162ae51ee77b52a8acf8c56b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;国泰安——Quantrader&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e592a4b18f4d129a520c1a6509efa387_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;量邦科技——大宽网&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-04904ee31dde4fa79967a545bb09f5f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;大鱼金融&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-48d7651a7894c4f65b1a151e94b21160_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;Quicklib&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ed9aa22992033902b419e29cd17b884_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;TB——交易开拓者&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0a24639e6a87030bfd7dbdf82a2bd49d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;文华财经&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-36f21f21b42e7caa0fcfa76788db67d3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;金字塔交易系统&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2e2a35a7b1b5af7c5f0e59071d283630_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;OpenQuant——SmartQuant&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-552fa8971c62a116e37ded132ec40314_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;镭矿&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-466a691847052dc32d8d98c8e702c473_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;h2&gt;&lt;b&gt;况客&lt;/b&gt;&lt;/h2&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6f750a56d0df894acedbe8565aeabd5c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;1000&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;b&gt;如果你是量化社会人快点个赞吧！&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c0691e5b33f4ca984165cb1b8e89259c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;806&quot; data-rawheight=&quot;800&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-04-02-35228649</guid>
<pubDate>Mon, 02 Apr 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>因子的有效性分析基于7种机器学习算法【系列54】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-14-34543283.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34543283&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f38a8880f244e1c99b1ba8cec08e9116_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;今天，继续我们的机器学习应用量化投资系列。本期我们介绍一篇研究报告，详细的介绍了7中机器学习算法在因子有效性上的展现。希望给大家在写策略时做一些参考借鉴。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;逻辑依旧明了，机器学习并非黑箱&lt;/b&gt; &lt;/p&gt;&lt;p&gt;谈到机器学习，大家最忌讳的便是黑箱问题。其实不必，理解机器学习算法，逻辑实则简单，比如相同的因子特征将会有相同的表现。在实战中，我们发现， 该逻辑十分有效，在我们的机器学习选股模型中，该逻辑连续十几年不曾被打破。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Adaboost 最稳定，朴素贝叶斯收益最高&lt;/b&gt; &lt;/p&gt;&lt;p&gt;全市场选股，市值中性选股等权加权，行业中性选股等权加权五种情况下， AdaBoost 年化波动率基本在 5%左右，表现非常稳定。朴素贝叶斯，年化收益分别达到 15.50%，15.75%， 12.89%，15.63%， 10.23%。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全市场训练明显优于市值中性或行业中性&lt;/b&gt; &lt;/p&gt;&lt;p&gt;分别在全市场、等市值组（按市值大小分 20 小组）、行业内部进行了训练预测。研究发现，在市值中性情况下，全市场训练得到的因子明显优于市值内部训练的因子；在行业中性情况下，全市场训练得到的因子亦优于行业内部训练得到的因子。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;机器学习因子单调性十分显著&lt;/b&gt; &lt;/p&gt;&lt;p&gt;把机器学习训练得到的个股相对强势值进行排序，按大小分成 5组， 研究发现排名靠前的小组明显优于排名靠后的小组，且单调性十分显著。 20090105 到 20171231， 前二组年化收益在 25%以上，而第四组收益不到 20%，最后一组收益不到 10%。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;人工智能有比较快速的自适应调整能力&lt;/b&gt; &lt;/p&gt;&lt;p&gt;20090105 到 20171231 期间，任何一年的月度平均 IC 均大于 0，且近一半的年份平均 IC 大于 8%，期间所有月度平均 IC 为 5.78%。虽然 IC 有负的情况，但在动态的训练中，人工智能模型能够快速的调整以适应市场。以 2017 年为例，前几个月 IC 几乎为负，但在后半年，模型迅速反应， IC 基本为本。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;人工智能选股模型策略(Logistic 为例)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;以传统因子滚动 12 个月值为特征值，个股下一期按收益大小排序， 排名前 30%作为强势股，排名靠后 30%作为弱势股。用机器学习算法进行训练预测。用当期因子作为输入，预测未来一个月个股相对走势的强弱。根据个股的相对强势，我们把排名靠前20%的作为多头，排名后 20%的作为空头进行了研究， 样本外20090105 到 20171231 期间， 行业中性等权年化多空收益差为16.45%， 年化波动率为 7.34%， 最大回撤为 10.84%。&lt;/p&gt;&lt;p&gt;人工智能从自诞生以来，理论和技术日益成熟，应用领域也不断扩大，金融领域也是将其改革的一大领域。谈到人工智能机器学习，大家最忌讳的便是黑箱问题，其实不必，理解机器学习算法，逻辑实则简单，比如相同的因子特征将会有相同的表现，以此简单的逻辑，我们实证中发现效果比较显著。人工智能机器人将能够自动生成研究报告替代分析师，发明策略替代主动基金经理进行投资等等，虽然这些目前来说不太普及，不是很成熟，但机器学习人工智能作为工具，为我们提高工作效率是毫无疑问的，并且随着技术的发展，金融领域将可能迎来颠覆性的改革。&lt;/p&gt;&lt;p&gt;本文主要尝试分类算法在投资中的运用，所以在此把文中涉及的分类算法都进行简单介绍。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、相关分类算法概述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. Logistic&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Logistic回归是研究二分类观察结果与一些影响因素之间关系的一种多变量分析方法。通常的问题是，研究某些因素条件下某个结果是否发生。根据线性回归可以预测连续的值，对于分类问题，我们需要输出0或者1。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. KNN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。通常情况下，k的取值为样本数量的开方。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. AdaBoost&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. SVM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;使用SVM算法的思路：（1）简单情况，线性可分情况，把问题转化为一个凸优化问题，可以用拉格朗日乘子法简化，然后用既有的算法解决；（2）复杂情况，线性不可分，用核函数将样本投射到高维空间，使其变成线性可分的情形，利用核函数来减少高纬度计算量。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. 朴素贝叶斯&lt;/b&gt;&lt;/p&gt;&lt;p&gt;贝叶斯定理也称贝叶斯推理，早在18世纪，英国学者贝叶斯(1702～1763)曾提出计算条件概率的公式用来解决如下一类问题：假设B[1],B[2]…,B[n]互斥且构成一个完全事件，已知它们的概率P(B[i]),i=1,2,…,n,现观察到某事件A与B[1],B[2]…,B[n]相伴随机出现，且已知条件概率P(A/B[i])，求P(B[i]/A)。朴素贝叶斯即特征条件相互独立。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.  决策树&lt;/b&gt;&lt;/p&gt;&lt;p&gt;决策树主要步奏（ID3）:1.对当前例子集合，计算属性的信息增益；2.选择信息增益最大的属性Ai；3.把在Ai处取值最大的例子归于Ai子集，Ai有几个属性就有几个子集;4.对依次对每种取值情况下的子集,递归调用建树算法，即返回1;5.若子集只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处。其中，1）信息熵其实是信息量的期望。2）熵：表示随机变量的不确定性。3）信息增益：在一个条件下，信息不确定性减少的程度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7. 随机森林&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、各分类算法的是与非&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;本文中，我们主要使用了七大分类算法进行个股打分分类，这七大算法基本涵盖了目前主流的比较成熟的传统机器学习分类算法，当我们实际使用时，可能会比较困惑，或者不知道哪个分类算法更适合我们，为此，我们把各大分类算法进行梳理。&lt;/p&gt;&lt;p&gt;首先，考虑一个算法是否符合我们的需求，我们需要明确自己的使用环境，我们的训练样本的数量如何？特征空间的维数如何？我们的分类是否是线性可分离呢？各因子特征是否相互独立？过度拟合是否将成为一个问题？对我们的系统在速度、性能、内存占用方面要求如何？对我们自己的需求有了个完整的了解之后，我们才可选择合适的算法。以下是各大分类算法的优与缺，是与非。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Logistic&lt;/b&gt;&lt;/p&gt;&lt;p&gt;逻辑回归是一种具有很好表现的分类算法，它抗噪声干扰能力强，并且你可以通过使用l2和l1正则化的方法来对特征进行选择从而避免过度拟合。适合当你需要一个概率框架（例如，通过简单的调整分类阈值，来得知不确定区间或置信区间confidence intervals）或者如果你希望将来能在训练集中加入更多的数据并很快的融入你的模型。缺点：1、容易欠拟合，一般准确度不太高；2、只能处理两分类问题，且必须线性可分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. KNN&lt;/b&gt;&lt;/p&gt;&lt;p&gt;思路简单，理论成熟，既可以用来做分类也可以用来做回归，可用于非线性分类，训练时间复杂度为O(n)，准确度高，对数据没有假设，对outlier不敏感。缺点是计算量大，消耗很多内存，因为要存储所有的实例，对低维空间效果更好，不适合高维空间。当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本，可以采用权值的方法（和该样本距离小的邻居权值大）来改进。样本容量较小的类域采用这种算法比较容易产生误分。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. AdaBoost&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Adaboost是一种有很高精度的分类器，容易实现，分类准确率较高，没有太多参数可以调，不会过拟合，可以使用各种方法构建子分类器,Adaboost算法提供的是框架，当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单，不用做特征筛选，不用担心overfitting(过度拟合)。缺点是容易受到噪声干扰，这也是大部分算法的缺，训练时间过长，执行效果依赖于弱分类器的选择，对outlier比较敏感。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. SVM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;原始的SVM只比较擅长处理二分类问题，可用于线性/非线性分类，也可以用于回归；低泛化误差；容易解释；计算复杂度较低。Support Vector Machines (SVMs) 使用与LR不同的损失函数（Hinge）。他们的解释也不相同（最大间距）。实际上，一个使用线性核的SVM与前面介绍的逻辑回归没有太大的区别。使用SVM代替逻辑回归的主要原因是因为你的问题可能不是线性可分的。这种情况下，你可能必须要使用一个SVM并且使用一个非线性的核（例如，RBF）。实际上，逻辑回归同样可以使用不同的核，但是重点是你可能在实际使用中发现SVM更好用。另一个使用SVM的原因是如果你的数据是在一个高维空间中。高精度、对过拟合有较好的理论保证，并且使用一个合适的核可以得到较好的效果，甚至你的数据在特征空间里不是线性可分的。特别是在高维空间的像文本识别的问题中效果好。SVMs的主要缺点是它的费劲、低效率的训练过程,对参数和核函数的选择比较敏感,当你有很多训练样本时，我不推荐你使用SVM来处理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5. Nbayes（朴素贝叶斯）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;表现简单，不能做丰富的假设,相互独立的属性的假设限制太多，不能学习不同特征之间的相互作用。如果假设的Nbayes条件相互独立性成立，那朴素贝叶斯比其他辨别模型如逻辑回归要快，你只需要较少的训练数据即可。并且即使假设条件不成立，朴素贝叶斯分类器在实际使用中也通常有较好的效果。如果你的训练集很小，高偏离/低方差的分类器（例如，朴素贝叶斯）比低偏离/高方差的分类器（如，KNN-K近邻）有着很大的优势，因为后者容易过拟合。但是低偏离/高方差的分类器随着你的训练集的增大变得越来越有优势（他们有很小的渐近误差），因为高偏离的分类器不能提供高精度的模型。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6. 决策树&lt;/b&gt;&lt;/p&gt;&lt;p&gt;决策树模型可读性好，理解和解释起来简单，具有描述性，有助于人工分析；效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。决策树模型可以想象,需要准备的数据量不大，决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数,使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，可能使用统计检验来验证模型，这是为了验证模型的可靠性。缺点是决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度，决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决,有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题，决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7. 随机森林&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随机森林是建立在决策树的基础上，决策树在分类时是选择所有变量，而随机森林则是产生很多决策树，然后每根决策树选择不同的变量，进行分析，最后选取决策树中的众数，作为最终结果。优点可以作用在高维数据中，具有很好的抗干扰能力,可以并行处理等。与逻辑回归相比有不同的优势。一个主要的优势就是它们不期望线性的特征或者相互之间有线性作用的特征。我在讲逻辑回归时没有提到的一点是，逻辑回归能很好的应对二值得特征（即绝对的特性，要么零，要么一）。随机森林里因为有许多的决策树组成，可以很好的处理连续型的特征。其它的主要优点是，因为它的结构组成（使用装袋或加速），这类算法可以很好的处理高纬空间的大数量的训练样本。随机森林在处理很多分类问题时效果更好可能会过拟合，可能会陷入局部最小值的情况，所以需要集（ensembles）来帮助降低变量，另一缺点是可能有很多相似的决策树，掩盖了真实的结果，对小数据或者底维数据可能不能产生最好的分类执行数据，虽然比bossting等快，但比单只决策树慢多了。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、策略概述&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;谈到机器学习，大家最忌讳的便是黑箱问题。其实不必，理解机器学习算法，逻辑实则简单，比如相同的因子特征将会有相同的表现。在实战中，我们发现，该逻辑十分有效，在我们的机器学习选股模型中， 首先构建机器学习因子，然后根据个股的相对强势即机器学习因子，我们把排名靠前 20%的作为多头，排名后 20%的作为空头进行回测；进一步，我们把相对强势分成 5 组，以考察这一指标的单调性，发现效果十分显著。同样，为了去除市值和行业的影响，我们也分别测试了市值中性及行业中性的情况下的表现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.1 机器学习因子构建&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Step1：以传统的成长因子，盈利因子，财务因子，市场因子，估值和规模因子等最近历史 12 个月（即滚动一年）为特征值。&lt;/p&gt;&lt;p&gt;Step2：把未来一期个股收益的大小排序，选取排名前 30%的作为强势股，标签为 1，选取排名靠后 30%的作为弱势股，标签为 0。&lt;/p&gt;&lt;p&gt;Step3：用 knn,Logistic,svm 等七大机器学习分类算法进行训练预测。&lt;/p&gt;&lt;p&gt;Step4：用最新一期因子作为特征输入，通过机器学习算法预测得到个股未来一期相对强势值，即机器学习因子。&lt;/p&gt;&lt;p&gt;Step5：分别在全市场、等市值中（按市值大小分 20 小组）、行业内部进行了训练预测。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1) 数据预处理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;a. 没满一年的新股不进行机器学习因子计算： 因为需要用最近历史一年的数据作为训练。&lt;/p&gt;&lt;p&gt;b. 对于缺失值，用平均值代替，当缺失达到 10%，则该因子丢弃。&lt;/p&gt;&lt;p&gt;c. z-score 标准化，要求原始数据的分布可以近似为高斯分布，否则效果不好。&lt;/p&gt;&lt;p&gt;对 a_value,turnover_1 等这一类不不符合高斯分布因子，需要用 ln(t1/t0）（同一个股当期与上期比值的对数）进行处理，才近似高斯分布。 但对 sec_return_1,MACD 等这一类变化率等相关因子，直接用原始值便可以，因为他们本身已经近似符合正态分布。&lt;/p&gt;&lt;p&gt;预处理之所有没有处理掉极值和去掉涨跌停个股因子，原因是因为此处只是训练特征， 而不是最终选股。再次，我们所选的因子是经过人工核对的，基本没有太多相似性，故也没有降维这一步。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2) 中性化处理&lt;/b&gt;&lt;br&gt;中性化处理我们包含二层含义，一是市值中性化，二是行业中性化。&lt;/p&gt;&lt;p&gt;首先， 我们都知道， 市值因子对个股的影响十分显著， 如果不考虑市值带来的干扰，则我们的策略可能被市值因子带来严重的影响。为此，我们市值分成 20 组，分别在不同市值组各选取 20%作为策略多头与空头， 使多头与空头有相同的市值分布，以消除市值可能带来的影响。&lt;/p&gt;&lt;p&gt;其次， 众所周知，不同行业，因子特征可能差异明显， 放在一起可能不具备可比性。为了去除行业带来的影响，我们也分别在不同行业选取 20%作为我们的空头与多头， 使多头与空头保持同样的行业暴露， 以消除行业带来的影响。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2 策略计算&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在进行策略计算时，考虑了以下几种情况：&lt;/p&gt;&lt;p&gt;a. 当期单个因子在全市场缺失达 40%时，则该因子丢弃，不进行计算。&lt;br&gt;b. 调仓当天停牌，涨停，跌停个股剔除。&lt;br&gt;c. 新股一个月之内不能作为候选股（上市小于 20 个交易日） 。&lt;/p&gt;&lt;p&gt;在 20090105 到 20171231 期间，我们分别进行了全市场选股，市值中性选股，行业中性选股，五种情况表现如下：&lt;/p&gt;&lt;p&gt;相关说明：&lt;br&gt;1） 所用因子：全市场训练得到的个股未来相对强势值。&lt;/p&gt;&lt;p&gt;2） L/S：全市场选股多空收益差净值。相对强势值排名靠前 20%作为多头，相对强势值排名后 20%作为空头。&lt;/p&gt;&lt;p&gt;3） aL/Se：市值等权多空收益差净值。分 20 小组，分别在组内选前 20%作为多头，后 20%作为空头，最后各组等权。&lt;/p&gt;&lt;p&gt;4） aL/Sw：市值加权多空收益差净值。分 20 小组，分别在组内选前 20%作为多头，后 20%作为空头，最后各组以市值组权重加权得到多空组合。&lt;/p&gt;&lt;p&gt;5） iL/Se：行业等权多空收益差净值。在中信一级行业，分别在行业内选前 20%作为多头，后 20%作为空头，最后各行业以等权到多空组合。&lt;/p&gt;&lt;p&gt;6） iL/Sw：行业加权多空收益差净值。在中信一级行业，分别在行业内选前 20%作为多头，后 20%作为空头，最后各行业以沪深 300 行业内权重加权得到多空组合。 &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、 AdaBoost、 knn 最稳定， Bayes 表现最好&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;经过实证研究， logistic， knn， AdaBoost， svm， Nbayes，随机森林，决策树七大分类算法表现虽然总体来说比较接近，但差别也比较明显，以下是实证得到的多空收益差表现。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c7e5f908ffa67551846b9a5b940b4666_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;380&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0d1c0f7cd72a845c1c59f701776c4c47_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;370&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ece7fcd91a80a4fd4a65e6f70bf833a0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;341&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6259464df5bca2505857670eaaaaf231_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;356&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5b3a67c7bff894b1ec53eec48bebf626_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;244&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0d7b64e7e4e03a8682b41cebb0d6492a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;236&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8d4cd6ea5e4376b0edc82a240879e194_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;236&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d64509b5085b3aebaadfdc3abe48311b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;44&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-02ddc719af471bb12b297f85acfd0719_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;196&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-21defcb9b6ae673944c380131cfb2b6d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;238&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3be5c812c926bc51ff4ef86b35ae59d4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;239&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9d9cd67a40f93861a497c01f57218a1c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;233&quot;&gt;&lt;p&gt;从以上结果可以看出， 在收益年化 10%以上的算法中， 在全市场选股，市值中性选股等权加权，行业中性选股等权加权五种情况下， AdaBoost 与 knn 算法年化波动率基本在 5%左右，表现非常稳定；而朴素贝叶斯则表现最好，年化收益分别达到 15.50%， 15.75%， 12.89%， 15.63%， 10.23%，在所有算法中，唯一的五种情况年化收益超过 10%的策略。 Logistic 表现非常接近朴素贝叶斯， svm 表现也较优， 而决策树和随机森林则表现不佳，年化收益基本在 3%左右。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、 人工智能有比较快速的自适应调整能力&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;七大分类算法中，我们分别计算出其月度 IC（即每个月因子排序与未来期个股收益排序的相关系数），统计各年度 IC 平均值，发现分类算法中除了决策树以外，其余算法得到的机器学习因子对个股未来收益皆有比较显著的相关性，且年度月 IC 均值都大于为 0，表示正相关性非常稳定。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bf588d6e809bda76ca90e6dcd2c96aa9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;369&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6b57c6cd2b95402d95ee6ff9c49e895e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;345&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-40870a6452f2dd9da26ac8804acacde2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;360&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d800eeea032a786bf5bc432ce689f149_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;362&quot;&gt;&lt;p&gt;20090105 到 20171231 期间， logistic， knn， AdaBoost， svm， Nbayes 五大算法任何一年的月度平均 IC 均大于 0；其中， 朴素贝叶斯期间月度平均 IC 最高，达到 5.88%，且年度月度平均 IC 几乎都大于 6%， logistic 近一半的年份平均 IC 等于 8%左右，期间所有月度平均 IC 为 5.31%。而决策树期间月度平均 IC 最低，为 0.36%且所有年度月度平均 IC 均小于 1%，说明预测性相对较差，几乎没有关联性。&lt;/p&gt;&lt;p&gt;动态的训练中，人工智能模型能够快速的调整以适应市场。以 2017 年 logistic 市值中性等权为例，前几个月 IC 几乎为负，但在后半年，模型迅速反应， IC 基本为本，且该年度平均月度 IC 依旧达到正的 1.49%。&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;六、 全市场训练明显优于市值中性或行业中性&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;前面讲道过，我们训练分 3 种情况， 即在全市场、等市值中（按市值大小分 20 小组）、行业内部进行了训练，分别得到相关的机器学习因子。以下我们分析哪种训练效果比较好。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d9d38c1ec5fba90640dc3f0d48869c3b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;395&quot;&gt;&lt;p&gt;1） 行业内训练因子等权：表示该因子为中信一级行业内训练得到；在中信一级行业，分别在行业内选前20%作为多头，后 20%作为空头，最后各行业以等权得到多空组合。&lt;/p&gt;&lt;p&gt;2） 全市场训练因子等权：表示该因子为全市场训练得到；在中信一级行业，分别在行业内选前 20%作为多头，后 20%作为空头，最后各行业以等权得到多空组合。&lt;/p&gt;&lt;p&gt;3） 行业内训练因子加权：表示该因子为中信一级行业内训练得到；在中信一级行业，分别在行业内选前20%作为多头，后 20%作为空头，最后各行业以沪深 300 行业内权重加权得到多空组合。&lt;/p&gt;&lt;p&gt;4） 全市场训练因子加权：表示该因子为全市场训练得到；在中信一级行业，分别在行业内选前 20%作为多头，后 20%作为空头，最后各行业以沪深 300 行业内权重加权得到多空组合。&lt;/p&gt;&lt;p&gt;相关说明如下：&lt;/p&gt;&lt;p&gt;1）等市值内训练因子等权：表示该因子为全市场按市值大小排序后分 20 小组，在组内训练得到；在 20小组内，分别选前 20%作为多头，后 20%作为空头，最后各小组以等权得到多空组合。&lt;/p&gt;&lt;p&gt;2）全市场训练因子等权：表示该因子为全市场训练得到；在 20 小组内，分别选前 20%作为多头，后 20%作为空头，最后各小组以等权得到多空组合。&lt;/p&gt;&lt;p&gt;3）等市值内训练因子加权：表示该因子为全市场按市值大小排序后分 20 小组，在组内训练得到；在 20小组内，分别选前 20%作为多头，后 20%作为空头，最后以各小组市值加权得到多空组合。&lt;/p&gt;&lt;p&gt;4）全市场训练因子加权：表示该因子为全市场训练得到；在 20 小组内，分别选前 20%作为多头，后 20%作为空头，最后以各小组市值加权得到多空组合。&lt;/p&gt;&lt;p&gt;从以上结果分析，可以看到，分别在全市场、等市值组、行业内部进行了训练预测。在市值中性情况下，全市场训练得到的因子明显优于市值内部训练的因子；在行业中性情况下，全市场训练得到的因子亦优于行业内部训练得到的因子。我觉得原因有二点：一是同市值或者同行业，因子特征相对不太明显； 二是分组后，样本明显减少，导致训练不够充分，降低了区分度。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;七、 机器学习因子相关性较高&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;为了考察这七大类因子的相关性，我们分别统计了各自之间的相关系数及协方差矩阵。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a4676d8268561ddd6ce0e7e8bbb708e7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;383&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b986b1c2407aa0614531207e228d40bf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;387&quot;&gt;&lt;p&gt;从相关系数矩阵可以看到，所有的相关系数皆为正，说明七大分类算法都是正相关，但除了决策树和随机森林，其它五大算法 logistic， knn， AdaBoost， svm，朴素贝叶斯相关系数基本是 60%以上，说明这五大算法相关性非常高，也说明这五大算法分类得到的结论比较一致，这也间接证明了这几大算法分类的正确性。&lt;/p&gt;&lt;p&gt;从协方差矩阵也可以看出， 所有的协方差皆为正，也说明七大分类算法都是正相关。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;八、 机器学习因子单调性十分显著&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从前面知道，几大分类算法相关性非常高，所以此处以 logistic 算法为代表，来展示机器学习因子的单调性。我们把机器学习训练得到的个股相对强势值进行排序，按大小分成 5 组，发现排名靠前的小组明显优于排名靠后的小组，且单调性十分显著。 20090105 到 20171231， 前二组年化收益在 25%以上，而第四组收益不到 20%，最后一组收益不到 10%。 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5db29748d042f01b17da0b2e78d64459_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;566&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-82a608ae5b7e982d42fbcd4389a06cae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;116&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;九、 总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;人工智能从自诞生以来，理论和技术日益成熟，应用领域也不断扩大，金融领域也是将其改革的一大领域。谈到人工智能机器学习，大家最忌讳的便是黑箱问题， 其实不必，理解机器学习算法，逻辑实则简单，比如相同的因子特征将会有相同的表现，以此简单的逻辑，我们实证中发现效果比较显著。 人工智能机器人将能够自动生成研究报告替代分析师，发明策略替代主动基金经理进行投资等等，虽然这些目前来说不太普及，不是很成熟，但机器学习人工智能作为工具，为我们提高工作效率是毫无疑问的，并且随着技术的发展，金融领域将可能迎来颠覆性的改革。&lt;/p&gt;&lt;p&gt;人工智能时代正在到来，这种颠覆性的改变将为金融行业带来巨变。 传统量化投资主要包括量化选股、量化择时、股指期货套利、商品期货套利、统计套利、算法交易，资产配置，风险控制等。传统的量化投资研究的数据来源一般是公司的财务指标、 交易行情数据、 政策宏观方面的投资信息等。 而随着量化投资这一领域的快速发展，这些传统数据中所包括的大部分投资信息已经被专业投资者所挖掘，想要从这些信息中获取收益难度将越来越大。我们将利用人工智能机器学习来提升投资能力。&lt;/p&gt;&lt;p&gt;谈到机器学习，大家最忌讳的便是黑箱问题。其实不必，理解机器学习算法，逻辑实则简单，比如相同的因子特征将会有相同的表现。在实战中，我们发现，该逻辑十分有效，在我们的机器学习选股模型中， 首先构建机器学习因子，然后根据个股的相对强势即机器学习因子，我们把排名靠前 20%的作为多头，排名后 20%的作为空头进行回测；进一步，我们把相对强势分成 5 组，以考察这一指标的单调性，发现效果十分显著。同样，为了去除市值和行业的影响，我们也分别测试了市值中性及行业中性的情况下的表现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;人工智能有比较快速的自适应调整能力&lt;/b&gt;&lt;/p&gt;&lt;p&gt;七大分类算法中，我们分别计算出其月度 IC（即每个月因子排序与未来期个股收益排序的相关系数），统计各年度 IC 平均值，发现分类算法中除了决策树以外，其余算法得到的机器学习因子对个股未来收益皆有比较显著的相关性，且年度月 IC 均值都大于为 0，表示正相关性非常稳定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;人全市场训练明显优于市值中性或行业中性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分别在全市场、等市值组、行业内部进行了训练预测。在市值中性情况下，全市场训练得到的因子明显优于市值内部训练的因子；在行业中性情况下，全市场训练得到的因子亦优于行业内部训练得到的因子。我觉得原因有二点&lt;b&gt;：&lt;/b&gt;一是同市值或者同行业，因子特征相对不太明显；二是分组后，样本明显减少，导致训练不够充分，降低了区分度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;机器学习因子相关性较高&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从相关系数矩阵可以看到，所有的相关系数皆为正，说明七大分类算法都是正相关，但除了决策树和随机森林，其它五大算法 logistic， knn， AdaBoost， svm，朴素贝叶斯相关系数基本是 60%以上，说明这五大算法相关性非常高，也说明这五大算法分类得到的结论比较一致，这也间接证明了这几大算法分类的正确性。&lt;/p&gt;&lt;p&gt;从协方差矩阵也可以看出，所有的协方差皆为正，也说明七大分类算法都是正相关。&lt;/p&gt;&lt;p&gt;&lt;b&gt;机器学习因子单调性十分显著&lt;/b&gt;&lt;/p&gt;&lt;p&gt;从前面知道，几大分类算法相关性非常高，所以此处以 logistic 算法为代表，来展示机器学习因子的单调性。我们把机器学习训练得到的个股相对强势值进行排序，按大小分成 5 组，发现排名靠前的小组明显优于排名靠后的小组，且单调性十分显著。 20090105 到 20171231， 前二组年化收益在 25%以上，而第四组收益不到 20%，最后一组收益不到 10%。&lt;/p&gt;&lt;p&gt;本报告中，我们先后介绍了人工智能与量化投资的关系，接着介绍了我们这次模型用到的分类算法，共包括 logistic， knn， AdaBoost， svm， Nbayes，随机森林，决策树七大分类算法。 分别简单介绍了相关概念，描述了他们的适用范围及优缺点。然后用在 A 股实证， 结论与我们分析的基本一致。 Adaboost 是一种有很高精度的分类器， 容易实现，分类准确率较高，没有太多参数可以调， 不会过拟合， KNN 虽然计算量大， 但准确度也高，对 outlier 不敏感。 所以 Adaboost 与 knn 分类得到的结果比较稳定， 在收益年化 10%以上的算法中，在全市场选股，市值中性选股等权加权，行业中性选股等权加权五种情况下， AdaBoost 与 knn 算法年化波动率基本在 5%左右，表现非常稳定。 而朴素贝叶斯只需要较少的训练数据即可有比较优秀的表现，在本研究中， 我们以一年为周期，训练数据量相对较小，且虽然朴素贝叶斯要求特征相互独立， 但即使假设条件不成立也能有比较好的表现，故朴素贝叶斯表现最好，年化收益分别达到 15.50%， 15.75%， 12.89%， 15.63%， 10.23%。决策树容易过拟合， 分类结果不稳定，精确度较低， 表现最差， 所以决策树表现不佳，年化收益基本在 3%左右，而随机森林是建立在决策树的基础上，决策树在分类时是选择所有变量，而随机森林则是产生很多决策树，然后每根决策树选择不同的变量，进行分析，最后选取决策树中的众数，作为最终结果， 所以表现比单个决策树好，但分类本质还是决策树，故不如别的分类算法。&lt;/p&gt;&lt;p&gt;提示：&lt;br&gt;1） 该文的分类算法所有若涉及到参数问题，皆默认参数，故不存在参数调整问题，也不存在最优问题，故相关算法皆比较有代表性。&lt;/p&gt;&lt;p&gt;2） 因为此研究主要考查各分类机器学习因子的有效性，故没有考虑手续费等问题。&lt;/p&gt;&lt;p&gt;http://weixin.qq.com/r/XzlIUEfE3xqgrQMz92xN (二维码自动识别)&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-14-34543283</guid>
<pubDate>Wed, 14 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【精选】使用Cryptory分析影响加密货币价格的因素（区块链系列3）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-14-34533755.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34533755&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d232a6668f8f47cd3886ef92cc20a9e2_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：David Sheehan&lt;/p&gt;&lt;p&gt;编译：编辑部&lt;/p&gt;&lt;p&gt;&lt;b&gt;这是机器学习应用区块链系列的第三篇文章&lt;/b&gt;&lt;/p&gt;&lt;p&gt;前2期文章传送门&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287256&amp;amp;idx=2&amp;amp;sn=483e71c66e863aec424ff0c1936411c6&amp;amp;chksm=802e314db759b85b505f7fd1ed0cb0cacf10ed4bbbf86cc50c7bbc9d07d4ac0fed3b783b2278&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;机器学习应用区块链系列（一）--如何开发一套自己的智能合约系统&lt;/a&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287287&amp;amp;idx=1&amp;amp;sn=a4efd248d74afa7360f2e32751066b5d&amp;amp;chksm=802e3162b759b8748907f4d889d648d89acd54837041a8f94cde00740b5a867012ae48fa1f5d&amp;amp;scene=21#wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;还是不靠谱！多维LSTM网络预测比特币价格【机器学习应用区块链系列二】&lt;/a&gt;&lt;p&gt;我们可以设计两种模型。&lt;/p&gt;&lt;p&gt;1、一个更复杂的模型（更多的隐含层）&lt;/p&gt;&lt;p&gt;2、一个可以识别更多输入信息数据源的模型。&lt;/p&gt;&lt;p&gt;虽然很容易把注意力放在第一个模型上，但garbage-in-garbage-out的原则依然存在。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/dashee87/cryptory%EF%BC%89%E3%80%82%E5%AE%83%E9%9B%86%E6%88%90%E4%BA%86%E5%90%84%E7%A7%8D%E5%8C%85%E5%92%8C%E5%8D%8F%E8%AE%AE%EF%BC%8C%E4%BB%A5%E4%BE%BF%E4%BD%A0%E5%8F%AF%E4%BB%A5%E5%9C%A8%E4%B8%80%E4%B8%AA%E5%9C%B0%E6%96%B9%E8%8E%B7%E5%BE%97%E5%8E%86%E5%8F%B2%E5%AF%86%E7%A0%81%E5%92%8C%E6%9B%B4%E5%B9%BF%E6%B3%9B%E7%9A%84%E7%BB%8F%E6%B5%8E/%E7%A4%BE%E4%BC%9A%E6%95%B0%E6%8D%AE%E3%80%82&quot;&gt;https://github.com/dashee87/cryptory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;安装&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;cryptor可以在PyPi和GitHub上使用，因此很容易运行pip install cryptory在你的command line/shell中。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e565d50cd079aac25c3688b9f88e23f6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;463&quot; data-rawheight=&quot;31&quot;&gt;&lt;p&gt;下一步是将包装加载到工作环境中。 具体来说，我们将导入Cryptory类。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# import package
from cryptory import Cryptory&lt;/code&gt;&lt;p&gt;假设没有返回任何错误，那么你现在可以开始提取一些数据。 但在此之前，值得一提的是，你可以通过运行help功能来检索有关每种方法的信息。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;help(Cryptory)

Help on class Cryptory in module cryptory.cryptory:

class Cryptory
|  Methods defined here:
|  
|  __init__(self, from_date, to_date=None, ascending=False, fillgaps=True, timeout=10.0)
 |      Initialise cryptory class
...&lt;/code&gt;&lt;p&gt;现在我们将创建我们自己的加密对象，我们将其称为my_cryptory。 你需要定义要检索数据的开始日期，同时还有一些可选的参数。例如，你可以设置结束日期，否则它将默认为当前日期 - 有关更多信请输入help(Cryptory.__init__)。 &lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;# initialise object
my_cryptory = Cryptory(from_date=&quot;2017-01-01&quot;)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;加密货币价格&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;首先我们将获取一些比特币的历史价格（从2017年1月1日开始）。 cryptory有几个选项对于这些数据种类：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# initialise object
my_cryptory = Cryptory(from_date=&quot;2017-01-01&quot;)
# for readability, reduce number of rows shown by default
import pandas as pd

pd.options.display.max_rows = 6

# get prices from coinmarketcap
my_cryptory.extract_coinmarketcap(&quot;bitcoin&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4dd8285a67b46972f7fd13a812ffbe0b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;202&quot;&gt;&lt;code lang=&quot;python&quot;&gt;# get prices from bitinfocharts
my_cryptory.extract_bitinfocharts(&quot;btc&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c10b2a8029c5cf231e2235ea8cd1144e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;468&quot; data-rawheight=&quot;336&quot;&gt;&lt;p&gt;这些单元说明如何从coinmarketcap和bitinfocharts中提取比特币价格。 每种方法返回的价格差异都可以通过计算每日价格的不同方法来解释（例如bitinfocharts代表当天的平均价格）。 出于这个原因，我不建议组合不同的价格来源。&lt;/p&gt;&lt;p&gt;你还可以通过extract_bitinfocharts。例如， 交易费用。请参阅帮助（Cryptory.extract_bitinfocharts）以获取更多信息。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# average daily eth transaction fee
my_cryptory.extract_bitinfocharts(&quot;eth&quot;, metric=&#39;transactionfees&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3061d50db17d4424150f4bafd822bf35_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;473&quot; data-rawheight=&quot;338&quot;&gt;&lt;p&gt;你可能已经注意到每种方法都会返回一个pandas dataframe。事实上，所有的加密方法都会返回一个pandas dataframe。这很方便，因为它允许你使用常见的pandas技术来处理输出。例如，我们可以轻松合并两个extract_bitinfocharts来结合每日bitcoin和 ethereum 的价格。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.extract_bitinfocharts(&quot;btc&quot;).merge(
my_cryptory.extract_bitinfocharts(&quot;eth&quot;), on=&#39;date&#39;, how=&#39;inner&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3e4d9428abd6e61515f0f7ad633caeac_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;466&quot; data-rawheight=&quot;338&quot;&gt;&lt;p&gt;extract_poloniex提供了加密价格的另一个来源，它从public poloniex API（https://poloniex.com/support/api/）中提取数据。例如，我们可以检索BTC / ETH汇率。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# btc/eth price
my_cryptory.extract_poloniex(coin1=&quot;btc&quot;, coin2=&quot;eth&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d5f28a35b9d32a5875cf0e7d6ead4b09_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;200&quot;&gt;&lt;p&gt;我们现在可以对加密货币价格进行一些基本分析。 &lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-931e004ad0675da38024b3ce2e55fe7c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;264&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-077748c6c519dc4f22f6020aa501e28c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;231&quot;&gt;&lt;p&gt;当然，上图并没有什么意义。 你不能只比较每个币的单位价格。 你需要考虑总供给和市值。 这就好像说日元与美元相比，美元被低估了。但我并不担心。这里更重要的是自2017年以来价格的相对变化，我们可以通过pandas magic (pct_change)：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5cff8c399379cfa5b19a00357de40576_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;287&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f7b41e0c1e7d86f017275a9bbcba03e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;231&quot;&gt;&lt;p&gt;这些币是在bitinfocharts上提供的，它们倾向于代表较旧的传统币。例如，从2017年表现最佳的硬币是Reddcoin。它从2017年开始，市值不到100万美元，但是完成价值约2.5亿美元，到2018年1月初达到7.5亿美元的高峰。你会注意到每个币都显示出相同的一个趋势：3月至6月持续上涨，随后在12月再次飙升，并在2018年1月出现明显的抛售。&lt;/p&gt;&lt;p&gt;在pandas的帮助下，我们可以生成密码价格关联图：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8409b0110b4aaa9cefee45f7078e5902_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;136&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-af0c5edb43c91db0d06596786f7d44e6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;413&quot;&gt;&lt;p&gt;它不像股市。以太坊和比特币没有Facebook和通用汽车那么不同。尽管股票价格与实现财务目标（即季度收益报告）和更广泛的宏观经济因素有关，但大多数密码（可能全部）目前都是围绕区块链技术在展开。这并不是说币偶尔不会逆市上涨，例如ripple (xrp)在十二月初。然而，过度表现之后往往是市场的表现不佳（例如2018年1月的波动）。&lt;/p&gt;&lt;p&gt;我承认我迄今为止所做的一切都是突破性的。你可以从Quandl api（https://www.quandl.com/tools/python）中获得类似的数据（我打算将quandl API调用集成到cryptory中）。当你想要将加密价格与其他数据源结合时，加密的真正好处cryptory就来了。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Reddit Metrics&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如果你熟悉密码，你很可能会意识到他们关联reddit的页面。 这就是加密投资者来讨论不同区块链实施的优点，剖析当天的主要话题和发布有趣的gif，它主要是GIF（https://www.reddit.com/r/Bitcoin/comments/7v438b/the_last_3_months_in_47_seconds/）。通过加密技术，您可以将reddit指标（订阅者总数，新订户，一分为二地被刮掉从redditmetrics网站（http://redditmetrics.com/）或与其他加密数据结合起来。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ba47219fd697213a57054ee7626ce2ef_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;360&quot; data-rawheight=&quot;94&quot;&gt;&lt;p&gt;让我们来看看iota和eos; 两个币于2017年6月出现，并于2017年底实现强劲增长。Their corresponding subreddits are r/iota and r/eos, respectively.&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.extract_reddit_metrics(&quot;iota&quot;, &quot;subscriber-growth&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-609b44dfc68a30ebbfc4500183f514f9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;472&quot; data-rawheight=&quot;336&quot;&gt;&lt;p&gt;现在我们可以分析价格和subreddit之间的关系。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3d5ee1eb8880bb8be94088b475c19f5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;179&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3c6e43481c4520e65e1e914bc07eb8a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;326&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1f1c9a7c2c96f516dde864aed361c0c2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;277&quot;&gt;&lt;p&gt;直观来看，价格和子分币成员增长之间显然存在某种相关性（y轴使用传统的min-max比例进行归一化）。 尽管两个币的Spearman等级相关性都相似，但是iota的Pearson相关系数显着更强，突出显示了不依赖单一测量的重要性。在撰写本文时，iota和eos的市值都在50亿美元左右（总体为第11和第9），但iota subreddit的用户数量比eos subreddit（分别为105k和30k）多3倍以上。虽然这并不能确定价格和reddit之间的关系是否具有预测性，但确实表明reddit指标可能是某些币有用的模型特征。 &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Google Trends&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Google Trends你会注意到在11月下旬和12月初，iota和eossubreddits的用户几乎同时激增。 这是更广泛的加密趋势的一部分，大多数币经历了前所未有的收益。 领先的是比特币，价格在11月15日和12月15日之间涨了两倍。 作为nocoiner最知名的加密技术，比特币（以及更广泛的区块链行业）在这次牛市期间受到了相当多的主流关注。据推测，这吸引了很多新的加密投资者，推高了价格。&lt;/p&gt;&lt;p&gt;通过cryptory，你可以轻松地将常规加密指标与Google Trends数据结合起来。 你只需要确定好你想要搜索术语。 如果你以前使用过Google Trends，那么你会注意到你只能检索最多90天的数据。 get_google_trends方法将重叠搜索拼接在一起，以年计算。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.get_google_trends(kw_list=[&#39;bitcoin&#39;])&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-66f1076692aa5bc88b3174819a870508_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;474&quot; data-rawheight=&quot;337&quot;&gt;&lt;p&gt;现在我们可以查看加密价格和谷歌搜索流行度之间的关系。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-19ba12ccf1d65a3d67ee281a19ff606a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;487&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fba7b2465db65b5a202bb7ee882b0b4a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;361&quot;&gt;&lt;p&gt;A few words on Verge (xvg): eccentric (i.e.crazy) crypto visionary John McAfee recommended (i.e. shilled) the unheraldedVerge to his twitter followers (i.e. fools), which triggered a huge surge inits price. As is usually the case with pump and dumps, the pump (from whichMcAfee himself potentially profitted) was followed by the dump. The sorry storyis retold in both the price and google search popularity. Unlike bitcoin andethereum though, you’d need to consider in your analysis that verge is also acommon search term for popular online technology news site The Verge (tronwould be a similar case).&lt;/p&gt;&lt;p&gt;无论如何，回到cryptory，你可以提供一个以上的关键字，使你可以看到不同条&lt;/p&gt;&lt;p&gt;款的相对流行度。让我们比较自2013年以来Kim Kardashian和比特币的历史人气：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d734c8fa495fa4b49c9c9e0a698ad92f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;124&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bc6cc2e2a91dec4ad53001aae2384e64_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;251&quot;&gt;&lt;p&gt;根据谷歌趋势，比特币在2017年6月成为一个更受欢迎的搜索词。也就是说，比特币从未在2014年11月13日达到Kim Kardashian的高度。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;股票市场价格&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;加密货币有点类似于传统的公司股票。 当然，主要区别在于你不可能通过投资股市来支付lambo。尽管如此，股票市场可能会提供有关总体经济表现如何的线索，甚至可能提供具体行业如何应对区块链革命的线索。&lt;/p&gt;&lt;p&gt;cryptory包含一个get_stock_prices方法，该方法基于雅虎财务并返回历史数据。请注意，你需要在雅虎金融网站上找到相关的公司/索引代码。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;# %5EDJI = Dow Jones
my_cryptory.get_stock_prices(&quot;%5EDJI&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0c072246d2cea399906875e6bab2434c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;202&quot;&gt;&lt;p&gt;你可能会注意到前一个收盘价格会在股市关闭的几天（例如周末）结转。你可以选择关闭此功能（请参阅help(Cryptort.__init__)）。&lt;/p&gt;&lt;p&gt;在pandas的帮助下，我们可以将比特币相对于某些特定股票和指数进行可视化操作。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cdf70d4d477c1c6f4cc068402cbe6dcc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;375&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e8aa7da60970dbfd51459370581059cc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;232&quot;&gt;&lt;p&gt;该图显示如果你在1月3日投资，你将收到的回报。由于比特币涨幅最大（&amp;gt; 10倍回报），客观上它是最好的投资。但AMD和NVIDIA（以及英特尔在某种程度上）是特殊情况，因为这些公司生产的显卡支持密码挖掘的工作。 柯达（不要与2012年前破产柯达相混淆），因为他们在2018年1月初宣布他们打算创建他们自己的“以照片为中心的加密货币”&lt;/p&gt;&lt;p&gt;和以前一样，通过pandas，你可以创建一个比特币股市的相关图。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6a4f6a7cee18ae6ffc8eb4c13e7a7f5a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;145&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9a2b5ca7a0c8c00b977d9c987682c56e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;426&quot;&gt;&lt;h2&gt;&lt;b&gt;商品价格&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;尽管比特币最初被设想为替代支付系统，但高额交易费和不断上涨的价值阻碍了其作为合法货币的使用。 这意味着比特币及其继任者已经演变成另一种价值储备 - 一种容易丢失的互联网黄金(http://uk.businessinsider.com/nearly-4-million-bitcoins-have-been-lost-forever-study-says-2017-11)。因此，调查比特币与更传统的价值储备之间的关系可能会很有趣。&lt;/p&gt;&lt;p&gt;cryptory包含一个get_metal_prices方法，可以检索各种贵金属的历史价格。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.get_metal_prices()&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a0ed43dc6f577ff9634cae040b6b817c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;199&quot;&gt;&lt;p&gt;我们可以轻松地绘制2017年和2018年商品的变化情况。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6a3c61c416b5c39b86b39d4de3461f1e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;279&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5f1ef4ceca038434ac0d99740c26a5ae_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;232&quot;&gt;&lt;p&gt;看看2017年和2018年慢慢升值的黄金，因此代表着稳定的财富储备。如前所述，我们可以绘制一个价格相关矩阵。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0a0208c06ad02835bc2c5ab6e3d00e26_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;168&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1bfb46e33b944d2aa239e6c2446a11c4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;540&quot; data-rawheight=&quot;400&quot;&gt;&lt;p&gt;不出所料，各种贵金属呈现出显着的相关性，而比特币价值看起来完全没有关联。 我认为，负相关可以提供证据表明人们正在摆脱传统的价值储备，但几乎没有证据支持这一理论。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;外汇汇率&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;比特币背后的动机之一是创造一种不受任何中央政府控制的货币。 当美国中央银行在2007年金融危机后通过印刷数万亿美元的新货币来支撑经济不景气时，美国中央银行可能贬值美元。因此，美元汇率和货币与加密货币之间可能存在关系。&lt;/p&gt;&lt;p&gt;cryptory包括一个get_exchange_rates方法，用于检索特定货币对之间的历史每日汇率。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.get_exchange_rates(from_currency=&quot;USD&quot;, to_currency=&quot;EUR&quot;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0d2c82fd2d9ab5b3d1e8ba513b52994a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;481&quot; data-rawheight=&quot;337&quot;&gt;&lt;p&gt;正如你所看到的，美元在过去的一年中已经跌破至欧元。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;原油价格&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;全球经济实力（如中国的需求）和地缘政治不稳定性（如中东，委内瑞拉）强烈影响油价。 当然，还有其他一些因素（页岩，走向可再生能源等），但你可能想要在你的加密价格模型中使用油价格来使用这些因素。&lt;/p&gt;&lt;p&gt;cryptory包括一个get_oil_prices方法，可以检索历史日常原油价格。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;my_cryptory.get_oil_prices()&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8a6a995d28d2c58294b780c22cf2dd48_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;476&quot; data-rawheight=&quot;337&quot;&gt;&lt;p&gt;自2017年初以来，原油价格已上涨约20％。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6176f70bd54a5e1da671f6f30e6122ff_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;127&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-92dd41ae5b585d797e4e4111b058a2e5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;554&quot; data-rawheight=&quot;232&quot;&gt;&lt;p&gt;&lt;b&gt;更多有关资料&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1、twitter statistics (specifically JohnMcAffee’s!!!)&lt;/p&gt;&lt;p&gt;2、media analysis (number of mainstreamarticles, sentiment, etc.- example)&lt;/p&gt;&lt;p&gt;3、more Asian-centric data sources (Japan andSouth Korea are said to account for 40% and 20% of global bitcoin volume,respectively)&lt;/p&gt;&lt;p&gt;4、more financial/crypto data (integrateQuandl api)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;原文代码链接：https://github.com/dashee87&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-14-34533755</guid>
<pubDate>Wed, 14 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【重磅】起薪50万，人才缺口20万的量化研究岗最深入调研！（文末专栏福利）</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-14-34532766.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34532766&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bb712a50ef652d23164d732e6a4e222e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;部分内容来自UniCareer（微信ID：UniCareer）和编辑部整理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2017至2018年，券商急招量化人才&lt;/b&gt;&lt;/p&gt;&lt;p&gt;3个人的量化团队可以做出一个亿的利润&lt;/p&gt;&lt;p&gt;让几乎所有的自营交易部眼睛瞪得发直&lt;/p&gt;&lt;p&gt;&lt;b&gt;有消息称，2018年，这个岗位将大幅扩招&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5dfe794c4d1db40c5ae0f126dfe21761_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;282&quot;&gt;&lt;p&gt;&lt;b&gt;金融工程和金融数学&lt;/b&gt;&lt;/p&gt;&lt;p&gt;正是进入量化岗最常见的两个专业&lt;/p&gt;&lt;p&gt;九大投行和券商量化岗位的要求都会明确要求：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8014407c93a45c30c83d69c206c5372_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;900&quot; data-rawheight=&quot;559&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-683d3d39e7f5b0541f1d601d6348ebca_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;759&quot; data-rawheight=&quot;217&quot;&gt;&lt;p&gt;MFE俗称金工，不只是镶了金边的码农。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Top MFE基本可以帮你保证一份投行的工作，买方Hedge Fund最爱招MFE的毕业生做Quant，MFE毕业的学生第一年薪资平均高达11万美金。&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d0048c7595a5c209525e295587f1fa2e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;263&quot;&gt;&lt;p&gt;那么，&lt;b&gt;哪些学校的MFE专业含金量最高？传说中毕业能进投行的是哪个学校的MFE？美国各名校的MFE金融工程专业分到底设置了哪些课程，其录取条件和就业形势如何呢？&lt;/b&gt;小编为大家吐血整理了这篇MFE专业大全。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;我们先看看国内情况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;（数据来自职友集）&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;上海&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上海量化研究员平均工资：&lt;/p&gt;&lt;p&gt;&lt;b&gt;¥ 21580/月&lt;/b&gt;&lt;/p&gt;&lt;p&gt;取自 363 份样本，较 2016 年，增长 97%。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-40d2f6ba4d7cd269bf9363db6b743227_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;675&quot; data-rawheight=&quot;430&quot;&gt;&lt;p&gt;历年工资变化趋势&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6c1f36ba3fff4f0065d96ebb6197c6f8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;695&quot; data-rawheight=&quot;380&quot;&gt;&lt;p&gt;按工作经验统计&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-125cc1c35220cf5f56638ac06ec19f58_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;687&quot; data-rawheight=&quot;380&quot;&gt;&lt;p&gt;相似岗位工资收入&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9ea1d0f5e392a014f9dbd83c72409532_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;841&quot; data-rawheight=&quot;571&quot;&gt;&lt;h2&gt;&lt;b&gt;北京&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;北京量化研究员平均工资：&lt;/p&gt;&lt;p&gt;&lt;b&gt;¥18150/月&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近1年 223 份样本。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fa96d407db9e75572fc688513eb47ad0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;675&quot; data-rawheight=&quot;435&quot;&gt;&lt;p&gt;按工作经验统计&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c0bced323c8b6f4eaa4cf53cf9a3db7e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;686&quot; data-rawheight=&quot;380&quot;&gt;&lt;p&gt;相似岗位工资收入&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-38edcb12530664474917c8f93e74139c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;868&quot; data-rawheight=&quot;572&quot;&gt;&lt;h2&gt;&lt;b&gt;深圳&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;深圳量化研究员平均工资：&lt;/p&gt;&lt;p&gt;&lt;b&gt;¥18160/月&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近1年 199 份样本。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b43ef5424b486ac5f77f7f3c1bd3c6b8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;674&quot; data-rawheight=&quot;434&quot;&gt;&lt;p&gt;按工作经验统计&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-63b5689bdbece3d9470ab849ba5f8ca5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;681&quot; data-rawheight=&quot;380&quot;&gt;&lt;p&gt;相似岗位工资收入&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-25a67e769c1996701d5393fd48b1feef_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;846&quot; data-rawheight=&quot;575&quot;&gt;&lt;h2&gt;&lt;b&gt;广州&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;工作量化研究员平均工资：&lt;/p&gt;&lt;p&gt;&lt;b&gt;¥12620/月&lt;/b&gt;&lt;/p&gt;&lt;p&gt;近1年 105 份样本。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bd032ed04175bd0892abb077b52699c1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;671&quot; data-rawheight=&quot;430&quot;&gt;&lt;p&gt;按工作经验统计&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a75b55998ba5031f809ccc9eef272186_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;680&quot; data-rawheight=&quot;380&quot;&gt;&lt;p&gt;相似岗位工资收入&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-05071bd89d8990b670bcb5ab735c52d1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;833&quot; data-rawheight=&quot;572&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们再看看国外情况&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;01 传说中毕业进投行的Top MFE&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Top MFE&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;毕业生基本可以保证一份投行的工作。 当然，也仅限于凤毛麟角的几个大学。这些项目要么声誉够好，要么学校资源、Career Service够好，要么校友给力。这些项目的毕业生无论在北美还是回国，发展都很好。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6b197ebdd58b70026cb3b086fd5a03d6_r.jpg&quot; data-caption=&quot;来源：知乎&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;577&quot; data-rawheight=&quot;623&quot;&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Tier 2 MFE &lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;以下项目比Top相较有些短板，不能保证在美国毕业后的工作，主要还是靠内推和海投。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-19bff054157c2c996b78822de4db452f_r.jpg&quot; data-caption=&quot;来源：知乎&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;736&quot; data-rawheight=&quot;519&quot;&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Tier 2以下的不推荐读&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;以上是MFE毕业的学生在行业和求职过程中总结出来的&lt;b&gt;美国金融工程项目含金量&lt;/b&gt;。关于美国金融工程硕士排名，目前比较主流的排名体系有QuantNet和TFE，近日QuantNet公布了最新的2018排名，一起来看一下吧！ &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;02 美国MFE项目排名大全及项目详解 &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;QuantNet根据GRE成绩、起薪、本科GPA、录取率、就业率等条件对美国的MFE进行了排名：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b77e17fb2452edb2af99b24927290378_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;901&quot; data-rawheight=&quot;3292&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-01380a867932465a4e5ba3e19d1e3fdb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;698&quot; data-rawheight=&quot;421&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7c51344f5315b43d414f7fc7abdff11a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;683&quot; data-rawheight=&quot;492&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-194c602c54b13bd641ae0176d2b360fb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;685&quot; data-rawheight=&quot;462&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c50279f5ad40142e3b467f1f76db8b25_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;688&quot; data-rawheight=&quot;465&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2f6373f0d171da04ccee77162d351d04_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;688&quot; data-rawheight=&quot;639&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9e5a24847ba4d799c16a3f14a5f1ce27_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;688&quot; data-rawheight=&quot;314&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-02d11518cffe8ff93c6eb34ff9a700a6_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;684&quot; data-rawheight=&quot;307&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b9f6ebc9c6e7418d5a2261313c5dee86_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;683&quot; data-rawheight=&quot;392&quot;&gt;&lt;h2&gt;&lt;b&gt;03 MFE专业就业前景与薪资&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;就业前景：&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;以下是最新的2017年Baruch MFE Employment Report，统计了MFE专业的学生去往的公司和职位。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7964bc668969b61a7fed01980098ca3a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1280&quot;&gt;&lt;p&gt;从就业报告中可以看出，毕业生的目标行业集中在投行，对冲基金，金融机构，咨询，资产管理等。&lt;b&gt;2015-2017年，54%的毕业生去往投行，31%的学生去往买方金融机构。&lt;/b&gt;具体工作方向以 Quant，Risk，Trading 居多。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f83453ffffe0f032c122939b2a7c6d96_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;563&quot;&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Quant，量化岗&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;量化分析师是近两年金融求职圈最火的职位，主要从事风险管理、优化投资组合、设计开发金融产品、财务分析、销售与交易等工作。去投行做量化也是很多人读MFE的目标。但现实中，面到最后一轮，&lt;b&gt;各大投行券商都只要“各类理工科PhD”&lt;/b&gt;。&lt;/p&gt;&lt;blockquote&gt;高盛HR：“我们对phd和master很公平的，不存在偏袒与否，但最后通过面试的也的确是phd为多”&lt;/blockquote&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Risk，风险管理岗&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;金融工程的本科在各大投行、基金、券商的风控岗都很吃香。相反，金融工程硕士可能有一些Over-qualified，有同学参加纽约JP Morgan的Risk面试，面试官曾明确表明，“ Risk 这种东西，本科生就足够了，没必要招个 Master 进来”。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Trader，交易岗&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;投行交易岗的最佳背景就是金融本科+MFE，懂金融、还要要懂编程和数学。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;&lt;u&gt;毕业薪资&lt;/u&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;在美国，&lt;b&gt;金融工程专业平均年薪为$80,000+&lt;/b&gt;, 比其他工作的平均薪酬水平&lt;b&gt;高31%&lt;/b&gt;；&lt;b&gt;Quants的平均年薪为$118,000&lt;/b&gt;，比其他工作的平均薪酬水平&lt;b&gt;高67%&lt;/b&gt;。而且工作时间和年限越长，薪资会越高。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-788097af03c7c6f8a08ac01d8201c32c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;160&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-490a7233b8ea40b4bebc302effdcd951_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;640&quot; data-rawheight=&quot;204&quot;&gt;&lt;p&gt;金融工程师让人羡艳的待遇自然让很多人跃跃欲试，国内还是国外，金融工程专业都是属于热门的专业之一。高能力配得上高薪资，该专业的门槛只会只高不低，所以它又是热门专业中很难申请的专业。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;04 &lt;b&gt;MFE毕业生，怎样才能进投行/券商量化岗？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;&lt;u&gt;证书：&lt;/u&gt;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;金融工程作为热门专，选择人数多，所以类似CFA（Chartered Financial Analyst）和FRM (Financial Risk Manager) 这类证书可以帮助你从竞争者中脱颖而出。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;数据语言：&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;作为对数据挖取、分析及结果可视化要求很高的领域，Python/C++/Java/C#/R这类编程语言的熟练应用是很不错的加分项，熟练使用数据统计类通用工具（例如matlab）也是必要技能。另外了解机器学习、数据挖掘，有相关经验则是非常有竞争力的为加分项。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;Gpa: &lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;要进入九大或知名券商，3.5/4.0为佳，很多公司的HR表示更看重高等数学、建模等有关数理逻辑能力的相关课程的成绩。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;学位：&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;作为研究方向明确，专业知识更加专精的硕士及以上学历者为佳。&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;i&gt;&lt;b&gt;实习经验：&lt;/b&gt;&lt;/i&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;作为交叉学科金融工程的毕业生，有金融背景实习（风险管理，证券投资，金融衍生品相关等）或数据编程方向（机器学习，数据可视化等）实习是大部分HR看中的宝贵经验。没有任何相关实习经验，即使是Top MFE毕业生也很难进投行。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;量化投资干货（公众号精选）&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284668&amp;amp;idx=1&amp;amp;sn=1d099b61ac8a378f39ef99203cfb85af&amp;amp;chksm=802e2b29b759a23f1ce824e84ab55601f8da41ace7877cac3fe97900f1a7147c97a732481841&amp;amp;scene=21#wechat_redirect&quot;&gt;2016年全年所有券商金工研报第1部分&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284678&amp;amp;idx=1&amp;amp;sn=0c29d884ada86f565b5849057fe5cdb6&amp;amp;chksm=802e2b53b759a245db87fe77c211e8f987464d0d188305808b412fb2d36cbc9f4bb707fedde9&amp;amp;scene=21#wechat_redirect&quot;&gt;2016年全年所有券商金工研报第2-4部分&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284702&amp;amp;idx=1&amp;amp;sn=c150e541adb6f852459b085a086bf97f&amp;amp;chksm=802e2b4bb759a25de30c981d25e8db6c90e409e0c8ec5303ad0b3fa673abfc01fd4832842c16&amp;amp;scene=21#wechat_redirect&quot;&gt;2016年全年所有券商金工研报第5-9部分&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653285014&amp;amp;idx=1&amp;amp;sn=7b008dd3b3c6362e2c8176e87934fc43&amp;amp;chksm=802e2883b759a195d086130716f83b01f88ed702b3db0dfc3bb0a30a0c3aae141ec823fcc5b6&amp;amp;scene=21#wechat_redirect&quot;&gt;国外优秀量化投资书籍推荐&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653285299&amp;amp;idx=1&amp;amp;sn=cef320cbac5c9155868a12c3e374613e&amp;amp;chksm=802e29a6b759a0b0527fac1276cdce6646484fe101d525e0373b58ab925074456faadc81b323&amp;amp;scene=21#wechat_redirect&quot;&gt;以色列神秘AI研究力量：深度学习的四大失败（视频+论文+ppt下载）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653285360&amp;amp;idx=1&amp;amp;sn=e90f550136b74dc8490a9d48886cdd71&amp;amp;chksm=802e29e5b759a0f3288d7c0434e46e2c7f116f21d056401de19df7b4aefda80434d6e509919c&amp;amp;scene=21#wechat_redirect&quot;&gt;高频交易（18篇论文）+（15本书籍）+（9篇研报）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653285408&amp;amp;idx=1&amp;amp;sn=a21c1ba55f954cf8875a45b736977645&amp;amp;chksm=802e2e35b759a72314dda71191036327642ebf168e1bb161ec37f6f56a563c73cc73d72fefda&amp;amp;scene=21#wechat_redirect&quot;&gt;文本挖掘在量化投资中的应用之（28篇最全券商研报）+（2个策略程序）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286032&amp;amp;idx=1&amp;amp;sn=f931e3de55ba425049553d524173b57e&amp;amp;chksm=802e2c85b759a5935002ab01161a92be5ba6c7a5ba64ad12d8be55490fa328973835008ab2dc&amp;amp;scene=21#wechat_redirect&quot;&gt;2017上半年所有券商金工研报（一）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286039&amp;amp;idx=2&amp;amp;sn=b6fda2baaff0af634531e3d2928755e0&amp;amp;chksm=802e2c82b759a59496553894c6e3a90e8a47622a228276d61c6c84a3b593b8a81e989926fb5c&amp;amp;scene=21#wechat_redirect&quot;&gt;2017上半年所有券商金工研报（二）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286133&amp;amp;idx=1&amp;amp;sn=c8ef7e2df827698971c71c270ec08a65&amp;amp;chksm=802e2ce0b759a5f63de0fb7f635e8959c4f25a5c761d165a0a2312d08e48e48e408dde572642&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年7月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286262&amp;amp;idx=1&amp;amp;sn=8fe879fc4a5189cf027b7496da82681f&amp;amp;chksm=802e2d63b759a47535c7a0dfe279672f10821edcdeb49c6f099a7388feef39e8faeb2aaf30e3&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年8月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286383&amp;amp;idx=1&amp;amp;sn=7c6b9f54ee5727ede261042510daa401&amp;amp;chksm=802e2dfab759a4ec6a3eb346d6e27fceae852aefae361bd93320ba4ffab7a2859899b28ace19&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年9月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286510&amp;amp;idx=1&amp;amp;sn=b64aab20dc1ba2e56776aa34090d361d&amp;amp;chksm=802e327bb759bb6d558caf6a2aaf4e86bfaf31a3558573f58c7f5f24d1526756ec0ac1d3a820&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年10月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286510&amp;amp;idx=1&amp;amp;sn=b64aab20dc1ba2e56776aa34090d361d&amp;amp;chksm=802e327bb759bb6d558caf6a2aaf4e86bfaf31a3558573f58c7f5f24d1526756ec0ac1d3a820&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年11月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286510&amp;amp;idx=1&amp;amp;sn=b64aab20dc1ba2e56776aa34090d361d&amp;amp;chksm=802e327bb759bb6d558caf6a2aaf4e86bfaf31a3558573f58c7f5f24d1526756ec0ac1d3a820&amp;amp;scene=21#wechat_redirect&quot;&gt;2017年12月全部券商金工研报汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286392&amp;amp;idx=1&amp;amp;sn=f3dd6c7926a797f6111701146d78b529&amp;amp;chksm=802e2dedb759a4fb8fd7806f820f874223246a8cf040e394b98a7333857be8ab39a88870cd66&amp;amp;scene=21#wechat_redirect&quot;&gt;有关行为经济学的（10多本书籍）+（130多篇论文）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286500&amp;amp;idx=1&amp;amp;sn=106e28d25e413c816193005ef64c479d&amp;amp;chksm=802e3271b759bb67f5088e176153a51dc945abb373d7eabc5c089adb2ecdd207138bfb667631&amp;amp;scene=21#wechat_redirect&quot;&gt;Quantitative Finance 杂志关于量化交易领域排名前十的文章&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653286702&amp;amp;idx=1&amp;amp;sn=e4416e23c8b5ab7955a258e1077153e3&amp;amp;chksm=802e333bb759ba2dea0b46da08d0d7b6268090033d6af98b2f72b14909e2935c1e7fbe4bec69&amp;amp;scene=21#wechat_redirect&quot;&gt;最全LSTM在量化交易中的应用汇总&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284202&amp;amp;idx=1&amp;amp;sn=f94bdefe70ddcb538ca463ba1c5e5205&amp;amp;chksm=802e257fb759ac69899d8544937600c22637697591fce25d1ed1b72414d975eeeba7cc58c9d8&amp;amp;scene=21#wechat_redirect&quot;&gt;【海通证券】 研报大放送（百篇）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284199&amp;amp;idx=1&amp;amp;sn=4ec9cac078f8057744349c9c953decb2&amp;amp;chksm=802e2572b759ac6438362451289132ab4bb631da5b41e9f2b2545eb5efe50e0d14d6bd3d3015&amp;amp;scene=21#wechat_redirect&quot;&gt;【广发证券】 研报大放送（最全）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653284196&amp;amp;idx=1&amp;amp;sn=85245caf9148fb965df1c56c963984ba&amp;amp;chksm=802e2571b759ac6772582aea40781bddd6f148f144edc9b8b08606749f3c2c012b907441d59d&amp;amp;scene=21#wechat_redirect&quot;&gt;【国泰君安】 研报大放送（精华）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283773&amp;amp;idx=1&amp;amp;sn=d4604682da0c5563be9da16717d11bf9&amp;amp;scene=21#wechat_redirect&quot;&gt;各大券商研报&lt;/a&gt;（一）&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653283257&amp;amp;idx=2&amp;amp;sn=49c78925e7f3535b9cad95bf91574519&amp;amp;scene=21#wechat_redirect&quot;&gt;各大券商研报&lt;/a&gt;（二）&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-14-34532766</guid>
<pubDate>Wed, 14 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>特征重要性在量化投资中的深度应用【系列56】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-14-34532295.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34532295&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-311ebd241a5460a3fc01021e805c5039_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287672&amp;amp;idx=1&amp;amp;sn=9f59afb57b99cab6692367578c0aaa70&amp;amp;chksm=802e36edb759bffbd644791bb16bbe1ee0042ac3118421976e13141b807f90ae1f3d752c3450&amp;amp;scene=0#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d9ceed78978badf52f685b50ced44c6&quot; data-image-width=&quot;358&quot; data-image-height=&quot;358&quot; data-image-size=&quot;ipico&quot;&gt;特征重要性在量化投资中的深度应用【系列56】&lt;/a&gt;&lt;p&gt;前五期传送门：&lt;/p&gt;&lt;p&gt;【系列55】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287642&amp;amp;idx=1&amp;amp;sn=a7c71f89c3ad6f60590585b1bf016780&amp;amp;chksm=802e36cfb759bfd9a629eae57fc430ff457753a138d053cb7666d55cd41f91809d821285a521&amp;amp;scene=21#wechat_redirect&quot;&gt;机器学习应用量化投资必须要踩的那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列54】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287512&amp;amp;idx=1&amp;amp;sn=14ee62549dab3c64468f78b3dbfd39a5&amp;amp;chksm=802e364db759bf5bb5abffc6a50f72d0e31722c178e01ce11a3d48fb28386055e741c9ecce8d&amp;amp;scene=21#wechat_redirect&quot;&gt;因子的有效性分析基于7种机器学习算法&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列53】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287481&amp;amp;idx=1&amp;amp;sn=dcb1dda1e2362d8297ae1a97845cf02e&amp;amp;chksm=802e362cb759bf3a3aaea75af824451a3dba7345ecc73e27facc4b917792835fdd2878403c8c&amp;amp;scene=21#wechat_redirect&quot;&gt;基于XGBoost的量化金融实战&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;今天，继续我们的机器学习应用量化投资系列。本期我们再介绍一篇杨勇团队撰写的研究报告。希望大家在写策略注意这些问题。 &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;从IC、IR到另类线性归因&lt;/b&gt;&lt;/p&gt;&lt;p&gt;基于IC、IR的单因子分析是传统多因子分析的基石。但是IC、IR分析出却不能考虑到多因子模型中因子与因子之间的相互影响。因此我们以之前报告介绍的标准神经网络回归为例，用另类线性归因对因子进行了分析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从线性归因到非线性归因&lt;/b&gt;&lt;/p&gt;&lt;p&gt;所有线性归因都是基于因子单调性（线性）的强假设。但是在机器学习的非线性世界中，这个强假设不复存在。非线性的机器学习算法需要非线性的归因方式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从相关性到因果性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;所有的传统归因方式都是基于相关性的而非因果性。因果分析也是机器学习未来的一个重点。我们以TMLE为例介绍机器学习下的因果性分析。&lt;/p&gt;&lt;p&gt;&lt;b&gt;机器学习归因的意义&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于传统模型，例如logit或者决策树而言，输入（自变量）和输出（因变量）的关系是非常明显的。你可以非常清楚的明白为什么一些样本被错误划分了，例如，比如输入因子中某个因子太小了。同样的，对于决策树，同样可以根据决策树每个分叉的逻辑（例如因子A&amp;gt;某个常数）向下推演，得出错误划分的原因。但是对于其他大多数的模型，由于它们的高维和非线性，要直观的理解是非常困难的。&lt;/p&gt;&lt;p&gt;尽管如此，让机器学习一个非常有前景的科技让人觉得处于黑箱的状态是非常不明智的。不透明性增加了误用的概率。亚马逊的算法，决定了大多数人今天在读什么书；NSA的算法决定了谁是潜在的恐怖分子；气候变化模型决定了二氧化碳排放量的安全范围。人不能干预和控制人所不明白的事情，这是什么要单独将机器学习归因的作为一篇报告的原因。&lt;/p&gt;&lt;p&gt;&lt;b&gt;特征工程与特征重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器学习的特征在量化投资当中也被称为因子。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1. 特征工程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;特征工程是用某些领域内的知识来构造特征的过程。&lt;/p&gt;&lt;p&gt;如果世界上有无穷的数据，和一个universal function approximator（一个可以表达任何事情的模型），那么就没有特征工程存在的必要。这正是目前在图像识别领域发生的事情，卷积神经网络直接学习每个像素点，然后对图像内容进行识别，而不借助任何人手制的特征。&lt;/p&gt;&lt;blockquote&gt;Coming up features isdifficult, time consuming, requires expert knowledge. “Applied machinelearning” is basically feature engineering.      &lt;br&gt;——AndrewNg, Machine Learning and AI via Brain simulations&lt;/blockquote&gt;&lt;p&gt;正如吴恩达所述，应用机器学习主要是特征工程。而金融领域的特征获取往往有两种方式。一种是从主观看盘经验来或者从经济学或者金融学的论文来；另一种是纯数据挖掘。后者经常被人诟病容易过度拟合而导致亏损。但是事实上事情可能并没有这么可怕。举例而言WorldQuant号称有四百万的Alpha因子，结合Alpha101来看，很多因子非常可能是数据挖掘出来的，但是从公开业绩来看，它的投资表现还是尚且可以令人满意的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2. 特征重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在构造出特征之后，我们需要了解这个特征究竟对我们的预测有没有用，这就需要了解特征重要性。&lt;/p&gt;&lt;p&gt;特征重要性的另一作用是可以进行特征选择，例如选出前五重要性的特征作为模型输入，剩下的可以舍弃。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;传统线性归因&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;传统量化投资是基于线性的世界，在这个世界中，衡量因子的重要度是IC、IR等等指标。除了IC，IR之外，还有一些值得介绍的传统线性归因的方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.1. 逐步回归&lt;/b&gt;&lt;/p&gt;&lt;p&gt;逐步回归的基本想法是，将变量逐个引入，引入变量的条件是偏回归平方和经检验是显著的，同时每引入一个新变量后，对已选入的变量要进行逐个检验，将不显著变量剔除，这样保证最后所得的变量子集中的所有变量都是显著的。这样经若干步以后便得“最优”变量子集。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2. Ridge, Lasso,Elastic Net&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在线性回归中，损失函数定义为：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ed56a9ae246009ac7742f3df75aaf639_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;287&quot; data-rawheight=&quot;55&quot;&gt;&lt;p&gt;也即RSS。&lt;/p&gt;&lt;p&gt;线性回归的目标在于找到一组系数(w1, w2, . . . , wd)使得RSS最小，但使用RSS作为损失函数可能会导致过拟合，尤其当训练集不够或者特征数量过多时（一个典型的例子是多重共线性），表现为即使实际解释力弱的特征，由于过拟合，它的系数值也较大。为了解决这个问题，在损失函数中对系数加入惩罚项：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-488c58cda850626e233e6103d8e85221_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;365&quot; data-rawheight=&quot;58&quot;&gt;&lt;p&gt;以上式最小为目标来寻找系数的方式就叫做RidgeRegression。其中 Lambda为调节参数，其大小标志着对系数的惩罚力度。 Lambda越大，系数就越小。但问题是，系数只能够趋近于0，当特征个数很多的时候，对那些本来系数就非常小的特征没什么影响，不能减小模型复杂度。&lt;/p&gt;&lt;p&gt;于是，将损失函数修改为：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-758cfc94f711f32043f60d27ad565706_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;313&quot; data-rawheight=&quot;59&quot;&gt;&lt;p&gt;以上式最小为目标来寻找系数的方式就叫做LassoRegression。损失函数在收敛的过程中会使一些系数变为0。变为0的权重对结果影响较小，即对应的特征相对不重要。因此LassoRegression可以筛选特征，有效减小线性模型的复杂度。&lt;/p&gt;&lt;p&gt;Elastic Net实际上式Ridge，Lasso的综合，其损失函数表示为：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-50f5088a0f104590fcaee99835c27b03_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;428&quot; data-rawheight=&quot;56&quot;&gt;&lt;p&gt;其中L1正则项（Lasso）产生稀疏的系数向量，减小模型复杂度。L2正则项（Ridge）减小过拟合，消除一定的L1稀疏性，以产生groupeffect，稳定L1正则项的路径。&lt;/p&gt;&lt;p&gt;所以从以上介绍可以看出，Ridge, Lasso，Elastic Net前面的正则化的系数的绝对值大小直接代表了该特征的重要性。下图代表了随着惩罚系数的增加，特征前面的系数也随之缩小。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-01d284fb4f8e8868161fc190e655b787_r.jpg&quot; data-caption=&quot;特征系数与惩罚系数关系&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;553&quot; data-rawheight=&quot;414&quot;&gt;&lt;p&gt;下图是之前上一篇标准神经网络回归策略的因子（特征）重要性排名，绝对值越大越重要，正负代表方向。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5de0fc74d63002cfb177b54df65363a1_r.jpg&quot; data-caption=&quot;Lasso Regression的系数 &quot; data-size=&quot;normal&quot; data-rawwidth=&quot;528&quot; data-rawheight=&quot;288&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-07c267be2ee992ed91e413f0d60b2419_r.jpg&quot; data-caption=&quot;Ridge Regression 的系数&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;565&quot; data-rawheight=&quot;300&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19620c7d9431a97e8d78301a6d3f33fd_r.jpg&quot; data-caption=&quot;ElasticNet 的系数&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;559&quot; data-rawheight=&quot;336&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-09fe1091efa130822b592d061ebe6d1d_r.jpg&quot; data-caption=&quot;变量间线性关系、非线性关系&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;697&quot; data-rawheight=&quot;244&quot;&gt;&lt;p&gt;例如，在上图左中，变量之间的关系是线性的，而在右图中，线性归因显然是不能反映出真实的变量之间的相关关系。&lt;/p&gt;&lt;p&gt;为了在非线性的世界中衡量因子的重要性，一系列不同的算法被开发出来了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;随机森林系列&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;4.1. 随机森林&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随机森林属于集成学习，可以视为是bagging算法在决策树上的运用。&lt;/p&gt;&lt;p&gt;机器学习中决策树主要用于分类和回归，树中的每一个节点表示某一特征的判断条件，其分支表示符合节点条件的对象。叶子节点表示对象所属的预测结果。&lt;/p&gt;&lt;p&gt;随机森林则由许多决策树构成，每棵决策树都由随机的部分样本的部分特征进行训练，它只接受了部分的训练数据，因此每棵决策树都是一个弱学习器。然后，通过bagging所有的弱学习器——决策树，比如投票（分类问题）或者取均值（回归问题），得到一个强学习器——随机森林。&lt;/p&gt;&lt;p&gt;由于每一棵树的输入样本不是全部的样本，每一棵树的特征不是全部特征，基于此基础上进行集成，预测结果相对不容易出现过拟合。并且由于训练的样本是随机、独立地进行选取，对各棵树的训练可以并行进行，训练速度相对快。&lt;/p&gt;&lt;p&gt;用随机森林计算因子重要性的方法有很多种，下面介绍其中一种&lt;/p&gt;&lt;p&gt;1：对于随机森林中的决策树i,使用相应的OOB(Outof Bag袋外数据)数据来计算它的袋外数据误差，记为errOOB1i。&lt;/p&gt;&lt;p&gt;2：随机地对袋外数据OOB所有样本的特征X加入噪声干扰(例如可以把X重新打乱顺序，常见的方法是就可以随机的改变样本在特征X处的值)，再次计算它的袋外数据误差，记为errOOB2i。 &lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-94e88c2bbdffe4c7dcda68df63ca9f83_r.jpg&quot; data-caption=&quot;随机森林计算因子重要性-打乱 X 前&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;658&quot; data-rawheight=&quot;174&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1391b095451af62503458c75d459bb1e_r.jpg&quot; data-caption=&quot;随机森林计算因子重要性-打乱 X 后&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;647&quot; data-rawheight=&quot;171&quot;&gt;&lt;p&gt;3：假设随机森林中有Ntree棵树,那么对于特征X的重要性为&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bfcff7256133df4d191e6a5663c246d2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;441&quot; data-rawheight=&quot;59&quot;&gt;&lt;p&gt;之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。下图是随机森林计算因子重要性的结果图。&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7ae42ba34278f01329df5171f8c20d68_r.jpg&quot; data-caption=&quot;随机森林计算因子（特征）重要性&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;553&quot; data-rawheight=&quot;385&quot;&gt;&lt;p&gt;&lt;b&gt;4.2. Burota&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Boruta是一种特征选择算法。精确地说，它是随机森林周围的一种延伸算法。&lt;/p&gt;&lt;p&gt;下面是Boruta算法运行的步骤：&lt;/p&gt;&lt;p&gt;1、首先，它通过创建混合副本的所有特征（即阴影特征）为给定的数据集增加了随机性。阴影特征就是把许多打乱后的特征作为新的特征。&lt;/p&gt;&lt;p&gt;2、然后，它训练一个随机森林分类的扩展数据集，并计算特征重要性，以评估的每个特征的重要性，越高则意味着越重要。&lt;/p&gt;&lt;p&gt;3、在每次迭代中，它检查一个真实特征是否比最好的阴影特征具有统计显著的更高（低）的重要性（即该特征是否比最大的阴影特征得分更高），如果是，则确认（拒绝）。它会删除它视为拒绝的特征，然后回到第1步。&lt;/p&gt;&lt;p&gt;4、最后，当所有特征得到确认或拒绝，或算法达到随机森林运行的一个规定的限制时，算法停止。&lt;/p&gt;&lt;p&gt;下图是之前上一篇标准神经网络回归大盘择时策略的因子（特征）重要性排名，从左到右依次从重要到不重要。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cd206e826d9096dbe364f8f63e4c5e71_r.jpg&quot; data-caption=&quot;标准神经网络回归大盘择时策略的因子（特征）重要性排名&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;550&quot; data-rawheight=&quot;531&quot;&gt;&lt;h2&gt;&lt;b&gt;遗传算法&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;遗传算法主要应用于优化问题，来源于种群进化的想法。首先需要确定题解的形式，一般为向量形式(x1,x2,...,xd)。开始时，随机生成大量的向量，作为初始种群。然后从该种群中挑选出最优题解，形成新的种群。然后，对它们做出修改，重新挑选出最优题解，依此反复进行这一过程。&lt;/p&gt;&lt;p&gt;修改题解的方法有变异和交叉：变异是对一个既有题解进行微小、简单、随机的改变，比如随机修改向量中一个元素Xi；交叉则是选取2个最优题解，将它们按某种方式结合，比如X1…Xi来自a向量，而…来自b向量，组成新的向量c。&lt;/p&gt;&lt;p&gt;变异如下图：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-def2491d8e81b63704fa6a4f1e935695_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;387&quot; data-rawheight=&quot;252&quot;&gt;&lt;p&gt;新的种群是通过对上一种群中的最优解，进行随机的变异和交叉构造出来的，它的大小通常与旧种群相同。这一过程会一直重复进行，达到指定的迭代次数，或者经数代后题解没有得到改善，结束整个过程。&lt;/p&gt;&lt;p&gt;遗传算法的归因往往需要结合特定的算法。举例来说，如果要从m个特征中，选出n个特征，使得一个线性回归的拟合效果最好。除了用(m,n)`的遍历方法之外，就可以用遗传算法来减少运算量。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TMLE&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;传统的机器学习模型往往是考虑相关性，但是不考虑因果性。相关性单纯指出A和B是有联系的，而因果性会指出是由于A导致了B还是由于B导致了A。&lt;/p&gt;&lt;p&gt;更复杂的因果性可以从下图看到，气温升高导致了冰淇淋的销量和啤酒的销量的增加，两者是因果性的关系。冰淇淋的销量和啤酒的销量的增加虽然有强相关，但是两者都是受气温驱动，两者没有因果联系。&lt;/p&gt;&lt;p&gt;同样的，在大盘择时策略中，我们也可以提出下列问题，昨日价格变化这个因子是否部分决定了今日高开低开幅度和 15:00价格/14:30价格-1？今日高开低开幅度和 15:00价格/14:30价格-1是不是只有相关性，没有因果性？&lt;/p&gt;&lt;p&gt;因此我们需要用全新的方法去解决因果性的问题。&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bfab4b9b515f1e5724b3ceed9add0371_r.jpg&quot; data-caption=&quot;大盘择时策略的因子重要性归因&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;329&quot; data-rawheight=&quot;128&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-644e87cd04f892d2ffc11f7a8292886c_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;705&quot; data-rawheight=&quot;831&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-af7346000f256443690914182ff37522_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;645&quot; data-rawheight=&quot;491&quot;&gt;&lt;p&gt;在之前的标准神经网络回归大盘择时策略当中，如果我们对“今日高低开幅度（昨日收盘价/昨日开盘价-1）”因子做TMLE，调用R包：&lt;/p&gt;&lt;p&gt;https://github.com/chizhangucb/tmleCommunity&lt;/p&gt;&lt;p&gt;可以得到如下结果：&lt;/p&gt;&lt;p&gt;&lt;b&gt;TMLE：-0.059&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;95%信心区间：[-0.0605,-0.0577]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;所以可以说明这个因子是有效的。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;杨勇团队&lt;/p&gt;&lt;p&gt;分析师：周袤&lt;/p&gt;&lt;p&gt;联系方式：18601798125&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-20e147d1234f759b40d3358135abec1e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-14-34532295</guid>
<pubDate>Wed, 14 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【必看】机器学习应用量化投资必须要踩的那些坑！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-03-12-34475061.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34475061&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1f33ef2a74bc4f6928f716dc70b99b1e_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;前四期传送门：&lt;/p&gt;&lt;p&gt;【系列54】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287512&amp;amp;idx=1&amp;amp;sn=14ee62549dab3c64468f78b3dbfd39a5&amp;amp;chksm=802e364db759bf5bb5abffc6a50f72d0e31722c178e01ce11a3d48fb28386055e741c9ecce8d&amp;amp;scene=21#wechat_redirect&quot;&gt;因子的有效性分析基于7种机器学习算法&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列53】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287481&amp;amp;idx=1&amp;amp;sn=dcb1dda1e2362d8297ae1a97845cf02e&amp;amp;chksm=802e362cb759bf3a3aaea75af824451a3dba7345ecc73e27facc4b917792835fdd2878403c8c&amp;amp;scene=21#wechat_redirect&quot;&gt;基于XGBoost的量化金融实战&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e38fa3ed8bcdd825f7746f42bd2e4775_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;570&quot;&gt;&lt;p&gt;今天，继续我们的机器学习应用量化投资系列。本期我们介绍一篇杨勇团队撰写的研究报告。希望大家在写策略注意这些问题。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;从高频到低频&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器学习在高频量化策略上应用更加容易。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从线性到非线性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;机器学习下的非线性比线性更能榨取数据的价值，但也更容易过度拟合，因此需要合理使用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从单次分析到推进分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;推进分析更加符合实盘状态下盘后更新模型的实际情况。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从分类到回归&lt;/b&gt;&lt;/p&gt;&lt;p&gt;回归经常能优于简单的分成两类。&lt;/p&gt;&lt;p&gt;&lt;b&gt;预测值相关&lt;/b&gt;&lt;/p&gt;&lt;p&gt;好的预测值不一定带来好的交易信号。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;标准神经网络回归大盘择时策略&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1.1. 设想和目标&lt;/b&gt;&lt;/p&gt;&lt;p&gt;运用机器学习对过去的模式进行识别，并预测未来。也即，用当前实时数据与过去所有数据进行模式匹配，若过去模式显示会大概率上涨下跌，则相应做多做空，否则不做操作。原本该模型是为日内策略设计的，也就是收盘平仓，但由于目前平今仓手续费昂贵，所以改为第二天开盘平。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2. 理论、方法及数据源&lt;/b&gt;&lt;/p&gt;&lt;p&gt;和所有量化策略相似，研究假设过去发生的事情未来会重复发生（也即挑战市场弱有效的假设）。另一重要假设是指数现货和指数期货之间相关性很高，接近1。这个假设是合理，因为在流动性充足的市场，如果现货和期货之间的任何偏差都可以造成套利机会。故可以用现货做期货。方法为传统的深度学习方法。数据源来自天软、万得，主要是中证500指数，沪深300指数，以及对应的期货主力合约。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3. 交易成本与策略执行&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在此策略的历史数据回测中，成交成本假设为日内单边千分之一，隔日单边万分之3。也即在成交中假设1.5个指数点的冲击成本。这样的假设充分包含了目前股指期货低流动性的现实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该算法共有7个模型，分别对应10:00,10:30,11:00,13:00,13:30,14:00,14:30的决策时间点。每个模型的本质是相似的，唯一的不同只在于越向后的模型，所能拥有的供机器决策数据越多。例如在10:30做决策会比10:00做决策多出半小时的数据。每个模型本身都是监督式学习。用价量指标来预测收益。若基于机器学习的预测值触及多头开仓阈值，则做多；若基于机器学习的预测值触及空头开仓阈值，则做空。反之维持原来仓位。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.5. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：3.55&lt;br&gt;最大回撤：17.05%&lt;br&gt;胜率：62.69&lt;br&gt;盈亏比：1.31&lt;br&gt;年化：80.36%&lt;/blockquote&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-969ba198529defa23be5ac24dd392b76_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;890&quot; data-rawheight=&quot;337&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-59e13eea274e23ab5a664f85cb34d77d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;887&quot; data-rawheight=&quot;351&quot;&gt;&lt;p&gt;&lt;b&gt;1.6. 策略因子归因&lt;/b&gt;&lt;/p&gt;&lt;p&gt;用前述的策略因子归因方法，可以看出一些非常重要的特点。&lt;/p&gt;&lt;p&gt;可以从图中看到，开盘的前一个小时行情数据产生的因子和收盘最后一个小时行情数据产生的因子是非常重要的，如14:30到15:00的收益。这与人的主观经验是一致的，开盘前一小时交投最活跃，基本能反应当天的市场情绪和主导全天的走势。而收盘最后一小时由于经常是对第二天情绪的猜测，所以从它的走势经常能推断出第二天市场的方向。&lt;/p&gt;&lt;p&gt;另外，盘中两个小时交投最不活跃，随机性也越大。单独用盘中两个小时作为因子去预测未来收益相对来说效果会差一些。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8885a17924d2d9cec6ddec3a38dc52ed_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;428&quot; data-rawheight=&quot;413&quot;&gt;&lt;p&gt;&lt;b&gt;1.7. 风险点及未来的改进方向&lt;/b&gt;&lt;/p&gt;&lt;p&gt;风险点主要有：&lt;/p&gt;&lt;p&gt;（1）期货和现货突然性的暂时偏离（在当前负基差的情况下和低成交量下，尤其可能发生）&lt;/p&gt;&lt;p&gt;（2）市场结构发生了深刻的变化（投资者类型，投资者风险偏好等等），导致过去的数据不再能预测未来。例如去年股灾期间国家队的大规模救市。&lt;/p&gt;&lt;p&gt;（3）市场流动性不足，导致成交需要付出巨大成本或者无法成交。以中证500为例，本模型单笔收益大约在千分之三左右，如果买卖价差长期超过5个指数点，将对策略的盈利能力造成毁灭性的打击。目前股指期货受限以来买卖价差大约为1-2个指数点。&lt;/p&gt;&lt;p&gt;（4）没有合适的报撤单逻辑。如果出现单边市场，简单的用限价单的报撤会导致以最不利的价格成交。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从低频到高频&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了比较高频分钟线与日线策略的区别，我们也设计了一个日线策略。这个策略是基于传统技术指标做特征，例如昨日收盘价相对于过去几日的均线的位置，以及高开低开情况等等。去预测未来的一日的收益。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该算法每日决策一次，每个模型本身都是监督式学习。用价量指标来预测收益。若基于机器学习的预测值触及多头开仓阈值，则做多；若基于机器学习的预测值触及空头开仓阈值，则做空。反之维持原来仓位。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：0.68&lt;br&gt;最大回撤：36.92%&lt;br&gt;胜率：53.21&lt;br&gt;盈亏比：0.99&lt;br&gt;年化： 19.02%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb9ce977f34138a563fde4edbb01e911_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;885&quot; data-rawheight=&quot;326&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d018a9cba0517416247afae7184babb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;889&quot; data-rawheight=&quot;342&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2c5ecabff105fc8b09af9ef450ac22c3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;805&quot; data-rawheight=&quot;163&quot;&gt;&lt;p&gt;&lt;b&gt;1.3. 高频背后的一些逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1. 数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通常来说，对机器学习模型，数据量越大越好。在假设能反映出目前市场的前提下，尽可能多的增加训练集的长度，对机器学习的模型算法收敛和模型稳定性是大有裨益的。假设是日线，以每年250个交易日为例，那么2010年到2017年就是大约1700个数据点。但如果是分钟线，同样是每年250个交易日，每个交易日240根分钟线，那么一共就有1700*240=408,000的数据点。显然后者就比前者多了好几个数量级。&lt;/p&gt;&lt;p&gt;但是并不是数据量的增加可以无限的，数据量的增加会收到其他客观条件所约束，如运算速度和交易成本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.1. 运算速度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;举例期货的例子来说，交易所每500毫秒推送一个tick，所以理论上，2010年到2017年就可以有49,000,000个数据点。如果假设交易策略是简单的每500毫秒预测一次，那么数据点的增加在实盘中就并没有什么用处。因为在CPU下，神经网络的计算用时不太可能在500毫秒之内。所以在一个决策时间点内没有算完，就已经进入了下一个决策时间点，实盘当中根本交易不到。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.2. 交易成本&lt;/b&gt;&lt;/p&gt;&lt;p&gt;同样上一个期货的例子。如果是非做市类策略，那么算上冲击成本后的交易成本通常双边至少要达到千分之一。在500毫秒乃至更长一些的时间尺度，由于时间时间尺度偏短，波动很难非常大，所以这是一个非常难覆盖交易成本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.3. 日内消息面&lt;/b&gt;&lt;/p&gt;&lt;p&gt;国内股市实行每日交易时间是四份小时。四个小时内出现基本面新消息的概率较小。而隔日的话，各种消息面容易打破股价自身的运行规律。使得预测的准确性大幅降低。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1.4.&lt;/b&gt; 行为金融&lt;/p&gt;&lt;p&gt;人的行为在短期内是比较固定的。比如日内短线的追涨杀跌等等，这些都是由人性所决定的。但是随着时间的拉长，特别是两个交易日之间，人会冷静下来，情绪会淡化。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从线性到非线性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了比较线性模型与非线性模型的区别，我们也设计了一个线性模型。这个策略是基于传统线性核函数的支持向量机回归，使用标准的神经网络回归策略一样的因子和预测目标。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，神经网络被替换成了线性核函数的支持向量机回归。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：0.95&lt;br&gt;最大回撤：29.71%&lt;br&gt;胜率：49.64&lt;br&gt;盈亏比：1.23&lt;br&gt;年化：17.67%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6f5f4fb6371357a81405e6feabb7fe4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;887&quot; data-rawheight=&quot;338&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-38f522e07fa61e47311117d06f41a886_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;894&quot; data-rawheight=&quot;367&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-83118faa54190d798b117e216be46a8f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;892&quot; data-rawheight=&quot;337&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6e3d0f4d07ea71a48f83947786d68033_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;163&quot;&gt;&lt;p&gt;&lt;b&gt;3.3. 非线性背后的一些逻辑和讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.1. 金融市场大概率是非线性的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;金融市场大概率是非线性的，举例而言，业内研究发现，不是高开幅度越大，当日的后续走势就越向上。如果当日高开0~0.5%左右，那么当日大概率是上扬的，但是如果高开的过大，当日就容易高开低走。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.2.  Bias-VarianceTrade off&lt;/b&gt;&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e463aeb59570d5950456182d6adeb3fe_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;723&quot; data-rawheight=&quot;1188&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1acc02f20b8c80ed566fa29a70784ddf_r.jpg&quot; data-caption=&quot;Dropout 算法&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;500&quot; data-rawheight=&quot;230&quot;&gt;&lt;p&gt;如上图所示，被舍弃的神经元用X表示出来了。所有指向它和从它发出的有向箭头都被斩断。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.3.3. 人的理解方式经常是线性的&lt;/b&gt;&lt;/p&gt;&lt;p&gt;正向线性思维的特点是，思维从某一个点开始，沿着正向向前以线性拓展，经过一个或是几个点，最终达到思维的正确结果。举例而言，经常有人会认为，如果过去100年平均每年的生产力提升是一千亿，那么未来一年统计上的期望生产力提升也是一千亿。这里就犯了一个常见错误。人类生产力的提升经常是指数级别上升的，所以未来一年统计上的期望生产力提升应该不止一千亿。&lt;/p&gt;&lt;p&gt;正是因为人的思维方式是线性的，在理解非线性的时候会直观上比较困难。为了增加读者对非线性的直观理解，我们将在下一篇中重点阐述。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从单次分析到推进分析&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;4.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，我们这次要比较第一篇报告中写了单次分析和推进分析。&lt;/p&gt;&lt;p&gt;下面是单次分析的常见方法：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-10eaa572edc2c9bb80992ac07ad96e93_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;724&quot; data-rawheight=&quot;74&quot;&gt;&lt;p&gt;上图给了一个单次分析的实例。实际上单次分析就是把整个样本分为互不重叠的两个部分。白色的是样本内，灰色的是样本外。首先用样本内的数据训练机器学习模型，然后用这个建立好的机器学习模型直接放入样本外数据进行检验，如果在样本外的数据依然说明该模型效果很好，那么在一定程度上说明该模型可以处理实际的问题而推进分析的样本内外常常变化。&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-47f7b1b20a6dda17406cb528e7ab7c56_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;730&quot; data-rawheight=&quot;194&quot;&gt;&lt;p&gt;上图是一种推进分析的方法。推进分析有个最为明显的特点，就是样本外的交易长度仅为一个交易周期。同样的，首先用样本内的数据训练机器学习模型，然后用这个建立好的机器学习模型直接放入样本外数据进行检验。在T1时刻，用0~T1的数据训练模型，然后在T1~T2的数据去检验模型；在T2时刻，用0~T2的数据训练模型，然后在T2~T3的数据去检验模型；在T3时刻，用0~T3的数据训练模型，然后在T3~T4的数据去检验模型，以此类推。最后将所有灰色框内的检验结果汇总，就是推进分析下总的样本外结果。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：2.66&lt;br&gt;最大回撤：17.24%&lt;br&gt;胜率：57.56&lt;br&gt;盈亏比：1.22&lt;br&gt;年化：56.38%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-93f1c4b4391948be4591dd72568ab5d4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;885&quot; data-rawheight=&quot;282&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a03e23c9026dca225287389073ab33a8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;298&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b029cadec3c51aa0d869a68378d78f88_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;343&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-bfd31babbaccc71c254dd3bd81034547_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;794&quot; data-rawheight=&quot;162&quot;&gt;&lt;p&gt;&lt;b&gt;4.3. 单次分析和推进分析的逻辑讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在逻辑上，推进分析更接近实盘。因为在实盘中，经常地，模型会每日在盘后更新。所以如果是这样，在回测时候，也应该假定，在T日末，模型会被重新训练，也即是站在T日的模型和站在T+1的模型也应当是不一样的。这样做的显著好处是，不论是回测还是实盘，每个模型都能用到站在当前时点上最新的数据。坏处也很显著，在回测时候，对历史上的每一天都要建立一个模型，这样的计算量是巨大的。&lt;/p&gt;&lt;p&gt;另外推进分析也有不同的方法。&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9627ee0c4ec83e3ce7b73450a6051e84_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;810&quot; data-rawheight=&quot;208&quot;&gt;&lt;p&gt;上图是另一种推进分析的方法（Rolling）。与之前推进分析的方法不同，在T2时刻，用0~T2的数据训练模型，然后在T2~T3的数据去检验模型；在T3时刻，我们并不像之前一样，用0~T3的数据训练模型，而是用T1~T3的数据训练模型，然后同样的在T3~T4的数据去检验模型，以此类推。最后将所有灰色框内的检验结果汇总，就是推进分析下总的样本外结果。&lt;/p&gt;&lt;p&gt;使用全样本做推进分析和使用过去n期样本做推进分析之间没有优劣之分。选择时候大体上要遵循两个基本原则，一个是数据要具有对当前市场状态的代表性，另一个数据量要尽可能多。使用过去n期样本通常能对当前市场状态的代表性，使用全样本做推进分析。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;从分类到回归&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;5.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样，不同的是，预测目标不再是某段时间的收益，而是一个二分类。也即，大于0的时候是上涨分类，小于0的时候是下跌分类。&lt;/p&gt;&lt;p&gt;&lt;b&gt;5.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：1.66&lt;br&gt;最大回撤：25.30%&lt;br&gt;胜率：49.72%&lt;br&gt;盈亏比：1.39&lt;br&gt;年化：30.91%&lt;/blockquote&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3362bc9ff3f7b250fcbf5675523f1925_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;294&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0d537f23041608c16a00817b2b13dfb1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;319&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-91b71a3b927eb85ca174dba987737674_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;895&quot; data-rawheight=&quot;341&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-edb04cc8fc9e8ed1d0c010294eddc976_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;162&quot;&gt;&lt;p&gt;&lt;b&gt;5.3. 分类与回归的逻辑讨论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分类的逻辑是，市场的状态是离散可分的。如果按照上例，涨0.1%和涨10%都会归结到上涨一栏。但是事实上，涨0.1%和涨10%是截然不同的，前者很可能是随机扰动，而后者一定是市场情绪的体现。但是随机扰动与市场情绪的分界点是很难确定的，涨0.1%是随机扰动，但是涨0.5%是不是随机扰动呢？所以分类有个天生的问题，以什么标准来划分类，如何划分类？我们也曾经尝试过划分成5类，7类，但是由于划分的类过多，效果也不及二分类。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;预测值相关&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;6.1. 算法和模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;算法与模型基本和标准的神经网络回归策略一样。在标准神经网络回归，我们只有当大于一个阈值的时候，做多。小于一个阈值的时候做空。但是在该策略中，再算出预测值后，直接预测值大于0就做多，小于0就做空。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6.2. 结论&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在日内单边千分之一，隔日单边万分之3的成交假设下，策略表现如下：&lt;/p&gt;&lt;blockquote&gt;夏普：2.17&lt;br&gt;最大回撤：26.05%&lt;br&gt;胜率：46.68%&lt;br&gt;盈亏比：1.75&lt;br&gt;年化：43.92%&lt;/blockquote&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4a1631f8ec1b942036d0c37561aca1e4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;318&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-1d079e829a45e76f1099cc8e856aed04_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;890&quot; data-rawheight=&quot;338&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2e27e4c6c26a27ecaa9f320b06d5db53_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;888&quot; data-rawheight=&quot;354&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-808a4d46bf83936d11d563571d80d7a9_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;164&quot;&gt;&lt;p&gt;&lt;b&gt;6.3. 预测值相关的逻辑&lt;/b&gt;&lt;/p&gt;&lt;p&gt;预测值可以从上述的回测看到，预测值如果按照简单的大于0，小于0交易，效果并不是特别出色。目前业内比较公认的结论是，预测值的强度代表方向的概率。举例而言，如果一个预测值是0.1%，一个是1%，那么后者实际上涨的概率大于前者。因此，选择一个合适的阈值变的至关重要。可能的确定阈值的方法可以是，历史上预测值的平均数加上一个历史上预测值的标准差作为看多阈值；历史上预测值的平均数减去一个历史上预测值的标准差作为看空阈值。&lt;/p&gt;&lt;p&gt;一个绝对值较大的预测阈值容易漏掉一些真正的上涨机会（统计上的Type II error），而一个绝对值比较小的预测阈值容易错误的开多仓（统计上的Type I error）。同时，绝对值预测阈值越小，越容易达到阈值，越容易触发交易，交易频率就越高，交易成本就越高。反之交易就越不频繁，交易成本就越低。&lt;/p&gt;&lt;p&gt;杨勇团队&lt;/p&gt;&lt;p&gt;分析师：周袤&lt;/p&gt;&lt;p&gt;联系方式：18601798125&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-03-12-34475061</guid>
<pubDate>Mon, 12 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>基于XGBoost的量化金融实战【系列53】</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-24-33948430.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33948430&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-72db6022f891e159901bddd2f66a3ad7_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;前两期传送门：&lt;/p&gt;&lt;p&gt;【系列52】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287306&amp;amp;idx=1&amp;amp;sn=9f374874636e7d6d52a9b3d92d6aa81b&amp;amp;chksm=802e319fb759b8896acf2ed9529da88a8fda0d76d6a3b816854e9ad5eeecfd6f4af75dd65804&amp;amp;scene=21#wechat_redirect&quot;&gt;基于Python预测股价的那些人那些坑&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【系列51】&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287197&amp;amp;idx=1&amp;amp;sn=9630389a52c7d0be4c1feaf3a534c2ce&amp;amp;chksm=802e3108b759b81ed11174f71b23fb73abe5c4ebad0f9d480b6efbd8f7e644de6b2232dc63fa&amp;amp;scene=21#wechat_redirect&quot;&gt;通过ML、Time Series模型学习股价行为&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;今天，我们介绍一篇&lt;b&gt;王老板&lt;/b&gt;写的文章，关于极度梯度提升(XGBoost)应用量化金融方向的，而且知道几乎每个参加 Kaggle 比赛的人都会用它。用今天我们来预测借贷俱乐部 (Lending Club) 的贷款的良恶性。&lt;/p&gt;&lt;p&gt;获取数据集，请在文末获取。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;XGBoost 基础版&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from xgboost import XGBClassifier&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;首先调用本章需要通用的包，在之后每小节中额外需要的包会特别指出来。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;numpy: 提供数组结构和辅助函数&lt;/li&gt;&lt;li&gt;pandas: 提供数据表结构来处理数据&lt;/li&gt;&lt;li&gt;matplotlib: 用来画图 &lt;/li&gt;&lt;li&gt;sklearn: 用来机器学习&lt;/li&gt;&lt;li&gt;xgboost: 极度梯度提升&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外&lt;/p&gt;&lt;ul&gt;&lt;li&gt;train_test_split 划分训练和测试集&lt;/li&gt;&lt;li&gt;accuracy_score 计算精度和得分&lt;/li&gt;&lt;li&gt;XGBclassifier 是 XGBoost 的分类器&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;整篇代码都用以下设置&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;seed = 7
test_size = 0.33&lt;/code&gt;&lt;p&gt;用一个固定的 seed 为了重现结果，训练集和测试集的样例个数比例为 2 比 1。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.1 模型初探&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;学陌生东西上手最快的方式就是用例子。先不管 XGBclassifier 每个参数是什么，先用它的默认值跑跑看看结果如何。&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;比马印第安人糖尿病 (Pima Indians Diabetes)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据根据医疗记录预测比马印第安人 5 年内糖尿病的发病情况。它是一个二元分类问题，一共有 768 个样例。该数据集包含了 8 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;怀孕次数&lt;/li&gt;&lt;li&gt;2 小时的血浆葡萄糖浓度。&lt;/li&gt;&lt;li&gt;舒张压&lt;/li&gt;&lt;li&gt;三头肌皮肤褶层厚度&lt;/li&gt;&lt;li&gt;2 小时血清胰岛素含量&lt;/li&gt;&lt;li&gt;体重指数&lt;/li&gt;&lt;li&gt;糖尿病家族史&lt;/li&gt;&lt;li&gt;年龄&lt;/li&gt;&lt;li&gt;类变量 (0 或 1）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [1]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1.1. 准备数据&lt;/b&gt;&lt;/p&gt;&lt;p&gt;读取 csv 文件，并打印出头尾 3 行。&lt;b&gt;点击下图&lt;/b&gt;发现该数据都是数值型，因此不用做任何转换。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# load data
data = pd.read_csv(&#39;pima-indians-diabetes.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e9532cb117669c05356f1a0be8af9754_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;210&quot;&gt;&lt;p&gt;&lt;b&gt;点击下图&lt;/b&gt;查看每列特征的统计值，比如个数和均值，没有发现什么异常。&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;data.describe()&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3606a337b7ab9c86e5cfa0b714b6d380_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;269&quot;&gt;&lt;p&gt;定义函数 splitXy，划分 data 中的特征 X 和标记 y。通常&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最后一列是 y，用 dataset[:, -1]&lt;/li&gt;&lt;li&gt;前面所有列是 X, 用 dataset[:, 0:-1]&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;def splitXy( data ):
   dataset = data.values
   X = dataset[:,0:-1]
   y = dataset[:,-1]
   return X, y&lt;/code&gt;&lt;p&gt;划分出训练集 (X_train, y_train) 和测试集 (X_test, y_test)&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# split data into train and test sets
X_train, X_test, y_train, y_test =
train_test_split(X, y, test_size=test_size,
random_state=seed)&lt;/code&gt;&lt;p&gt;打印每个变量大小，确保集合划分正确。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;print( &#39;The size of X is&#39;, X.shape )
print( &#39;The size of y is&#39;, y.shape )
print( &#39;The size of X_train is&#39;, X_train.shape )
print( &#39;The size of y_train is&#39;, y_train.shape )
print( &#39;The size of X_test is&#39;, X_test.shape )
print( &#39;The size of y_test is&#39;, y_test.shape )
The size of X is (768, 8)
The size of y is (768,)
The size of X_train is (514, 8)
The size of y_train is (514,)
The size of X_test is (254, 8)
The size of y_test is (254,)&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.2. 训练模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;定义函数 fit 训练 XGBClassifier(&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def fit( X, y ):
   model = XGBClassifier()
   model.fit(X, y)
   return model&lt;/code&gt;&lt;p&gt;在训练集上调用函数 fit 并打印模型，注意模型用 binary:logistic 作为目标，因为该问题是二元分类问题。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;model = fit(X_train, y_train)
print(model)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
     gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
     min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
     objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
     scale_pos_weight=1, seed=0, silent=True, subsample=1)&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.3. 预测结果&lt;/b&gt;&lt;/p&gt;&lt;p&gt;定义函数 predict 预测新示例并计算精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def predict( model, X, y ):
   # make predictions for X
   y_pred = model.predict(X)
   predictions = [round(value) for value in y_pred]
   # evaluate predictions
   accuracy = accuracy_score(y, predictions)
   return predictions, accuracy&lt;/code&gt;&lt;p&gt;在测试集上调用函数 predict 并打印精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;_, accuracy = predict(model, X_test, y_test)
print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))
Accuracy: 77.95%&lt;/code&gt;&lt;p&gt;&lt;b&gt;对于一个简单的没有经过调参的模型，在测试集上的 77.95% 精度还算不错。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1.4 保存和加载模型&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有时候训练一次模型会花很长时间，保存训练好的模型以便下次直接用可以节省很多时间和资源。首先引进 pickle 包。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;import pickle&lt;/code&gt;&lt;p&gt;用 dump 函数来保存模型，将 model 命名成 pima.dat。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# save model to file
pickle.dump(model, open(&quot;pima.dat&quot;, &quot;wb&quot;))&lt;/code&gt;&lt;p&gt;用 load 函数来加载数据 pima.dat 命名为模型 pima_model。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# load model from file
pima_model = pickle.load(open(&quot;pima.dat&quot;, &quot;rb&quot;))&lt;/code&gt;&lt;p&gt;最后检查模型，发现 pima_model 在测试集的精度为 77.95%，和上节的结果一样。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;_, accuracy = predict(pima_model, X_test, y_test)
print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))
Accuracy: 77.95%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.1.5 可视化树&lt;/b&gt;&lt;/p&gt;&lt;p&gt;安装并引入 graphviz 包里面的 Digraph 和 xgboost 包里面的 plot_tree 用于画出模型中的某一棵树。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from graphviz import Digraph
from xgboost import plot_tree&lt;/code&gt;&lt;p&gt;比如你想看第 5 棵树是如何分裂的，设 num_trees = 4，注意 python 第一个对应的是 index 0。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;plot_tree(model, num_trees = 4)
plt.show()
plt.savefig(&#39;Tree from Top to Bottom.png&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-cdebdeb6f8711e858687c4760c9ea29f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;525&quot;&gt;&lt;p&gt;对上图的一些解释：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;f0, f1, ... f7 代表 pima 数据里的 8 个特征简写，其中 f1 对应着 “2 小时的血浆葡萄糖浓度”，树的根部一开始由这个特征在 127.5 的特征值上分裂。&lt;/li&gt;&lt;li&gt;XGBoost 可以自动处理缺失值，从上图可看出 missing 和 yes 归成一起&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外你想看第 1 棵树是如何分裂的，并且喜欢树从左往右分裂，而不是从上往下 (默认方式) 分裂，只需设置 rankdir = &#39;LR&#39;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;plot_tree(model, num_trees=0, rankdir=&#39;LR&#39;)
plt.show()
plt.savefig(&#39;Tree from Left to Right.png&#39;)&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ff41d578069fa5b9b6526077ba460313_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;694&quot;&gt;&lt;h2&gt;&lt;b&gt;1.2 数据预处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;先载入 LabelEncoder 和 OneHotEncoder&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder&lt;/code&gt;&lt;p&gt;LabelEncoder (LE) 将字符转换成整数&lt;/p&gt;&lt;p&gt; LE(猫, 狗, 鸡) = (0, 1, 2)&lt;/p&gt;&lt;p&gt;OneHotEncoder (OHE) 将字符转换成向量&lt;/p&gt;&lt;p&gt; OHE(猫, 狗, 鸡) = [1,0,0; 0 1 0; 0 0 1]&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2.1. 解码字符型输出&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;鸢尾花数据集 (Iris Flower)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Iris 以鸢尾花的特征作为数据来源，它是一个多元分类问题，一共有 150 个样例。该数据集包含了 4 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;花萼长度&lt;/li&gt;&lt;li&gt;花萼宽度&lt;/li&gt;&lt;li&gt;花瓣长度&lt;/li&gt;&lt;li&gt;花瓣宽度&lt;/li&gt;&lt;li&gt;类变量：Iris Setosa (山鸢尾, Iris Versicolour (杂色鸢尾), Iris Virginica (维吉尼亚鸢尾)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [2]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的类别是字符型变量，用 LabelEncoder。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;iris.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-022a372a2d303a22a2c8efae846669a5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;449&quot; data-rawheight=&quot;185&quot;&gt;&lt;p&gt;用 fit_transform 函数将 (Iris-setosa, Iris-versicolor, Iris-virginica) 转换成 (0, 1, 2)。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)
# split data into train and test sets
X_train, X_test, y_train, y_test 
= train_test_split(X, label_encoded_y, test_size=test_size,
random_state=seed)&lt;/code&gt;&lt;p&gt;定义函数 fit_predict 整合 fit 和 predict 函数，并当 verbose 设置为 True 时打印模型和精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def fit_predict(X_train, y_train, X_test, y_test, verbose=True):
   model = fit(X_train, y_train)
   _, accuracy = predict(model, X_test, y_test)
   if verbose == True:
       print(model)
       print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))&lt;/code&gt;&lt;p&gt;调用 fit_predict 函数得到 92% 的精度，而且注意该模型用 multi:softprob 作为目标，因为该问题是个多元分类问题，而且 XGBoost 内部自动将“类变量”作独热编码，要不然目标应该是 multi:softmax。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;multi:softprob&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 92.00%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.2.2 独热编码分类型特征&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;肯特岗乳癌 (Breast + Cancer)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据集可用于进行患者乳腺癌治疗结果预测。它是一个二元分类问题，一共有 286 个样例。该数据集包含了 9 个特征和 1 个类变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;age&lt;/li&gt;&lt;li&gt;menopause&lt;/li&gt;&lt;li&gt;tumor-size&lt;/li&gt;&lt;li&gt;inv-nodes&lt;/li&gt;&lt;li&gt;node-caps&lt;/li&gt;&lt;li&gt;deg-malig&lt;/li&gt;&lt;li&gt;breast&lt;/li&gt;&lt;li&gt;breast-quad&lt;/li&gt;&lt;li&gt;irradiat&lt;/li&gt;&lt;li&gt;类变量：复发，未复发&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [3]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的 9 个特征都是字符型变量，这里用 OneHotEncoder。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;datasets-uci-breast-cancer.csv&#39;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9b0f77dc4bc4cd726d20afce671edacf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;792&quot; data-rawheight=&quot;186&quot;&gt;&lt;p&gt;从下表 unique 那行可看出每个特征的类数，比如 age 有 6 类，breast 有 2 类等&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data.describe()&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8a48cc8cc6d07983e7d1d69d5734836e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;896&quot; data-rawheight=&quot;160&quot;&gt;&lt;p&gt;对每个特征 X 做独热编码，对类 y 做普通编码。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
X = X.astype(str)
# encode string input values as integers
columns = []
for i in range(0, X.shape[1]):
   feature = LabelEncoder().fit_transform(X[:,i]).reshape(X.shape[0], 1)
   onehot_encoder = OneHotEncoder(sparse=False)
   feature = onehot_encoder.fit_transform(feature)
   print(&quot;X[&quot;,i, &quot;] shape ::&quot;, feature.shape)
   columns.append(feature)
# collapse columns into array
encoded_x = np.column_stack(columns)
print(&quot;X shape ::&quot;, encoded_x.shape)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)
# split data into train and test sets
X_train, X_test, y_train, y_test = 
train_test_split(encoded_x, label_encoded_y,
test_size=test_size, random_state=seed)
X[0] shape :: (286, 6)
X[1] shape :: (286, 3)
X[2] shape :: (286, 11)
X[3] shape :: (286, 7)
X[4] shape :: (286, 3)
X[5] shape :: (286, 3)
X[6] shape :: (286, 2)
X[7] shape :: (286, 6)
X[8] shape :: (286, 2)
X shape :: (286, 43)&lt;/code&gt;&lt;p&gt;上面的结果有些奇怪，比如 X[4] 的大小是 (286, 3), 286 代表数据个数，3 代表类别个数，但是从前一张图得知这个特征是 node-caps，对应的 unique 数为 2。看了 csv 文件才知道，这一特征栏下面有缺失值，读进 pandas 的数据表中赋值为 NaN, 也当成了一类&lt;/p&gt;&lt;p&gt;最后调用 fit_predict 函数得到 71.58% 的精度，结果不算太好，那是因为该数据中有不少缺失值，下节就来说明如何处理它们，即便 XGBoost 模型也可以自行处理。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 71.58%&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.2.3 缺失值处理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;病马疝气症 (Horse Colic)&lt;/b&gt; &lt;/p&gt;&lt;p&gt;该数据集可用于进行患者乳腺癌治疗结果预测。它是一个二元分类问题，一共有 368 个样例。该数据集包含了 27 个特征 (其中有 30% 的缺失值) 和 1 个类变量。由于数据太多就一一列出，用这个数据集是为了测试 XGBoost 处理缺失值的能力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [4]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;该数据的类型都是数值型，其中符号 ? 代表缺失值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&quot;horse-colic.csv&quot;)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20738cac42da42901d3e36cde4e97444_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;711&quot; data-rawheight=&quot;226&quot;&gt;&lt;p&gt;首先将缺失值用 0 来填充，填充完记得将 X 转化成 float 格式。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# split data into X and y
X, y = splitXy(data)
# set missing values to 0
X[X == &#39;?&#39;] = 0
# convert to numeric
X = X.astype(&#39;float32&#39;)
# encode Y class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)&lt;/code&gt;&lt;p&gt;该模型达到了 83.84% 的精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;fit_predict(X_train, y_train, X_test, y_test)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
      gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
      min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
      objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
      scale_pos_weight=1, seed=0, silent=True, subsample=1)
Accuracy: 83.84%&lt;/code&gt;&lt;p&gt;接着我们做一下实验，将缺失值分别 1 和 NaN 来填充，得到的精度分别为 79.80% 和 85.86%。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;X[X == &#39;?&#39;] = 1
X[X == &#39;?&#39;] = np.nan&lt;/code&gt;&lt;p&gt;&lt;b&gt;发现将缺失值设为独一的 NaN 最好，得到的精度最高，因为其独一性 XGBoost 把缺失值也当成“一类”。设为 0 或 1 都不太好，因为数据本身可能也含有一些 0 或 1。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.3 交叉验证&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;交叉验证在选取超参数时非常重要，首先载入 KFold, StratifiedKFold 和 cross_val_score。其中&lt;/p&gt;&lt;ul&gt;&lt;li&gt;KFold 适用于二分类且类别平衡&lt;/li&gt;&lt;li&gt;StratifiedKFold 适用于多分类或类别不平衡&lt;/li&gt;&lt;li&gt;cross_val_score 计算一些指标&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score&lt;/code&gt;&lt;p&gt;这里用 5 折交叉验证，分别用 KFold 和 StratifiedKFold 来跑。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# CV model for binary class or balanced class
kfold = KFold(n_splits=5, random_state=7)
results = cross_val_score(model, X, y, cv=kfold)
print(&quot;Accuracy: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))
Accuracy: 76.44% (5.09%)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# CV model for multi-class or inbalanced class
kfold = StratifiedKFold(n_splits=5, random_state=7)
results = cross_val_score(model, X, y, cv=kfold)
print(&quot;Accuracy: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))
Accuracy: 76.57% (3.74%)&lt;/code&gt;&lt;p&gt;该数据有 768 个，其中 268 正类 500 反类，属于不平衡的二分类问题，从上面结果来看，的确&lt;/p&gt;&lt;ul&gt;&lt;li&gt;StratifiedKFold 的精度 76.57% 比 KFold 的精度 76.44% 要高&lt;/li&gt;&lt;li&gt;StratifiedKFold 的标准差 3.74% 比 KFold 的标准差 5.09% 要低&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;1.4 特征选择&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;特征选择是一个重要课题，由于 XGBoost 包含随机森林的性质，因此也可以用来排序特征重要性并选择特征。&lt;/p&gt;&lt;p&gt;首先载入 plot_importance 和 SelectFromModel&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from xgboost import plot_importance
from sklearn.feature_selection import SelectFromModel&lt;/code&gt;&lt;p&gt;&lt;b&gt;1.4.1 特征重要性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;训练好的模型会给特征打分，用 feature_importances_ 属性查看其分数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# feature importance
print(pima_model.feature_importances_)
# feature
feature = data.columns.tolist()[0:-1]
print(feature)
[ 0.07094595  0.1858108   0.08952703  0.08445946  0.07263514  0.16047297
 0.12837838  0.20777027]
[&#39;Number of times pregnant &#39;, &#39;Plasma glucose concentration&#39;, &#39;Diastolic blood pressure&#39;, &#39;Triceps skin fold thickness&#39;, &#39;Serum insulin&#39;, &#39;BMI&#39;, &#39;Diabetes pedigree function&#39;, &#39;Age &lt;/code&gt;&lt;p&gt;一图胜万字，手动画图可看出年龄 (灰色柱体) 特征最重要，怀孕次数 (蓝色柱体) 特征最不重要。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# mannually plot
df = pd.DataFrame(data=model.feature_importances_, index=feature).T
df.plot.bar(figsize=(12,6));&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cbe992e9bb20248beac84a4aafac7fa2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;723&quot; data-rawheight=&quot;357&quot;&gt;&lt;p&gt;别傻了，这年头谁还手动画图，直接用 XGBoost 自带函数 plot_importance 就可以了。(虽然方便，但是丑，有没有颜色区分，而且特征都用没有实际意义的 f0, ..., f8 代表)&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;# Using the XGBoost built-in function to plot
plot_importance(pima_model)
plt.show()&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2b888755e373ec27b8db9be01985eae1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;744&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;查查 Pima 数据的特征名字，发现 f7 对应着是年龄，f0 对应着是怀孕次数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4.2 特征选择&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算出特征重要性后就可以选择特征了。用 SelectFromModel 将特征重要性作为阈值 (threshold)，得到一个 X_train 和 X_test 在特征维度上的子集，然后重新训练并打印出精度。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;thresholds = np.sort(pima_model.feature_importances_)
for c in thresholds:
   selection = SelectFromModel(pima_model, threshold=c, prefit=True)
   select_X_train = selection.transform(X_train)
   select_X_test = selection.transform(X_test)
   # train model
   selection_model = fit(select_X_train, y_train)
   # eval model
   _, accuracy = predict(selection_model, select_X_test, y_test)
   print(&quot;Threshold = %.3f, n = %d, Accuracy: %.2f%%&quot; % (c, select_X_train.shape[1], accuracy*100.0))
Threshold = 0.071, n = 8, Accuracy: 77.95%
Threshold = 0.073, n = 7, Accuracy: 76.38%
Threshold = 0.084, n = 6, Accuracy: 77.56%
Threshold = 0.090, n = 5, Accuracy: 76.38%
Threshold = 0.128, n = 4, Accuracy: 76.38%
Threshold = 0.160, n = 3, Accuracy: 74.80%
Threshold = 0.186, n = 2, Accuracy: 71.65%
Threshold = 0.208, n = 1, Accuracy: 63.78%&lt;/code&gt;&lt;p&gt;从上面结果可知，一般特征个数越多，精度越高，但是 n = 4 对应着 76.38% 的精度不比 n = 8 对应着 77.95% 的精度低很多。权衡精度和模型复杂度，我们会选择 n = 4。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.5 提前终止&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Pima 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;提前终止 (early stopping) 可以防止过拟合 (overfitting)。具体做法可分三步：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;选评估指标 - error 或 logloss 或都选&lt;/li&gt;&lt;li&gt;选评估集 - 训练集或测试集或都选&lt;/li&gt;&lt;li&gt;调用 fit 函数，设置 verbose = True (&lt;b&gt;重要&lt;/b&gt;) 并监控&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下例选了测试集和 error 来评估，打印出 error 值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, eval_metric=&quot;error&quot;, eval_set=eval_set, verbose=True)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;[0]  validation_0-error:0.259843
[1]  validation_0-error:0.26378
[2]  validation_0-error:0.26378
...
[17]  validation_0-error:0.228346
[18]  validation_0-error:0.224409
[19]  validation_0-error:0.232283
[20]  validation_0-error:0.232283
[21]  validation_0-error:0.23622
[22]  validation_0-error:0.23622&lt;/code&gt;&lt;p&gt;发现 error 一开始在减低，但从 [19] 回合开始就升高了，这说明模型有可能过拟合了。&lt;/p&gt;&lt;p&gt;下例选了 [训练集, 测试集] 和 [error, logloss] 来评估，并画图。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_train, y_train), (X_test, y_test)]
model.fit(X_train, y_train, eval_metric=[&quot;error&quot;, &quot;logloss&quot;], eval_set=eval_set,
verbose=False)&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5ec0025a201c8c23a10c748c7b93016f_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;762&quot; data-rawheight=&quot;391&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-386c1cd97545e521ad96e4a79f98fd3d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;773&quot; data-rawheight=&quot;383&quot;&gt;&lt;p&gt;从上两图看出，训练集 (蓝线) 上的 logloss 和 error 一直稳定下降，但是测试集 (黄线) 上的 logloss 和 error 在 20-40 之间开始上升，我们需要在 20 和 40 之间某个点提前终止训练。&lt;/p&gt;&lt;p&gt;XGBoost 中设置 early_stopping_rounds 可以提前终止，当该值设为 10，意思是说如果 logloss &lt;b&gt;在某一回合连续 10 个回合上升，那么在这个回合停止&lt;/b&gt;。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=&quot;logloss&quot;,
eval_set=eval_set, verbose=True)
[0]  validation_0-logloss:0.660186
Will train until validation_0-logloss hasn&#39;t improved in 10 rounds.
[1]  validation_0-logloss:0.634854
[2]  validation_0-logloss:0.612239
[3]  validation_0-logloss:0.593118
...
[29]  validation_0-logloss:0.491407
[30]  validation_0-logloss:0.488828
[31]  validation_0-logloss:0.487867
[32]  validation_0-logloss:0.487297
[33]  validation_0-logloss:0.487562
[34]  validation_0-logloss:0.487788
[35]  validation_0-logloss:0.487962
[36]  validation_0-logloss:0.488218
[37]  validation_0-logloss:0.489582
[38]  validation_0-logloss:0.489334
[39]  validation_0-logloss:0.490969
[40]  validation_0-logloss:0.48978
[41]  validation_0-logloss:0.490704
[42]  validation_0-logloss:0.492369
Stopping. Best iteration:
[32]  validation_0-logloss:0.487297

Accuracy: 78.35%&lt;/code&gt;&lt;p&gt;上图的信息非常明确，logloss 从 [32] 回合一直上升到 [42] 回合，因此在此停止。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.6 多线程运行&lt;/b&gt;&lt;/h2&gt;&lt;code lang=&quot;python&quot;&gt;from time import time&lt;/code&gt;&lt;p&gt;本小节使用的数据是&lt;/p&gt;&lt;p&gt;&lt;b&gt;Otto Group Product&lt;/b&gt; &lt;/p&gt;&lt;p&gt;训练集里包含了 6 万多个样本，每个样本有一个 id，93 个特征值 feat_1 ~ feat_93,以及类别 target，一共 9 种类别：class_1 ~ class_9。测试集里有 14 万多测试样本。它是一个多元分类问题&lt;/p&gt;&lt;p&gt;&lt;b&gt;具体信息见参考文献 [5]&lt;/b&gt;&lt;/p&gt;&lt;p&gt;读取并预处理数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;Otto train.csv&#39;)
X, y = splitXy(data)
# encode string class values as integers
label_encoded_y = LabelEncoder().fit_transform(y)&lt;/code&gt;&lt;p&gt;XGBoost 是用 C++ 实现的而且用 OpenMP API 做并行处理。将模型里面 nthread 设为 -1 代表使用系统里所有的线程，这也是默认设置。&lt;/p&gt;&lt;p&gt;此外将 nthread 设为 1, 2, 3, 4 看看用时比较。由下面结果可看出线程越多耗时越少，但从 3 到 4 时间减少并不是很明显。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;results = []
num_threads = [1, 2, 3, 4]
for n in num_threads:
   start = time()
   model = XGBClassifier(nthread=n)
   model.fit(X_train, y_train)
   elapsed = time() - start
   print(n, elapsed)
   results.append(elapsed)
1 78.58481550216675
2 50.53288269042969
3 44.90712261199951
4 39.1385235786438&lt;/code&gt;&lt;p&gt;除了 XGBClassifier, k-Fold 验证也可以并行化，它是通过设置 n_jobs 来实现的。接下来做三个实验：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只并行化 k-Fold 验证 (n_jobs = -1, nthread = 1)&lt;/li&gt;&lt;li&gt;只并行化 XGBClassifier (n_jobs = 1, nthread = -1)&lt;/li&gt;&lt;li&gt;两个都并行化 (n_jobs = -1, nthread = -1)&lt;/li&gt;&lt;/ol&gt;&lt;code lang=&quot;python&quot;&gt;# Single Thread XGBoost, Parallel Thread CV
model = XGBClassifier(nthread=1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=-1)

# Parallel Thread XGBoost, Single Thread CV
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=1)

# Parallel Thread XGBoost and CV
model = XGBClassifier(nthread=-1)
results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=&#39;neg_log_loss&#39;,
n_jobs=-1)
Single XGB Parallel CV: 187.554039
Parallel XGB Single CV: 145.135988
Parallel XGB and CV:    149.078977&lt;/code&gt;&lt;p&gt;结果显示只并行化 k-Fold 验证耗时最长，因为真正慢的是 XGBClassifier。此外奇怪的是两个都并行耗时并不是最短 (按道理应该是)。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1.7 调整超参数&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;本节用 Otto 的数据。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于 XGBoost 模型，很多学者和实践者通过无数尝试，给出了一些超参数的合理范围，这些建议在调参时非常有用，起码可以给个初始值尝试。关于那些参数建议来源，可参考 [6], [7], [8], [9], [10], [11], [12], [13], [14]。里面有一些学术大牛比如 Friedman 和 Hastie，有业界大牛 Owen Zhang，有 R, scikit-learn 和 XGBoost 官方参数建议。&lt;/p&gt;&lt;p&gt;接下来我们从三组最重要的超参数对来看是调参，这里需要用的到 GridSearchCV。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;from sklearn.model_selection import GridSearchCV&lt;/code&gt;&lt;p&gt;GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。它的好处是，只需增加几行代码，就能遍历多种组合。比如模型 M 有三个参数 P1, P2, P3, 其中&lt;/p&gt;&lt;p&gt; P1 可选值 1, 2&lt;/p&gt;&lt;p&gt; P2 可选值 4, 5, 6&lt;/p&gt;&lt;p&gt; P3 可选值 1, 3, 5, 7&lt;/p&gt;&lt;p&gt;建立一个参数字典 Para = {P1: [1 2], P2: [4 5 6], P3: [1 3 5 7]}，直接调用&lt;/p&gt;&lt;p&gt; GS = GridSearchCV(M, Para)&lt;/p&gt;&lt;p&gt;遍历所有 Para 组合，跑模型 24 遍，最后输出为 GS。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.7.1 树的个数和深度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;XGBoost 整个过程就是一个按顺序加树的过程，因此树的个数和树的深度绝对算是一组重要的超参数。接下来做三组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调树的个数&lt;/li&gt;&lt;li&gt;只调树的深度&lt;/li&gt;&lt;li&gt;同时调树的个数和深度&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;调树的个数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数从 50 到 350，以 50 为间隔，在 5 折交叉验证中要运行模型 30 次，最后最佳树的个数是 200。观察 logloss 发现其实树的个数在 100 和 350 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = range(50, 400, 50)
param_grid = dict(n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001034 using {&#39;n_estimators&#39;: 200}
-0.010886 (0.000810) with: {&#39;n_estimators&#39;: 50}
-0.001098 (0.001313) with: {&#39;n_estimators&#39;: 100}
-0.001038 (0.001348) with: {&#39;n_estimators&#39;: 150}
-0.001034 (0.001344) with: {&#39;n_estimators&#39;: 200}
-0.001035 (0.001346) with: {&#39;n_estimators&#39;: 250}
-0.001036 (0.001349) with: {&#39;n_estimators&#39;: 300}
-0.001037 (0.001351) with: {&#39;n_estimators&#39;: 350}&lt;/code&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d8c29bfb56a30f19bf120b71d5d69bad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;780&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;&lt;b&gt;调树的深度&lt;/b&gt; &lt;/p&gt;&lt;p&gt;树的深度从 1 到 9，以 2 为间隔，在 5 折交叉验证中要运行模型 25 次，最后最佳树的深度是 5。观察 logloss 发现其实树的深度在 3 和 9 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;max_depth = range(1, 11, 2)
param_grid = dict(max_depth=max_depth)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold,
verbose=1)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001098 using {&#39;max_depth&#39;: 5}
-0.026149 (0.000743) with: {&#39;max_depth&#39;: 1}
-0.001098 (0.001313) with: {&#39;max_depth&#39;: 3}
-0.001098 (0.001295) with: {&#39;max_depth&#39;: 5}
-0.001100 (0.001295) with: {&#39;max_depth&#39;: 7}
-0.001100 (0.001295) with: {&#39;max_depth&#39;: 9}&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-9946e313afa58382c7d1be3aa006352b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;779&quot; data-rawheight=&quot;402&quot;&gt;&lt;p&gt;&lt;b&gt;调树的个数和深度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数为 [50, 100, 150, 200]，树的深度为 [2, 4, 6, 8]，在 5 折交叉验证中要运行模型 80 次，最后最佳树的个数和深度是 200 和 4，这个和单独调试的 最佳树的个数 200 和最佳树的深度 5 很接近。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = [50, 100, 150, 200]
max_depth = [2, 4, 6, 8]
param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold,
verbose=1)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001010 using {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 200}&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-41a037a10ccd4f3f0b90adf571846f2e_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;483&quot; data-rawheight=&quot;286&quot;&gt;&lt;p&gt;&lt;b&gt;1.7.2 学习率和树的个数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一般来说，学习率越小，需要增加树的个数就越大。接下来做两组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调学习率&lt;/li&gt;&lt;li&gt;同时调树的个数和学习率&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;调学习率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;学习率为 [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]，在 5 折交叉验证中要运行模型 30 次，最后最佳学习率是 0.2。观察 logloss 发现其实学习率在 0.1 和 0.3 之间差距都很小。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]
param_grid = dict(learning_rate=learning_rate)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001028 using {&#39;learning_rate&#39;: 0.2}
-2.155502 (0.000038) with: {&#39;learning_rate&#39;: 0.0001}
-1.841039 (0.000362) with: {&#39;learning_rate&#39;: 0.001}
-0.597303 (0.000606) with: {&#39;learning_rate&#39;: 0.01}
-0.001098 (0.001313) with: {&#39;learning_rate&#39;: 0.1}
-0.001028 (0.001336) with: {&#39;learning_rate&#39;: 0.2}
-0.001029 (0.001349) with: {&#39;learning_rate&#39;: 0.3}&lt;/code&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d3ef31a3b7316af22f0b808528d8de22_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;801&quot; data-rawheight=&quot;418&quot;&gt;&lt;p&gt;&lt;b&gt;调树的个数和学习率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;树的个数为 [100, 200, 300, 400, 500]，学习率为 [0.0001, 0.001, 0.01, 0.1]，在 5 折交叉验证中要运行模型 100 次，最后最佳树的个数和学习率是 200 和 0.1，这个和单独调试的 最佳树的个数 200 和最佳学习率 0.2 很接近。 &lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;n_estimators = [100, 200, 300, 400, 500]
learning_rate = [0.0001, 0.001, 0.01, 0.1]
param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring=&quot;neg_log_loss&quot;, n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X, label_encoded_y)
Best: -0.001034 using {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 200}&lt;/code&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2aa1d095364cfbd4b206ee06e1deccd1_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;288&quot;&gt;&lt;p&gt;&lt;b&gt;1.7.3 采样比率&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随机森林有列采样和行采样，XGBoost 也有。 接下来做两组调试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只调行采样比率&lt;/li&gt;&lt;li&gt;只调列采样比率&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;行采样比率和列采样比率都从 0.1 到 1，以 0.1 为间隔，那么分别在 5 折交叉验证中要运行模型 50 次，最后发现最佳行采样比率是 0.3 和最佳列采样比率是 0.7。图表展示如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7842e64e5ece90e899741face90d70bb_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;486&quot; data-rawheight=&quot;300&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e51db7d6ce0f4beec3ee55523205ec89_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;477&quot; data-rawheight=&quot;311&quot;&gt;&lt;h2&gt;&lt;b&gt;2. XGBoost 进阶版&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本章介绍如何将 XGBoost 在 Lending Club 的预测贷款的应用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.1数据预处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;读取并概览数据。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;data = pd.read_csv(&#39;lending-club-data.csv&#39;, low_memory=False)
data.head(3).append(data.tail(3))&lt;/code&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6ec4b58bf8a5bdea10c78dff30c2beee_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;829&quot; data-rawheight=&quot;357&quot;&gt;&lt;p&gt;处理数据步骤有三步：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;平衡样本 (sample balancing)&lt;/li&gt;&lt;li&gt;特征子集 (feature subset)&lt;/li&gt;&lt;li&gt;独热编码 (one-hot encoding)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本帖，我们不会平衡样本，保留原来良性贷款和恶性贷款的比例。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;N1 = len(data[data[target] == 0])
N2 = len(data[data[target] == 1])

print( &quot;%% of safe loans  : %.2f%%&quot; %(N1/(N1+N2)*100.0) )
print( &quot;%% of risky loans : %.2f%%&quot; %(N2/(N1+N2)*100.0) )
% of safe loans  : 81.12%
% of risky loans : 18.88%&lt;/code&gt;&lt;h2&gt;&lt;b&gt;2.2 模型比对&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先我们用相同的参数，比对决策树 (DT)、随机森林 (RF)、梯度提升树 (GBT) 和极度梯度提升 (XGB)。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT = DecisionTreeClassifier(max_depth=5)
RF = RandomForestClassifier(n_estimators=10, max_depth=5)
GBT = GradientBoostingClassifier(n_estimators=10, max_depth=5)
XGB = XGBClassifier(n_estimators=10, max_depth=5)&lt;/code&gt;&lt;p&gt;训练这四个模型但是发现错误，原因是数据里有缺失值，XGB 可以自动处理，但是 DT, RF 和 GBT 不能。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT.fit( X_train, y_train )
RF.fit( X_train, y_train )
GBT.fit( X_train, y_train )
XGB.fit( X_train, y_train )
ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;).&lt;/code&gt;&lt;p&gt;将缺失值用 NaN 来填充。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;X_train1 = np.nan_to_num(X_train)
X_test1 = np.nan_to_num(X_test)&lt;/code&gt;&lt;p&gt;再训练模型并打印出它们的精度值。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;DT  - Accuracy (Train): 0.8199
DT  - Accuracy (Test):  0.8172
RF  - Accuracy (Train): 0.8127
RF  - Accuracy (Test):  0.8128
GBT - Accuracy (Train): 0.8197
GBT - Accuracy (Test):  0.8173
XGB - Accuracy (Train): 0.8217
XGB - Accuracy (Test):  0.8179&lt;/code&gt;&lt;p&gt;从测试误差来看，确实&lt;/p&gt;&lt;p&gt;XGB &amp;gt; GBT &amp;gt; RF &amp;gt; DT &lt;/p&gt;&lt;h2&gt;&lt;b&gt;2.3参数介绍&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;XGBoost 的设置有三种参数：&lt;b&gt;一般参数&lt;/b&gt;，&lt;b&gt;提升参数&lt;/b&gt;和&lt;b&gt;学习参数&lt;/b&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一般参数&lt;/b&gt; 取决于提升器，通常是树或线性模型&lt;/li&gt;&lt;li&gt;&lt;b&gt;提升参数&lt;/b&gt; 取决于选择的提升器的相关参数&lt;/li&gt;&lt;li&gt;&lt;b&gt;学习参数&lt;/b&gt; 取决于指定学习任务和相应的学习目标&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;一般参数 (general parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;booster&lt;/i&gt;：选择提升器，默认是 tree&lt;/li&gt;&lt;li&gt;&lt;i&gt;silent&lt;/i&gt;：是否打印信息，默认是 0 不打印&lt;/li&gt;&lt;li&gt;&lt;i&gt;nthread&lt;/i&gt;：线程数，默认为最大可用线程数&lt;/li&gt;&lt;li&gt;&lt;i&gt;num_pbuffer&lt;/i&gt;：缓冲区大小，默认为训练实例的数量&lt;/li&gt;&lt;li&gt;&lt;i&gt;num_feature&lt;/i&gt;：特征纬度，默认为特征的最高纬&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;提升参数 (booster parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;eta&lt;/i&gt;：学习率，范围 [0, 1]，默认为 0.3。该参数越小，计算速度越慢；该参数越大，有可能无法收敛&lt;/li&gt;&lt;li&gt;&lt;i&gt;gamma&lt;/i&gt;：控制叶子个数的参数，范围 [0, +∞)，默认为 0。该参数越大，越不容易过拟合&lt;/li&gt;&lt;li&gt;&lt;i&gt;max_depth&lt;/i&gt;：每颗树的最大深度，范围 [0, +∞)，默认为 6。该参数越大，越容易过拟合 &lt;/li&gt;&lt;li&gt;&lt;i&gt;min_child_weight&lt;/i&gt;：每个叶子里面的最小权重和，范围 [0, +∞)，默认为 1。该参数越大，越不容易过拟合&lt;/li&gt;&lt;li&gt;&lt;i&gt;subsample&lt;/i&gt;：样本采样比率，范围 (0, 1]，默认为 1。如果取 0.5 代表随机用 50% 的样本集用来训练&lt;/li&gt;&lt;li&gt;&lt;i&gt;colsample_bytree&lt;/i&gt;：列采样比率，范围 (0, 1]，默认为 1。对每棵树的生成用的特征进行列采样，类似于随机森林的列采样&lt;/li&gt;&lt;li&gt;&lt;i&gt;lambda&lt;/i&gt;：L2 正则化参数，范围 [0, +∞)，默认为 1。该参数越大，越不容易过拟合。 &lt;/li&gt;&lt;li&gt;&lt;i&gt;alpha&lt;/i&gt;：L1 正则化参数，范围 [0, +∞)，默认为 0。该参数越大，越不容易过拟合。 &lt;/li&gt;&lt;li&gt;&lt;i&gt;scale_pos_weight&lt;/i&gt;：控制正反类的平衡参数，范围 [0, +∞)，默认为 1。该参数通常设为“反类的总和/正类的总和”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;学习参数 (learning parameters)&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;objective&lt;/i&gt;：损失函数，默认为 linear。其他常见类型有： &lt;/li&gt;&lt;ul&gt;&lt;li&gt;reg:logistic – 二分类&lt;/li&gt;&lt;li&gt;binary:logistic  – 二分类概率&lt;/li&gt;&lt;li&gt;multi:softmax – 多分类&lt;/li&gt;&lt;li&gt;multi:softprob – 多分类概率&lt;/li&gt;&lt;li&gt;rank:pairwise – 排序&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;i&gt;base_score&lt;/i&gt;：预测分数，默认为 0.5。最初每个样例的预测分数。&lt;/li&gt;&lt;li&gt;&lt;i&gt;eval_metric&lt;/i&gt;：评估指标。该指标用在验证集上，比如回归任务默认的是 rmse；分类任务默认为 error；排序任务默认为 map。其他常见类型有：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;rmse – root mean square error &lt;/li&gt;&lt;li&gt;mae – mean absolute error &lt;/li&gt;&lt;li&gt;logloss – negative log-likelihood &lt;/li&gt;&lt;li&gt;error – binary classification error rate&lt;/li&gt;&lt;li&gt;merror – multiclass classification error rate &lt;/li&gt;&lt;li&gt;mlogloss – multiclass logloss &lt;/li&gt;&lt;li&gt;auc – area under the curve&lt;/li&gt;&lt;li&gt;map – mean average precision&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;i&gt;seed&lt;/i&gt;：随机种子，默认为 0，用于产生可复现的结果 &lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;2.4 调参步骤&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;调参像是一门艺术，需要足够的经验和前人的建议。一个不错的调参思路有如下四步：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;开始用一个业界公认的学习率 0.1，然后根据交叉验证误差来确定“最优树的个数”&lt;/li&gt;&lt;li&gt;决定好学习率和树的个数之后，调“树相关的参数”，比如 &lt;i&gt;max_depth, min_child_weight, gamma, subsample, colsample_bytree&lt;/i&gt;&lt;/li&gt;&lt;li&gt;为了进一步避免过拟合，调“正则化参数”，比如 &lt;i&gt;lambda&lt;/i&gt;, &lt;i&gt;alpha&lt;/i&gt;&lt;/li&gt;&lt;li&gt;用之前调好的参数，尝试小一点的学习率，看是否能提高模型表现&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这里我编写了一个函数 modelfit，主要为了方便打印精度值和画图，由于程序比较长，大家可以去 notebook 上去看。&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤 1&lt;/b&gt;：固定学习率调最优树的个数&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb1 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=1, gamma=0, subsample=0.8,
                     colsample_bytree=0.8, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb1, X_train, X_test, y_train, y_test )
Best Iteration: 402

Model Report
Accuracy (Train): 0.8481
Accuracy (Test): 0.8247
AUC Score (Train): 0.850696
AUC Score (Test): 0.762747&lt;/code&gt;&lt;p&gt;固定学习率 0.1，以交叉误差作为评估指标，得到 &lt;b&gt;402&lt;/b&gt; 作为最优树的个数。此外，特征重要性的图如下：&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a515f41bf869c45c856cd34dbcc70476_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;798&quot; data-rawheight=&quot;291&quot;&gt;&lt;p&gt;&lt;b&gt;步骤 2&lt;/b&gt;：调“树相关参数”&lt;/p&gt;&lt;p&gt;在这一步中，树的个数用步骤 1 确定好的 &lt;b&gt;402&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;5&lt;/b&gt; 和 &lt;b&gt;3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test1 = { &#39;max_depth&#39;:range(3,10,2), &#39;min_child_weight&#39;:range(1,6,2) }
Best: 0.764367 using {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;5&lt;/b&gt; 和 &lt;b&gt;3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test2 = { &#39;max_depth&#39;:[4,5,6], &#39;min_child_weight&#39;:[2,3,4] }
Best: 0.764367 using {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3}&lt;/code&gt;&lt;p&gt;再确定 gamma 最优值 &lt;b&gt;0.3&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test3 = { &#39;gamma&#39;:[i/10.0 for i in range(0,5)] }
Best: 0.764989 using {&#39;gamma&#39;: 0.3}&lt;/code&gt;&lt;p&gt;将上面三个参数最优值 (下图红色) 带入模型重新再调树的个数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb2 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8,
                     colsample_bytree=0.8, objective= &#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb2, X_train, X_test, y_train, y_test )
Best Iteration: 482

Model Report
Accuracy (Train): 0.8505
Accuracy (Test): 0.8249
AUC Score (Train): 0.856505
AUC Score (Test): 0.763606&lt;/code&gt;&lt;p&gt;现在最优树的个数为 &lt;b&gt;482&lt;/b&gt;，而且发现 AUC 变好了，从 &lt;b&gt;0.762747&lt;/b&gt; 提升到 &lt;b&gt;0.763606&lt;/b&gt;，再接再厉继续调整其他“采样参数”。&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;0.8&lt;/b&gt; 和 &lt;b&gt;0.8&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test4 = { &#39;subsample&#39;: [i/10.0 for i in range(6,10)],
               &#39;colsample_bytree&#39;: [i/10.0 for i in range(6,10)] }
Best: 0.764960 using {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.8}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;0.8&lt;/b&gt; 和 &lt;b&gt;0.8&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test5 = { &#39;subsample&#39;: [i/100.0 for i in range(70,95,5)],
               &#39;colsample_bytree&#39;: [i/100.0 for i in range(70,95,5)] }
Best: 0.764960 using {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.8}&lt;/code&gt;&lt;p&gt;&lt;b&gt;步骤 3&lt;/b&gt;：调“正则化参数”&lt;/p&gt;&lt;p&gt;先粗调参，大概定下最优值 &lt;b&gt;1&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test6 = { &#39;reg_alpha&#39;:[1e-5, 1e-2, 0.1, 1, 100] }
Best: 0.765376 using {&#39;reg_alpha&#39;: 1}&lt;/code&gt;&lt;p&gt;再细调参，最终确定最优值 &lt;b&gt;5&lt;/b&gt;。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;param_test7 = { &#39;reg_alpha&#39;:[0.1, 0.5, 1, 5, 10] }
Best: 0.766167 using {&#39;reg_alpha&#39;: 5}&lt;/code&gt;&lt;p&gt;发现 Best Score 的确增加了。最后把所有调好参数最优值 (下图红色) 带入模型再调树的个数。&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb3 = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8, colsample_bytree=0.8, 
                     reg_alpha=5, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb3, X_train, X_test, y_train, y_test )
Best Iteration: 361

Model Report
Accuracy (Train): 0.8417
Accuracy (Test): 0.8245
AUC Score (Train): 0.830397
AUC Score (Test): 0.763912&lt;/code&gt;&lt;p&gt;现在最优树的个数为 &lt;b&gt;361&lt;/b&gt;，AUC 继续变好，从 &lt;b&gt;0.763606 &lt;/b&gt;提升到 &lt;b&gt;0.763912&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;步骤 4&lt;/b&gt;：尝试小一点的学习率&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;xgb4 = XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=5,
                     min_child_weight=3, gamma=0.3, subsample=0.8, colsample_bytree=0.8, 
                     reg_alpha=5, objective=&#39;binary:logistic&#39;,
                     nthread=4, scale_pos_weight=1, seed=seed )

modelfit( xgb4, X_train, X_test, y_train, y_test )
Best Iteration: 3702

Model Report
Accuracy (Train): 0.8424
Accuracy (Test): 0.8256
AUC Score (Train): 0.832793
AUC Score (Test): 0.767145&lt;/code&gt;&lt;p&gt;学习率从 0.1 降低到 0.01，其他参数不变，AUC 从 &lt;b&gt;0.763912 &lt;/b&gt;提升到 &lt;b&gt;0.767145&lt;/b&gt;。再回顾一路从来 AUC 的提升历程：&lt;/p&gt;&lt;p&gt;&lt;b&gt;0.767145 &amp;gt; 0.763912 &amp;gt; 0.763606 &amp;gt; 0.762747&lt;/b&gt; &lt;/p&gt;&lt;p&gt;可不要小瞧这一点点的提升哦，在 kaggle 比赛中可会压不少人的。当然本节也只是对调参给个思路，抛砖引玉。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4. 总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本贴总结的东西超越了 XGBoost 带来的东西，有着更广的使用范围，一些心得如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;学新模型最好从具体例子开始，用模型的默认值先&lt;/li&gt;&lt;li&gt;尝试不同类型的数据，用编码技巧，处理缺失值&lt;/li&gt;&lt;li&gt;用提前终止来防止过拟合&lt;/li&gt;&lt;li&gt;能画图就画图，一图胜千字&lt;/li&gt;&lt;li&gt;能并行就并行，时间就是生命&lt;/li&gt;&lt;li&gt;调参是门艺术，没有捷径只能积累，多看大师的推荐，从重要的参数开始，先粗调再细调&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;参考文献&lt;/b&gt;&lt;/p&gt;&lt;p&gt;https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes&lt;/p&gt;&lt;p&gt;http://archive.ics.uci.edu/ml/datasets/Iris&lt;/p&gt;&lt;p&gt;http://archive.ics.uci.edu/ml/datasets/Breast+Cancer&lt;/p&gt;&lt;p&gt;https://archive.ics.uci.edu/ml/datasets/Horse+Colic&lt;/p&gt;&lt;p&gt;https://www.kaggle.com/c/otto-group-product-classification-challenge/data&lt;/p&gt;&lt;p&gt;Greedy Function Approximation: A Gradient Boosting Machine, Jerome Friedman&lt;/p&gt;&lt;p&gt;Stochastic Gradient Boosting, Jerome Friedman&lt;/p&gt;&lt;p&gt;Gradient Boosting Machine Learning, Trevor Hastie&lt;/p&gt;&lt;p&gt;https://cran.r-project.org/web/packages/gbm/gbm.pdf&lt;/p&gt;&lt;p&gt;http://www.saedsayad.com/docs/gbm2.pdf&lt;/p&gt;&lt;p&gt;http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting&lt;/p&gt;&lt;p&gt;https://xgboost.readthedocs.io/en/latest/python/python_api.html&lt;/p&gt;&lt;p&gt;Winning Data Science Competitions, Owen Zhang&lt;/p&gt;&lt;p&gt;Open Source Tools and Data Science Competitions, Owen Zhang&lt;/p&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-24-33948430</guid>
<pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>【顶级资源】掌握线性代数为机器学习打下坚实基础！</title>
<link>https://henix.github.io/feeds/zhuanlan.Lhtz-Jqxx/2018-02-22-33926699.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/33926699&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-a687d42513ea2550967a69eaecedeffe_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;原文：&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287475&amp;amp;idx=1&amp;amp;sn=39e41d221b8fca580825abea97b499a3&amp;amp;chksm=802e3626b759bf30d9bb761ad3fc02522531d746afa281e8a41a5da5df408e5b1610618397ce#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【顶级资源】掌握线性代数为机器学习打下坚实基础！&lt;/a&gt;&lt;p&gt;线性代数是数学领域，也是机器学习领域重要的支柱。对于初学者来说，要想学好机器学习，线性代数的掌握是必不可少的，也可以说是十分十分重要的。&lt;/p&gt;&lt;p&gt;春节后的第一天，公众号特此为大家分享一份这样的顶级学习清单。希望大家在新的一年里，学业有成，事业更旺。同时也感谢大家对公众号一直以来的支持与厚爱！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;维基百科&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一些&lt;b&gt;高层次&lt;/b&gt;的学习网页：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;线性代数&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Linear_algebra&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;矩阵（数学）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Matrix_(mathematics)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;矩阵分解&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Matrix_decomposition&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;线性代数主题列表&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/List_of_linear_algebra_topics&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;书籍教程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;所有书籍，编辑部费劲千辛万苦给大家都下载好了。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;在文末获取。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Introduction to Linear Algebra, Fifth Edition, Gilbert Strang, 2016.&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-249687c6b0d02f7c57b32c600d0c0415_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;403&quot; data-rawheight=&quot;500&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra Done Right, Third Edition, 2015.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1a91b998d65a9d9d9d674f06fa388cd0_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;872&quot; data-rawheight=&quot;1304&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;No Bullshit Guide To Linear Algebra, Ivan Savov, 2017.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-75c1eb06399ab9e84f48096274ec8931_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;874&quot; data-rawheight=&quot;1298&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Matrix Computations, Gene Golub and Charles Van Loan, 2012.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4b533353b62b768daa0ee5a21ec48c68_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1034&quot; data-rawheight=&quot;1502&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Numerical Linear Algebra with Applications: Using MATLAB.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7e81490146d8ee7eb0f6bb5186a77375_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;872&quot; data-rawheight=&quot;1326&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Numerical Linear Algebra with Applications Using MATLAB.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cf8d15ee18c005d0e20ae8606749845b_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1318&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Applied Multivariate Statistical Analysis, Richard Johnson and Dean Wichern, 2012.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-54d9bfa93329419a66aa5c9506432bce_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;378&quot; data-rawheight=&quot;500&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebraand Its Applications David C. Lay, 2016.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-90bbd86eaf96c44cf0a92810480313c2_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1343&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;ADVANCED LINEAR ALGEBRA NICHOLAS LOEHR, Virginia Polytechnic Institute and State University Blacksburg, USA, 2014.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8d8ebc7f7c0c440601209fc228f24668_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;1432&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Elementary Linear Algebra, 8e, Ron Larson, 2017.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-067e6be22032609e40d0d1dd69309f3a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1383&quot;&gt;&lt;ul&gt;&lt;li&gt;还有更多优秀的免费在线图书。 在维基百科上查看线性代数页面的末尾，可以看到更多的书籍列表。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Linear_algebra#Further_reading&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7f1ef9839ca59d4a5e08a06f06aaed3_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;706&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;大学课程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;现在许多大学课程提供PDF版本的讲义幻灯片，笔记和阅读材料。 有些甚至提供预先录制的视频讲座，这是非常宝贵的。&lt;/p&gt;&lt;p&gt;美国顶尖学校推荐的一些课程包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;MIT的线性代数——Gilbert Strang&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e306ddfee63efc35d8c3adaba5e43eb7_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;849&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;The Matrix in Computer Science at Brown by Philip Klein&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://cs.brown.edu/courses/cs053/current/index.htm&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-113c6a675f47282b4ad3323bf5d18c54_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;781&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Computational Linear Algebra for Coders at University of San Francisco by Rachel Thomas.&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://github.com/fastai/numerical-linear-algebra/&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-076f974480db3e6641bb1a4b94c78bbf_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;970&quot;&gt;&lt;h2&gt;&lt;b&gt;在线课程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra on Khan Academy&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.khanacademy.org/math/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5c161d361ec7f56b6d815147b032688d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;692&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra: Foundations to Frontiers on edX&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.edx.org/course/laff-linear-algebra-foundations-to-frontiers&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0456811498a219958c3939ab411e7605_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;757&quot;&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;问答平台&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;考虑到当前有大量的问答平台，有很多地方可以在线提出有关线性代数的问题。&lt;/p&gt;&lt;p&gt;以下是推荐的最热门的平台，供大家参考学习：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on the Mathematics Stack Exchange&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://math.stackexchange.com/?tags=linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-16596c2e55adf15f10d92370e7db02a5_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;388&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on Cross Validated&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://stats.stackexchange.com/questions/tagged/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-12f62762aba07795a9c2dcc1be1672ad_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;280&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra tag on Stack Overflow&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://stackoverflow.com/questions/tagged/linear-algebra&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5a0dfe7e08537514c473bef3f9023acc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;235&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Linear Algebra on Quora&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.quora.com/topic/Linear-Algebra&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-937b7d492f28e424a412dcf31e04f0d8_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Math Subreddit&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://www.reddit.com/r/math/&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-774fe1d8508ea88587a0e2b10fc56c78_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;435&quot;&gt;&lt;h2&gt;&lt;b&gt;NumPy资源&lt;/b&gt; &lt;/h2&gt;&lt;p&gt;在Python中实现线性代数时，您可能需要NumPy的帮助。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Reference&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Array Creation Routines&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Array Manipulation Routines&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;NumPy Linear Algebra&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/numpy/reference/routines.linalg.html&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;SciPy Linear Algebra&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;https://docs.scipy.org/doc/scipy/reference/linalg.html&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;链接:https://pan.baidu.com/s/1daPb7C  &lt;/p&gt;&lt;p&gt;密码:1bqf&lt;/p&gt;&lt;p&gt;&lt;b&gt;开工大吉，新年旺旺旺&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&amp;amp;mid=2653287475&amp;amp;idx=1&amp;amp;sn=39e41d221b8fca580825abea97b499a3&amp;amp;chksm=802e3626b759bf30d9bb761ad3fc02522531d746afa281e8a41a5da5df408e5b1610618397ce#rd&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-7f8c6ac23492b4d33c149764579319ed&quot; data-image-width=&quot;475&quot; data-image-height=&quot;475&quot; data-image-size=&quot;ipico&quot;&gt;【顶级资源】掌握线性代数为机器学习打下坚实基础！&lt;/a&gt;&lt;b&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;367&quot;&gt;&lt;/b&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>量化投资机器学习</author>
<guid isPermaLink="false">2018-02-22-33926699</guid>
<pubDate>Thu, 22 Feb 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
