<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>编程语言与高级语言虚拟机杂谈（仮）</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/</link>
<description>探讨编程语言的设计与实现</description>
<language>zh-cn</language>
<lastBuildDate>Fri, 03 Aug 2018 23:02:26 +0800</lastBuildDate>
<item>
<title>Scala macro 2018 实用指南</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-07-12-39379432.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/39379432&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scala的metaprogramming 目前比较混乱，有runtime reflection,compile time macro,scalameta,macro paradise ,而下一代scala(dotty)的元编程也还没完全定下来。&lt;/p&gt;&lt;p&gt;scalameta : 只是一个库，不带任何编译器插件，只是用来变换语法数，没有任何reflection的功能，所以需要结合macro使用。&lt;/p&gt;&lt;p&gt;查找资料后发现macro paradise + scalameta这个应该是比较好用的。但是需要注意Scalameta的版本最高是1.8.0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;工程设置：注意macro必须单独开个project,要调用macro的project需要指定dependency&lt;/p&gt;&lt;code lang=&quot;scala&quot;&gt;val macroSettings = Seq(
  libraryDependencies ++= macroDeps,
  addCompilerPlugin(&quot;org.scalameta&quot; %% &quot;paradise&quot; % &quot;3.0.0-M11&quot; cross CrossVersion.full)
)

lazy val macros = (project in file(&quot;macros&quot;))
  .settings(macroSettings: _*)

lazy val macroUsage = project
  .in(file(&quot;macroUsage&quot;))
  .dependsOn(macros) // macro!
  .settings(macroSettings: _*)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;macro有好几种，比较常见的是用于生成代码的annotation macro和更加general的def macro&lt;/p&gt;&lt;code lang=&quot;scala&quot;&gt;import scala.annotation.{StaticAnnotation, compileTimeOnly}
import scala.language.experimental.macros
import scala.meta._

@compileTimeOnly(&quot;enable macro paradise to expand macro annotations&quot;)
class mappable extends StaticAnnotation {
  inline def apply(ast: Any): Any = meta {
    ast match {
      case xx@q&quot;..$mods class $tName (..$params) extends $template &quot; =&amp;gt;
        val add=q&quot;&quot;&quot;def printit=println(&quot;macros print1111 !&quot;)&quot;&quot;&quot;
        val result = q&quot;&quot;&quot;
          $xx
        &quot;&quot;&quot;
        println(mods)
        println(tName)
        q&quot;&quot;&quot;..$mods class $tName(..$params) {
      $add
    }&quot;&quot;&quot;
    }
  }
}
&lt;/code&gt;&lt;p&gt;annotation usage:&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;  @mappable
  case class sdfsdf(a: Int)&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;def macro:&lt;/p&gt;&lt;code lang=&quot;scala&quot;&gt;import scala.language.experimental.macros

object defMacros {
  import scala.reflect.macros.blackbox
  def isEvenLog(number: Int): Unit = macro isEvenLogImplementation

  def isEvenLogImplementation(c: blackbox.Context)(number: c.Expr[Int]): c.Tree = {
    import c.universe._
    println(&quot;isEvenLogImplementation&quot;)
    println(number.toString())
    q&quot;&quot;&quot;
      if ($number%2==0){
        println($number.toString + &quot; is even&quot;)
      }else {
        println($number.toString + &quot; is odd&quot;)
      }
    &quot;&quot;&quot;
  }

  import scala.reflect.macros.blackbox.Context

  /*
  def anylen[t](x: Seq[t]) = macro map[t]

  def map[T : c.WeakTypeTag](c: blackbox.Context)(p: c.Expr[Seq[T]]): c.Tree = {
    import c.universe._
    q&quot;&quot;&quot;1&quot;&quot;&quot;
  }
 */
}&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;关于macro的未来()：&lt;/p&gt;&lt;a href=&quot;https://www.scala-lang.org/blog/2018/04/30/in-a-nutshell.html&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2f3c06cf5e89f8a53d50972874d33cc5&quot; data-image-width=&quot;399&quot; data-image-height=&quot;648&quot; data-image-size=&quot;120x160&quot;&gt;Macros: the Plan for Scala 3&lt;/a&gt;&lt;a href=&quot;https://www.scala-lang.org/blog/2017/10/09/scalamacros.html&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2f3c06cf5e89f8a53d50972874d33cc5&quot; data-image-width=&quot;399&quot; data-image-height=&quot;648&quot; data-image-size=&quot;120x160&quot;&gt;Roadmap towards non-experimental macros&lt;/a&gt;&lt;a href=&quot;https://dotty.epfl.ch/docs/reference/principled-meta-programming.html&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;Principled Meta Programming&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>Martin awodey</author>
<guid isPermaLink="false">2018-07-12-39379432</guid>
<pubDate>Thu, 12 Jul 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>tensorflow on idris学习项目</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-07-06-39147186.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/39147186&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;希望在idris中实现安全的深度学习api。目前已经成功调用C api获取tf版本&lt;/p&gt;&lt;a href=&quot;https://github.com/doofin/tensorflow-idris&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-03ab6f034de1e41a73ac9fd84abbf1bf&quot; data-image-width=&quot;420&quot; data-image-height=&quot;420&quot; data-image-size=&quot;ipico&quot;&gt;doofin/tensorflow-idris&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>Martin awodey</author>
<guid isPermaLink="false">2018-07-06-39147186</guid>
<pubDate>Fri, 06 Jul 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器学习与编译优化的入门介绍</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-05-15-36884760.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36884760&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;该篇文章也算凑个AI的热点::)。 对编译器后端技术基本熟悉的研究工作者应该会感觉写中后端优化策略相当地依赖于既有领域经验，所以说设计一个比较好的启发是优化策略是相当的困难并且玄学(说的就是GCC中的LRA和LLVM中的Spiller)。机器学习算法的主要优势就是学习已有的经验，从而降低对人类的工作负担，实现一个相对较优的决策。比如：根据以往的气候数据预测将来多少天的天气，根据地质结构数据等预测某个地区的石油、天然气等矿产的储量。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;编译器研究人员在设计Loop unrolling策略的时候，会考虑多种不同的因子对展开策略的影响。比如：目标机器的向量寄存器的数据宽度，循环内部的指令数，分支语句的数目，算术运算指令的数目，访存操作的数目等。以往的算法都是根据已有的benchmark得到这个决策结果。很显然这个过程可以使用机器利用机器学习算法来辅助编译器设计人员[1]。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;另外一方面，当前所有的寄存器分配算法中用到的Spilling策略都是基于启发式的优先级策略，这方面也可以从机器特性，程序结构中抽取对应的特征向量来决定溢出哪一个物理寄存器是最合适的[2]。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;当然，还可以利用机器学习的算法，从输入的命令行选项，目标机器的特性（如：机器字长，寄存器数目，寄存器类别，指令种类，指令支持的操作，cache特征）来学习得到一个相对比较合适的pass调度顺序，当然这个模型的决策变量非常之多，训练的数据量也比较大。而且模型中使用到的参数的选择对模型的效果非常重要，目前学界还没有比较好的模型[3][4]。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;文章[5]中给出了当前两者结合的研究热点。&lt;/p&gt;&lt;p&gt;[1]. &lt;a href=&quot;ftp://nozdr.ru/biblio/kolxo3/Cs/CsLn/A/Artificial%20Intelligence..%20Methodology,%20Systems,%20and%20Applications,%2010%20conf.,%20AIMSA%202002%28LNCS2443,%20Springer,%202002%29%28ISBN%203540441271%29%28287s%29_CsLn_.pdf#page=49&quot;&gt;ftp://nozdr.ru/biblio/kolxo3/Cs/CsLn/A/Artificial%20Intelligence..%20Methodology,%20Systems,%20and%20Applications,%2010%20conf.,%20AIMSA%202002%28LNCS2443,%20Springer,%202002%29%28ISBN%203540441271%29%28287s%29_CsLn_.pdf#page=49&lt;/a&gt; (p41).&lt;/p&gt;&lt;p&gt;[2]. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3402&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3402&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3]. &lt;a href=&quot;https://www.eecis.udel.edu/~cavazos/cgo-2006-talk.pdf&quot;&gt;https://www.eecis.udel.edu/~cavazos/cgo-2006-talk.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4].&lt;a href=&quot;https://www.researchgate.net/profile/Abdul_Wahid_Memon/publication/225732776_Milepost_GCC_Machine_Learning_Enabled_Self-tuning_Compiler/links/0fcfd50ca24ef6fdcb000000/Milepost-GCC-Machine-Learning-Enabled-Self-tuning-Compiler.pdf&quot;&gt;https://www.researchgate.net/profile/Abdul_Wahid_Memon/publication/225732776_Milepost_GCC_Machine_Learning_Enabled_Self-tuning_Compiler/links/0fcfd50ca24ef6fdcb000000/Milepost-GCC-Machine-Learning-Enabled-Self-tuning-Compiler.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5]. &lt;a href=&quot;https://arxiv.org/abs/1801.04405&quot;&gt;[1801.04405] A Survey on Compiler Autotuning using Machine Learning&lt;/a&gt;&lt;/p&gt;</description>
<author>XlousZeng</author>
<guid isPermaLink="false">2018-05-15-36884760</guid>
<pubDate>Tue, 15 May 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>又发现JDK的一个bug，这次是：ConcurrentHashMap</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-05-01-36297733.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36297733&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更新：ConcurrentHashMap的作者Doug Lea确认这是一个逻辑缺陷：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6084f2b7252c5ad32039a682737aade4_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1498&quot; data-rawheight=&quot;320&quot; data-watermark=&quot;watermark&quot; data-original-src=&quot;v2-6084f2b7252c5ad32039a682737aade4&quot; data-watermark-src=&quot;v2-c19ea84d64f48f2b9a9efb9590aeccf9&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8202422?focusedCommentId=14182353&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14182353&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;Bug链接&lt;/a&gt;&lt;p&gt;并在他维护的JSR166规范里提交了修改：&lt;/p&gt;&lt;a href=&quot;http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.310&amp;amp;r2=1.311&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.310&amp;amp;r2=1.311&lt;/a&gt;&lt;p&gt;分割线以下是我当时给JDK提这个bug后写的：&lt;/p&gt;&lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt;&lt;p&gt;JDK的源码都没人仔细看吗？我刚开始看JDK-1.8的ConcurrentHashMap的源码，就发现构造函数有问题，给Java提了bug，果然如此。&lt;/p&gt;&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8202422&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot;&gt;https://bugs.openjdk.java.net/browse/JDK-8202422&lt;/a&gt;&lt;p&gt;而且assign给了大神Doug Lea:&lt;/p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0a0bd99d31f7c543ef66788dab694bbc_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;534&quot; data-rawheight=&quot;64&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;我在so上也问了这个问题，有人认为这是一个优化技巧，但在逻辑上有一点说不太通，如果增加一个文档说明，把3/2这个计算参数的说明加进去，也勉强可以说得过去。总之，目前来看，确实是有逻辑缺陷的。&lt;/p&gt;&lt;p&gt;SO链接：&lt;/p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/50083966/bug-parameter-initialcapacity-of-concurrenthashmaps-construct-method&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;v2-2d47e939feed796bcf7483d306661c88&quot; data-image-width=&quot;316&quot; data-image-height=&quot;316&quot; data-image-size=&quot;ipico&quot;&gt;Bug: parameter &#39;initialCapacity&#39; of ConcurrentHashMap&#39;s construct method?&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>Old Driver</author>
<guid isPermaLink="false">2018-05-01-36297733</guid>
<pubDate>Tue, 01 May 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>榨干机器硬件性能: JVM＆GPU</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-04-30-36284998.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/36284998&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-657d4c369db90c1bfe3ea9dd06a173e0_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着过去几年机器学习 模型训练，以及区块链领域中（币圈和链圈） 对计算力的要求，人们对硬件计算速度的要求越来越高。很自然的，作为传统科学计算领域， 基于GPU的加速也获得了大量的关注：Tensorflow 底层利用GPU来计算；大量的挖矿软件( e.g., ethminer)直接对GPU暴力使用。在16,17年，由于币价格的爆发，对GPU显卡挖矿的需求，直接导致Nvida的股票价格翻了好几倍。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一　需求&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;然而很遗憾的是，目前GPU显卡API操作，主要是基于两大框架: Opencl 和CUDA。这两种框架实际上需要开发人员对框架底层有大量的了解, 主要体现在： &lt;/p&gt;&lt;ol&gt;&lt;li&gt;自己需要在底层实现kernel函数&lt;/li&gt;&lt;li&gt;自己申请，管理GPU的内存，并负责 Host memory和GPU memory的通讯。&lt;/li&gt;&lt;li&gt;自己去手动优化Kernle 方法的实现，比如基于数据类型的优化。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;由于这种复杂性，GPU的应用(e.g, coin mining, Machine Learning) 对高层语言开发者, 比如作为目前最通用的编程平台JVM(Java或者其他JVM语言),　是一件非常复杂的实现. 开发者只能通过自己写JNI的方式，对GPU做封装，然后自己在上层通过Java调用（这种方式对于绝大部分的程序员来说，可行性不高）。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;本文的目的并不真正深入J9对 GPU的具体技术细节，更多的还是从上层的科普角度出发。自己在14,15年，以及后来的17年了解 参与，讨论了部分J9这方面的调研和工作，包括后来15年在pppj也接触了Rice＆IBM Tokyo RD那边的Akihiro和Kazuaki等博士 关于这方面的沟通，理了几遍代码。&lt;/p&gt;&lt;p&gt;首先，需要特别澄清的是:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;JVM Specification 并没有制定JVM要对GPU的支持。这个Feature只是IBM J9 Java8自己一个特有的属性。(Hotspot不清楚，谁知道的喊声？)&lt;/li&gt;&lt;li&gt;J9 Java 8是最早开始支持 Cuda GPU的，至少我当时15年是，今天可能还有其它家的也支持（待考证）。&lt;/li&gt;&lt;li&gt;本文也区分另外一个Java &amp;amp; GPU的开源项目 &lt;b&gt;&lt;a href=&quot;https://github.com/Syncleus/aparapi&quot;&gt;aparapi&lt;/a&gt;&lt;/b&gt; 。Aparapi是在Java 语言中支持GPU, 但是需要开发者自己操作类似Kernel函数的，开发者需要知道GPu开发理论知识背景。而J9 不需要高层感知底下任何关于CUDA/OPENCL等知识, 它是直接在Runtime这一层去支持的。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;本文阅读需要一些背景：　&lt;/p&gt;&lt;ul&gt;&lt;li&gt;简单知道JVM以及bytecode&lt;/li&gt;&lt;li&gt;简单知道GPU以及CUDA是什么，以及知道为什么会有GPU这玩意&lt;/li&gt;&lt;li&gt;知道JIT 以及编译器是用来干什么的。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;二　大概原理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;J9 对 GPU 的支持主要是在以下两个方面&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CUDA GPU (或者严格说基于Nvida家的CUDA框架)。&lt;/li&gt;&lt;li&gt;支持仅限于 Java 8中的Stream API.  比如： &lt;/li&gt;&lt;/ol&gt;&lt;code lang=&quot;text&quot;&gt;     LongStream.range(low, up).parallel().forEach(i -&amp;gt; &amp;lt;lambda&amp;gt;)&lt;/code&gt;&lt;p&gt;Java8中的 Stream API 是从高层应用中去抽象出来了一个Parallel ，而GPU本身是在物理硬件上实现了 Same Instruction Multiple Data (SIMD)的数据并行。所以，直接通过GPU实现上层并行的逻辑是一个很自然而然的想法。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;大概的框架流程图如下：&lt;/p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-57abb2c1b1faafd601b0de246072263d_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1000&quot; data-rawheight=&quot;245&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;1， J9 Interpreter 解释bytecode指令的时候，检测 并识别出来 Stream API中的foreach (for_loops) 以及lambda closure (这中间涉及到Invokedynamic). &lt;/p&gt;&lt;p&gt;2, JIT compiler (TR in J9) 此时进行优化，将产生两部分代码: Host machine  code, and target GPU code（NVVM IR）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Host Machine Code (i.e., CPU here)运行的，这部分将包括：在GPU申请内存，GPU-CPU 内存之间的相互复制，调用Nvida的driver 来编译，启动NVVM IR在GPU上的执行。 &lt;/li&gt;&lt;li&gt;NVVM IR: 严格上来说是对应着lambda closure， 这部分最后会有变成Parallel Thread Execution (PTX) 指令，并最终由Nvida编译器生成具体GPU上的指令。综合起来，这块的转化为：　&lt;/li&gt;&lt;/ul&gt;&lt;code lang=&quot;text&quot;&gt;bytecode-&amp;gt; NVVM IR-&amp;gt;PTX instruction-&amp;gt; Nvida GPU  instruction.&lt;/code&gt;&lt;h2&gt;&lt;b&gt;三　优化方案&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;直接利用GPU实现上层Java 语言的并行，能提高上层应用的运效率。这个是在比较理想的状况下能够成立的。&lt;/p&gt;&lt;p&gt;所以，在对lambda closure-&amp;gt; NVVM转化的时候，JVM还需要做一些其他方面的优化。对于计算能力，或者说 对于计算速度的提升，或者说 在考虑榨干 现有硬件的条件下，一般是从两个方面去考虑：内存，指令。&lt;/p&gt;&lt;h2&gt;3.1 内存　（Array aligning）&lt;/h2&gt;&lt;p&gt;通过内存优化Runtime的性能，这本身是一个非常大的范围： 比如通过memory management, GC, cache, locality等种种。细说起来需要一本书来完成。&lt;/p&gt;&lt;p&gt;J9对GPU的支持中，我们说的内存优化是指的是GPU中的array(i.e., device memory)的处理, 而非host memory中的array。在cuda原先memory allocation方法中 。原先的管理方式是直接讲array object (array header and array body) 放入连续的一块地址中(starting from 0)。在新的优化，实际上重新对array object进行placement 使得body从128 的整数位(e.g., index 31) 开始。如下图：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f0023fae5dd4e0537cdf0487cf2dc45a_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;483&quot; data-rawheight=&quot;174&quot; data-watermark=&quot;&quot; data-original-src=&quot;&quot; data-watermark-src=&quot;&quot; data-private-watermark-src=&quot;&quot;&gt;&lt;p&gt;这么做的理由主要是基于两种条件：　a) 内存的操作更多的是对元素中read and write，对array header的操作并没有那么频繁（对比而言）, 所以header的读写的重要性不高; b) 　在128 index对齐后，读或者写可以在一个GPU指令周期内完成，而前者需要两个指令周期(第一个：　０－１２7，　第二个周期: １２８－３８４ )．&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;当然，除了array aligning, J9 也对内存其他方面进行的优化(不细说了)，比如基于jit对内存region 读写进行识别,　对指令进行re-order,　以达到high memory cache hit in GPU, 又或者减少不必要的复制指令 for copying from GPU memory to host memory，有或对 array header elimination. &lt;/p&gt;&lt;h2&gt;3.2 指令优化（Lambda Closure Optimization）&lt;/h2&gt;&lt;p&gt;指令优化属于传统的JIT 编译器方面的内容, 所以传统的JIT  optimization（e.g. , Deadcode elimination, ）基本上都可以拿过来用，毕竟lambda closure里面也是bytecode。这一部分就不需要细讲。&lt;/p&gt;&lt;p&gt;可以拿出来说的是 cross lambda method calling.  Exactly speaking, the caller is inside of lambda while the callee is out of lambda closure. 比如下面这个例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;public class Sample{
    public void myMethod(args...){}
    public void anotherMethod(){
        intSteam...forEach( i -&amp;gt;{
                myMethod(receiver, other-args);
                obj.anotherMethod();..
        });
    }
    public void anotherMethod(){}
} &lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在这个例子中， myMethod 和anotherMethod调用都是跨lambda closure (直白点：myMethod, anotherMethod 都不是closure内的方法). 这就产生一个问题或者中间有两个gap:  &lt;/p&gt;&lt;p&gt;a)  GPU上高速执行的代码调用CPU上低速执行的代码　－_－!!&lt;/p&gt;&lt;p&gt;b)  GPU上还要在runtime 决定　方法的具体实现方法(Java上若无显示标注方法是 invokevirtiual)。 =_=&lt;/p&gt;&lt;p&gt;　为了决定具体某个方法的实现，the receiver of a method call 和　virtual method table 需要在runtime时候从host memory 复制到device memory中去。为了使得程序跑的正确，这两个Gap将会kill GPU的效率。&lt;/p&gt;&lt;p&gt;　　To resolve both Gaps,  J9中JIT 编译器直接进行inline Caching (IC)优化（Akihiro认为是Method Inling). 在VM中，Inline Caching之前是SELF　language中(For detail, please refer Dr Urs Hölzle&#39;s PhD thesis [3])。&lt;/p&gt;&lt;p&gt;　　简单的归纳下就是在生成NVVM的时候，先直接假定 the type of method call receiver是哪一种，然后将被调用的方法实现直接inline到方法调用处。为了生成代码的正确性，在原先call site之前插入一个guard (中文不知道如何翻译？？)进行检测。若检测没有成功，则jmp到原先低速的方法（也就是这个时候GPU停下来去要求CPU执行：看当前receiver具体是哪路神仙(这个GPU也可以做，但是需要先copy from host memory to device memory)，CPU执行callee&#39;s　method）.　所以对于上面lambda内 obj.anotherMethod();　变成了&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;if(receiver is someType){
    SomeType&#39;s anotherMethod real Implementation and will be executed at GPU kernel.
}else{
    invoke receiver.anotherMethod(..)  //execute on CPU
}&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;四  附注：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;性能benchmark结果可以参考文章4. &lt;/li&gt;&lt;li&gt;本文的图来自4,5&lt;/li&gt;&lt;li&gt;转载请保留原作者的名字&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;【１】Project &lt;a href=&quot;https://github.com/Syncleus/aparapi&quot;&gt;Syncleus/aparapi&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【２】&lt;a href=&quot;https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html&quot;&gt;CUDA Toolkit Documentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;【３】Urs Hölzle, Adaptive Optimization for Self: Reconciling High Performance with Exploratory Programming. Ph.D. thesis, Stanford, CA, USA, 1995, UMI Order No. GAX95-12396.&lt;/p&gt;&lt;p&gt;【４】Kazuaki Ishizaki, Akihiro Hayashi, Gita Koblents and Vivek Sarkar, Compiling and Optimizing Java 8 Programs for GPU Execution/&lt;/p&gt;&lt;p&gt;【５】Akihiro Hayashi, Kazuaki Ishizaki, and Gita Koblents, Machine-Learning-based Performance Heuristics for Runtime CPU/GPU Selection. PPPJ 2015.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>Shijie XU</author>
<guid isPermaLink="false">2018-04-30-36284998</guid>
<pubDate>Mon, 30 Apr 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>[新闻] 大部分Intel hardware intrinsic 将在 .NET Core 2.1 中启用</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-03-26-34960352.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34960352&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;随着 .NET Core 2.1 发布的临近，上周&lt;a href=&quot;https://github.com/dotnet/coreclr&quot;&gt;CoreCLR&lt;/a&gt; 已经停止向master 分支中提交新功能。&lt;/p&gt;&lt;p&gt;所以目前已经确定，&lt;a href=&quot;https://github.com/dotnet/corefx/issues/22940&quot;&gt;Intel hardware intrinsic 项目&lt;/a&gt;将作为一个&lt;a href=&quot;https://github.com/dotnet/corefx/issues/27486&quot;&gt;Preview 特性&lt;/a&gt;在 .NET Core 2.1 正式发布版中启用。&lt;/p&gt;&lt;p&gt;1. .&lt;b&gt;NET Core 2.1 正式发布版中将启用所有的SSE，SSE2，SSE3，SSSE3，SSE4.1，AVX，LZCNT，POPCNT intrinsic，以及大约70%的AVX2 和SSE4.2 中的Crc32 intrinsic.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2. 作为一个Preview 特性，用户在程序中必须显式安装/引用一个Nuget 包（System.Runtime.Intrinsics.Experimental），这个包中并没有任何实际代码，但它负责向用户暴露可用的intrinsic API。在成为正式特新之前，这些API 可能还会轻微改动。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;我会随后在专栏里写一个系列文章来教大家使用这个hardware intrinsic 功能（安装，配置，C# SIMD编程，等等）。也希望大家在GitHub 上向我们多多提交意见和bug report :)&lt;/p&gt;</description>
<author>彭飞</author>
<guid isPermaLink="false">2018-03-26-34960352</guid>
<pubDate>Mon, 26 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>一个让Netty作者也感到惊讶的错误</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-03-16-34609401.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34609401&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;为了表示我不是标题党，先来个截图：&lt;/p&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ce8476a71905621c52ff6e1003e96baa_r.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;747&quot; data-rawheight=&quot;135&quot;&gt;&lt;p&gt;Netty是Java世界里网络编程框架的明星，也是我非常喜欢和推崇的开源项目之一。但其核心的Future接口实现却犯了一个基本的逻辑错误，本文就指出了Netty中的核心Future接口在实现cancel和isDone方法时违反了约定规则的问题。&lt;/p&gt;&lt;p&gt;首先来看java.util.concurrent.Future#cancel方法的javadoc约定：&lt;/p&gt;&lt;blockquote&gt;After this method returns, subsequent calls to isDone will always return true.&lt;br&gt;&lt;/blockquote&gt;&lt;p&gt;就是说，在调用了cancel方法后，再调用isDone将永远返回true。&lt;/p&gt;&lt;p&gt;Netty的Future接口继承了Java的Future接口:&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;inline&quot;&gt;public interface Future&amp;lt;V&amp;gt; extends java.util.concurrent.Future&amp;lt;V&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;所以在实现的时候理应遵循这一约定，但Netty截止到目前的最新版本中(4.1.21)，并没有遵循这一约定，参见下面的代码例子：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;import io.netty.util.concurrent.GlobalEventExecutor;

import io.netty.util.concurrent.Promise;

public class DefaultPromiseIsDoneTest {

    private final Promise defaultPromise = GlobalEventExecutor.INSTANCE.newPromise();

    public static void main(String args[]) {

        DefaultPromiseIsDoneTest main = new DefaultPromiseIsDoneTest();

        main.isDoneTest();

    }

    private void isDoneTest() {

        defaultPromise.setUncancellable();

        defaultPromise.cancel(false);

        boolean isDone = defaultPromise.isDone();

        System.out.println(isDone);

    }
}&lt;/code&gt;&lt;p&gt;运行后，控制台打印的是 false。 而按照约定，应该打印true才对。&lt;/p&gt;&lt;p&gt;Netty其它几个实现类也没有遵循这一约定：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;io.netty.channel.group.VoidChannelGroupFuture#isDone
io.netty.channel.VoidChannelPromise#isDone&lt;/code&gt;&lt;p&gt;我在Netty的github上提出了这个问题，得到了Netty官方的确认。&lt;/p&gt;&lt;p&gt;https://github.com/netty/netty/issues/7712&lt;/p&gt;&lt;p&gt;但由于修改会导致当前版本用户受到影响，所以Netty准备在下一个大版本中修复这一问题。&lt;/p&gt;&lt;p&gt;我们在异步编程，实现Future接口的时候，对cancel和isDone方法的理解很可能会依赖于直觉，而没有严格对照接口文档中约定的逻辑来实现。甚至像Netty这样的老司机也犯了大意的错误。特写出这篇文章，跟大家共勉。&lt;/p&gt;&lt;p&gt;具体的讨论可以参见我的stackoverflow帖子（本文开头的图片就截自下面的帖子）：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/48792248/does-netty-violate-the-contract-of-future-cancel-method&quot;&gt;does-netty-violate-the-contract-of-future-cancel-method&lt;/a&gt;&lt;/p&gt;</description>
<author>Old Driver</author>
<guid isPermaLink="false">2018-03-16-34609401</guid>
<pubDate>Fri, 16 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>对JDK-lang包里一个方法的改进</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2018-03-16-34608787.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34608787&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JDK-lang包里的一个方法：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;java.lang.Integer.numberOfLeadingZeros(int)&lt;/code&gt;&lt;p&gt;这个方法是用来计算int的二进制值从左到右有连续多少个0。&lt;br&gt;我们知道Java里的int型是有负数的，负数的二进制第1位肯定是1，所以如果参数i小于0，应该直接返回0就可以了。&lt;br&gt;但我们看JDK里的实现：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;public static int numberOfLeadingZeros(int i) {
        // HD, Figure 5-6
        if (i == 0)
            return 32;
        int n = 1;
        if (i &amp;gt;&amp;gt;&amp;gt; 16 == 0) { n += 16; i &amp;lt;&amp;lt;= 16; }
        if (i &amp;gt;&amp;gt;&amp;gt; 24 == 0) { n +=  8; i &amp;lt;&amp;lt;=  8; }
        if (i &amp;gt;&amp;gt;&amp;gt; 28 == 0) { n +=  4; i &amp;lt;&amp;lt;=  4; }
        if (i &amp;gt;&amp;gt;&amp;gt; 30 == 0) { n +=  2; i &amp;lt;&amp;lt;=  2; }
        n -= i &amp;gt;&amp;gt;&amp;gt; 31;
        return n;
    }&lt;/code&gt;&lt;p&gt;可以看到JDK里的实现里并没有对负数的情况进行判断，而是走了下面4个if分支判断和位移操作，这在参数i 为负数的情况下肯定是不必要的，而且也会影响性能。&lt;/p&gt;&lt;p&gt;如果继续挖掘的话，可以看到代码里有一行注释：&lt;/p&gt;&lt;blockquote&gt;// HD, Figure 5-6&lt;br&gt;&lt;/blockquote&gt;&lt;p&gt;注释里HD的意思是指《Hacker&#39;s Delight》这本书，意思是该方法的实现逻辑参考了这本书，那我们就来看下《Hacker&#39;s Delight》书里的实现代码：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;int nlz(unsigned x) {
int n;
if (x == 0) return(32);
n = 1;
if ((x &amp;gt;&amp;gt; 16) == 0) {n = n +16; x = x &amp;lt;&amp;lt;16;}
if ((x &amp;gt;&amp;gt; 24) == 0) {n = n + 8; x = x &amp;lt;&amp;lt; 8;}
if ((x &amp;gt;&amp;gt; 28) == 0) {n = n + 4; x = x &amp;lt;&amp;lt; 4;}
if ((x &amp;gt;&amp;gt; 30) == 0) {n = n + 2; x = x &amp;lt;&amp;lt; 2;}
n = n - (x &amp;gt;&amp;gt; 31);
return n;
}&lt;/code&gt;&lt;p&gt;可以看到这是C语言的实现，乍一看，貌似和JDK里的Java实现是一样的，但仔细看，你会发现方法参数x是unsigned类型的，因为unsigned类型不会有负数，不需要判断负数的情况，所以《Hacker&#39;s Delight》书里的实现是没问题的，而JDK实现里的方法参数是int类型，是需要判断负数的情况的。&lt;/p&gt;&lt;p&gt;所以我们可以大胆的推测，JDK的程序员当初在实现这个方法的时候，只是把&lt;br&gt;《Hacker&#39;s Delight》书里的实现改为了Java实现，而没有考虑到参数类型的区别。&lt;/p&gt;&lt;p&gt;这个方法在JDK里是被标记为intrinsic的，意思是该方法针对不同的硬件有更底层的原语级的实现，但这并不代表Java实现的逻辑就不需要被优化，因为：&lt;br&gt;1.在有些硬件上可能没有intrinsic支持。&lt;br&gt;2.用户显式关闭了intrinsic调用。&lt;br&gt;3.没有开启JIT编译。&lt;/p&gt;&lt;p&gt;除了Integer类之外，Long类里也有相同功能的方法需要改进：&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;java.lang.Long.numberOfLeadingZeros(long)&lt;/code&gt;&lt;p&gt;我给OpenJDK提了bug，其实应该叫改进/增强(Enhancement)，OpenJDK官方经过测试，发现我修改后的方案确实会提升性能，于是愉快的修复了这个问题：&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8189230&quot;&gt;bug链接&lt;/a&gt;&lt;br&gt;不过大家可能要等到Java-11发布的时候才能看到这个修改了。:)&lt;/p&gt;</description>
<author>Old Driver</author>
<guid isPermaLink="false">2018-03-16-34608787</guid>
<pubDate>Fri, 16 Mar 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>外部函数接口 FFI —— 虚拟机中重要但不起眼的组件</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2017-12-19-32134367.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32134367&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2619f90ef86a0cb5119dd3d56869b921_r.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;——或许你没听说过它，但是，它为虚拟机的世界与二进制的世界搭起了一道桥梁。它就是FFI。&lt;/p&gt;&lt;h2&gt;一个看似简单的问题&lt;/h2&gt;&lt;p&gt;用过 Scheme, Python 等各种动态语言的人，可能会对里面的 apply 情有独钟。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Scheme 中，你可以这样写：&lt;/p&gt;&lt;code lang=&quot;scheme&quot;&gt;(define (add-int a b) (+ a b))
(add-int 5 6)
(apply add-int &#39;(5 6))&lt;/code&gt;&lt;p&gt;Python2 中，你可以这样写：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;def add_int(a, b):
  return a + b

add_int(5, 6)
apply(add_int, (5, 6))&lt;/code&gt;&lt;p&gt;当然这在 Python3 中更改了，要写成这样：&lt;/p&gt;&lt;code lang=&quot;python&quot;&gt;add_int(*(5, 6))&lt;/code&gt;&lt;p&gt;Ruby 也是类似的写法。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;那么问题来了，在 C语言 中，能不能实现类似于如上的效果呢？&lt;/p&gt;&lt;p&gt;比如这样：&lt;/p&gt;&lt;code lang=&quot;c&quot;&gt;int add_int(int a, int b)
{
    return a + b;
}

add_int(5, 6); // OK!

int data[] = { 5, 6 };
apply(add_int, data);  // ????&lt;/code&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;一个更加实用的问题&lt;/h2&gt;&lt;p&gt;在虚拟机中经常遇到这么一个问题，我在一个二进制库，里面有一组函数，但是我 &lt;b&gt;不能通过C语言来调用 &lt;/b&gt;它们，或者是，我能使用C语言来调用，却 &lt;b&gt;不能在调用的时候知道它们的函数声明&lt;/b&gt;。怎么办？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;什么是 FFI ？&lt;/h2&gt;&lt;p&gt;FFI 全名 Foreign Function Interface ，中文名 外部函数接口。&lt;/p&gt;&lt;p&gt;其实 FFI 很简单，就是一个接口。这个接口可以让你在不清楚参数的个数和类型的情况下调用一个函数。&lt;/p&gt;&lt;p&gt;它可以用在问题一的场景，作为一个apply使用。但它更常被用于问题二的场景，成为 &lt;b&gt;高级语言的虚拟机与底层二进制代码之间的交互 &lt;/b&gt;的桥梁。&lt;/p&gt;&lt;p&gt;FFI 的主流实现，libffi 在它的 &lt;a href=&quot;https://sourceware.org/libffi/&quot;&gt;官网&lt;/a&gt; 上列出了它被哪些项目所使用：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CPython&lt;/li&gt;&lt;li&gt;OpenJDK&lt;/li&gt;&lt;li&gt;js-ctypes&lt;/li&gt;&lt;li&gt;Dalvik&lt;/li&gt;&lt;li&gt;Java Native Access (JNA)&lt;/li&gt;&lt;li&gt;Ruby-FFI&lt;/li&gt;&lt;li&gt;fsbv&lt;/li&gt;&lt;li&gt;JSCocoa&lt;/li&gt;&lt;li&gt;PyObjC&lt;/li&gt;&lt;li&gt;RubyCocoa&lt;/li&gt;&lt;li&gt;PLT Scheme&lt;/li&gt;&lt;li&gt;gcj&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;你一定用过其中某一款虚拟机/解释器，但是你未必知道 FFI。&lt;/p&gt;&lt;p&gt;没错，FFI 就是这么默默无闻，但它的贡献巨大。因为它实现了一种不通过 C语言 进行底层交互的功能，因为它，虚拟机解决问题的范围大大地扩展了。两个世界联系在了一起，世界变得更加宽广。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;对两个问题的分析&lt;/h2&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;第一个问题：如何实现apply？&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个问题示例的写法非常方便，而且支持这种写法有着重要的意义。&lt;/p&gt;&lt;p&gt;因为与直接调用 add_int(5, 6) 相比， add_int(*(5, 6)) 表示，它传递的是某个列表/元组/数组的解引。这个列表/元组/数组可以是任意的长度，尽管 add_int 只支持传递2个参数。&lt;/p&gt;&lt;p&gt;在 Python 中，书写 add_int(a, b) 表示，传递给 add_int 两个参数，每个参数的类型和数据是未知的。而书写 add_int(*a) 则表示，传递给 add_int 一组参数，但是参数的个数、类型和数据都是未知的。&lt;/p&gt;&lt;p&gt;无疑， add_int(*a) 拥有着比 add_int(a, b) 更高的自由度。相对于 add_int(*a) 来说， add_int(a, b)  属于硬编码了参数的个数，而 add_int(*a) 则取消了对参数个数的硬编码。&lt;/p&gt;&lt;p&gt;既然是硬编码，那么可以认为，add_int(a, b) 参数个数的信息位于&lt;b&gt;代码&lt;/b&gt;中，add_int(*a) 参数个数的信息位于&lt;b&gt;数据&lt;/b&gt;中。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;之所以强调这点，是因为，在 C语言 中，大部分的东西都是 &lt;b&gt;硬编码 &lt;/b&gt;的。&lt;/p&gt;&lt;p&gt;在 C 语言中，使用 add_int ，必须确定 函数的地址（通常使用函数名或函数指针来指定），函数声明，传递的参数的个数、类型，参数的数据等。这几个东西，在调用函数时，都是&lt;b&gt;必须&lt;/b&gt;确定的。&lt;/p&gt;&lt;p&gt;也就是说，&lt;b&gt;一旦无法确定其中任意一个，那么就不能在 C 中调用函数&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;好了，我们来问一下， C 中的 apply ，到底能不能实现呢？&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;apply(add_int, data);&lt;/code&gt;&lt;p&gt;问： 这一句中，有哪些是已知的，哪些是未知的？&lt;/p&gt;&lt;p&gt;答：函数的地址 已知，数据 已知，参数的个数未知，类型未知（如果想要那种单类型的apply，可以算作已知）。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;因为至少有一点是未知的，那么就不能通过这种方式调用。&lt;/p&gt;&lt;p&gt;因此，在目前的C标准下， 仅仅使用标准C语言，apply 的实现是&lt;b&gt;不可能&lt;/b&gt;的。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;那么真的是没有办法吗？&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/54627596/answer/142304962&quot;&gt;gkmail：C 语言有没有办法实现类似 Scheme 里的 apply 函数？&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个回答提到了 libffi 。没错， libffi = lib + ffi，&lt;b&gt;libffi&lt;/b&gt; 就是我们这篇文章的主角 FFI 的主流实现。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;第二个问题：如何不通过C来调用C函数？&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;要解决这个问题，首先我们要知道如何调用一个函数。&lt;/p&gt;&lt;p&gt;大体上来说，从汇编层面来讲，调用一个函数，就是先将参数放置在函数所要求的寄存器、栈等位置，然后从该函数的最初的指令开始执行，执行后从特定的位置取出调用的结果来使用。（在后面的小节我会结合具体的平台详细地说明这一点。）&lt;/p&gt;&lt;p&gt;总的来说，调用函数经历了这样一个过程：&lt;b&gt;参数传递 -&amp;gt; 函数调用 -&amp;gt; 返回值传递&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;C语言所编译的调用函数，硬编码了 参数传递 和 返回值传递。传递方式 是代码指定，而不是数据指定。&lt;/p&gt;&lt;p&gt;也就是说，如果想要能够自由地调用函数，必须拥有 &lt;b&gt;指定传递方式&lt;/b&gt; 的能力。将数据放置在什么位置，将结果从哪里取出，只要解决了这个问题，就解决了动态调用函数的问题。&lt;/p&gt;&lt;p&gt;而传递方式，是由 ABI 来规定的。函数的传递方式，就是通过ABI根据该函数的 函数声明来指定的。&lt;/p&gt;&lt;p&gt;所以，理论上，只要知道了 &lt;b&gt;函数声明 和 ABI&lt;/b&gt;，就能过获取函数的 参数和返回值的传递方式，再加上 函数的地址以及数据，就可以&lt;b&gt;不通过 C语言 的编译，来调用这个函数&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;code lang=&quot;text&quot;&gt;函数声明 + ABI --- 某种工具 --&amp;gt; 传递方式
传递方式 + 函数地址 + 数据 ---调用--&amp;gt; 结果&lt;/code&gt;&lt;p&gt;这个过程，就是 主流的 FFI 库，libffi 的调用原理。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;FFI 的基本原理&lt;/h2&gt;&lt;p&gt;FFI 的本质其实是 &lt;b&gt;参数传递 + 函数调用 + 返回值传递&lt;/b&gt;，而参数传递和返回值传递的方式，都是由 &lt;b&gt;具体的 ABI 所规定 &lt;/b&gt;的。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;我们从 x86-64 CPU 下 System V AMD64 ABI （Linux中一种主流的64位ABI） 来详细说明。&lt;/p&gt;&lt;p&gt;我们有函数 sub_int ：&lt;/p&gt;&lt;code lang=&quot;c&quot;&gt;int64_t sub_int(int64_t a, int64_t b)
{
    return a - b;
}&lt;/code&gt;&lt;p&gt;使用 gcc -O0 编译，使用 objdump 分析可得：&lt;/p&gt;&lt;code lang=&quot;objdump&quot;&gt;0000000000000000 &amp;lt;sub_int&amp;gt;:
   0:   55                      push   %rbp
   1:   48 89 e5                mov    %rsp,%rbp
   4:   48 89 7d f8             mov    %rdi,-0x8(%rbp)
   8:   48 89 75 f0             mov    %rsi,-0x10(%rbp)
   c:   48 8b 45 f8             mov    -0x8(%rbp),%rax
  10:   48 2b 45 f0             sub    -0x10(%rbp),%rax
  14:   5d                      pop    %rbp
  15:   c3                      retq&lt;/code&gt;&lt;p&gt;稍微简化一下，大概就是：&lt;/p&gt;&lt;code lang=&quot;nasm&quot;&gt;sub_int:
    mov    %rdi,%rax    ; rax = rdi
    sub    %rsi,%rax    ; rax = rax - rsi
    retq&lt;/code&gt;&lt;p&gt;在 SysV64 ABI 下，传递 int64_t，第一个参数使用 rdi，第二个参数使用 rsi，结果使用 rax。&lt;/p&gt;&lt;p&gt;也就是说，这个函数进行了这样的过程 ：  rax = rdi - rsi 。&lt;/p&gt;&lt;p&gt;在C语言中调用 sub_int(5, 6)，是进行了如下的过程：&lt;/p&gt;&lt;code lang=&quot;nasm&quot;&gt;caller:
    mov    $0x6,%rsi    ; rsi = 6
    mov    $0x5,%rdi    ; rdi = 5
    call   sub_int      ; rax = rdi - rsi
    ...                 ; Use the result in rax
    retq&lt;/code&gt;&lt;p&gt;也就是说，只要将第一个参数放到 rdi 寄存器，将第二个参数放到 rsi 寄存器，然后使用 call 指令，就可以正确地调用函数了。&lt;/p&gt;&lt;p&gt;而这个 call 指令，也可以调用一个函数指针，如 call (%rax)。&lt;/p&gt;&lt;p&gt;因此，函数的地址不需要确定，需要确定的只是参数放置的位置而已。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;基于这种情况，我们可以把 参数传递、函数调用、返回值传递 这三个过程分开。&lt;/p&gt;&lt;p&gt;函数调用就是调用 call 指令，它上面就是 参数传递，下面就是返回值传递。&lt;/p&gt;&lt;p&gt;参数传递 + 函数调用 + 返回值传递，这三者组合就能构成一个调用函数。&lt;/p&gt;&lt;p&gt;FFI 的本质就是这些的组合，上面给出的 caller 函数其实就是 FFI 函数的雏形。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;那么参数放置位置以及返回值的位置如何确定？这个是 ABI 所规定的。&lt;/p&gt;&lt;p&gt;在 SysV64 ABI 下， (int64_t, int64_t) -&amp;gt; int64_t 这种声明，一定是按照 (rdi, rsi) -&amp;gt; rax 这种方式来传递的。&lt;/p&gt;&lt;p&gt;所以，只需要确定 函数声明 和 ABI ，就能够确定 参数和返回值的传递方式。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;传递方式&lt;/b&gt; 是 基于 函数声明 和 ABI 的固定不变的东西，再加上能够变化的 &lt;b&gt;函数地址&lt;/b&gt;、&lt;b&gt;数据&lt;/b&gt;，就可以调用这个函数。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;libffi 的调用过程如下：&lt;/p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6ef5327e618a12eee546a243690602fa_r.jpg&quot; data-caption=&quot;libffi 调用过程&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;498&quot; data-rawheight=&quot;386&quot;&gt;&lt;p&gt;libffi 将 调用方式-&amp;gt;汇编调用过程 都写在了 ffi_call 中，ffi_call 的核心是使用汇编书写的。其本质就是如上的过程。&lt;/p&gt;&lt;p&gt;另外有一个使用 Jit 编译出定制 FFI 函数的 JitFFI，&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32044108&quot;&gt;JitFFI —— 对于外部函数接口FFI的Jit编译器&lt;/a&gt;。这个项目与 libffi 本质并无不同，但处理方式上有所区别。JitFFI 直接编译出 专门的接口函数，调用函数时直接调用接口函数。目前项目还不完善，有兴趣的读者可以去看一下。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;后记&lt;/h2&gt;&lt;p&gt;之所以写这篇文章，是希望大家能够对 FFI 这个默默无闻的组件有着基本的认识。&lt;/p&gt;&lt;p&gt;FFI 看似可有可无，其实在虚拟机与二进制的交互中扮演着重要的角色。&lt;/p&gt;&lt;p&gt;有了FFI，虚拟机可以轻松地调用二进制程序。&lt;/p&gt;&lt;p&gt;那么如何在二进制程序中调用虚拟机，进而达到两个虚拟机之间的交互？&lt;/p&gt;&lt;p&gt;这就是另一个问题了。&lt;/p&gt;</description>
<author>Chill Magic</author>
<guid isPermaLink="false">2017-12-19-32134367</guid>
<pubDate>Tue, 19 Dec 2017 00:00:00 +0800</pubDate>
</item>
<item>
<title>推荐部分学习寄存器分配的材料</title>
<link>https://henix.github.io/feeds/zhuanlan.hllvm/2017-11-25-31406621.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/31406621&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关注编译优化这个领域的学习和研究人员最近几年会发现，这个领域的研究已经非常的成熟，但是最近几年还是有一些新东西出现，寄存器分配问题进行建模的新方法，如：基于PBQP[1][2][3]，基于puzzling[4]，基于Multi-commodity network flow模型[5]的方法。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;在另一方面，自从2005年学界证明了SSA形式的寄存器干涉图满足弦图(chordal graph)的性质之后，学术界对寄存器分配问题的研究基本上都集中在如何更好的利用SSA形式提供的良好特性。当然在SSA形式的MIR (machine immediate representation)之上进行寄存器分配的时候，也有多种的算法思想，如：基于graph coloring[6]，基于linear scan[7][8][9]。当然还有一些融合多种思想的算法。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;然后有研究团队发现[10]，既然SSA形式下的graph coloring这么简单。而且LSRA在处理live interval分割的时候非常困难，时间代价非常高。因为他们绝对也没要使用线性扫描算法了（当然这是就高性能的优化编译器而言，对于优先考虑编译速度的JIT来说，LSRA仍然是个不错的选择）。他们这个组产出了相当多的SSA形式下基于graph coloring的寄存器分配器的研究工作，他们组里面的Hack博士写了一篇重点关于SSA形式下的寄存器分配问题的书[11]&lt;/p&gt;&lt;p&gt;[1]. &lt;a href=&quot;http://www.complang.tuwien.ac.at/scholz/pbqp.html&quot;&gt;Partitioned Boolean Quadratic Programming (PBQP)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2.] &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.4551&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Nearly Optimal Register Allocation with PBQP&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3]. &lt;a href=&quot;http://compilers.cs.uni-saarland.de/ssasem/talks/Jens.Palsberg.pdf&quot;&gt;http://compilers.cs.uni-saarland.de/ssasem/talks/Jens.Palsberg.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4]. &lt;a href=&quot;http://compilers.cs.ucla.edu/fernando/publications/drafts/long_PereiraPalsberg07.pdf&quot;&gt;http://compilers.cs.ucla.edu/fernando/publications/drafts/long_PereiraPalsberg07.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5]. &lt;a href=&quot;http://www.cs.cmu.edu/~dkoes/research/gpra.pdf&quot;&gt;http://www.cs.cmu.edu/~dkoes/research/gpra.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6]. &lt;a href=&quot;http://www.cs.utexas.edu/users/mckinley/380C/lecs/briggs-thesis-1992.pdf&quot;&gt;register allocator via graph coloring&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7]. &lt;a href=&quot;https://www.cs.purdue.edu/homes/suresh/502-Fall2008/papers/linear-scan.pdf&quot;&gt;Linear scan register allocation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[8]. &lt;a href=&quot;http://cgo.org/cgo2010/talks/cgo10-ChristianWimmer.pdf&quot;&gt;http://cgo.org/cgo2010/talks/cgo10-ChristianWimmer.pdf&lt;/a&gt; (OpenJDK C1中寄存器分配器的作者之一)&lt;/p&gt;&lt;p&gt;[9]. &lt;a href=&quot;http://www.christianwimmer.at/Publications/Wimmer10a/Wimmer10a.pdf&quot;&gt;http://www.christianwimmer.at/Publications/Wimmer10a/Wimmer10a.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[10]. &lt;a href=&quot;http://compilers.cs.uni-saarland.de/projects/ssara/&quot;&gt;SSA-based Register Allocation&lt;/a&gt; (saarland大学编译器组)&lt;/p&gt;&lt;p&gt;[11]. Register allocator for program in SSA form&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>XlousZeng</author>
<guid isPermaLink="false">2017-11-25-31406621</guid>
<pubDate>Sat, 25 Nov 2017 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
